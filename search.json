[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LSE DS202 (2022/2023)",
    "section": "",
    "text": "📑 Course Brief\nFocus: learn and understand the most fundamental machine learning algorithms\nHow: practical use of machine learning techniques and its metrics, applied to relevant data sets\n\n\n🎯 Learning Objectives\n\nUnderstand the fundamentals of the data science approach, with an emphasis on social scientific analysis and the study of the social, political, and economic worlds;\nUnderstand how classical methods such as regression analysis or principal components analysis can be treated as machine learning approaches for prediction or for data mining.\nKnow how to fit and apply supervised machine learning models for classification and prediction.\nKnow how to evaluate and compare fitted models, and to improve model performance.\nUse applied computer programming, including the hands-on use of programming through course exercises.\nApply the methods learned to real data through hands-on exercises.\nIntegrate the insights from data analytics into knowledge generation and decision-making.\nUnderstand an introductory framework for working with natural language (text) data using techniques of machine learning.\nLearn how data science methods have been applied to a particular domain of study (applications).\n\n\n\n🧑🏻‍🏫 Our Team\n\nTeacher ResponsibleTeaching StaffStudentsAdministrative Support\n\n\n\nDr. Jonathan Cardoso-Silva  Assistant Professorial Lecturer  LSE Data Science Institute 📧 J.Cardoso-Silva at lse dot ac dot uk\nOffice Hours:\n\n15-min slots, on Wednesdays 13:00 – 15:00 during Term Time\nRoom: PEL 9.01c (check out the 🗺️ campus map)\nBook via Student Hub up to 12 hours in advance\n\n\n\n\nDr. Stuart Bramwell  ESRC Postdoctoral Fellow  Department of Methodology PhD in Politics (Oxford) 📧 s.bramwell at lse dot ac dot uk\n\nYijun Wang  Guest Teacher at the LSE Data Science Institute PhD candidate in Health Informatics (KCL)  MSc in Data Science (KCL)  📧 y.wang508 at lse dot ac dot uk\n\nMustafa Can Ozkan  Guest Teacher at the LSE Data Science Institute PhD candidate in the Spacetime Lab (UCL)  MSc in Transport (Imperial/UCL)  📧 M.C.Ozkan at lse dot ac dot uk\n\nXiaowei Gao  Guest Teacher at the LSE Data Science Institute PhD candidate in the Spacetime Lab (UCL)  MSc in Data Science (KCL)  📧 X.Gao23 at lse dot ac dot uk\n\nAnton Boichenko  Guest Teacher at the LSE Data Science Institute Product Developer at Decoded  MSc in Applied Social Data Science (LSE)  📧 A.Boichenko at lse dot ac dot uk\n\n\n\nZhang Ruishan (Yoyo)  1st Year BSc Economics Student  Course Representative for DS202\n\nRachitha Raghuram  2nd Year BSc Economics Student  Course Representative for DS202\n\n\n\nNathaniel Ocquaye  Teaching Support and Events Officer Office: PEL 9.01 Email: DSI.UG at lse dot ac dot uk\n\nJill Beattie  Institute Coordinator Office: PEL 9.01E Tel: +44 (0) 20 7955 7759 Email: DSI.Admin at lse dot ac dot uk\n\n\n\n\n\nClass Groups\n\nGroup 01\n\n📆 Mondays\n⌚ 09:00 — 10:30\n📍 PAN.1.03\n🧑‍🏫 Xiaowei\n\n\n\nGroup 02\n\n📆 Mondays\n⌚ 10:30 — 12:00\n📍 PAN.1.03\n🧑‍🏫 Xiaowei\n\n\n\nGroup 03\n\n📆 Mondays\n⌚ 13:00 — 14:30\n📍 MAR.1.09\n🧑‍🏫 Stuart\n\n\n\nGroup 04\n\n📆 Fridays\n⌚ 16:00 — 17:30\n📍 NAB.1.04\n🧑‍🏫 Stuart\n\n\n\nGroup 05\n\n📆 Mondays\n⌚ 09:00 — 10:30\n📍 32L.LG.11\n🧑‍🏫 Mustafa\n\n\n\nGroup 06\n\n📆 Mondays\n⌚ 10:30 — 12:00\n📍 32L.LG.11\n🧑‍🏫 Mustafa\n\n\n\nGroup 07\n\n📆 Fridays\n⌚ 09:30 — 11:00\n📍 CBG.2.06\n🧑‍🏫 Yijun"
  },
  {
    "objectID": "blog/main.html",
    "href": "blog/main.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n2 min\n\n\n\nweek03\n\n\nlinear regression\n\n\noutliers\n\n\n\n\n\n\n\n23 October 2022\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nweek04\n\n\np-values\n\n\n\n\n\n\n\n19 October 2022\n\n\n\n\n\n\n\n\n\n\n\n\n0 min\n\n\n\nweek03\n\n\nbayesian statistics\n\n\nequations\n\n\n\n\n\n\n\n17 October 2022\n\n\n\n\n\n\n\n\n\n\n\n\n3 min\n\n\n\nweek02\n\n\nR-squared\n\n\nequations\n\n\n\n\n\n\n\n10 October 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/bayes-theorem.html",
    "href": "blog/posts/bayes-theorem.html",
    "title": "Understanding Bayes’ Theorem",
    "section": "",
    "text": "On Week 03 (Part II), I showed you the following equation and mentioned that it forms the basis for one of the algorithms we explore in this course, Naive Bayes.\n\\[\nP(\\mathbf{Y} = k | \\mathbf{X} = x) = \\frac{P(k)P(\\mathbf{X}|\\mathbf{Y}=k)}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}\n\\]\nThe following video might help you understand the intuition behind the equation. Enjoy!"
  },
  {
    "objectID": "blog/posts/outliers.html",
    "href": "blog/posts/outliers.html",
    "title": "Identifying outliers in linear model",
    "section": "",
    "text": "TLDR\n\n\n\nHere you will find R code to single out points with high “studentized” residuals\nDuring Week 03 lab, we looked at diagnostic plots we can generate in R to obtain insights about the fit of a linear model.\nImport required libraries\nFit a linear model then produce diagnostic plots\nA frequent question we got in the labs was about the first of these plots, “Residuals vs Fitted”."
  },
  {
    "objectID": "blog/posts/outliers.html#residual-vs-fitted-plot",
    "href": "blog/posts/outliers.html#residual-vs-fitted-plot",
    "title": "Identifying outliers in linear model",
    "section": "Residual vs Fitted plot",
    "text": "Residual vs Fitted plot\nIdeally, the residual plot, also called the null residual plot, should show a random scatter of points centered around 0 and forming an approximately constant width band.\n\nbase Rtidyverse\n\n\n\n\nCode\n# Add dots\nplot(predict(lm.fit), rstudent(lm.fit), \n     xlab=\"Fitted values\",\n     ylab=\"Residuals\",\n     main=\"Residuals vs Fitted\")\n\n# Add lines\nabline(h = 3, lwd = 5,col = 'red')\nabline(h = 0, lwd = 5,col = 'yellow')\n\n\n\n\n\n\n\n\n\nCode\nplot_df <- data.frame(fitted_vals=predict(lm.fit),\n                      residuals=rstudent(lm.fit))\n\ng <- ggplot(plot_df, aes(x=fitted_vals, y=residuals)) +\n\n    # Add dots\n     geom_point(alpha=0.4, size=3.5) +\n     xlab(\"Fitted values\") +\n     ylab(\"Residuals\") +\n     ggtitle(\"Residuals vs Fitted\") +\n\n    # Add lines\n    geom_hline(yintercept=0, size=1.5, color='yellow') +\n    geom_hline(yintercept=3, size=1.5, color='red') +\n\n    # Customising the plot +\n     theme_bw()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nCode\ng"
  },
  {
    "objectID": "blog/posts/outliers.html#selecting-outliers",
    "href": "blog/posts/outliers.html#selecting-outliers",
    "title": "Identifying outliers in linear model",
    "section": "Selecting outliers",
    "text": "Selecting outliers\nHow do you identify the data points that have a high value of residuals (potential outliers)?\nTo produce this plot, we used the function rstudent() to calculate the so-called “studentized” residuals. This function returns a vector with the same length as the number of data points:\n\nstudentized_residuals <- rstudent(lm.fit)\nlength(studentized_residuals)\n\n[1] 506\n\n\n\nnrow(Boston)\n\n[1] 506\n\n\nWe can use this information to filter and select those data points that produced a studentized-residual above 3:\n\ndf_potential_outliers <- Boston[studentized_residuals > 3, ]\ndf_potential_outliers\n\n       crim zn indus chas   nox    rm   age    dis rad tax ptratio lstat medv\n187 0.05602  0  2.46    0 0.488 7.831  53.6 3.1992   3 193    17.8  4.45 50.0\n215 0.28955  0 10.59    0 0.489 5.412   9.8 3.5875   4 277    18.6 29.55 23.7\n226 0.52693  0  6.20    0 0.504 8.725  83.0 2.8944   8 307    17.4  4.63 50.0\n258 0.61154 20  3.97    0 0.647 8.704  86.9 1.8010   5 264    13.0  5.12 50.0\n263 0.52014 20  3.97    0 0.647 8.398  91.5 2.2885   5 264    13.0  5.91 48.8\n268 0.57834 20  3.97    0 0.575 8.297  67.0 2.4216   5 264    13.0  7.44 50.0\n372 9.23230  0 18.10    0 0.631 6.216 100.0 1.1691  24 666    20.2  9.53 50.0\n373 8.26725  0 18.10    1 0.668 5.875  89.6 1.1296  24 666    20.2  8.88 50.0\n\n\nData frame df_potential_outliers above contain all potential outliers according to this criteria. In a real-life setting, you would check the values of these data points in comparison with the rest of the dataset to understand what makes them different."
  },
  {
    "objectID": "blog/posts/outliers.html#what-to-do-next",
    "href": "blog/posts/outliers.html#what-to-do-next",
    "title": "Identifying outliers in linear model",
    "section": "What to do next?",
    "text": "What to do next?\nIf the process that generated the dataset is indeed linear, it is possible that these are “true” outliers, rare cases that deviate from the norm. But often, this indicates that a linear model is not able to capture all the nuances present in the data. Maybe the data generating procedure is nonlinear? Or maybe it depends on other features that are not present in your dataset?\nAs for your next actions, it all depends on what you plan to do with this model or how much risk you can take by predicting data similar to these edge cases. If you do not want or cannot afford to ignore these errors, you can try to collect more data, fit more complex algorithms, or talk to domain experts to try to understand these cases a bit more."
  },
  {
    "objectID": "blog/posts/p-values.html",
    "href": "blog/posts/p-values.html",
    "title": "Don’t give p-values more credit than they deserve",
    "section": "",
    "text": "When you run a linear or logistic regression and find out that a regression coefficient has a low associated p-value, it is tempting to scream THIS FEATURE IS SIGNIFICANT AND I CAN PROVE!\nIn reality, although p-values might suggest a non-zero relationship between variables, you shouldn’t judge the performance or explainability of a model simply by the p-values of coefficients, nor the p-value associated with the full model (say, the F-statistic).\nWhen assessing a model, look beyond goodness-of-fit. Perform train/test splits, cross-validation, bootstrap, and use appropriate measures of success to the problem you have at hand. Come to 🗓️ Week 04 workshop (the lecture) this Friday 21 October to learn more about this.\nThe reason I am saying all this is because p-values are very easy to hack. In fact, there is even a term for misuse of p-values in the scientific literature: p-hacking.\nWhere do I inform myself about this?\nI have separated a list of articles and commentaries about this topic. Check them out:\n\nNahm, Francis Sahngun. 2017. “What the P Values Really Tell Us.” The Korean Journal of Pain 30 (4): 241.\nAmrhein, Valentin, Sander Greenland, and Blake McShane. 2019. “Scientists Rise up Against Statistical Significance.” Nature 567 (7748): 305–7.\nAschwanden, Christie. 2015. “Science Isn’t Broken.” FiveThirtyEight.\nSterne, Jonathan A C, and George Davey Smith. 2001. “Sifting the Evidence—What’s Wrong with Significance Tests?” BMJ : British Medical Journal 322 (7280): 226–31.\n\n💡 If you are in a hurry and want to read just ONE thing, read the “Science Isn’t Broken.” piece at FiveThirtyEight. They have a cool visualisation to illustrate the problem."
  },
  {
    "objectID": "blog/posts/r-squared.html",
    "href": "blog/posts/r-squared.html",
    "title": "What happens to R-squared when we add more predictors?",
    "section": "",
    "text": "TLDR\n\n\n\nWhen using OLS, \\(R^2\\) will always stay the same or increase you add more features to a linear regression, even if those features are “useless”."
  },
  {
    "objectID": "blog/posts/r-squared.html#about-r2",
    "href": "blog/posts/r-squared.html#about-r2",
    "title": "What happens to R-squared when we add more predictors?",
    "section": "About \\(R^2\\)",
    "text": "About \\(R^2\\)\nAs we saw briefly on Week 02 and as defined in Chapter 3 of the textbook (James et al. 2021), \\(R^2\\) represents the proportion of variability in the response variable that can be explained using the dependent variables (features). More specifically:\n\\[\nR^2 = 1 - \\frac{\\operatorname{RSS}}{\\operatorname{TSS}}\n\\]\nwhere \\(\\operatorname{TSS}\\) represents the Total Sum of Squares:\n\\[\n\\operatorname{TSS} =  \\sum_i^n{\\left(y_i - \\bar{y}\\right)^2},\n\\]\nand \\(\\bar{y}\\) is the mean of \\(\\mathbf{y}\\).\nAs for the \\(\\operatorname{RSS}\\), it represents the Residual Sum of Squares.\nTo understand why \\(R^2\\) can never decrease if we add more features, I think it is useful to visualise linear regression in matrix format."
  },
  {
    "objectID": "blog/posts/r-squared.html#data-and-variables",
    "href": "blog/posts/r-squared.html#data-and-variables",
    "title": "What happens to R-squared when we add more predictors?",
    "section": "Data and Variables",
    "text": "Data and Variables\nSuppose you have the following data:\n\\[\n\\mathbf{X} = \\left[\n\\begin{array}{cc}\nx_{11}\\\\\nx_{21}\\\\\n\\vdots\\\\\nx_{n1}\n\\end{array}\\right],\n\\]\nthat is, there i only \\(p=1\\) feature for the \\(n\\) observations of data, and you want to fit a linear regression to attempt to predict the \\(\\mathbf{y}\\) column below:\n\\[\n\\mathbf{y} = \\left[\n\\begin{array}{c}\ny_{1}\\\\\ny_2\\\\\n\\vdots\\\\\ny_n\n\\end{array}\\right].\n\\]"
  },
  {
    "objectID": "blog/posts/r-squared.html#simple-linear-regression-in-matrix-format",
    "href": "blog/posts/r-squared.html#simple-linear-regression-in-matrix-format",
    "title": "What happens to R-squared when we add more predictors?",
    "section": "Simple Linear Regression in matrix format",
    "text": "Simple Linear Regression in matrix format\nWe can also represent the regression coefficients obtained by Ordinary Least Squares (OLS) in matrix format. \\(\\hat{\\boldsymbol{\\beta}}\\) is a \\(1 \\times (p+1)\\) matrix:\n\\[\n\\hat{\\boldsymbol{\\beta}} = \\left[\n\\begin{array}{c}\n\\hat{\\beta}_{0}\\\\\n\\hat{\\beta}_1\n\\end{array}\\right].\n\\]\nWhen thinking about regression this way, it is often useful to add an additional column of 1s to \\(\\mathbf{X}\\):\n\\[\n\\mathbf{X} = \\left[\n\\begin{array}{cc}\n1 & x_{11}\\\\\n1 & x_{21}\\\\\n1 & \\vdots\\\\\n1 & x_{n1}\n\\end{array}\\right],\n\\]\nso that:\n\\[\n\\hat{\\boldsymbol{\\beta}}\\mathbf{X} =\\left[\n\\begin{array}{c}\n\\hat{\\beta}_{0} \\times 1 + \\hat{\\beta}_1 \\times x_{11} \\\\\n\\hat{\\beta}_{0} \\times 1 + \\hat{\\beta}_1 \\times x_{21} \\\\\n\\vdots  \\\\\n\\hat{\\beta}_{0} \\times 1 + \\hat{\\beta}_1 \\times x_{n1}\\\\\n\\end{array}\\right] = \\hat{\\mathbf{y}}\n\\]\nrepresents the estimated values \\(\\hat{\\mathbf{y}}\\).\nResiduals can then be represented as \\(\\mathbf{e} = (\\mathbf{y} - \\hat{\\mathbf{y}})\\), or\n\\[\n\\mathbf{e} =\\left[\n\\begin{array}{c}\ny_{1} - (\\hat{\\beta}_{0} \\times 1 + \\hat{\\beta}_1 \\times x_{11}) \\\\\ny_{2} - (\\hat{\\beta}_{0} \\times 1 + \\hat{\\beta}_1 \\times x_{21} )\\\\\n\\vdots  \\\\\ny_{n} - (\\hat{\\beta}_{0} \\times 1 + \\hat{\\beta}_1 \\times x_{n1})\\\\\n\\end{array}\\right]\n\\]\nOLS provides an optimal way to find coefficients \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes \\(\\operatorname{RSS}\\).\nIn matrix format, the \\(\\operatorname{RSS}\\) can be represented as:\n\\[\n\\operatorname{RSS} = \\mathbf{e}^T\\mathbf{e}.\n\\]\nOLS will find an optimal solution to \\(\\text{minimise } \\operatorname{RSS}\\). Let’s call it \\(\\operatorname{RSS}_{(p=1)}\\)."
  },
  {
    "objectID": "blog/posts/r-squared.html#what-if-i-add-another-feature",
    "href": "blog/posts/r-squared.html#what-if-i-add-another-feature",
    "title": "What happens to R-squared when we add more predictors?",
    "section": "What if I add another feature?",
    "text": "What if I add another feature?\n💡 What would happen if I added a second feature and ran a regression with \\(p=2\\)?\nThink of the implications this has for the matrix of coefficients \\(\\hat{\\boldsymbol{\\beta}}\\) and matrix of residuals \\(\\mathbf{e}\\) if I decided to add a second column of data to \\(\\mathbf{X}\\).\nThe new extended feature matrix, let’s call it \\(\\mathbf{X}'\\), would look like the following:\n\\[\n\\mathbf{X} = \\left[\n\\begin{array}{ccc}\n1 & x_{11}&x_{12}\\\\\n1 & x_{21}&x_{22}\\\\\n1 & \\vdots&\\vdots\\\\\n1 & x_{n1}&x_{n2}\n\\end{array}\\right],\n\\]\nOLS would find new coefficients, let’s call them \\(\\hat{\\boldsymbol{\\beta}}'\\):\n\\[\n\\hat{\\boldsymbol{\\beta}} = \\left[\n\\begin{array}{c}\n\\hat{\\beta}'_{0}\\\\\n\\hat{\\beta}'_1\\\\\n\\hat{\\beta}'_2\n\\end{array}\\right].\n\\]\nSimilarly, the new \\(\\mathbf{e}'\\) could be represented as:\n\\[\n\\mathbf{e}' =\\left[\n\\begin{array}{c}\ny_{1} - (\\hat{\\beta}'_{0} \\times 1 + \\hat{\\beta}'_1 \\times x_{11} + \\hat{\\beta}'_2 \\times x_{12})\\\\\ny_{2} - (\\hat{\\beta}'_{0} \\times 1 + \\hat{\\beta}'_1 \\times x_{21} + \\hat{\\beta}'_2 \\times x_{22})\\\\\n\\vdots  \\\\\ny_{n} - (\\hat{\\beta}'_{0} \\times 1 + \\hat{\\beta}'_1 \\times x_{n1} + \\hat{\\beta}'_2 \\times x_{n2})\\\\\n\\end{array}\\right]\n\\]\nAgain, if we run OLS, the algorithm will find an optimal solution to the the residuals above, a minimum \\(\\operatorname{RSS}_{(p=2)}\\). What can we expect of \\(\\operatorname{RSS}_{(p=2)}\\) in relation to \\(\\operatorname{RSS}_{(p=1)}\\)? Let’s think through some scenarios:\nIf it turned out that \\(\\hat{\\beta}'_0 = \\hat{\\beta}_0\\) and \\(\\hat{\\beta}'_1 = \\hat{\\beta}_1\\):\n\nIf \\(\\hat{\\beta}'_2 = 0\\), then we could conclude that \\(\\mathbf{e}' = \\mathbf{e}\\) and \\(\\operatorname{RSS}_{(p=2)} = \\operatorname{RSS}_{(p=1)}\\). That is, by minimising the sum of squares, OLS cannot find a better solution that involves \\(\\hat{\\beta}'_2\\).\nIf \\(\\hat{\\beta}'_2 \\neq 0\\) then notice that the new residuals are related to the previous ones: \\({e_i}' = (e_i - \\hat{\\beta}'_2x_{i2}) ~~\\forall i\\). That means \\(({e_i}')^2 \\geq e_i^2~~\\forall i\\) and therefore \\(\\operatorname{RSS}_{(p=2)} \\geq \\operatorname{RSS}_{(p=1)}\\).\n\nA similar logic applies to the case where \\(\\hat{\\beta}'_0 \\neq \\hat{\\beta}_0\\) and \\(\\hat{\\beta}'_1 \\neq \\hat{\\beta}_1\\) — check 1 for a more formal proof."
  },
  {
    "objectID": "blog/posts/r-squared.html#conclusion",
    "href": "blog/posts/r-squared.html#conclusion",
    "title": "What happens to R-squared when we add more predictors?",
    "section": "Conclusion",
    "text": "Conclusion\nBy adding a new feature, given the fact that OLS uses squares of errors, it is inevitable that \\(({e_i}')^2 \\geq e_i^2~~\\forall i\\). OLS will always find an equivalent or a lower \\(\\operatorname{RSS}\\) value. If you check the \\(R^2\\) definition once again, you will realise that \\(\\operatorname{TSS}\\) will never change — as it is not related to \\(\\mathbf{X}\\), only to \\(\\mathbf{y}\\) — only \\(\\operatorname{RSS}\\) can vary when you add new features. Since \\(\\operatorname{RSS}\\) can only decrease or stay the same, \\(R^2\\) will always increase or stay the same, never decrease, if you add more features.\nTo correct this misleading tendency of R-squared, an adjusted index has been proposed. The ajusted R-squared takes the number of features into account and it is what you should rely on when assessing the goodness-of-fit of a linear regression."
  },
  {
    "objectID": "main/assessments.html",
    "href": "main/assessments.html",
    "title": "✍️ Assessments",
    "section": "",
    "text": "Click on the assignments below to go to the Moodle page and submit your responses:\n\n✍️ Formative Problem Set (01) | W03-W05: Moodle link & associated RMarkdown file.\n\nYou can find model solutions here\n\n✍️ Summmative Problem Set (01) | W05-W07: Moodle link & associated webpage."
  },
  {
    "objectID": "main/assessments.html#problem-sets-60",
    "href": "main/assessments.html#problem-sets-60",
    "title": "✍️ Assessments",
    "section": "📝 Problem Sets (60%)",
    "text": "📝 Problem Sets (60%)\n\nSummative problem sets released on Weeks 5, 8 & 11.\nThese will have a similar style to the formative problem sets, a mix of R tasks and your written interpretation of the analyses.\nTypically, you will have 4-6 days to submit your solutions.\nEach of the three summative problem sets is worth 20% of the final mark, and will be graded on a 100 point scale."
  },
  {
    "objectID": "main/assessments.html#exam-40",
    "href": "main/assessments.html#exam-40",
    "title": "✍️ Assessments",
    "section": "✍️ Exam (40%)",
    "text": "✍️ Exam (40%)\n\nAn open-book take-home exam, taken during the January exams period.\nExam questions will be comparable in style to the problem sets.\nThe exam questions will be released on Moodle\n\n\n\n\n\n\n\n⚠️ Import Update 11/10/2022\n\n\n\nLast year, DS202 exam was performed entirely online due to COVID-19 mitigation procedures. We want to run it online via our own Moodle page again this academic term, we just need to understand LSE regulations about exams for this year.\nWe will update you on this very soon (hopefully by the end of W04)."
  },
  {
    "objectID": "main/communication.html",
    "href": "main/communication.html",
    "title": "LSE DS202",
    "section": "",
    "text": "Find out how to reach out to your peers, teaching and administrative staff during this course.\n\n\n\nMost of our “informal” communication and interactions will happen through Slack.\nSlack is a platform used by many companies and institutions where teams can collaborate and communicate about a specific project.\nThere will be channels dedicated to discussing each week’s content, a channel for sharing useful links and events, plus a random channel to share random stuff about data science.\n\nYou will receive an invitation to join our Slack group via e-mail. Send an e-mail to Jon (J.Cardoso-Silva at lse dot ac dot uk) if you have not received an invite by the time of the first lecture.\nCheck out the following links to understand more about this tool:\n\nSlack tutorials\nCollaborate effectively in channels\n\n\n\n\n\nIt is probably a good idea to book office hours if:\n\nyou struggled with a technical or theoretical aspect of a problem set in the previous week,\nyou have queries about careers in data science,\nyou want guidance in how to apply data science to other things you are studying outside this course.\n\nCome prepared. You only have 15 minutes.\nAsk for help sooner rather than later.\nBook slots via StudentHub up to 12 hours in advance.\n\n⚠️ Reserve 📧 e-mail for formal requests: extensions, deferrals, etc. No need to e-mail to inform you will skip a class, for example."
  },
  {
    "objectID": "main/courserep.html",
    "href": "main/courserep.html",
    "title": "Course Representative",
    "section": "",
    "text": "The Data Science Institute (DSI) is excited to announce that the elections for Student Academic Representatives have now gone live! You can nominate yourself if you are taking DS105M or DS202 this Term, and all students get to to cast their votes anonymously on the candidates. The nomination and voting process will be conducted via Slack.\nAs a Student Academic Representative (Course Rep), you’ll work with staff to ensure your peers’ feedback is seen and acted on, and that student voices are represented in institutional decision-making. It is also a fantastic way to develop new skills, get to know the Institute better, and bolster your CV.\nYou can see the positions available in the table below:"
  },
  {
    "objectID": "main/courserep.html#engagement",
    "href": "main/courserep.html#engagement",
    "title": "Course Representative",
    "section": "Engagement",
    "text": "Engagement\n\nYou will have a direct channel of communication to voice suggestions, concerns and recommendations to the teaching and administrative staff at the DSI.\nYou will be allowed to use the DSI space on certain periods during the week.\nYou will have direct contact to PhD students and researchers who are visiting the DSI.\nYou will be the first to know about internal research projects available to undergraduate students."
  },
  {
    "objectID": "main/courserep.html#teaching-committees",
    "href": "main/courserep.html#teaching-committees",
    "title": "Course Representative",
    "section": "Teaching Committees",
    "text": "Teaching Committees\nAs a course rep, you will be invited to attend two Teaching Committees at the DSI during Michaelmas Term:\n\nthe first on Week 07\nthe second on Week 11\n\n\n\n\n\n\n\nWhat are Teaching Committees?\n\n\n\n\n\nTeaching Committees are official meetings where academic and professional service staff at the DSI discuss how the courses are going, devise adjustments for what is not going as planned, and plan new ways to boost recognition of innovative work of students enrolled in DSI courses. These meetings typically involve Lecturers, the Teaching Support Officer, the Communications Officer, the Institute Manager, and the Director of the DSI."
  },
  {
    "objectID": "main/courserep.html#nominations",
    "href": "main/courserep.html#nominations",
    "title": "Course Representative",
    "section": "1. Nominations",
    "text": "1. Nominations\n\nWrite down a short paragraph (max. ~50 words) explaining why you feel you would be great for the role!\nHead to your Slack group and post your short paragraph on the #student-rep channel.\nYou can nominate yourself anytime but no later than Monday 10 October 12 p.m.."
  },
  {
    "objectID": "main/courserep.html#voting",
    "href": "main/courserep.html#voting",
    "title": "Course Representative",
    "section": "2. Voting",
    "text": "2. Voting\n\nWe will set up an (anonymous) poll on the #student-rep channel using the app Polly\nNo one will be able to see who is winning/losing. Only when the poll closes will we know the results.\nAll students will be able to cast their votes anonymously on their favourite candidates from Tuesday 11 October – Thursday 13 October (end of Week 03).\nResults will be announced on the #student-rep channel."
  },
  {
    "objectID": "main/syllabus.html",
    "href": "main/syllabus.html",
    "title": "LSE DS202",
    "section": "",
    "text": "References\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York NY: Springer. https://www.statlearning.com/.\n\n\nSumner, Chris, Alison Byers, Rachel Boochever, and Gregory J. Park. 2012. “Predicting Dark Triad Personality Traits from Twitter Usage and a Linguistic Analysis of Tweets.” In 2012 11th International Conference on Machine Learning and Applications, 2:386–93. https://doi.org/10.1109/ICMLA.2012.218."
  },
  {
    "objectID": "assessments/formative1_solutions.html",
    "href": "assessments/formative1_solutions.html",
    "title": "✔️ Formative Problem Set 01 | Solutions",
    "section": "",
    "text": "Import required libraries.No need to change if you are not planning to use other packages.\n\nlibrary(car)\nlibrary(ISLR2)\nlibrary(tidyverse)\n# If you use other packages, add them here\n\nlibrary(ggcorrplot)\nlibrary(tidymodels) ## to use the tidy() function to extract data from lm models"
  },
  {
    "objectID": "assessments/formative1_solutions.html#questions",
    "href": "assessments/formative1_solutions.html#questions",
    "title": "✔️ Formative Problem Set 01 | Solutions",
    "section": "🎯 Questions",
    "text": "🎯 Questions\nUse the Carseats data set in the ISLR2 package to answer the following questions:\n\nQ1. Variables\nList the names of the variables in this dataset and whether they are quantitative or qualitative. (Worth 1/100 mock marks)\n\nQuantitative:\n\nSales\nCompPrice\nIncome\nAdvertising\nPopulation\nPrice\nAge\nEducation\n\nQualitative:\n\nShelveLoc\nUrban\nUS\n\n\n\n\nQ2. Dimension\nUse R to list the number of rows in the dataset. (Worth 1/100 mock marks)\n\nnrow(Carseats)\n\n[1] 400\n\n\n\n\nQ3. Visual\nSelecting only the quantitative variables, plot the correlation between variables (Worth 5/100 mock marks)\nTip: If you want to use other R packages, add them to the list of libraries at the top.\n\nCarseats_quant <- Carseats %>% select(-c(ShelveLoc, Urban, US))\ncorr_Carseats <- cor(Carseats_quant)\n\nggcorrplot(corr_Carseats, lab=TRUE, digits=2, lab_size=3)\n\n\n\n\n\n\nQ4. Initial Variable Identification\nBased on just an initial inspection of the data, which variables would you select to train an algorithm to predict Sales? Why? (Worth 7/100 mock marks)\n\nPrice is clearly the best first candidate. It is the variable most highly correlated with the dependent variable (Sales) in absolute terms. The correlation between Price and Sales is -0.44. As price increases, sales decreases in an approximately linear fashion.\n\n\ng <- (\n  ggplot(Carseats, aes(x=Price, y=Sales))\n  \n  + geom_point(size=3, alpha=0.5)\n  \n  + theme_bw()\n  \n  )\n\ng\n\n\n\n\n\nIf we want to add more variables, the next candidates would be Age and Advertising, as these variables also exhibit a correlation with Sales that is not too close from zero, in absolute terms. Another interesting point about these two variables is the fact that they are not correlated to each other; that is, correlation Age vs Advertising and Age vs Price and Price vs Advertising is close to zero.\n\n\ng <- (\n  ggplot(Carseats, aes(x=Age, y=Sales))\n  \n  + geom_point(size=3, alpha=0.5)\n  \n  + theme_bw()\n  \n  )\n\ng\n\n\n\n\n\ng <- (\n  ggplot(Carseats, aes(x=Advertising, y=Sales))\n  \n  + geom_point(size=3, alpha=0.5)\n  \n  + theme_bw()\n  \n  )\n\ng\n\n\n\n\n\n\nQ5. Simple Linear Regression\nChose ONE SINGLE variable – any variable – and fit a linear regression to predict Sales. Show the summary of this linear model. (Worth 3/100 mock marks)\n\nsimple_model <- lm(Sales ~ Price, data=Carseats)\n\nsummary(simple_model)\n\n\nCall:\nlm(formula = Sales ~ Price, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5224 -1.8442 -0.1459  1.6503  7.5108 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 13.641915   0.632812  21.558   <2e-16 ***\nPrice       -0.053073   0.005354  -9.912   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.532 on 398 degrees of freedom\nMultiple R-squared:  0.198, Adjusted R-squared:  0.196 \nF-statistic: 98.25 on 1 and 398 DF,  p-value: < 2.2e-16\n\n\n\n\nQ6. Simple Linear Regression - Interpretation\nProvide an interpretation of each coefficient in the model, stating their values, whether they are significant and what they represent. Be careful—some of the variables in the model are qualitative! (Worth 10/100 mock marks)\n\nWhen Price = 0, the model predicts Sales = 13.64.\n\nFor every $100 increase in Price, Sales decrease by approximately 5.3 units.\n\nBoth the intercept and the regression coefficient for Price are statistically significant, their p-values are close to zero.\n\n\n\nQ7. Simple Linear Regression - Formula\nWrite the model in equation form, carefully handling the qualitative variables properly. (Worth 5/100 mock marks)\n\\[Sales = - 0.053073 \\times Price + 13.641915 + \\epsilon\\]\n\n\nQ8. Multiple Linear Regression\nChose ONLY THREE variables and fit a linear regression to predict Sales. Show the summary of this linear model. (Worth 3/100 mock marks)\n\nmultiple_model <- lm(Sales ~ Price + Age + Advertising, data=Carseats)\n\nsummary(multiple_model)\n\n\nCall:\nlm(formula = Sales ~ Price + Age + Advertising, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.6247 -1.5288  0.0148  1.5220  6.2925 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 16.003472   0.718754  22.266  < 2e-16 ***\nPrice       -0.058028   0.004827 -12.022  < 2e-16 ***\nAge         -0.048846   0.007047  -6.931 1.70e-11 ***\nAdvertising  0.123106   0.017095   7.201 3.02e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.269 on 396 degrees of freedom\nMultiple R-squared:  0.3595,    Adjusted R-squared:  0.3547 \nF-statistic:  74.1 on 3 and 396 DF,  p-value: < 2.2e-16\n\n\n\n\nQ9. Multiple Linear Regression - Interpretation\nProvide an interpretation of each coefficient in the model, stating their values, whether they are significant and what they represent. Be careful—some of the variables in the model are qualitative! (Worth 10/100 mock marks)\n\nThis multiple regression model is statistically significant (p-value associated with the F-statistic is very small). In addition, all regression coefficients were also found to be statistically significant (p-value close to zero).\nIn the hypothetical scenario in which \\(Price = 0\\) and \\(Age = 0\\) and \\(Advertising = 0\\) , the multiple regression model predicts \\(Sales \\approx 16\\) units.\n\nFor every fixed combination of Age and Advertising, an increase in \\(100\\) dollars in Price is associated with a Sales decrease by approximately 5.8 units.\nIf Price and Advertising are fixed, the Age of a car impacts negatively the number of sales. The regression coefficient is \\(-0.048846\\) , which means we would expect Sales to decrease by \\(1\\) unit for every \\(\\approx 20\\) years of age of the Car (since \\(\\frac{1}{0.048846} \\approx 20\\) ) .\nAdvertising, on the other hand, is associated with an increase in the number of Sales. If Price and Age are fixed, the model predicts that 12 more items of Sales for each increase in \\(100\\) Advertising units (dollars?)\n\n\n\n\nQ10. Multiple Linear Regression - Formula\nWrite the model in equation form, carefully handling the qualitative variables properly. (Worth 5/100 mock marks)\n\\[Sales = 16.003472 - 0.058028 \\times Price - 0.048846 \\times Age + 0.123106 \\times Advertising + \\epsilon\\]\n\n\nQ11. Model comparison\nWhich of the two models you created, in questions Q5 and Q8 provide a better fit? (Worth 10/100 mock marks)\n\nThe multiple model in Q8 fitted the Sales data better.\n\nThe adjusted R-squared is higher in the Q8 versus the Q5 model, that is, adding Age and Advertising explains more of the variance in Sales than the model with just Price.\nWe also see a reduction in the Residual Standard Error.\nThis is also somewhat apparent from the diagnostic plot of residual vs fitted (shown below). The standardized residuals look more concentrated around zero, on the multiple model.\n\n\nplot_df <- data.frame(fitted_vals=predict(simple_model),\n                      residuals=rstudent(simple_model))\n\ng <- (\n  ggplot(plot_df, aes(x=fitted_vals, y=residuals))\n\n  # Add dots\n  + geom_point(alpha=0.4, size=3.5) \n  + xlab(\"Fitted values\") \n  + ylab(\"Residuals\") \n  + ylim(c(-4,4))\n  + ggtitle(\"Residuals vs Fitted (Simple Model)\")\n\n  # Add lines\n  + geom_hline(yintercept=0, size=1.5, color='yellow') \n  + geom_hline(yintercept=3, size=1.5, color='red')\n\n  # Customising the plot +\n  + theme_bw()\n)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\ng\n\n\n\n\n\nplot_df <- data.frame(fitted_vals=predict(multiple_model),\n                      residuals=rstudent(multiple_model))\n\ng <- (\n  ggplot(plot_df, aes(x=fitted_vals, y=residuals))\n\n  # Add dots\n  + geom_point(alpha=0.4, size=3.5) \n  + xlab(\"Fitted values\") \n  + ylab(\"Residuals\") \n  + ylim(c(-4, 4))\n  + ggtitle(\"Residuals vs Fitted (Multiple Model)\")\n\n  # Add lines\n  + geom_hline(yintercept=0, size=1.5, color='yellow')\n  + geom_hline(yintercept=3, size=1.5, color='red')\n\n  # Customising the plot +\n  + theme_bw()\n)\n\ng\n\n\n\n\n\n\nQ12. Collinearity\nWhat is the Variance Inflation Factor (VIF) of each variable? (Worth 3/100 mock marks)\nConsidering only the model built on Q8:\n\nvif(multiple_model)\n\n      Price         Age Advertising \n   1.012538    1.010550    1.001987 \n\n\nAlternatively, if you interpreted the question to refer to all variables, the following would also be accepted:\n\nvif(lm(Sales ~ ., data=Carseats))\n\n                GVIF Df GVIF^(1/(2*Df))\nCompPrice   1.554618  1        1.246843\nIncome      1.024731  1        1.012290\nAdvertising 2.103136  1        1.450219\nPopulation  1.145534  1        1.070296\nPrice       1.537068  1        1.239785\nShelveLoc   1.033891  2        1.008367\nAge         1.021051  1        1.010471\nEducation   1.026342  1        1.013086\nUrban       1.022705  1        1.011289\nUS          1.980720  1        1.407380\n\n\n\n\nQ13. Collinearity (cont.)\nBased on your responses to Q3 and the output of Q12, would you consider ignoring any variables when building a linear model? Why/Why not? (Worth 7/100 mock marks)\n\nIn terms of vif – which measures the linear dependency of each predictor to ALL the other predictors – there are no “problematic variables”. If there were, we would find variables with vif above 5 or 10.\nIn terms of pairwise collinearity (plot in Q3), we also don’t find any problematic colinearities.\n\n\n\nQ14. Modelling\nConsidering ALL possible combinations of TWO variables in this dataset, find the one linear model that has the smallest Residual Standard Error. Explain how you reached that conclusion, show us the summary of that model and write the model in equation form. (Worth 15/100 mock marks)\n\n💡Tip: Here I show an “elegant” way of solving this question by selecting variables by their names and by using dataframes, the pipe and other R functions. There are simpler ways to solve this question, the simplest perhaps would be to solve it by iterating over the column indices instead of column names. If you did it like that, your answer will be accepted, if done correctly.\n\n\n# Select all columns in the dataset, except Sales and get a list of their names\nall_predictors <- Carseats %>% select(-Sales) %>% names()\nall_predictors\n\n [1] \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\"  \"Price\"      \n [6] \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"       \"US\"         \n\n\nUse the function combn to produce a list of all possible combination of pairs of predictors. In addition to that, let’s reshape the data and transform it to a nicely formatted dataframe with two columns. Since these columns do not have names, the function as.data.frame() automatically name them V1 and V2.\n💡 Tip: type ?combn and hit ENTER in the R console to read more about the combn function.\n\nall_combn_predictors <- combn(all_predictors, 2) %>% t() %>% as.data.frame() \n\nall_combn_predictors\n\n            V1          V2\n1    CompPrice      Income\n2    CompPrice Advertising\n3    CompPrice  Population\n4    CompPrice       Price\n5    CompPrice   ShelveLoc\n6    CompPrice         Age\n7    CompPrice   Education\n8    CompPrice       Urban\n9    CompPrice          US\n10      Income Advertising\n11      Income  Population\n12      Income       Price\n13      Income   ShelveLoc\n14      Income         Age\n15      Income   Education\n16      Income       Urban\n17      Income          US\n18 Advertising  Population\n19 Advertising       Price\n20 Advertising   ShelveLoc\n21 Advertising         Age\n22 Advertising   Education\n23 Advertising       Urban\n24 Advertising          US\n25  Population       Price\n26  Population   ShelveLoc\n27  Population         Age\n28  Population   Education\n29  Population       Urban\n30  Population          US\n31       Price   ShelveLoc\n32       Price         Age\n33       Price   Education\n34       Price       Urban\n35       Price          US\n36   ShelveLoc         Age\n37   ShelveLoc   Education\n38   ShelveLoc       Urban\n39   ShelveLoc          US\n40         Age   Education\n41         Age       Urban\n42         Age          US\n43   Education       Urban\n44   Education          US\n45       Urban          US\n\n\nSo, there are a total of 45 combinations of pairs of features. Now, we need to fit linear regression models for all combinations and compute the Residual Standard Error of each.\nHow do we get the Residual Standard Error of a model? The R documentation says I can do it with the sigma() function. Read more about it here (or just type ?sigma in the R console and hit ENTER):\n\nsigma(simple_model)\n\n[1] 2.532326\n\n\nHow do we create these 45 models ? We can iterate over each line of the all_combn_predictors using a for loop, grab the names of the variables and create a linear model with them.\nLinear models in R are typically defined as a formula, but how do I build a formula without typing the name of the variables manually? We can do it by using the as.formula as illustrated in this Stackoverflow response.\nFirst, let me show you how the as.formula combined with paste works. Look at all the formulas produced in this loop:\n\nfor(i in 1:nrow(all_combn_predictors)){\n  \n   # get the i-th row of the dataframe\n  selected_vars <- all_combn_predictors[i,]\n  \n  # build the formula\n  model_formula <- as.formula(paste(\"Sales ~\", paste(selected_vars, collapse=\"+\")))\n  print(model_formula)\n}\n\nSales ~ CompPrice + Income\nSales ~ CompPrice + Advertising\nSales ~ CompPrice + Population\nSales ~ CompPrice + Price\nSales ~ CompPrice + ShelveLoc\nSales ~ CompPrice + Age\nSales ~ CompPrice + Education\nSales ~ CompPrice + Urban\nSales ~ CompPrice + US\nSales ~ Income + Advertising\nSales ~ Income + Population\nSales ~ Income + Price\nSales ~ Income + ShelveLoc\nSales ~ Income + Age\nSales ~ Income + Education\nSales ~ Income + Urban\nSales ~ Income + US\nSales ~ Advertising + Population\nSales ~ Advertising + Price\nSales ~ Advertising + ShelveLoc\nSales ~ Advertising + Age\nSales ~ Advertising + Education\nSales ~ Advertising + Urban\nSales ~ Advertising + US\nSales ~ Population + Price\nSales ~ Population + ShelveLoc\nSales ~ Population + Age\nSales ~ Population + Education\nSales ~ Population + Urban\nSales ~ Population + US\nSales ~ Price + ShelveLoc\nSales ~ Price + Age\nSales ~ Price + Education\nSales ~ Price + Urban\nSales ~ Price + US\nSales ~ ShelveLoc + Age\nSales ~ ShelveLoc + Education\nSales ~ ShelveLoc + Urban\nSales ~ ShelveLoc + US\nSales ~ Age + Education\nSales ~ Age + Urban\nSales ~ Age + US\nSales ~ Education + Urban\nSales ~ Education + US\nSales ~ Urban + US\n\n\nNow, let’s actually create the linear models and identify the one with the smallest RSE:\n\nbest_RSE        <- Inf   # we don't know yet \nbest_predictors <- c()   # start empty, fill later\nbest_model      <- NULL  # we don't know yet\n\nfor(i in 1:nrow(all_combn_predictors)){\n  \n   # get the i-th row of the dataframe\n  selected_vars <- all_combn_predictors[i,]\n  \n  # build the formula\n  model_formula <- as.formula(paste(\"Sales ~\", paste(selected_vars, collapse=\"+\")))\n  \n  fitted_model      <- lm(model_formula, data=Carseats)\n  current_model_RSE <- sigma(fitted_model)\n  \n  if(current_model_RSE < best_RSE){\n    best_RSE <- current_model_RSE\n    best_predictors <- selected_vars\n    best_model <- fitted_model\n    cat(\"Found a better model!\\n\")\n    cat(paste0(\"  Vars: [\", paste(selected_vars, collapse=\",\"), \"]\\n\"))\n    cat(paste0(\"  RSE:\", best_RSE, \"\\n\\n\"))\n  }\n  \n}\n\nFound a better model!\n  Vars: [CompPrice,Income]\n  RSE:2.7899309103294\n\nFound a better model!\n  Vars: [CompPrice,Advertising]\n  RSE:2.71911905676241\n\nFound a better model!\n  Vars: [CompPrice,Price]\n  RSE:2.26880676843526\n\nFound a better model!\n  Vars: [Advertising,ShelveLoc]\n  RSE:2.24418017219941\n\nFound a better model!\n  Vars: [Price,ShelveLoc]\n  RSE:1.91725333683979\n\n\nAs we can see from above, the best model – if we consider only the minimum RSE – is:\n\nsummary(best_model)\n\n\nCall:\nlm(formula = model_formula, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.8229 -1.3930 -0.0179  1.3868  5.0780 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     12.001802   0.503447  23.839  < 2e-16 ***\nPrice           -0.056698   0.004059 -13.967  < 2e-16 ***\nShelveLocGood    4.895848   0.285921  17.123  < 2e-16 ***\nShelveLocMedium  1.862022   0.234748   7.932 2.23e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.917 on 396 degrees of freedom\nMultiple R-squared:  0.5426,    Adjusted R-squared:  0.5391 \nF-statistic: 156.6 on 3 and 396 DF,  p-value: < 2.2e-16\n\n\n\\[Sales = 12.001802 - 0.056698\\times Price + 4.895848 \\times ShelveLoc[Good] + 1.862022 \\times ShelveLoc[Medium] + \\epsilon\\]\nTeeechnically, the model above has more than just two variables, it uses three variables (two combinations of ShelveLoc. This is because R converts categorical variables to independent binary variables automatically. Since we didn’t distinguish this case in the question, this is an acceptable response.\n\n\nQ15. Interaction Effects\nUse the * and : symbols to fit linear regression models with interaction effects. Could you find any model with interactions that fit better than the models you built in Questions Q5 & Q8 & Q14? Feel free to use as many interactions and variables as you prefer. Justify your answer. Explain how you reached that conclusion, show us the summary of that model and write the model in equation form. (Worth 15/100 mock marks)\n\n💡 Here, you were not expected to test ALL combinations. You could solve this by trial and error, combining multiple combinations of variables until you reached a model with better RSE, R-squared or other metric.\nFor didactic purposes, I will re-use the solution from Q14 and test all possible linear models with only two variables, this time considering * and : operator instead of the + operator.\nI will look for the model that optimizes for RSE.\n\n\n# Using the tidyverse function crossing\nnew_combn_predictors <- crossing(all_combn_predictors, tibble(interaction_type=c(\"*\", \":\")))\nnew_combn_predictors\n\n# A tibble: 90 × 3\n   V1          V2         interaction_type\n   <chr>       <chr>      <chr>           \n 1 Advertising Age        :               \n 2 Advertising Age        *               \n 3 Advertising Education  :               \n 4 Advertising Education  *               \n 5 Advertising Population :               \n 6 Advertising Population *               \n 7 Advertising Price      :               \n 8 Advertising Price      *               \n 9 Advertising ShelveLoc  :               \n10 Advertising ShelveLoc  *               \n# … with 80 more rows\n\n\nThis time there are 90 possible combinations:\n\nfor(i in 1:nrow(new_combn_predictors)){\n  \n   # get the i-th row of the dataframe\n  selected_vars <- new_combn_predictors[i,c(\"V1\", \"V2\")]\n  \n  # build the formula\n  # Notice that this time I will use whatever is in the column `interaction_type`\n  model_formula <- as.formula(paste(\"Sales ~\", paste(selected_vars, collapse=new_combn_predictors[i, ]$interaction_type)))\n  \n  print(model_formula)\n  \n}\n\nSales ~ Advertising:Age\nSales ~ Advertising * Age\nSales ~ Advertising:Education\nSales ~ Advertising * Education\nSales ~ Advertising:Population\nSales ~ Advertising * Population\nSales ~ Advertising:Price\nSales ~ Advertising * Price\nSales ~ Advertising:ShelveLoc\nSales ~ Advertising * ShelveLoc\nSales ~ Advertising:Urban\nSales ~ Advertising * Urban\nSales ~ Advertising:US\nSales ~ Advertising * US\nSales ~ Age:Education\nSales ~ Age * Education\nSales ~ Age:Urban\nSales ~ Age * Urban\nSales ~ Age:US\nSales ~ Age * US\nSales ~ CompPrice:Advertising\nSales ~ CompPrice * Advertising\nSales ~ CompPrice:Age\nSales ~ CompPrice * Age\nSales ~ CompPrice:Education\nSales ~ CompPrice * Education\nSales ~ CompPrice:Income\nSales ~ CompPrice * Income\nSales ~ CompPrice:Population\nSales ~ CompPrice * Population\nSales ~ CompPrice:Price\nSales ~ CompPrice * Price\nSales ~ CompPrice:ShelveLoc\nSales ~ CompPrice * ShelveLoc\nSales ~ CompPrice:Urban\nSales ~ CompPrice * Urban\nSales ~ CompPrice:US\nSales ~ CompPrice * US\nSales ~ Education:Urban\nSales ~ Education * Urban\nSales ~ Education:US\nSales ~ Education * US\nSales ~ Income:Advertising\nSales ~ Income * Advertising\nSales ~ Income:Age\nSales ~ Income * Age\nSales ~ Income:Education\nSales ~ Income * Education\nSales ~ Income:Population\nSales ~ Income * Population\nSales ~ Income:Price\nSales ~ Income * Price\nSales ~ Income:ShelveLoc\nSales ~ Income * ShelveLoc\nSales ~ Income:Urban\nSales ~ Income * Urban\nSales ~ Income:US\nSales ~ Income * US\nSales ~ Population:Age\nSales ~ Population * Age\nSales ~ Population:Education\nSales ~ Population * Education\nSales ~ Population:Price\nSales ~ Population * Price\nSales ~ Population:ShelveLoc\nSales ~ Population * ShelveLoc\nSales ~ Population:Urban\nSales ~ Population * Urban\nSales ~ Population:US\nSales ~ Population * US\nSales ~ Price:Age\nSales ~ Price * Age\nSales ~ Price:Education\nSales ~ Price * Education\nSales ~ Price:ShelveLoc\nSales ~ Price * ShelveLoc\nSales ~ Price:Urban\nSales ~ Price * Urban\nSales ~ Price:US\nSales ~ Price * US\nSales ~ ShelveLoc:Age\nSales ~ ShelveLoc * Age\nSales ~ ShelveLoc:Education\nSales ~ ShelveLoc * Education\nSales ~ ShelveLoc:Urban\nSales ~ ShelveLoc * Urban\nSales ~ ShelveLoc:US\nSales ~ ShelveLoc * US\nSales ~ Urban:US\nSales ~ Urban * US\n\n\n\nbest_RSE        <- Inf   # we don't know yet \nbest_predictors <- c()   # start empty, fill later\nbest_model      <- NULL  # we don't know yet\n\nfor(i in 1:nrow(new_combn_predictors)){\n  \n   # get the i-th row of the dataframe\n  selected_vars <- new_combn_predictors[i,c(\"V1\", \"V2\")]\n  \n  # build the formula\n  # Notice that this time I will use whatever is in the column `interaction_type`\n  model_formula <- as.formula(paste(\"Sales ~\", paste(selected_vars, collapse=new_combn_predictors[i, ]$interaction_type)))\n  \n  fitted_model      <- lm(model_formula, data=Carseats)\n  current_model_RSE <- sigma(fitted_model)\n  \n  if(current_model_RSE < best_RSE){\n    best_RSE <- current_model_RSE\n    best_predictors <- selected_vars\n    best_model <- fitted_model\n    cat(\"Found a better model!\\n\")\n    cat(paste0(model_formula, \"\\n\"))\n    cat(paste0(\"  RSE:\", best_RSE, \"\\n\\n\"))\n  }\n  \n}\n\nFound a better model!\n~\n Sales\n Advertising:Age\n  RSE:2.77861087594811\n\nFound a better model!\n~\n Sales\n Advertising * Age\n  RSE:2.65041673414083\n\nFound a better model!\n~\n Sales\n Advertising * Price\n  RSE:2.40120656790555\n\nFound a better model!\n~\n Sales\n Advertising * ShelveLoc\n  RSE:2.24885583915041\n\nFound a better model!\n~\n Sales\n Price:ShelveLoc\n  RSE:1.96438627687264\n\nFound a better model!\n~\n Sales\n Price * ShelveLoc\n  RSE:1.91821983511205\n\n\nWe didn’t find a better model, but we found a model that leads to almost the same RSE as the one in Q14:\n\nsummary(best_model)\n\n\nCall:\nlm(formula = model_formula, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.9037 -1.3461 -0.0595  1.3679  4.9037 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           11.832984   0.965788  12.252  < 2e-16 ***\nPrice                 -0.055220   0.008276  -6.672 8.57e-11 ***\nShelveLocGood          6.135880   1.392844   4.405 1.36e-05 ***\nShelveLocMedium        1.630481   1.171616   1.392    0.165    \nPrice:ShelveLocGood   -0.010564   0.011742  -0.900    0.369    \nPrice:ShelveLocMedium  0.001984   0.010007   0.198    0.843    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.918 on 394 degrees of freedom\nMultiple R-squared:  0.5444,    Adjusted R-squared:  0.5386 \nF-statistic: 94.17 on 5 and 394 DF,  p-value: < 2.2e-16\n\n\n\\[\n\\begin{eqnarray}\nSales &=& 11.832984 - 0.055220 \\times Price \\\\\n&&+ 6.135880 \\times ShelveLoc[Good] \\\\ &&+1.630481 \\times ShelveLoc[Medium] \\\\\n&&-0.010564 \\times (Price \\times ShelveLoc[Good] ) \\\\\n&&+0.001984 \\times (Price \\times ShelveLoc[Medium] )\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "assessments/summative1.html",
    "href": "assessments/summative1.html",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "",
    "text": "Welcome to the first Summative Problem Set of DS202 (2022/23)!\nThings to know before you start:"
  },
  {
    "objectID": "assessments/summative1.html#how-to-get-help",
    "href": "assessments/summative1.html#how-to-get-help",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "How to get help",
    "text": "How to get help\n\nIt is okay to use whatever additional R packages you can find to help you explore the data and/or models better.\nIt is okay to team up with class colleagues to brainstorm ideas and these problems together.\n\nMost questions do not have a single objective response. It is unlikely that you will all write the exact same response so we will spot plagiarism and full copy-pastes very easily.\n\nIt is ok to use Slack or a shared Google Drive document to share links to useful content. For example, you can share things like:\n\n“Tip: I am using this package called tidymodels and it is much simpler than writing for loops!”\n‘I found this link useful: What is the difference between type=“class” and type=“response” in the predict function?’\nThis website has many examples of charts in R — and it has the source code!\n‘This R package called lubridate helped me work with dates a lot easier!’\n\nIt is ok ask clarification about questions of this problem set publicly on Slack. For example, you can ask questions like:\n\n“I am a little confused about Question X. Where it says ... does it mean ....? Am I getting this right?”\n\nIt is also ok to ask generic programming-related questions publicly on Slack. For example, you can ask questions like:\n\n“How do I get just the last 10 items of a list in R/tidyverse?” or\n“How do I sum the number of occurrences of a value in a column?”\n“Anyone else getting an unequal lengths error when creating a new vector? How do I solve this?”\n“I am having a hard time understanding the code below (from Week X lab):\nnew_list <- seq(length(dim(some_dataframe)[1]))\nWhy so many parentheses? Does anyone how to interpret this? #help” or even\n“How do I select specific columns of a dataframe by their names?” or maybe\n“The pairs plot is too messy, anyone knows of a better way to visualise pairs of variables?”\n\nWhat we CANNOT accept:\n\nsharing your entire script or RMarkdown with others. But it is ok to share snippets of code with best practices, or to ask for help, like the type of code people share on Stackoverflow\nasking others to do your work for you (LSE regulations on plagiarism applies to computer code too)\nWe will run TurnitIn on your submissions to help flag cases of plagiarism"
  },
  {
    "objectID": "assessments/summative1.html#setup",
    "href": "assessments/summative1.html#setup",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "⚙️ Setup",
    "text": "⚙️ Setup\nImport required libraries.No need to change if you are not planning to use other packages.\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n# If you use other packages, add them here"
  },
  {
    "objectID": "assessments/summative1.html#data-dictionary",
    "href": "assessments/summative1.html#data-dictionary",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Data Dictionary",
    "text": "Data Dictionary\nWe will use a dataset of Algerian forest fires used by Faroudja & Izeboudjen (2019) 2 and sourced from the UCI ML repository. It has observations on 244 days in Algeria from June to September 2012 in two regions:\n\nBejaia, and\nSidi Bel-abbes\n\nThe dataset contains the following variables:\n\nDate columns\n\n\n\nColumn\nDescription\n\n\n\n\nday\nday of monitoring\n\n\nmonth\nmonth of the monitoring (‘june’ to ‘september’)\n\n\nyear\nFixed: 2012\n\n\n\n\n\nColumns related to weather data observations\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nTemperature\ntemperature at noon (max temperature) in Celsius degrees\n\n\nRH\nRelative Humidity in %\n\n\nWs\nWind speed in km/h\n\n\nRain\ntotal day in mm\n\n\n\n\n\nColumns related to FWI components 3\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nFFMC\nFine Fuel Moisture Code index from the FWI system\n\n\nDMC\nDuff Moisture Code index from the FWI system\n\n\nDC\nDrought Code index from the FWI system\n\n\nISI\nInitial Spread Index index from the FWI system\n\n\nBUI\nBuildup Index index from the FWI system\n\n\nFWI\nFire Weather Index Index\n\n\n\n\n\nThe column we want to predict\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nClasses\ntwo classes representing the ocurrence of fire"
  },
  {
    "objectID": "assessments/summative1.html#loading-the-data",
    "href": "assessments/summative1.html#loading-the-data",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Loading the data",
    "text": "Loading the data\nUse the code below to load the two datasets used in this first problem set.\nIMPORTANT: Ensure all the dataset files are in the exact same directory as this RMarkdown.\n# read_csv is a function of the tidyverse package\n\ndf_forest_fires_bejaia <- read_csv(\"./Algeria_Forest_Fires_Bejaia_Region_Dataset.csv\")\n\ndf_forest_fires_sidi   <- read_csv(\"./Algeria_Forest_Fires_Sidi_Bel_Abbes_Region_Dataset.csv\")\n\n\nTake a look at the data\n# Look at the first few lines of the dataframe\ndf_forest_fires_bejaia %>% head()\n# Look at the first few lines of the dataframe\ndf_forest_fires_sidi %>% head()\n# What are the dimensions of the dataframes?\n\ndf_forest_fires_bejaia %>% dim()\n# What are the dimensions of the dataframes?\n\ndf_forest_fires_sidi %>% dim()"
  },
  {
    "objectID": "assessments/summative1.html#what-we-want-from-you",
    "href": "assessments/summative1.html#what-we-want-from-you",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "What we want from you",
    "text": "What we want from you\nYou main goal will be to predict the occurrence of fires and understand what this ocurrence is associated with.\nNext, you will go through a sequence of tasks. For each of them you are given a code cell where you are supposed to write solutions to the tasks."
  },
  {
    "objectID": "assessments/summative1.html#q1.-fire-days",
    "href": "assessments/summative1.html#q1.-fire-days",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Q1. Fire days",
    "text": "Q1. Fire days\nUsing R, count the number of fire days observed in the two regions. (2 points)\n\nBejaia region\n\n# Replace this by your code\n\nSidi Bel-abbes region\n\n# Replace this by your code"
  },
  {
    "objectID": "assessments/summative1.html#q2.-fire-days-in-common",
    "href": "assessments/summative1.html#q2.-fire-days-in-common",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Q2. Fire days in common",
    "text": "Q2. Fire days in common\nUsing R, calculate how many days of fire the two regions had in common, and explain how you calculated it. (3 points)\n# Replace this by your code\nExplain what you did in the code above:\n\nReplace this with your text. Use multiple lines if needed."
  },
  {
    "objectID": "assessments/summative1.html#q3.-exploratory-data-analysis---part-i",
    "href": "assessments/summative1.html#q3.-exploratory-data-analysis---part-i",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Q3. Exploratory Data Analysis - Part I",
    "text": "Q3. Exploratory Data Analysis - Part I\nRun the code below to look at the plot it produces. In your own words, explain what you see: what dataset was used in the plot, what are the variables in the X and Y axis and what do the colours mean? (2 points)\ng <-\n    (ggplot(df_forest_fires_bejaia,\n            aes(x = Temperature, y = RH, colour = Classes))\n        + geom_point(size = 3, alpha = 0.6)\n\n        # OPTIONAL: Customising the plot.\n        # You can delete these lines below if you don't like the theme\n        # Or you can choose other themes from\n        # https://ggplot2.tidyverse.org/reference/ggtheme.html\n        + theme_bw()\n    )\ng\n\nYour text goes here"
  },
  {
    "objectID": "assessments/summative1.html#q4.-exploratory-data-analysis---part-ii",
    "href": "assessments/summative1.html#q4.-exploratory-data-analysis---part-ii",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Q4. Exploratory Data Analysis - Part II",
    "text": "Q4. Exploratory Data Analysis - Part II\nNow, create a scatterplot using any two predictors from the Sidi Bel-abbes region data. Colour the dots according to their Classes (3 points)\nYou can use either base R or ggplot. 4\n# Your code here"
  },
  {
    "objectID": "assessments/summative1.html#q5.-exploratory-data-analysis---part-iii",
    "href": "assessments/summative1.html#q5.-exploratory-data-analysis---part-iii",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Q5. Exploratory Data Analysis - Part III",
    "text": "Q5. Exploratory Data Analysis - Part III\nCan you spot differences in the distributions of predictors between the two regions (Sidi Bel-abbes vs Bejaia)? Describe the differences for at least one variable. Write your response and provide evidence using R code. You could use, for example, cross-tabulation, descriptive statistics or visualisations to support your point. (8 points)\n\nReplace this with your text. Use multiple lines if needed.\n\n# Your code here"
  },
  {
    "objectID": "assessments/summative1.html#q6.-logistic-regression-model",
    "href": "assessments/summative1.html#q6.-logistic-regression-model",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Q6. Logistic Regression Model",
    "text": "Q6. Logistic Regression Model\nBuild a logistic regression model for the Bejaia dataset using THREE predictors to predict the ocurrence of fire (the Classes variable). You can also add interaction effects amongst these three predictors if you wish. Save it as a variable named model and use R to print its summary. (7 points)\n💡 Tip: you might need to convert Classes to a factor.\n💡 If you have questions about R programming or conceptual questions about logistic regression, it’s ok to ask questions to teachers and colleagues. What you are not allowed to ask: things like “is my solution correct?” or “which variables did you use?”, etc..\nYou can choose to print the summary using base R or any of the functions from the broom package (part of tidymodels).\n# Your code here\n\n# If you won't answer this question, erase or comment out the line of code below. Otherwise, you will get an error when knitting this notebook.\nmodel <-"
  },
  {
    "objectID": "assessments/summative1.html#q7.-logistic-regression-model---justification",
    "href": "assessments/summative1.html#q7.-logistic-regression-model---justification",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Q7. Logistic Regression Model - Justification",
    "text": "Q7. Logistic Regression Model - Justification\nProvide a reasonable explanation for your choice of the three predictors in Q6. Why did you chose those variables? (10 points)\n(Optional: add additional R code/visualisations that you feel might help support your answer)\n\nReplace this with your text. Use multiple lines if needed."
  },
  {
    "objectID": "assessments/summative1.html#q8.-logistic-regression-model---diagnostics",
    "href": "assessments/summative1.html#q8.-logistic-regression-model---diagnostics",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Q8. Logistic Regression Model - Diagnostics",
    "text": "Q8. Logistic Regression Model - Diagnostics\nRun the code below to look at the plot it produces. In your own words, explain what you see and what this plot tells you about your model. (8 points)\n⚠️ If you didn’t build a model in Q6, erase or comment out the block of code below. Otherwise, you will get an error when knitting this notebook.\n\ntrain_classes     <- df_forest_fires_bejaia$Classes\ntrain_predictions <- predict(model, df_forest_fires_bejaia, type = \"response\")\n\nplot_df <- data.frame(train_classes     = train_classes,\n                      train_predictions = train_predictions)\n\ng <-\n    (ggplot(plot_df, aes(x = train_predictions, fill = train_classes))\n        + geom_histogram(alpha = 0.8, binwidth = 0.05, position = \"stack\")\n\n        # OPTIONAL: Customising the plot.\n        # You can delete these lines below if you don't like the theme\n        # Or you can choose other themes from\n        # https://ggplot2.tidyverse.org/reference/ggtheme.html\n        + theme_bw()\n        + labs(x = \"Predictions on the training set\",\n               y = \"Count\")\n        + scale_fill_brewer(name = \"Target\", type = \"qual\", palette = 2)\n        + scale_x_continuous(labels = scales::percent, breaks = seq(0, 1, 0.1))\n        + ggtitle(\"Histogram of probability distributions fitted to the data\")\n    )\ng\n\nYour text goes here"
  },
  {
    "objectID": "assessments/summative1.html#q9.-logistic-regression-model---confusion-matrix",
    "href": "assessments/summative1.html#q9.-logistic-regression-model---confusion-matrix",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Q9. Logistic Regression Model - Confusion Matrix",
    "text": "Q9. Logistic Regression Model - Confusion Matrix\nRun the code below to look at the table it produces. What does this table show and what does it tell you about your model? (3 points)\ntrain_classes           <- df_forest_fires_bejaia$Classes\ntrain_class_predictions <- apply_threshold(model, df_forest_fires_bejaia, threshold=0.50)\n\nconfusion_matrix <- table(train_classes, train_class_predictions)\nprint(confusion_matrix)"
  },
  {
    "objectID": "assessments/summative1.html#q10.-logistic-regression-model---classification-metrics",
    "href": "assessments/summative1.html#q10.-logistic-regression-model---classification-metrics",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Q10. Logistic Regression Model - Classification metrics",
    "text": "Q10. Logistic Regression Model - Classification metrics\nNow, consider three other options of threshold: \\(t \\in \\{0.20, 0.40, 0.60\\}\\). Which of these three options lead to the best f1-score for your model? Write the R code for this and justify your answer. (7 points)\n# Your code here\n\nReplace this with your text. Use multiple lines if needed."
  },
  {
    "objectID": "assessments/summative1.html#q11.-logistic-regression-model---optimal-threshold-challenging",
    "href": "assessments/summative1.html#q11.-logistic-regression-model---optimal-threshold-challenging",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Q11. Logistic Regression Model - Optimal Threshold (Challenging)",
    "text": "Q11. Logistic Regression Model - Optimal Threshold (Challenging)\nNow, consider another set of possible thresholds, \\(t \\in \\{0.00, 0.01, 0.02, \\ldots, 0.98, 0.99, 1.00\\}\\). Find the optimal threshold \\(t^*\\), the one that leads to the best f1-score. Write the R code for this and justify your answer. (12 points)\n# Your code here\n\nReplace this with your text. Use multiple lines if needed."
  },
  {
    "objectID": "assessments/summative1.html#q12.-test-set-predictions",
    "href": "assessments/summative1.html#q12.-test-set-predictions",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Q12. Test set predictions",
    "text": "Q12. Test set predictions\nFollow the instructions below to apply the model you trained in Q6 to predict the probability of forest fires in the Sidi Bel-abbes dataset and produce a plot similar to that of Q8. (7 points)\n\nCreate a vector named test_classes that contains the true observed data (fire vs not fire) of Sidi Bel-abbes (You might need to convert it to factor)\nCreate a vector named test_predictions that contains the predict probability of forest fires in the Sidi Bel-abbes region\nIf the plot is produced and correct, you will get full marks. No need to justify the response.\n\n⚠️ If you don’t want to answer this question, erase or comment out the block of code below. Otherwise, you will get an error when knitting this notebook.\n# Your code here\n\ntest_classes <- \ntest_predictions <- \n\nplot_df <- data.frame(test_classes     = test_classes,\n                      test_predictions = test_predictions)\n\ng <-\n    (ggplot(plot_df, aes(x = test_predictions, fill = test_classes))\n        + geom_histogram(alpha = 0.8, binwidth = 0.05, position = \"stack\")\n\n        # OPTIONAL: Customising the plot.\n        # You can delete these lines below if you don't like the theme\n        # Or you can choose other themes from\n        # https://ggplot2.tidyverse.org/reference/ggtheme.html\n        + theme_bw()\n        + labs(x = \"Predictions on the test set\",\n               y = \"Count\")\n        + scale_fill_brewer(name = \"Target\", type = \"qual\", palette = 2)\n        + scale_x_continuous(labels = scales::percent, breaks = seq(0, 1, 0.1))\n        + ggtitle(\"Histogram of probability distributions when applied to Sidi Bel-abbes data\")\n    )\ng"
  },
  {
    "objectID": "assessments/summative1.html#q13.-diagnostics",
    "href": "assessments/summative1.html#q13.-diagnostics",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Q13. Diagnostics",
    "text": "Q13. Diagnostics\nUsing the best threshold you found in either Q10 or Q11, write R code to produce a confusion matrix for the test set (Sidi Bel-abbes dataset). What is the True Positive Rate and True Negative Rate of your model in the test set? Did your model generalise well from the training to test set? (8 points)\n# Your code here\n\n\nReplace this with your text. Use multiple lines if needed."
  },
  {
    "objectID": "assessments/summative1.html#q14.-alternative-models-challenging",
    "href": "assessments/summative1.html#q14.-alternative-models-challenging",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Q14. Alternative Models (Challenging)",
    "text": "Q14. Alternative Models (Challenging)\nFollow the instructions below to build and explore an alternative classification model. Add as many chunks of code, text and equations as you prefer. (20 points)\n\nChose another algorithm (either Naive Bayes, Decision Tree or Support Vector Machine) to build a new classification model.\nUse the same training data you used to build your logistic regression in Q6 (same predictors)\nIf the algorithm requires a threshold, chose one that maximises the F1-score using the same logic as in Q10 or Q11.\nUse the same test data you used to validate your logistic regression as in Q12\nIf the algorithm does not require a threshold, try to tweak the parameters of the algorithm so as to avoid overfitting the model.\nUse whatever means you find appropriate (for example metrics, matrices, tables, plots) to compare your new model to the logistic model you built in the rest of this notebook.\nWrite about what you think makes your alternative model better/worse.\nProvide the full R code you used to build and test your alternative model\n\n\n💡 TIPS\n\nUse what you have learned about up until Week 05 (lecture)\nRefer to your readings of the textbook if you want to understand more about how the alternative algorithms work. You can find links to the appropriate chapters here.\n\n\n# Your code here. Copy this chunk of code if you need.\n\n\nReplace this with your text. Use multiple lines if needed."
  },
  {
    "objectID": "assessments/summative1.html#decompress",
    "href": "assessments/summative1.html#decompress",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Decompress",
    "text": "Decompress\nHow do you plan to reward yourself for completing this problem set?\n\n<replace this text with your reward. A cookie? take X days off? etc.>"
  },
  {
    "objectID": "weeks/week01.html",
    "href": "weeks/week01.html",
    "title": "🗓️ Week 01 - Introduction, Context & Key Concepts",
    "section": "",
    "text": "In this first week, we will cover what you can expect to learn from this course and the course logistics: all you need to know about the structure of the lectures, classes, assessments and how we will interact throughout this course.\nWe will also cover some of the basics: what do we mean by data science & machine learning, and what do you need to do to get the most of this course?\nJoin the lecture on 30 September 2022 at 2pm at NAB LG.01 (Wolfson Theatre)."
  },
  {
    "objectID": "weeks/week01.html#links",
    "href": "weeks/week01.html#links",
    "title": "🗓️ Week 01 - Introduction, Context & Key Concepts",
    "section": "Links",
    "text": "Links\n\n👨‍🏫 Lecture slides\n📒 Preparing for next week’s lab\n🔖 Appendix"
  },
  {
    "objectID": "weeks/week02.html",
    "href": "weeks/week02.html",
    "title": "🗓️ Week 02 - Simple and Multiple Linear Regression",
    "section": "",
    "text": "If you have already taken stats level at university, it is likely that you have met the linear regression method.\nIt is the most fundamental algorithm for regression and if you truly master it, you will be way ahead of the curve when it comes to the most advanced Machine Learning algorithms.\nJoin the lecture 7 October 2022 2pm at NAB LG.01 (Wolfson Theatre).."
  },
  {
    "objectID": "weeks/week02.html#links",
    "href": "weeks/week02.html#links",
    "title": "🗓️ Week 02 - Simple and Multiple Linear Regression",
    "section": "Links",
    "text": "Links\n\n👨‍🏫 Lecture slides\n✅ Take a look at this week’s checklist\n💻 This week’s lab\n✔️ Lab Solutions"
  },
  {
    "objectID": "weeks/week03.html",
    "href": "weeks/week03.html",
    "title": "🗓️ Week 03 - Classifiers",
    "section": "",
    "text": "On 🗓️ Week 02, we learned how to make predictions about numerical variables. But what if you wanted to predict whether someone will perform an action (a Yes or No question)? Or, say, you were interested in assessing how the risk of fraud increases depending on the behaviour of a customer? These problems can be modelled using classifiers, a type of supervised learning.\nThis week, we will explore two classifier algorithms: the Logistic Regression and the Naive Bayes classifiers. We will learn how those methods relate (or not) to linear regression, and how to interpret its results. You will also meet a few new metrics and will learn of new ways to assess the ‘accuracy’ of models. These metrics will be incrediblly important on Week 04!\nJoin the lecture 14 October 2022 2pm at NAB LG.01 (Wolfson Theatre)."
  },
  {
    "objectID": "weeks/week03.html#links",
    "href": "weeks/week03.html#links",
    "title": "🗓️ Week 03 - Classifiers",
    "section": "Links",
    "text": "Links\n\n👨‍🏫 Lecture slides\n✅ Take a look at this week’s checklist\n💻 This week’s lab\n✔️ Lab Solutions"
  },
  {
    "objectID": "weeks/week04.html",
    "href": "weeks/week04.html",
    "title": "🗓️ Week 04 - Resampling Methods",
    "section": "",
    "text": "We have made it to Week 04! In the labs, you will get a chance to explore different Classification algorithms (Logistic Regression and Naive Bayes) and how to think about their output. What can we say about predicted probabilities?\nThis week’s lecture (on Friday 21 October 2022, Wolfson Theatre NAB.LG.01 2pm-4pm) will be a bit different. We will not learn about new algorithms. Instead, we will learn how to compare models built from the three algorithms we learned about (linear regression, logistic regression and naive bayes). You will learn about the different metrics one could use to assess the predictive performance on a test set rather than just the goodness-of-fit of the training data.\nThe structure of the lecture will also be a bit different, it will resemble a workshop more than a taught lecture. See you there!"
  },
  {
    "objectID": "weeks/week04.html#links",
    "href": "weeks/week04.html#links",
    "title": "🗓️ Week 04 - Resampling Methods",
    "section": "Links",
    "text": "Links\n\n👨‍🏫 Lecture\n✅ Take a look at this week’s checklist\n💻 This week’s lab\n✔️ Lab Solutions"
  },
  {
    "objectID": "weeks/week05.html",
    "href": "weeks/week05.html",
    "title": "🗓️ Week 05 - Non-linear algorithms",
    "section": "",
    "text": "Welcome to Week 05 of DS202!\nIn the labs, you will practice training/test splits, cross-validation and you will learn about the bootstrap. Check Chapter 5 of our textbook to reinforce these concepts during the week.\nAt the end of the week, we will explore two new algorithms: Support Vector Machine and Decision Trees.\nStarting this week, our lecture will take place in a new classroom. Join us this Friday, 28 October 2022 from 2 p.m. to 4 p.m. at NAB.LG.08."
  },
  {
    "objectID": "weeks/week05.html#links",
    "href": "weeks/week05.html#links",
    "title": "🗓️ Week 05 - Non-linear algorithms",
    "section": "Links",
    "text": "Links\n\n👨‍🏫 Lecture\n✅ Take a look at this week’s checklist\n💻 This week’s lab\n✔️ Lab Solutions\n✍️ First Summative W05-W07"
  },
  {
    "objectID": "weeks/week06.html",
    "href": "weeks/week06.html",
    "title": "🗓️ Week 06 - Reading Week",
    "section": "",
    "text": "We know many of you will be using the Reading Week to work on your summative assessments (deadline: 9 Nov).\nTo support you with your problem sets, we have three drop-in sessions this week:\nYou don’t need to book, just show up. But if you want to have the session on your calendar, you can book your visit via Student Hub."
  },
  {
    "objectID": "weeks/week06.html#links",
    "href": "weeks/week06.html#links",
    "title": "🗓️ Week 06 - Reading Week",
    "section": "Links",
    "text": "Links\n\n✅ Take a look at this week’s checklist\n💻 RNotebook created during Drop-In Session (Jon)"
  },
  {
    "objectID": "weeks/week07.html",
    "href": "weeks/week07.html",
    "title": "🗓️ Week 07 - Recap of tidyverse + Clustering Algorithms",
    "section": "",
    "text": "Welcome to Week 07 of DS202!\nThis week’s lab draws on 🗓️ Week 05 lecture content and on feedback given by the course representatives about the main struggles you are facing with R (tidyverse).\nIn the lecture, we will recap some tidyverse concepts – things we wish we had taught you in the first weeks of this course! – and then, on the second part, we will introduce the idea of unsupervised learning (clustering).\nJoin us on Friday, 11 November 2022 from 2 p.m. to 4 p.m. at NAB.LG.08."
  },
  {
    "objectID": "weeks/week07.html#links",
    "href": "weeks/week07.html#links",
    "title": "🗓️ Week 07 - Recap of tidyverse + Clustering Algorithms",
    "section": "Links",
    "text": "Links\n\n✍️ First Summative W05-W07, worth 20%, deadline: 9 November 2022.\n💻 This week’s lab\n👨‍🏫 Lecture\n✅ Take a look at this week’s checklist\n✔️ Lab Solutions"
  },
  {
    "objectID": "weeks/week08.html",
    "href": "weeks/week08.html",
    "title": "🗓️ Week 08 - Dimensionality Reduction",
    "section": "",
    "text": "Welcome to Week 08 of DS202!"
  },
  {
    "objectID": "weeks/week08.html#links",
    "href": "weeks/week08.html#links",
    "title": "🗓️ Week 08 - Dimensionality Reduction",
    "section": "Links",
    "text": "Links\n\n💻 This week’s lab\n✨ This week’s bonus lab (advanced)\n👨‍🏫 Lecture 🔜\n✅ Take a look at this week’s checklist 🔜\n✔️ Lab Solutions 🔜"
  },
  {
    "objectID": "weeks/week01/appendix.html",
    "href": "weeks/week01/appendix.html",
    "title": "🔖 Week 01 - Appendix",
    "section": "",
    "text": "This week’s indicative reading: (James et al. 2021, chaps. 2, 2.1–2.2)\n\n\nNeed to recap probability and statistics concepts? Check the suggested readings below:\n\n(Warne 2018, chaps. 1-3,5,6,11-12)\n(Gelman, Hill, and Vehtari 2020, chaps. 1–4)\nIf you are a PBS student, you can revisit the content of PB130 (MT3, MT4, MT8-MT11)"
  },
  {
    "objectID": "weeks/week01/appendix.html#recommended-additional-reading",
    "href": "weeks/week01/appendix.html#recommended-additional-reading",
    "title": "🔖 Week 01 - Appendix",
    "section": "Recommended (additional) reading",
    "text": "Recommended (additional) reading\nWhat are different ways one can approach a modelling problem?\nCheckout the upcoming book ‘Modeling Mindsets’ (Molnar 2022, chaps. 2–3) (it’s free to read online) to learn about the traditional frequentist statistics vs Bayesian statistics vs Machine Learning approaches.\nThe following twitter thread also summarises the main points of these different paradigms:\n\n\nIn a perfect world, you could effortlessly switch between modeling mindsets (statistics, machine learning, causal inference, …).Realistically, you only have time to master a few mindsets.So what to do? A thread 🧵\n\n— Christoph Molnar (@ChristophMolnar) August 30, 2022"
  },
  {
    "objectID": "weeks/week01/appendix.html#lse-digital-skills-lab",
    "href": "weeks/week01/appendix.html#lse-digital-skills-lab",
    "title": "🔖 Week 01 - Appendix",
    "section": "LSE Digital Skills Lab",
    "text": "LSE Digital Skills Lab\nLSE Digital Skills Lab offers R and python workshops during Term time and they will also give DSI students access to self-paced programming courses via Dataquest.\nFollow the links below to take the pre-sessional self-paced courses:\n\nR for Data Science Pre-sessional Course 22/23\n\nAlso, keep an eye on the following pages for news of the in-person workshops:\n\nR workshops"
  },
  {
    "objectID": "weeks/week01/appendix.html#other-resources",
    "href": "weeks/week01/appendix.html#other-resources",
    "title": "🔖 Week 01 - Appendix",
    "section": "Other resources",
    "text": "Other resources\n\nCheckout this summer’s LSE Careers Skill Accelerator programme. Some of the self-paced courses will remain open to LSE students until the end of the year.\n\n\n\nReady to develop the key skills employers are looking for in 2022?Join this summer's LSE Careers Skills Accelerator programme to expand your skillset!⭐Apply on CareerHub by 11.59pm, Wed 15 June for your chance to join the programme⭐https://t.co/J5sI1NRaMA\n\n— LSE Careers (@LSECareers) June 9, 2022\n\n\n\nThe book R for Data Science is free to read online and is a great resource to advance your R skills."
  },
  {
    "objectID": "weeks/week01/lecture.html",
    "href": "weeks/week01/lecture.html",
    "title": "👨‍🏫 Week 01 - Slides",
    "section": "",
    "text": "Either click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides."
  },
  {
    "objectID": "weeks/week01/lecture.html#coffee-break-10-min",
    "href": "weeks/week01/lecture.html#coffee-break-10-min",
    "title": "👨‍🏫 Week 01 - Slides",
    "section": "☕ Coffee Break (10 min)",
    "text": "☕ Coffee Break (10 min)\nUse this time to chat, stretch, drink some coffee or just relax for a bit by yourself."
  },
  {
    "objectID": "weeks/week01/lecture.html#part-ii---key-concepts-45-50-min",
    "href": "weeks/week01/lecture.html#part-ii---key-concepts-45-50-min",
    "title": "👨‍🏫 Week 01 - Slides",
    "section": "Part II - Key Concepts (45-50 min)",
    "text": "Part II - Key Concepts (45-50 min)\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides."
  },
  {
    "objectID": "weeks/week01/prep.html",
    "href": "weeks/week01/prep.html",
    "title": "📒 Week 01 - Preparing for next week’s lab",
    "section": "",
    "text": "We do not have classes this week. Instead, we recommend you use this time to learn or revisit the basics of the programming language R.\nSee sections below for advice on how to do that."
  },
  {
    "objectID": "weeks/week01/prep.html#preparing-for-the-next-week",
    "href": "weeks/week01/prep.html#preparing-for-the-next-week",
    "title": "📒 Week 01 - Preparing for next week’s lab",
    "section": "📒 Preparing for the next week",
    "text": "📒 Preparing for the next week\n\nJoin the Slack group of this course (more info)\nUse this time to learn or revisit basic R programming skills.\nWrite down your R questions. Next week’s lab, we have a roadmap for a revision of basic commands but we can tailor it to address any questions you might have with R."
  },
  {
    "objectID": "weeks/week01/prep.html#recommended-reading-other-resources",
    "href": "weeks/week01/prep.html#recommended-reading-other-resources",
    "title": "📒 Week 01 - Preparing for next week’s lab",
    "section": "🔖 Recommended reading & other resources",
    "text": "🔖 Recommended reading & other resources\nCheck out the Appendix page."
  },
  {
    "objectID": "weeks/week02/checklist.html",
    "href": "weeks/week02/checklist.html",
    "title": "✅ Week 02 - Checklist",
    "section": "",
    "text": "Your Checklist:\nYour Checklist:\n\n🖥️ Before you come to the class, skim through the W02 lab roadmap page to have an idea of what we are going to do.\n👨‍💻 New to R? Or perhaps you are in one of the Monday sessions and struggled to follow the lab? There is still time to take the R pre-sessional course. (Read the Getting access and using Dataquest session carefully)\n📚 Before you attend the lecture on Friday, try to catch up on the recommended reading of last week.\n💻 Assess your own understanding: did you understand all the exercises of the lab?\n📝 Take note of anything that is still not clear to you.\n📟 Share your conceptual or programming-related questions on #week02 channel on Slack.\n📤 Have anything else to share? If came across an interesting resource for R beginners, or curious articles about exploratory data analysis, feel free to share it on /week02 or /random channels in our Slack group.\n\nFollowing this will keep you well prepared for the Linear Regression lab of Week 03."
  },
  {
    "objectID": "weeks/week02/lab.html",
    "href": "weeks/week02/lab.html",
    "title": "💻 Week 02 - Lab Roadmap (90 min)",
    "section": "",
    "text": "This week, we will recap some basic R commands for social data science and then apply these commands to a practical case. We will learn about data structures and some simple data visualisation skills in R .\nIt is expected that R has been downloaded locally. We recommend that you run R within an integrated development environment (IDE) such as RStudio, which can be freely downloaded."
  },
  {
    "objectID": "weeks/week02/lab.html#step-1-basic-commands",
    "href": "weeks/week02/lab.html#step-1-basic-commands",
    "title": "💻 Week 02 - Lab Roadmap (90 min)",
    "section": "Step 1: Basic commands",
    "text": "Step 1: Basic commands\nWe will follow the instructions below step by step together while answering whatever questions you might encounter along the way.\n\nOpen R or RStudio. You can either run the folllowing commands in a R script or in the console window.\nCreate a vetor of numbers with the function c() and name it x. When we type x, it gives us back the vector:\n> x <- c(1, 3, 2, 5)\n> x\n[1] 1 3 2 5\nNote that the > is not part of the command; rather, it is printed by R to indicate that it is ready for another command to be entered. We can also save things using = rather than <-:\n> x = c(1, 3, 2, 5)\nCheck the length of vector x using the length() function:\n> length(x)\n[1] 4\nCreate a matrix of numbers with the function matrix() and name it y. When we type y, it gives us back the matrix:\n> y <- matrix(data = c(1:16), nrow = 4, ncol = 4)\n> y\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\nIf you want to learn about the meaning of some arguments like nrow or ncol:\n> ?matrix\nSelect one element in the matrix y:\n> y[2,3]\n[1] 10\nThe first number after the open-bracket symbol [ always refers to the row, and the second number always refers to the column\nSelect multiple rows and column at a time in the matrix y:\n> y[c(1, 3), c(2, 4)]\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n> y[1:3, 2:4]\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n> y[1:2, ]\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n> y[-c(1, 3), ]\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\nNo index for the columns or the rows indicates that R should include all columns or all rows, respectively. The use of a negative sign - in the index tells R to keep all rows or columns except those indicated in the index.\nCheck the number of rows and columns in a matrix:\n> dim(y)\n[1] 4 4\nGenerate a vector of random normal variables:\n> set.seed(1303)\n> x <- rnorm(50)\n> y <- x + rnorm(50, mean = 50, sd = .1)\n> cor(x, y)\n[1] 0.9942128\nBy default, rnorm() creates standard normal random variables with a mean of 0 and a standard deviation of 1. However, the mean and standard deviation can be altered as illustrated above.\nEach time we call the function rnorm(), we will get a different answer. However, sometimes we want our code to reproduce the exact same set of random numbers; we can use the set.seed() function to do this. We use set.seed() throughout the labs whenever we perform calculations involving random quantities.\nLet’s check some descriptive statistics of these vectors:\n> mean(y)\n[1] 50.18446\n> var(y)\n[1] 0.8002002\n> sqrt ( var (y))\n[1] 0.8945391\n> sd(y)\n[1] 0.8945391\n> cor (x, y)\n[1] 0.9942128\nThe mean() and var() functions can be used to compute the mean and variance of a vector of numbers. Applying sqrt() to the output of var() will give the standard deviation. Or we can simply use the sd() function. The cor() function is to compute the correlation between vector x and y."
  },
  {
    "objectID": "weeks/week02/lab.html#step-2-graphics",
    "href": "weeks/week02/lab.html#step-2-graphics",
    "title": "💻 Week 02 - Lab Roadmap (90 min)",
    "section": "Step 2: Graphics",
    "text": "Step 2: Graphics\nWe will plot and save plots in R.\n\nProduce a scatterplot between two vectors of numbers using the function plot():\n> set.seed(1303)\n> x <- rnorm(100)\n> y <- rnorm(100)\n> plot(x,y)\n> plot(x, y, xlab = \" this is the x- axis \",\n       ylab = \" this is the y- axis \",\n       main = \" Plot of X vs Y\")\nBy default, the output plot will show in Plots window in the lower right cornor.\nSave the scatterplot in a pdf or a jpeg file:\n> pdf(\"Figure.pdf\")\n> plot(x, y, col = \"green\")\n> dev.off()\nnull device\n        1    \nTo create a jpeg, we use the function jpeg() instaed of pdf(). The function dev.off() indicates to R that we are done creating the plot.\nProduce a contour plot (like a topographical map) to represent 3-Dimentional data using the function contour():\n> x <- seq(1, 10)\n> y <- x\n> f <- outer(x, y, function (x, y) cos(y) / (1 + x^2))\n> contour(x, y, f)\n> contour(x, y, f, nlevels = 45, add = T)\n> fa <- (f - t(f)) / 2\n> contour(x, y, fa, nlevels = 15)\nThe image() function works the same way as contour(). Explore it if you are interested.\nUsing ggplot2 package for graphic:\nIn R, the data is stored in a structure called dataframe. Dataframe can be seen as a 2-dimensional table consisting of rows and columns and their values. These values might be in different types such as numeric, character or logical. However, each column should have the exactly same data type.\nWe can use the open-source data visualization package - ggplot2 to construct aesthetic mappings based on our data.\n\nSince tidyverse library includes ggplot2, if you install tidyverse you will have access to ggplot2; installation can be done;\n\n> install.packages(\"tidyverse\")\n\nAlternatively, ggplot2 package can be installed\n\n> install.packages(\"ggplot2\")\nAfter the installation is completed, it should be called in R environment:\n> library(ggplot2)\nThere are some ready datasets to play with in the package ggplot2. Let’s explore and plot a dataset called diamonds showing the prices and some features of over 50000 diamonds. You can explore the meanings of the variables with ?diamonds command.\nPlease type:\n> View(diamonds)\nthe View() function can be used to view it in a spreadsheet-like window.\nwe can plot this dataset with desired variables.\n > ggplot(diamonds[0:50,], aes(x=carat, y=price)) +\ngeom_point() + \ngeom_text(label=diamonds[0:50,]$cut)\nx and y in aes shows the axis which are the carat and the price info each diamond. diamonds is the dataframe used in the plot and We used only the first 50 lines for clear visualisation. geom_point defines the shape of data to be plot and geom_text adds the labels. With $ sign, you can access a column in your dataset.\nWe can also plot a histogram showing price\n > ggplot(diamonds,aes(x=price)) + geom_histogram(binwidth=100)\nThis time all dataset is used for the visualisation.. For more detailed information and some examples you can use ?ggplot and ?aes\n\n\n\n\n\n\n\nFurther Study - Heatmap Example\n\n\n\n\n\nCreating a heatmap with ggplot2 package:\nThis time we will create a dummy dataframe with country names, a time period and random GDP for each country.\ncountries <- c(\"Canada\", \"France\",\"Greece\",\"Libya\",\"Malta\")\nyears <- c(2012:2021)\nLet’s gather them together and see what our dataframe looks like:\ndata <- expand.grid(Country=countries, Year=years)\ndata\nexpand.grid creates a dataframe from all combinations of the supplied vectors.\nto create random GPD for each country and for each year, and to add these values into our dataframe as GDP column::\ngdps  <- runif(50, min=20000, max=500000)\ndata$GDP = gdps\nrunif generates a certain number of random values between min and maximum values with a uniform distribution. Since we have 5 countries and 10 year, we generated 50 random GPD value.\nTo check the data and type of the variable data:\nView(data)\nclass(data)\nWe can plot now a very basic heatmap\nggplot(data, aes(Year, Country, fill= GDP)) + geom_tile()\nTo create a heatmap, our dataframe should look like a tabular dataset with three columns. aes defines X,Y axis and the values filling these pairs in the heatmap. geom_tile creates a heatmap with rectangulars with different options. For detailed information ?geom_tile"
  },
  {
    "objectID": "weeks/week02/lab.html#step-3-loading-data",
    "href": "weeks/week02/lab.html#step-3-loading-data",
    "title": "💻 Week 02 - Lab Roadmap (90 min)",
    "section": "Step 3: Loading data",
    "text": "Step 3: Loading data\nNow, we will learn how to import a data set into R and explore the data set. For this lab session, we will use a ready-to-use dataset AUTO in the book “Introduction to Statistical Learning, with Applications in R”. With the package ISLR2, we can use all the datasets in the book.\n\nFirst, we need to install ISLR2 into our R environment for future use.\n> install.packages(\"ISLR2\")\nTo use ISLR2 package and the datasets in our analyses, we need to call it in each R session with;\n> library(ISLR2)\nThat’s it! We now can use all datasets by calling them by their names. The package includes numerous datasets and you can explore them with R.\nAUTO dataset is ready to be used in the analyse. You can explore the dataset by using:\n> View(Auto)\n> head(Auto)\nThe head() function can also be used to view the first few rows of the data\nYou may want to save this dataset on a local computer, which is useful for your future analyses while doing some changes on it. To save a dataset as a csv file:\n> write.csv(DataFrameName, file=\"Path to save the DataFrame//File Name.csv\", row.names = FALSE)\nThe option row.names = FALSE deletes the row names when you are saving the dataset. In this case, it will remove basic incremental indexes such as 1,2,… from the data. A detailed explanation of write.csv and its options could be found by typing ?write.csv\n\n\n\n\n\n\nExample\n\n\n\nYou need to include the path where you would like to save the dataset on your computer. For example, if you work in a folder called Test in your desktop in a Windows machine. The code:\n> write.csv(Auto, \"C:Users//LSE//Desktop//Test//autodataset.csv\", row.names = FALSE )\n\n\nTo use the dataset in the future, you need to load it into a dataframe by importing the csv file.\nWe will load this dataset in a dataframe called Auto. Dataframe name is changable, however we would like to use words understandable and readable.\n> Auto <- read.csv(\"C://Users//LSE//Desktop//Test//autodataset.csv\", na.strings = \"?\")\nUsing the option na.strings tells R that any time it sees a particular character or set of characters (such as a question mark), it should be treated as a missing element of the data matrix.\nYou can check the dataset:\n> View(Auto)\n> head(Auto)\nDeal with the missing data by removing rows with missing observations:\n> Auto <- na.omit(Auto)\n> dim(Auto)\n[1] 392 9\nThe function dim() is to check the size of the data frame.\nProduce a numerical summary of each variable in the particular data frame:\n> summary(Auto)"
  },
  {
    "objectID": "weeks/week02/lab.html#step-4-practical-exercises-in-pairs",
    "href": "weeks/week02/lab.html#step-4-practical-exercises-in-pairs",
    "title": "💻 Week 02 - Lab Roadmap (90 min)",
    "section": "Step 4: Practical exercises (in pairs)",
    "text": "Step 4: Practical exercises (in pairs)\nSo far, we have learnt some basic commands in R. In this practical case, we will continues with the data set Auto studied in Step 3. Make sure that the missing values have been removed from the data.\nSix questions are listed below. You are required to try to answer these questions in pair using R commands. We will go over the solutions once everyone has finished these questions.\n🎯 Questions\n\nWhich of the predictors are quantitative, and which are qualitative?\nWhat is the range of each quantitative predictor? (hint: You can answer this using the range() function)\nWhat is the mean and standard deviation of each quantitative predictor?\nNow remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?\nUsing the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings.\nSuppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer."
  },
  {
    "objectID": "weeks/week02/lab_solutions.html",
    "href": "weeks/week02/lab_solutions.html",
    "title": "✔️ Week 02 - Lab Solutions",
    "section": "",
    "text": "Use the function View()to identify the type of a variable (quantitative or qualitative):\nView(Auto)\nVariables mpg, cylinders, horsepower, weight, acceleration, year are quantitative variable. Variables origin, name are qualitative variable\nUse the function range() to check the range of each quantitative predictor:\nrange(Auto$mpg)\n[1]  9.0 46.6\nTo refer to a variable, we must type the data set and the variable name joined with a $ symbol.\nUsing summary() to have an overall look at all variables and statistical features (like mean and standard deviation) are included in the outputs:\nsummary(Auto)\nor\nmean(Auto$mpg)\nsd(Auto$mpg)\nRemove the 10th through 85th observations from the original data frame and store it as another new data frame:\nAuto_tmp = Auto[-c(10:85), ]\nsummary(Auto_tmp)\nmean(Auto_tmp$mpg)\nsd(Auto_tmp$mpg)\nCreate a scatterplot matrix using the function pairs():\npairs( ~ mpg + displacement + horsepower + weight + \n        acceleration + year + origin + cylinders, \n        data = Auto)\nNotice the linear or non-linear trends in the scatterplots.Then create a histogram of the variable mpg:\nhist (Auto$mpg , col = 2, breaks = 15)\nUse the hist() function to produce some histograms with differing numbers of bins for a few of the quantitative variables. You may find the command par(mfrow = c(2, 2)) useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways.\nAfter observing the first row of the scatterplot matrix which indicates the relationship between gas mileage (mpg) and other variables, you will find evident linear or non-linear trends exist in the scatterplots with variables displacement, horsepower, weight, year and origin. Therefore, these varibles might be useful in predicting mpg.\n\nIf you want to achieve ststistical robust when exploring the relationship between variables, you need to culculate some statistics (like the correlation using the function cor()) and conduct statistical tests. This will be further illustrated in Week 03."
  },
  {
    "objectID": "weeks/week02/lecture.html",
    "href": "weeks/week02/lecture.html",
    "title": "👨‍🏫 Week 02 - Slides",
    "section": "",
    "text": "Either click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides. For a PDF copy, check Moodle."
  },
  {
    "objectID": "weeks/week02/lecture.html#coffee-break-10-min",
    "href": "weeks/week02/lecture.html#coffee-break-10-min",
    "title": "👨‍🏫 Week 02 - Slides",
    "section": "☕ Coffee Break (10 min)",
    "text": "☕ Coffee Break (10 min)\nUse this time to chat, stretch, drink some coffee or just relax for a bit by yourself."
  },
  {
    "objectID": "weeks/week02/lecture.html#part-ii---multiple-linear-regression-45-50-min",
    "href": "weeks/week02/lecture.html#part-ii---multiple-linear-regression-45-50-min",
    "title": "👨‍🏫 Week 02 - Slides",
    "section": "Part II - Multiple Linear Regression (45-50 min)",
    "text": "Part II - Multiple Linear Regression (45-50 min)\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides. For a PDF copy, check Moodle."
  },
  {
    "objectID": "weeks/week03/checklist.html",
    "href": "weeks/week03/checklist.html",
    "title": "✅ Week 03 - Checklist",
    "section": "",
    "text": "Your Checklist:\n\n📙 Read (James et al. 2021, chap. 3) to reinforce your theoretical knowledge of Linear Regression. The textbook is available online for free.\n🧑‍💻 If you already know linear regression from previous courses you have taken, why not take this knowledge to next level?\n\nTry to find a dataset online that contains a numerical variable you could predict by fitting a linear regression to it. I will be curious to see what you find. Share your findings on the #week03 channel in our Slack.\n\n🖥️ Before you come to the class, skim the W03 lab roadmap page to have an idea of what we are going to do.\n\nThis week, instead of just typing things in the terminal, we will use R Markdown. You can read about it here. This is also how you will be submitting solutions to formative and summative assignments in the future.\nI will post solutions to the practical exercises at the end of the week.\n\n💻 Assess yourself: did you understand all the exercises in the lab?\n\nIf you are new to linear regression and you are enrolled in the Monday sessions, it is likely that you will struggle a bit in the lab. During the week, reserve some time to read about Linear Regression and then practice the exercises again.\n\n📟 Struggling with something? Don’t know what a particular R command do? Share your questions on the #week03 channel in our Slack.\n\nI will also be posting follow up questions on Slack during the week.\n\n📝 Keep in mind that: after the lecture on Friday, 14 October 2022, we will post the first formative assignment on Moodle.\n\nYou will have until Thursday of the following week (20 October 2022) to submit your solutions.\nThis assignment is not marked, it doesn’t count towards your final grade, but you will receive feedback if you submit.\nThe assignment will have a similar format as the questions we explore in the lab.\n\n👨‍🏫 Attend the lecture. It will help you remember concepts more easily when revising later.\n\n\n\n\n\n\nReferences\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York NY: Springer. https://www.statlearning.com/."
  },
  {
    "objectID": "weeks/week03/formative.html",
    "href": "weeks/week03/formative.html",
    "title": "📝 Week 03 - Formative homework",
    "section": "",
    "text": "Use the Carseats data set in the ISLR2 package to answer the following questions:\n\nFit a multiple linear regression model to predict Sales using Price, Urban, and US. Show the summary output.\nProvide an interpretation of each coefficient in the model. Be careful—some of the variables in the model are qualitative!\nWrite the model in equation form, carefully handling the qualitative variables properly.\nFor which of the predictors can you reject the null hypothesis \\(H_0: \\beta_j = 0\\)?\nBased on your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of an association with the outcome. Justify your choices.\nHow well do the models in Questions 1 & 5 fit the data?\nUsing the model from question 5, obtain 95% confidence intervals for the coefficient(s).\nUse the * and : symbols to fit linear regression models with interaction effects. Could you find any model with interactions that fit better than the models you built in Questions 1 & 5? Justify your answer."
  },
  {
    "objectID": "weeks/week03/formative_solutions.html",
    "href": "weeks/week03/formative_solutions.html",
    "title": "📝 Week 03 - Formative homework",
    "section": "",
    "text": "Use the Carseats data set in the ISLR2 package to answer the following questions:\n\nFit a multiple linear regression model to predict Sales using Price, Urban, and US. Show the summary output.\n> library(ISLR2)\n> lm.fit <- lm(Sales ~ Price + Urban + US, data = Carseats)\n> summary(lm.fit)\nProvide an interpretation of each coefficient in the model. Be careful—some of the variables in the model are qualitative!\nβ1: Holding Urban and US fixed, the Sales decrease 54.459 units on average when the Price company charges for car seats at each site increases 1000 units.\nβ2: Holding Price and US fixed, the Sales decrease 0.021916 units on average when the store is in urban area.\nβ3: Holding Price and Urban fixed, the Sales increase 1.200573 units on average when the store is in US.\nWrite the model in equation form, carefully handling the qualitative variables properly.\nSales = β0 + β1 x Price + β2 + β3, when the store is in urban and in US\nSales = β0 + β1 x Price + β3, when the store is not in urban and but in US\nSales = β0 + β1 x Price + β2, when the store is in urban and but not in US\nSales = β0 + β1 x Price, when the store is not in urban and not in US\nFor which of the predictors can you reject the null hypothesis H0: βj = 0?\nFrom the p-values of t-test, we could reject the null hypothesis H0: β1 = 0 for the predictor Price and the null hypothesis H0: β3 = 0 for the predictor US.\nBased on your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of an association with the outcome. Justify your choices.\n> lm.fit1 <- lm(Sales ~ Price + US, data = Carseats)\n> summary(lm.fit1)\nHow well do the models in Questions 1 & 5 fit the data?\nNeither model fits the data well according to the small value of R2 and Adjusted R2. It means that the predictors included in both models can only interpret a small part of the change pattern of the response.\nUsing the model from question 5, obtain 95 % confidence intervals for the coefficient(s).\n> confint(lm.fit1)\n    2.5 %      97.5 %\n(Intercept) 11.79032020 14.27126531\nPrice       -0.06475984 -0.04419543\nUSYes        0.69151957  1.70776632\n95% confidence interval for β0 = [11.79032020, 14.27126531]; 95% confidence interval for β1 = [-0.06475984 -0.04419543]; 95% confidence interval for β2 = [0.69151957 1.70776632].\nUse the * and : symbols to fit linear regression models with interaction effects. Could you find any model with interactions that fit better than the models you built in Questions 1 & 5? Justify your answer.\n> lm.fit2 <- lm(Sales ~ Price * US, data = Carseats)\n> summary(lm.fit2)\n> lm.fit3 <- lm(Sales ~ Price * Urban, data = Carseats)\n> summary(lm.fit3)\n> lm.fit4 <- lm(Sales ~ Price * US + Urban, data = Carseats)\n> summary(lm.fit4)\n> lm.fit5 <- lm(Sales ~ Price * Urban + US, data = Carseats)\n> summary(lm.fit5)\n> lm.fit6 <- lm(Sales ~ Price + US + Urban + Price:US + Price:Urban, data = Carseats)\n> summary(lm.fit6)\n> lm.fit7 <- lm(Sales ~ Price + US + Urban + Price:US + Price:Urban + Urban:US, data = Carseats)\n> summary(lm.fit7)\nDifferent multiple linear regression models with interactions have been built. However, after comparision there is almost no difference between the values of the \\(R^2\\) and adjusted \\(R^2\\), which means the goodness of fit is not significantly improved. The reason is that there is no interaction effects between Price and US, and between Price and Urban, and between US and Urban which is supported by the significance of these coeficients’ t-tests."
  },
  {
    "objectID": "weeks/week03/lab.html",
    "href": "weeks/week03/lab.html",
    "title": "💻 Week 03 - Lab Roadmap (90 min)",
    "section": "",
    "text": "This week, we will fit simple and multiple linear regression models in R and learn to interpret the R output. We will apply this method to practical cases and deal with problems that commonly arise during this process."
  },
  {
    "objectID": "weeks/week03/lab.html#step-1-simple-linear-regression",
    "href": "weeks/week03/lab.html#step-1-simple-linear-regression",
    "title": "💻 Week 03 - Lab Roadmap (90 min)",
    "section": "Step 1: Simple linear regression",
    "text": "Step 1: Simple linear regression\nWe will follow the instructions below step by step together while answering whatever questions you might encounter along the way.\n\nInstall and load the ISLR2 package, which contains a large collection of data sets and functions.\ninstall.packages(\"ISLR2\").\nlibrary (ISLR2)\nThe function install.packages() is used to download packages that don’t come with R. This installation only needs to be done the first time you use a package. However, the library() function must be called within each R session to load packages.\nUse the Boston data set in the ISLR2 library. It records medv (median house value) for 506 census tracts in Boston. Have a look at the first few rows of the Boston data set:\nhead (Boston)\n    crim zn indus chas   nox    rm  age    dis rad tax ptratio lstat medv\n1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3  4.98 24.0\n2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8  9.14 21.6\n3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8  4.03 34.7\n4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7  2.94 33.4\n5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7  5.33 36.2\n6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7  5.21 28.7\nWe want to predict medv using the available predictors, such as rm (average number of rooms per house), age (average age of houses), and lstat (percentage of households with low socioeconomic status). To find out more about the data set, we can type ?Boston.\nFit a simple linear regression lm() model, with medv as the response and lstat as the predictor:\n> lm.fit <- lm(medv ~ lstat , data = Boston)\nThe basic syntax is lm(y ∼ x, data), where y is the response, x is the predictor, and data is the data set in which we keep these two variables.\nUse the tidy function to create a dataframe with columns for the estimate, standard error, f-statistic (estimate/standard error), p-values, and 95 percent confidence intervals:\ninstalling/loading broom:\ninstall.packages(\"broom\")\nlibrary(broom)\n> tidy(lm.fit, conf.int = TRUE)\n\n\n# A tibble: 2 × 7\nterm        estimate std.error statistic   p.value conf.low conf.high\n<chr>          <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)   34.6      0.563       61.4 3.74e-236    33.4     35.7  \n2 lstat         -0.950    0.0387     -24.5 5.08e- 88    -1.03    -0.874\nBecause lm.fit is a simple linear regression model, there are only two coefficients: \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). The goodness-of-fit of the model can be measured by the \\(R^2\\) in the output, which can be obtained (along with other model statistics) using the glance function.\n> glance(lm.fit)$r.squared\n[1] 0.5441463\nPlot medv and lstat along with the least squares regression line using the geom_point() and geom_abline() functions.::\nlibrary(tidyverse)\n\n> ggplot(data = Boston, aes(x = lstat, y = medv)) +\ngeom_point() + \ngeom_abline(intercept = lm.fit$coefficients[1], slope = lm.fit$coefficients[2])"
  },
  {
    "objectID": "weeks/week03/lab.html#step-2-multiple-linear-regression",
    "href": "weeks/week03/lab.html#step-2-multiple-linear-regression",
    "title": "💻 Week 03 - Lab Roadmap (90 min)",
    "section": "Step 2: Multiple linear regression",
    "text": "Step 2: Multiple linear regression\nWe will still use the Boston data set to fit multiple linear regression. The fitting process is similar to simple linear regression.\n\nFit a multiple linear regression lm() model, with medv as the response, lstat and age as the predictors:\n> lm.fit <- lm(medv ~ lstat + age , data = Boston)\n> tidy(lm.fit, conf.int = TRUE)\n\n# A tibble: 3 × 7\nterm        estimate std.error statistic   p.value conf.low conf.high\n<chr>          <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)  33.2       0.731      45.5  2.94e-180  31.8      34.7   \n2 lstat        -1.03      0.0482    -21.4  8.42e- 73  -1.13     -0.937 \n3 age           0.0345    0.0122      2.83 4.91e-  3   0.0105    0.0586\nThe syntax lm(y ~ x1 + x2 + x3) is used to fit a model with three predictors, x1, x2, and x3. The tidy() function now outputs the regression coefficients for all the predictors.\nFit a multiple linear regression lm() model, with medv as the response, all rest variables as the predictors:\n> lm.fit <- lm(medv ~ ., data = Boston)\n> tidy(lm.fit, conf.int = TRUE)\n\n# A tibble: 13 × 7\nterm         estimate std.error statistic  p.value conf.low conf.high\n<chr>           <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)  41.6       4.94        8.43  3.79e-16  31.9     51.3    \n2 crim         -0.121     0.0330     -3.68  2.61e- 4  -0.186   -0.0565 \n3 zn            0.0470    0.0139      3.38  7.72e- 4   0.0197   0.0742 \n4 indus         0.0135    0.0621      0.217 8.29e- 1  -0.109    0.136  \n5 chas          2.84      0.870       3.26  1.17e- 3   1.13     4.55   \n6 nox         -18.8       3.85       -4.87  1.50e- 6 -26.3    -11.2    \n7 rm            3.66      0.420       8.70  4.81e-17   2.83     4.48   \n8 age           0.00361   0.0133      0.271 7.87e- 1  -0.0226   0.0298 \n9 dis          -1.49      0.202      -7.39  6.17e-13  -1.89    -1.09   \n10 rad           0.289     0.0669      4.33  1.84e- 5   0.158    0.421  \n11 tax          -0.0127    0.00380    -3.34  9.12e- 4  -0.0202  -0.00521\n12 ptratio      -0.938     0.132      -7.09  4.63e-12  -1.20    -0.678  \n13 lstat        -0.552     0.0507    -10.9   6.39e-25  -0.652   -0.452  \n\nWe can access the individual components of a summary object by name (type ?glance to see what is available). Hence glance(lm.fit)$r.squared gives us the \\(R^2\\).\n\nSelect variables:\nIn these two multiple linear regression models, the t-tests and F-test results suggest that many of the predictors are significant for the response variable. However, some do not achieve statistical significance. Can you see which variables these are?\nWe call the process of determining which predictors are associated with the response as variable selection.\nIf the number of predictors is very small, we could perform the variable selection by trying out a lot of different models, each containing a different subset of the predictors. We can then select the best model out of all of the models we have considered.\nUsing the template below, try figuring out the model which produces the highest adjusted \\(R^2\\). The adjusted \\(R^2\\) has a similar interpretation to \\(R^2\\), only it is an advantage here as it penalises models that include insignificant parameters.\nlm.fit <- lm(medv ~ ., data = Boston)\nglance(lm.fit)$adj.r.squared\n[1] 0.7278399\nWe found that if you remove indus and age, the adjusted \\(R^2\\) becomes slightly larger compared to including all predictors.\nlm.fit <- lm(medv ~ ., data = Boston[,-c(3,7)])\nglance(lm.fit)$adj.r.squared\n[1] 0.7288734"
  },
  {
    "objectID": "weeks/week03/lab.html#step-3-some-potential-problems",
    "href": "weeks/week03/lab.html#step-3-some-potential-problems",
    "title": "💻 Week 03 - Lab Roadmap (90 min)",
    "section": "Step 3: Some potential problems",
    "text": "Step 3: Some potential problems\nMany problems may occur when we fit a linear regression model to a particular data set. These problems will lead to inaccurate estimation. In this step, we will identify and overcome potential problems such as outliers, collinearity and interaction effects.\nWe present a few of the many methods available, but those interested can explore more after class.\n\nHandle interaction terms:\nIn regression, an interaction effect exists when the effect of an independent variable on the response variable changes, depending on the values of one or more independent variables. When you believe there is an interaction effect, it is easy to include interaction terms in a linear model using the lm() function.\n> tidy(lm(medv ~ lstat * age , data = Boston))\n\n# A tibble: 4 × 5\nterm         estimate std.error statistic  p.value\n<chr>           <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept) 36.1        1.47      24.6    4.91e-88\n2 lstat       -1.39       0.167     -8.31   8.78e-16\n3 age         -0.000721   0.0199    -0.0363 9.71e- 1\n4 lstat:age    0.00416    0.00185    2.24   2.52e- 2\nThe syntax lstat:age tells R to include an interaction term between lstat and age. The syntax lstat*age simultaneously includes lstat, age, and the interaction term lstat×age as predictors; it is a shorthand for lstat+age+lstat:age.\nIdentify outliers through residual plots:\nAn outlier is a point for which \\(\\hat{y}_i\\) is far from the value predicted by the model. Outliers can arise for a variety of reasons, such as incorrect recording of observation during data collection. Outliers could be identified through residual plots:\n> par(mfrow = c(2, 2))\n> plot(lm.fit)\nThe plot function automatically produces four diagnostic plots when you pass the output from lm(). Plots on the left column are residual plots, indicating the relationship between residuals and fitted values.\nIn practice, it can be difficult to decide how large a residual needs to be before we consider the point to be an outlier. Instead of plotting the residuals, we can address this problem by plotting the studentized residuals. These are computed by dividing each residual ei by its estimated standard studentized residual error. Observations with studentized residuals greater than 3 in absolute value are possible outliers. Using the plot() function to plot the studentized residuals:\n> plot(predict(lm.fit), rstudent(lm.fit)\nHandle outliers:\nIf we believe an outlier is due to an error in data collection or recording, then one solution is to simply remove the observation. However, care should be taken, as an outlier may instead signal a deficiency with our model, such as a missing predictor.\nDetect multicollinearity using the correlation matrix:\nMulticollinearity refers to the situation in which two or more predictor variables are highly correlated to one another. It can be detected through the correlation matrix:\ncor(Boston)\nIgnoring the last row and the last column in the matrix, which indicate the relationship with response variable medv, an element of this matrix that is large in absolute value indicates a pair of highly correlated variables, and therefore a collinearity problem in the data.\nWe can detect multicollinearity quantitatively using vif() function in the `car’ package:\ninstall.packages(\"car\"))\nlibrary(car)\n> vif(lm.fit)\nInstead of inspecting the correlation matrix, a better way to assess multicollinearity is to compute the variance inflation factor (VIF). As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity.\n\n\n\n\n\n\n\nRead more about VIF\n\n\n\n\n\nCheck out our textbook (James et al. 2021, 99–103) for a description of the Variance Inflation Factor (VIF).\n\n\n\n\nHandle collinearity:\nWhen faced with the problem of multicollinearity, there are two simple solutions.\n-The first is to drop one of the problematic variables from the regression.\n-The second solution is to combine the collinear variables into a single predictor, where such combination makes theoretical sense."
  },
  {
    "objectID": "weeks/week03/lab.html#step-4-practical-exercises-in-pairs",
    "href": "weeks/week03/lab.html#step-4-practical-exercises-in-pairs",
    "title": "💻 Week 03 - Lab Roadmap (90 min)",
    "section": "Step 4: Practical exercises (in pairs)",
    "text": "Step 4: Practical exercises (in pairs)\nSo far, we have learnt to fit simple and multiple linear regression models in R. In this practical case, we will continue to use the data set Auto studied in the last lab. Make sure that the missing values have been removed from the data.\nEight questions are listed below. You are required to try to answer these questions in pairs using R commands. We will go over the solutions once everyone has finished these questions.\n🎯 Questions\n\nUse the lm() function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the tidy() function to print the results. Comment on the output. For example:\n\nIs there a relationship between the predictor and the response?\nHow strong is the relationship between the predictor and the response?\nIs the relationship between the predictor and the response positive or negative?\nWhat is the predicted mpg associated with a horsepower of 98? What are the associated 95 % confidence intervals?\n\nPlot the response and the predictor. Use the geom_abline() function to display the least squares regression line.\nUse the plot() function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.\nProduce a scatterplot matrix that includes all the variables in the data set.\nCompute the matrix of correlations between the variables using the function cor(). You will need to exclude the name and origin variable, which are qualitative.\nUse the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the tidy() function to print the results. Comment on the output. For instance:\n\nIs there a relationship between the predictors and the response?\nWhich predictors appear to have a statistically significant relationship to the response?\nWhat does the coefficient for the year variable suggest?\n\nUse the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers?\nUse the * and : symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?\n\n\n\n\n\n\n\nTip\n\n\n\nIf you could not finish all eight questions during the lab, take that as a home exercise.\nUse the #week03 channel on Slack if you have any questions."
  },
  {
    "objectID": "weeks/week03/lab.html#solutions-to-exercises",
    "href": "weeks/week03/lab.html#solutions-to-exercises",
    "title": "💻 Week 03 - Lab Roadmap (90 min)",
    "section": "🔑 Solutions to exercises",
    "text": "🔑 Solutions to exercises\n\nlibrary(tidyverse)\nlibrary(ISLR2)\n\n\nQ1\nUse the lm() function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the tidy() function to print the results. Comment on the output.\nFor example:\n\nIs there a relationship between the predictor and the response?\nHow strong is the relationship between the predictor and the response?\nIs the relationship between the predictor and the response positive or negative?\nWhat is the predicted mpg associated with a horsepower of 98? What are the associated 95 % confidence intervals?\n\n\nAuto <- na.omit(Auto)\nlm.fit <- lm(mpg ~ horsepower, data = Auto)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = mpg ~ horsepower, data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5710  -3.2592  -0.3435   2.7630  16.9240 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 39.935861   0.717499   55.66   <2e-16 ***\nhorsepower  -0.157845   0.006446  -24.49   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.906 on 390 degrees of freedom\nMultiple R-squared:  0.6059,    Adjusted R-squared:  0.6049 \nF-statistic: 599.7 on 1 and 390 DF,  p-value: < 2.2e-16\n\n\nRegarding to the p-values of t-test and F-test, there is a strong relationship between the predictor horsepower and the response mpg. From the sign of coefficients, the relationship between the predictor and the response is negative. Using the function predict() to predict the value of response and the confidence interval, we get:\n\npredict(lm.fit, data.frame(horsepower = 98), interval = \"confidence\")\n\n       fit      lwr      upr\n1 24.46708 23.97308 24.96108\n\n\nTherefore, the predicted mpg associated with a horsepower of 98 is 24.47, and the associated 95 % confidence interval is [23.97308, 24.96108].\n\n\nQ2\nPot the response and the predictor. Use the geom_abline() function to display the least squares regression line.\nIn base R (without using any tidverse or any other package):\n\nplot(Auto$horsepower, Auto$mpg, xlim = c(0, 250))\nabline (lm.fit, lwd = 3, col = \"red\")\n\n\n\n\nUsing ggplot (from tidyverse):\n\nggplot(data = Auto, aes(x = horsepower, y = mpg)) +\n  geom_point(alpha=0.6, size=2.5) +\n  geom_abline(intercept = lm.fit$coefficients[1], \n              slope = lm.fit$coefficients[2],\n              color=\"red\", size=1.2) +\n  theme_bw()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\nQ3\nUse the plot() function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.*\n\npar(mfrow = c(2, 2))\nplot(lm.fit)\n\n\n\n\nBy observing four diagnostic plots, we could find non-linear pattern in residual plots. The quadratic trend of the residuals could be a problem. Then we plot studentized residuals to identify outliers:\n\nplot(predict(lm.fit), rstudent(lm.fit))\n\n\n\n\nThere are possible outliers as seen in the plot of studentized residuals because there are data with a value greater than 3.\n\n\nQ4\nProduce a scatterplot matrix that includes all the variables in the data set.\n\npairs(Auto)\n\n\n\n\n\n\nQ5\nCompute the matrix of correlations between the variables using the function cor(). You will need to exclude the name and origin variable, which is qualitative.\n\ncor(subset(Auto, select = -name))\n\n                    mpg  cylinders displacement horsepower     weight\nmpg           1.0000000 -0.7776175   -0.8051269 -0.7784268 -0.8322442\ncylinders    -0.7776175  1.0000000    0.9508233  0.8429834  0.8975273\ndisplacement -0.8051269  0.9508233    1.0000000  0.8972570  0.9329944\nhorsepower   -0.7784268  0.8429834    0.8972570  1.0000000  0.8645377\nweight       -0.8322442  0.8975273    0.9329944  0.8645377  1.0000000\nacceleration  0.4233285 -0.5046834   -0.5438005 -0.6891955 -0.4168392\nyear          0.5805410 -0.3456474   -0.3698552 -0.4163615 -0.3091199\norigin        0.5652088 -0.5689316   -0.6145351 -0.4551715 -0.5850054\n             acceleration       year     origin\nmpg             0.4233285  0.5805410  0.5652088\ncylinders      -0.5046834 -0.3456474 -0.5689316\ndisplacement   -0.5438005 -0.3698552 -0.6145351\nhorsepower     -0.6891955 -0.4163615 -0.4551715\nweight         -0.4168392 -0.3091199 -0.5850054\nacceleration    1.0000000  0.2903161  0.2127458\nyear            0.2903161  1.0000000  0.1815277\norigin          0.2127458  0.1815277  1.0000000\n\n\nA nicer way to plot correlations is through the package ggcorrplot:\n\nlibrary(ggcorrplot)\n\nggcorrplot(cor(Auto %>% select(-c(name))))\n\n\n\n\n\n\nQ6\nUse the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the tidy() function to print the results. Comment on the output.\nFor instance:\n\nIs there a relationship between the predictors and the response?\nWhich predictors appear to have a statistically significant relationship to the response?\nWhat does the coefficient for the year variable suggest?\n\n\nlm.fit1 <- lm(mpg ~ . -name, data = Auto)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = mpg ~ . - name, data = Auto)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5903 -2.1565 -0.1169  1.8690 13.0604 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -17.218435   4.644294  -3.707  0.00024 ***\ncylinders     -0.493376   0.323282  -1.526  0.12780    \ndisplacement   0.019896   0.007515   2.647  0.00844 ** \nhorsepower    -0.016951   0.013787  -1.230  0.21963    \nweight        -0.006474   0.000652  -9.929  < 2e-16 ***\nacceleration   0.080576   0.098845   0.815  0.41548    \nyear           0.750773   0.050973  14.729  < 2e-16 ***\norigin         1.426141   0.278136   5.127 4.67e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.328 on 384 degrees of freedom\nMultiple R-squared:  0.8215,    Adjusted R-squared:  0.8182 \nF-statistic: 252.4 on 7 and 384 DF,  p-value: < 2.2e-16\n\n\n\nYes, there is a relationship between the predictors and the response by testing the null hypothesis of whether all the regression coefficients are zero. The F-statistic is far from 1 (with a small p-value), indicating evidence against the null hypothesis.\nObserving the p-values associated with each predictor’s t-statistic, we see that displacement, weight, year, and origin have a statistically significant relationship, while cylinders, horsepower and acceleration do not.\nThe regression coefficient for year is 0.75. This suggests that, considering all other predictors fixed, mpg increases by additional 0.75 unit. In other words, cars become more fuel efficient every year by almost 1 mpg/year.\n\n\n\nQ7\nUse the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers?\n\npar(mfrow = c(2, 2))\nplot(lm.fit1)\n\n\n\n\nFrom the leverage plot, we see that point 14 appears to have a high leverage, although not a high magnitude residual. Besides, the quadratic trend of the residuals could be a problem. Maybe linear regression is not the best fit for this prediction.\nWe plot studentized residuals to identify outliers:\n\nplot(predict(lm.fit1), rstudent(lm.fit1))\n\n\n\n\nThere are possible outliers as seen in the plot of studentized residuals because there are data with a value greater than 3.\n\n\nQ8\nUse the * and : symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?\n\nlm.fit2 <-  lm(mpg ~ cylinders * displacement + displacement * weight, data = Auto)\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = mpg ~ cylinders * displacement + displacement * \n    weight, data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.2934  -2.5184  -0.3476   1.8399  17.7723 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             5.262e+01  2.237e+00  23.519  < 2e-16 ***\ncylinders               7.606e-01  7.669e-01   0.992    0.322    \ndisplacement           -7.351e-02  1.669e-02  -4.403 1.38e-05 ***\nweight                 -9.888e-03  1.329e-03  -7.438 6.69e-13 ***\ncylinders:displacement -2.986e-03  3.426e-03  -0.872    0.384    \ndisplacement:weight     2.128e-05  5.002e-06   4.254 2.64e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.103 on 386 degrees of freedom\nMultiple R-squared:  0.7272,    Adjusted R-squared:  0.7237 \nF-statistic: 205.8 on 5 and 386 DF,  p-value: < 2.2e-16\n\n\nInteraction between displacement and weight is statistically signifcant, while the interaction between cylinders and displacement is not."
  },
  {
    "objectID": "weeks/week03/lab_solutions.html",
    "href": "weeks/week03/lab_solutions.html",
    "title": "✔️ Week 03 - Lab Solutions",
    "section": "",
    "text": "Use the lm() function to perform a simple linear regression，and use the summary() function to print the results:\n> library(ISLR2)\n> Auto <- na.omit(Auto)\n> lm.fit <- lm(mpg ~ horsepower, data = Auto)\n> summary(lm.fit)\n\nCall:\nlm(formula = mpg ~ horsepower, data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5710  -3.2592  -0.3435   2.7630  16.9240 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 39.935861   0.717499   55.66   <2e-16 ***\nhorsepower  -0.157845   0.006446  -24.49   <2e-16 ***\n---\n Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 4.906 on 390 degrees of freedom\nMultiple R-squared:  0.6059,    Adjusted R-squared:  0.6049 \nF-statistic: 599.7 on 1 and 390 DF,  p-value: < 2.2e-16\nRegarding to the p-values of t-test and F-test, there is a strong relationship between the predictor horsepower and the reponse mpg. From the sign of coefficients, the relationship between the predicator and the response is negative. Using the function predict() to predict the value of reponse and the confidence interval, we get:\n> predict(lm.fit, data.frame(horsepower = 98), interval = \"confidence\")\n       fit      lwr      upr\n1 24.46708 23.97308 24.96108\nTherefore, the predicted mpg associated with a horsepower of 98 is 24.47, and the associated 95 % confidence interval is [23.97308, 24.96108].\nUse the function plot() and abline():\n> attach(Auto)\n> plot(mpg, horsepower, ylim = c(0, 250))\n> abline (lm.fit, lwd = 3, col = \"red\")\n> dev.off()\nUsing plot() function to produce diagnostic plots:\n> par(mfrow = c(2, 2))\n> plot (lm.fit)\n> dev.off()\nBy observing four diagnostic plots, we could find non-linear patttern in residual plots. The quadratic trend of the residuals could be a problem. Then we plot studentized residuals to identify outliers:\n> plot(predict(lm.fit), rstudent(lm.fit))\n> dev.off()\nThere are possible outliers as seen in the plot of studentized residuals because there are data with a value greater than 3.\nUse the pairs() function to produce a scatterplot matrix:\n> pairs(Auto)\n> dev.off()\nUse the cor() function to compute the matrix of correlations between the variables while excluding the name variable:\n> cor(subset(Auto, select = -name))\nUse the lm() function to perform a multiple linear regression，and use the summary() function to print the results:\n> lm.fit1 <- lm(mpg ~ . -name, data = Auto)\n> summary (lm.fit1)\n\nCall:\nlm(formula = mpg ~ . - name, data = Auto)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5903 -2.1565 -0.1169  1.8690 13.0604 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -17.218435   4.644294  -3.707  0.00024 ***\ncylinders     -0.493376   0.323282  -1.526  0.12780    \ndisplacement   0.019896   0.007515   2.647  0.00844 ** \nhorsepower    -0.016951   0.013787  -1.230  0.21963    \nweight        -0.006474   0.000652  -9.929  < 2e-16 ***\nacceleration   0.080576   0.098845   0.815  0.41548    \nyear           0.750773   0.050973  14.729  < 2e-16 ***\norigin         1.426141   0.278136   5.127 4.67e-07 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 3.328 on 384 degrees of freedom\nMultiple R-squared:  0.8215,    Adjusted R-squared:  0.8182 \nF-statistic: 252.4 on 7 and 384 DF,  p-value: < 2.2e-16\n\nYes, there is a relationship between the predictors and the response by testing the null hypothesis of whether all the regression coefficients are zero. The F-statistic is far from 1 (with a small p-value), indicating evidence against the null hypothesis.\nObverving the p-values associated with each predictor’s t-statistic, we see that displacement, weight, year, and origin have a statistically significant relationship, while cylinders, horsepower and acceleration do not.\nThe regression coefficient for year is 0.75. This suggests that, considering all other predictors fixed, mpg increases by additional 0.75 unit. In other words, cars become more fuel efficient every year by almost 1 mpg/year.\n\nUse the plot() function to produce diagnostic plots:\n> par(mfrow = c(2, 2))\n> plot (lm.fit1)\n> dev.off()\nFrom the leverage plot, we see that point 14 appears to have a high leverage, although not a high magnitude residual. Besides, the quadratic trend of the residuals could be a problem. Maybe linear regression is not the best fit for this prediction.\nWe plot studentized residuals to identify outliers:\n> plot(predict(lm.fit1), rstudent(lm.fit1))\n> dev.off()\nThere are possible outliers as seen in the plot of studentized residuals because there are data with a value greater than 3.\nUse the * and : symbols to fit linear regression models with interaction effects:\n> lm.fit2 <-  lm(mpg ~ cylinders * displacement + displacement * weight, data = Auto)\n> summary(lm.fit2)\nInteraction between displacement and weight is statistically signifcant, while the interaction between cylinders and displacement is not."
  },
  {
    "objectID": "weeks/week03/lecture.html",
    "href": "weeks/week03/lecture.html",
    "title": "👨‍🏫 Week 03 - Slides",
    "section": "",
    "text": "Either click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides. For a PDF copy, check Moodle."
  },
  {
    "objectID": "weeks/week03/lecture.html#coffee-break-10-min",
    "href": "weeks/week03/lecture.html#coffee-break-10-min",
    "title": "👨‍🏫 Week 03 - Slides",
    "section": "☕ Coffee Break (10 min)",
    "text": "☕ Coffee Break (10 min)\nUse this time to chat, stretch, drink some coffee or just relax for a bit by yourself."
  },
  {
    "objectID": "weeks/week03/lecture.html#part-ii---classifiers-naive-bayes-45-50-min",
    "href": "weeks/week03/lecture.html#part-ii---classifiers-naive-bayes-45-50-min",
    "title": "👨‍🏫 Week 03 - Slides",
    "section": "Part II - Classifiers (Naive Bayes) (45-50 min)",
    "text": "Part II - Classifiers (Naive Bayes) (45-50 min)\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides. For a PDF copy, check Moodle."
  },
  {
    "objectID": "weeks/week04/checklist.html",
    "href": "weeks/week04/checklist.html",
    "title": "✅ Week 04 - Checklist",
    "section": "",
    "text": "Here is a suggestion of how to program your week in relation to this course:\nExtra:"
  },
  {
    "objectID": "weeks/week04/checklist.html#if-your-lab-is-on-monday",
    "href": "weeks/week04/checklist.html#if-your-lab-is-on-monday",
    "title": "✅ Week 04 - Checklist",
    "section": "If your lab is on Monday:",
    "text": "If your lab is on Monday:\n\n📥 Download: Before or once you arrive at the classroom, download the DS202_2022MT_w04_lab_rmark.Rmd file that contains the lab roadmap or browse the webpage version here.\n🧑‍💻Participate: Actively engage with the material in the lab. Ask your class teacher for help if anything is unclear. Work with others whenever possible and take notes of theoretical concepts or practical coding skill you might want to revisit later in the week.\n\nIt is ok if you feel a bit lost in the lab. After all, you haven’t had much time to read the material to reinforce the concepts in your mind. That is why participation and note-taking is so important.\n\n📙 Read: Find some time before Thursday to read (James et al. 2021, chap. 4) and reinforce your theoretical knowledge of Classifiers (Logistic Regression & other methods, including Naïve Bayes); the textbook is available online for free.\n\nAs you go through the text, try to connect what you read to the things you heard about in the lecture or the examples you explored in the lab.\n\n✍️ Solve: Also before Thursday, take a look at the problem sets of the first formative assessment. Either try to complete it before Thursday or at least have a look to see what it contains. Take note of whatever questions you might have about R or linear regression.\n\nThe Formative Problem Set 01 is available on Moodle and you can submit your solutions until Tuesday 25 October 2022, 23:50 UK time.\n\n✋Teacher Support: Stuart will host a drop-in session on Thursday 20 October 2022 2pm-4pm (provisionally at the FAW 9.04 room). Bring your notes and questions or just simply use this shared space to work on your problem set.\n🧑‍🏫 Attend the lecture: This week, the lecture will look more like a workshop. We won’t explore new algorithms but we will work on regression/classification metrics and explore the concepts of train/test splits and cross-validation. You will need to use those in your first summative problem set."
  },
  {
    "objectID": "weeks/week04/checklist.html#if-your-lab-is-on-friday",
    "href": "weeks/week04/checklist.html#if-your-lab-is-on-friday",
    "title": "✅ Week 04 - Checklist",
    "section": "If your lab is on Friday:",
    "text": "If your lab is on Friday:\n\n📙 Read: Find some time before Thursday to read (James et al. 2021, chap. 4) and reinforce your theoretical knowledge of Classifiers (Logistic Regression & other methods, including Naïve Bayes); the textbook is available online for free. As you go through the text, try to connect what you read to the things you heard about in the lecture.\n\nYou can ask questions on Slack (#week04) or take them with you to the drop-in session on Thursday.\n\n✍️ Solve: Also before Thursday, take a look at the problem sets of the first formative assessment. Either try to complete it before Thursday or at least have a look to see what it contains. Take note of whatever questions you might have about R or linear regression.\n\nThe Formative Problem Set 01 is available on Moodle and you can submit your solutions until Tuesday 25 October 2022, 23:50 UK time.\n\n✋Teacher Support: Stuart will host a drop-in session on Thursday 20 October 2022 2pm-4pm (provisionally at the FAW 9.04 room). Bring your notes and questions or just simply use this shared space to work on your problem set.\n📥Download: Before or once you arrive at the classroom, download the DS202_2022MT_w04_lab_rmark.Rmd file that contains the lab roadmap or browse the webpage version here.\n🧑‍💻Participate: Actively engage with the material in the lab. Ask your class teacher for help if anything is unclear. Keep your notes and the textbook handy so you can consult them during the session.\n🧑‍🏫Attend the lecture: This week, the lecture will look more like a workshop. We won’t explore new algorithms but we will work on regression/classification metrics and explore the concepts of train/test splits and cross-validation. You will need to use those in your first summative problem set."
  },
  {
    "objectID": "weeks/week04/lab.html",
    "href": "weeks/week04/lab.html",
    "title": "💻 Week 04 - Lab Roadmap (90 min)",
    "section": "",
    "text": "This week, we will build diverse classification models to deal with situation when the response variable is qualitative in R. We predict these qualitative variables through some widely-used classifiers including Logistic Regression (one of many Generalized Linear Models) and Naïve Bayes in this lab session. We will also apply these classification models into practical practices and compare their performance on different data sets.\nWe will follow the instructions below step by step together while answering whatever questions you might encounter along the way.\nR packages you will need:"
  },
  {
    "objectID": "weeks/week04/lab.html#step-1-explore-the-dataset",
    "href": "weeks/week04/lab.html#step-1-explore-the-dataset",
    "title": "💻 Week 04 - Lab Roadmap (90 min)",
    "section": "Step 1: Explore the dataset",
    "text": "Step 1: Explore the dataset\n\nStep 1.1 Load the Data\nLoad the ISLR2 package, which contains a large collection of data sets and functions. We will begin by examining some numerical and graphical summaries of the Smarket data, which is part of the ISLR2 library.\n\nlibrary(\"ISLR2\")\nhead(Smarket)\n\n  Year   Lag1   Lag2   Lag3   Lag4   Lag5 Volume  Today Direction\n1 2001  0.381 -0.192 -2.624 -1.055  5.010 1.1913  0.959        Up\n2 2001  0.959  0.381 -0.192 -2.624 -1.055 1.2965  1.032        Up\n3 2001  1.032  0.959  0.381 -0.192 -2.624 1.4112 -0.623      Down\n4 2001 -0.623  1.032  0.959  0.381 -0.192 1.2760  0.614        Up\n5 2001  0.614 -0.623  1.032  0.959  0.381 1.2057  0.213        Up\n6 2001  0.213  0.614 -0.623  1.032  0.959 1.3491  1.392        Up\n\n\nThis data set consists of percentage returns for the S&P 500 stock index over 1250 days, from the beginning of 2001 until the end of 2005. We use the command names() to obtain the variable names of this data set:\n\nnames(Smarket)\n\n[1] \"Year\"      \"Lag1\"      \"Lag2\"      \"Lag3\"      \"Lag4\"      \"Lag5\"     \n[7] \"Volume\"    \"Today\"     \"Direction\"\n\n\nHow many rows and columns do we have in this dataset?\n\ndim(Smarket)\n\n[1] 1250    9\n\n\nLet’s add another column day to index the number of days in this dataset:\n\nSmarket$day <- 1:nrow(Smarket)\nhead(Smarket)\n\n  Year   Lag1   Lag2   Lag3   Lag4   Lag5 Volume  Today Direction day\n1 2001  0.381 -0.192 -2.624 -1.055  5.010 1.1913  0.959        Up   1\n2 2001  0.959  0.381 -0.192 -2.624 -1.055 1.2965  1.032        Up   2\n3 2001  1.032  0.959  0.381 -0.192 -2.624 1.4112 -0.623      Down   3\n4 2001 -0.623  1.032  0.959  0.381 -0.192 1.2760  0.614        Up   4\n5 2001  0.614 -0.623  1.032  0.959  0.381 1.2057  0.213        Up   5\n6 2001  0.213  0.614 -0.623  1.032  0.959 1.3491  1.392        Up   6\n\n\nFor each date, we have recorded the percentage returns for each of the five previous trading days, Lag1 through Lag5. We have also recorded Volume (the number of shares traded on the previous day, in billions), Today (the percentage return on the date in question) and Direction (whether the market was Up or Down on this date). Our goal is to predict Direction (a qualitative response) using the other features.\nLet’s look at a generic summary of this data and how each pair of variables are related:\n\nsummary(Smarket)\n\n      Year           Lag1                Lag2                Lag3          \n Min.   :2001   Min.   :-4.922000   Min.   :-4.922000   Min.   :-4.922000  \n 1st Qu.:2002   1st Qu.:-0.639500   1st Qu.:-0.639500   1st Qu.:-0.640000  \n Median :2003   Median : 0.039000   Median : 0.039000   Median : 0.038500  \n Mean   :2003   Mean   : 0.003834   Mean   : 0.003919   Mean   : 0.001716  \n 3rd Qu.:2004   3rd Qu.: 0.596750   3rd Qu.: 0.596750   3rd Qu.: 0.596750  \n Max.   :2005   Max.   : 5.733000   Max.   : 5.733000   Max.   : 5.733000  \n      Lag4                Lag5              Volume           Today          \n Min.   :-4.922000   Min.   :-4.92200   Min.   :0.3561   Min.   :-4.922000  \n 1st Qu.:-0.640000   1st Qu.:-0.64000   1st Qu.:1.2574   1st Qu.:-0.639500  \n Median : 0.038500   Median : 0.03850   Median :1.4229   Median : 0.038500  \n Mean   : 0.001636   Mean   : 0.00561   Mean   :1.4783   Mean   : 0.003138  \n 3rd Qu.: 0.596750   3rd Qu.: 0.59700   3rd Qu.:1.6417   3rd Qu.: 0.596750  \n Max.   : 5.733000   Max.   : 5.73300   Max.   :3.1525   Max.   : 5.733000  \n Direction       day        \n Down:602   Min.   :   1.0  \n Up  :648   1st Qu.: 313.2  \n            Median : 625.5  \n            Mean   : 625.5  \n            3rd Qu.: 937.8  \n            Max.   :1250.0  \n\n\n\npairs(Smarket)\n\n\n\n\n\n\nStep 1.2 Initial exploratory data analysis\nProduce a matrix that contains all of the pairwise correlations among the predictors in a data set:\n\ncor(Smarket[, -9])\n\n             Year         Lag1         Lag2         Lag3         Lag4\nYear   1.00000000  0.029699649  0.030596422  0.033194581  0.035688718\nLag1   0.02969965  1.000000000 -0.026294328 -0.010803402 -0.002985911\nLag2   0.03059642 -0.026294328  1.000000000 -0.025896670 -0.010853533\nLag3   0.03319458 -0.010803402 -0.025896670  1.000000000 -0.024051036\nLag4   0.03568872 -0.002985911 -0.010853533 -0.024051036  1.000000000\nLag5   0.02978799 -0.005674606 -0.003557949 -0.018808338 -0.027083641\nVolume 0.53900647  0.040909908 -0.043383215 -0.041823686 -0.048414246\nToday  0.03009523 -0.026155045 -0.010250033 -0.002447647 -0.006899527\nday    0.97977289  0.035414677  0.036022487  0.038988767  0.041437137\n               Lag5      Volume        Today        day\nYear    0.029787995  0.53900647  0.030095229 0.97977289\nLag1   -0.005674606  0.04090991 -0.026155045 0.03541468\nLag2   -0.003557949 -0.04338321 -0.010250033 0.03602249\nLag3   -0.018808338 -0.04182369 -0.002447647 0.03898877\nLag4   -0.027083641 -0.04841425 -0.006899527 0.04143714\nLag5    1.000000000 -0.02200231 -0.034860083 0.03502515\nVolume -0.022002315  1.00000000  0.014591823 0.54634793\nToday  -0.034860083  0.01459182  1.000000000 0.03527333\nday     0.035025152  0.54634793  0.035273325 1.00000000\n\n\nThe function cor() can only take quantitative variables. Because the Direction variable is qualitative, therefore we exclude it when calculating the correlation matrix.\nAs one would expect, the correlations between the lag variables and today’s returns are close to zero. In other words, there appears to be little correlation between today’s returns and previous days’ returns. The only substantial correlation is between Year and Volume. We could explore how Volume changed chronologically.\n\nlibrary(tidyverse)\n\nggplot(data = Smarket, aes(x = day, y = Volume)) +\n    geom_col(fill=\"#665797\") +\n\n    ggtitle(\"Volume of shares traded over the period of this dataset\") +\n\n    xlab(\"Day\") + ylab(\"Volume (in billion US dollars)\") +\n\n    theme_minimal() +\n    theme(aspect.ratio = 2/5)\n\n\n\n\nBy plotting the data, which is ordered chronologically, we see that Volume is increasing over time. In other words, the average number of shares traded daily increased from 2001 to 2005."
  },
  {
    "objectID": "weeks/week04/lab.html#step-2-logistic-regression",
    "href": "weeks/week04/lab.html#step-2-logistic-regression",
    "title": "💻 Week 04 - Lab Roadmap (90 min)",
    "section": "Step 2: Logistic Regression",
    "text": "Step 2: Logistic Regression\nWe will still use the Smarket data set to fit a Logistic Regression Model to predict Direction.\n\nStep 2.1 Separate some data just for training\nBuild a training and a testing dataset. In practice we will be interested in our model’s performance not on the data that we used to fit the model, but rather on days in the future for which the market’s movements are unknown. Therefore, we will first create a training data set corresponding to the observations from 2001 through 2004. We will then create a testing data set of observations from 2005.:\n\ntrain <- (Smarket$Year < 2005)\nSmarket.before.2005 <- Smarket [ train , ]\n\nSmarket.2005 <- Smarket[ Smarket$Year==2005, ]\nDirection.2005 <- Smarket$Direction [ Smarket$Year==2005 ]\n\nSmarket$Year < 2005 returns True for the values satisfying <2005 (smaller than 2005) condition in Year column in Smarket dataset. The same logic can be applied to >, <= , >=, ==(equal) or != (not equal).\nTo access corresponding rows, we can create a vector (train) and put it in open brackets to make it more readable and reusable, or we can explicitly specify in the open brackets.\nUsing Smarket[ Smarket$Year==2005,] gives all the rows satisfying this condition with all columns. The same logic can be applied to < , >, <= , >= or != (not equal).\nSmarket$Direction [ Smarket$Year==2005 ] tells the direction values corresponding to the rows equal to 2005 in the year column are requested.\nThus, we can construct a training data set named Smarket.before.2005, and a testing data set named Smarket.2005.\nHow many observations we have in the training set (< 2005)?\n\nnrow(Smarket.before.2005)\n\n[1] 998\n\n\nWhat about the test set (2005)?\n\nnrow(Smarket.2005)\n\n[1] 252\n\n\n\n\nStep 2.2 Fit a logistic regression model\nFit a Logistic Regression Model in order to predict Direction using Lag1 through Lag5 and Volume based on training data set:\n\nglm.fits <- \n    glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, \n        data = Smarket.before.2005, family = binomial)\n\nThe generalized linear model syntax of the glm() function is similar to that of lm(), except that we must pass in the argument family = binomial in order to tell R to run a logistic regression rather than some other type of generalized linear model.\nWe now fit a logistic regression model using only the subset of the observations that correspond to dates before 2005, using the subset argument.\n\n\nStep 2.3 Inspect the model\nHave a look at p-values of this Logistic Regression Model:\n\nsummary(glm.fits)\n\n\nCall:\nglm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + \n    Volume, family = binomial, data = Smarket.before.2005)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.302  -1.190   1.079   1.160   1.350  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)\n(Intercept)  0.191213   0.333690   0.573    0.567\nLag1        -0.054178   0.051785  -1.046    0.295\nLag2        -0.045805   0.051797  -0.884    0.377\nLag3         0.007200   0.051644   0.139    0.889\nLag4         0.006441   0.051706   0.125    0.901\nLag5        -0.004223   0.051138  -0.083    0.934\nVolume      -0.116257   0.239618  -0.485    0.628\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1383.3  on 997  degrees of freedom\nResidual deviance: 1381.1  on 991  degrees of freedom\nAIC: 1395.1\n\nNumber of Fisher Scoring iterations: 3\n\n\nOr, alternatively, you can gather the same information using the broom package:\n\nlibrary(broom)\n\ntidy(glm.fits)\n\n# A tibble: 7 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)  0.191      0.334     0.573    0.567\n2 Lag1        -0.0542     0.0518   -1.05     0.295\n3 Lag2        -0.0458     0.0518   -0.884    0.377\n4 Lag3         0.00720    0.0516    0.139    0.889\n5 Lag4         0.00644    0.0517    0.125    0.901\n6 Lag5        -0.00422    0.0511   -0.0826   0.934\n7 Volume      -0.116      0.240    -0.485    0.628\n\n\nThe smallest p-value here is associated with Lag1. The negative coefficient for this predictor suggests that if the market had a positive return yesterday, then it is less likely to go up today. However, at a value of 0.295, the p-value is still relatively large, and so there is no clear evidence of a real association between Lag1 and Direction. So do other predictors.\n\n\nStep 2.4 Make predictions about the future (2005)\nObtain predicted probabilities of the stock market going up for each of the days in our testing data set, that is, for the days in 2005:\n\nglm.probs <- predict(glm.fits, Smarket.2005, type = \"response\") \nglm.probs[1:10]   \n\n      999      1000      1001      1002      1003      1004      1005      1006 \n0.5282195 0.5156688 0.5226521 0.5138543 0.4983345 0.5010912 0.5027703 0.5095680 \n     1007      1008 \n0.5040112 0.5106408 \n\n\nThe predict() function can be used to predict the probability that the market will go up, given values of the predictors. The type = \"response\" option tells R to output probabilities of the form P(Y = 1|X), as opposed to other information such as the logit. If no data set is supplied to the predict() function, then the probabilities are computed for the training data set that was used to fit the logistic regression model. Here we have printed only the first ten probabilities.\n\ncontrasts(Smarket$Direction)\n\n     Up\nDown  0\nUp    1\n\n\nWe know that these values correspond to the probability of the market going up, rather than down, because the contrasts() function indicates that R has created a dummy variable with a 1 for Up.\nIn order to make a prediction as to whether the market will go up or down on a particular day, we must convert these predicted probabilities into class labels, Up or Down. The following commands create a vector of class predictions based on whether the predicted probability of a market increase is greater than or less than 0.5.\n\nglm.pred <- rep(\"Down\", 252)\nglm.pred[glm.probs > .5] <- \"Up\"\n\nThe first command creates a vector of 252 Down elements. The second line transforms to Up all of the elements for which the predicted probability of a market increase exceeds 0.5.\n\n\nStep 2.5 Create a confusion matrix\nConstruct confusion matrix in order to determine how many observations in testing data set were correctly or incorrectly classified.\n\ntable(glm.pred, Direction.2005)    \n\n        Direction.2005\nglm.pred Down Up\n    Down   77 97\n    Up     34 44\n\n\nGiven the predictions, the table() function can be used to produce a confusion matrix in order to determine how many observations were correctly or incorrectly classified.\n\n\nStep 2.6 What is the error in the test set?\nCalculate the test set error rate:\n\nmean(glm.pred == Direction.2005)\n\n[1] 0.4801587\n\n\n\nmean(glm.pred != Direction.2005)\n\n[1] 0.5198413\n\n\nThe != notation means not equal to, and so the last command computes the test set error rate. The results are rather disappointing: the test error rate is 52%, which is worse than random guessing! Of course this result is not all that surprising, given that one would not generally expect to be able to use previous days’ returns to predict future market performance.\n\n\nStep 2.7 Can we find a better combination of features?\nRemove the variables that appear not to be helpful in predicting Direction and fit a new Logistic Regression model. We recall that the logistic regression model had very underwhelming p-values associated with all of the predictors, and that the smallest p-value, though not very small, corresponded to Lag1. Perhaps by removing the variables that appear not to be helpful in predicting Direction, we can obtain a more effective model. After all, using predictors that have no relationship with the response tends to cause a deterioration in the test error rate (since such predictors cause an increase in variance without a corresponding decrease in bias), and so removing such predictors may in turn yield an improvement.\n\nglm.fits <- glm(Direction ~ Lag1 + Lag2, \n                data = Smarket.before.2005, family = binomial)\nsummary(glm.fits)\n\n\nCall:\nglm(formula = Direction ~ Lag1 + Lag2, family = binomial, data = Smarket.before.2005)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.345  -1.188   1.074   1.164   1.326  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept)  0.03222    0.06338   0.508    0.611\nLag1        -0.05562    0.05171  -1.076    0.282\nLag2        -0.04449    0.05166  -0.861    0.389\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1383.3  on 997  degrees of freedom\nResidual deviance: 1381.4  on 995  degrees of freedom\nAIC: 1387.4\n\nNumber of Fisher Scoring iterations: 3\n\n\n\nglm.probs <- predict(glm.fits, Smarket.2005, type = \"response\")\nglm.pred <- rep(\"Down\", 252)\nglm.pred[glm.probs > .5] <- \"Up\"\ntable(glm.pred , Direction.2005)\n\n        Direction.2005\nglm.pred Down  Up\n    Down   35  35\n    Up     76 106\n\n\n\nmean(glm.pred == Direction.2005)\n\n[1] 0.5595238\n\n\nCheck proportion:\n\n106 / (106 + 76)\n\n[1] 0.5824176\n\n\nAbove we have refit the logistic regression using just Lag1 and Lag2, which seemed to have the highest predictive power in the original logistic regression model.\nNow the results appear to be a little better: 56% of the daily movements have been correctly predicted. It is worth noting that in this case, a much simpler strategy of predicting that the market will increase every day will also be correct 56% of the time! Hence, in terms of overall error rate, the logistic regression method is no better than the naive approach However, the confusion matrix shows that on days when logistic regression predicts an increase in the market, it has a 58% accuracy rate. This suggests a possible trading strategy of buying on days when the model predicts an increasing market, and avoiding trades on days when a decrease is predicted. Of course one would need to investigate more carefully whether this small improvement was real or just due to random chance.\nSuppose that we want to predict the returns associated with particular values of Lag1 and Lag2. In particular, we want to predict Direction on a day when Lag1 and Lag2 equal 1.2 and 1.1, respectively, and on a day when they equal 1.5 and −0.8. We do this using the predict() function.\n\npredict(glm.fits, \n        newdata = data.frame (Lag1 = c(1.2, 1.5), Lag2 = c(1.1, -0.8)), \n        type = \"response\")\n\n        1         2 \n0.4791462 0.4960939 \n\n\n\n\nStep 2.8 Logistic regression siblings\nIn this lab we used the glm() function with family = binomial to perform logistic regression. Other choices for the family argument can be used to fit other types of GLMs. For instance, family = Gamma fits a gamma regression model. You can alwarys use the following command to explore more about family argument and possible choices.\n?glm()"
  },
  {
    "objectID": "weeks/week04/lab.html#step-3-naive-bayes",
    "href": "weeks/week04/lab.html#step-3-naive-bayes",
    "title": "💻 Week 04 - Lab Roadmap (90 min)",
    "section": "Step 3: Naive Bayes",
    "text": "Step 3: Naive Bayes\nThe Smarket data set will still be utilised to fit a naive Bayes classifier to predict Direction.\n\nStep 3.1 Let’s fit a Naive Bayes model\nFit a naive Bayes model to predict Direction using Lag1 and Lag2:\n\nlibrary(e1071)\nnb.fit <- naiveBayes (Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\nnb.fit\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n    Down       Up \n0.491984 0.508016 \n\nConditional probabilities:\n      Lag1\nY             [,1]     [,2]\n  Down  0.04279022 1.227446\n  Up   -0.03954635 1.231668\n\n      Lag2\nY             [,1]     [,2]\n  Down  0.03389409 1.239191\n  Up   -0.03132544 1.220765\n\n\nNaive Bayes is implemented in R using the naiveBayes() function, which is part of the e1071 library. By default, this implementation of the naive Bayes classifier models each quantitative feature using a Gaussian distribution. However, a kernel density method can also be used to estimate the distributions.\nThe output contains the estimated mean and standard deviation for each variable in each class. For example, the mean for Lag1 is 0.0428 for Direction=Down, and the standard deviation is 1.23. We can easily verify this:\n\nmean(Smarket$Lag1[train][Smarket$Direction[train] == \"Down\"])\n\n[1] 0.04279022\n\n\n\nsd(Smarket$Lag1[train][Smarket$Direction[train] == \"Down\"])\n\n[1] 1.227446\n\n\n\n\nStep 3.2 Predict the test set\nPredict Direction in testing data set:\n\nnb.class <- predict(nb.fit, Smarket.2005)\ntable(nb.class, Direction.2005)\n\n        Direction.2005\nnb.class Down  Up\n    Down   28  20\n    Up     83 121\n\n\n\nmean(nb.class == Direction.2005)\n\n[1] 0.5912698\n\n\nThe predict() function is straightforward. From the confusion matrix, Naive Bayes performs very well on this data, with accurate predictions over 59% of the time. This is better than Logistic Regression Model.\nThe predict() function can also generate estimates of the probability that each observation belongs to a particular class.\n\nnb.preds <- predict(nb.fit, Smarket.2005, type = \"raw\")\nnb.preds[1:5, ]\n\n          Down        Up\n[1,] 0.4873164 0.5126836\n[2,] 0.4762492 0.5237508\n[3,] 0.4653377 0.5346623\n[4,] 0.4748652 0.5251348\n[5,] 0.4901890 0.5098110"
  },
  {
    "objectID": "weeks/week04/lab.html#step-4-practical-exercises-in-pairs",
    "href": "weeks/week04/lab.html#step-4-practical-exercises-in-pairs",
    "title": "💻 Week 04 - Lab Roadmap (90 min)",
    "section": "Step 4: Practical exercises (in pairs)",
    "text": "Step 4: Practical exercises (in pairs)\nSo far, we have learnt to fit some kinds of classification models in R. In this practical case, we will continue to use the data set Auto. Make sure that the missing values have been removed from the data.\nSix questions are listed below. In this part, you will develop a model to predict whether a given car gets high or low gas mileage based on the Auto data set.\n🎯 Questions\n\nCreate a binary variable, mpg01, that contains a 1 if mpg contains a value above its median, and a 0 if mpg contains a value below its median. You can compute the median using the median() function. Note you may find it helpful to use the data.frame() function to create a single data set containing both mpg01 and the other Auto variables.\nExplore the data graphically in order to investigate the association between mpg01 and the other features. Which of the other features seem most likely to be useful in predicting mpg01? Scatterplots and boxplots may be useful tools to answer this question. Describe your findings.\nSplit the data into a training set and a test set. Train set contains observations before 1979. Test set contains the rest of the observations.\nPerform logistic regression on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in question 2. What is the test error of the model obtained?\nPerform naive Bayes on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in question 2. What is the test error of the model obtained?\nWhich of these two methods appears to provide the best results on this data? Justify your choice."
  },
  {
    "objectID": "weeks/week04/lab_solutions.html",
    "href": "weeks/week04/lab_solutions.html",
    "title": "✔️ Week 04 - Lab Solutions",
    "section": "",
    "text": "Create a binary variable, mpg01, that contains a 1 if mpg contains a value above its median, and a 0 if mpg contains a value below its median. You can compute the median using the median() function. Note you may find it helpful to use the data.frame() function to create a single data set containing both mpg01 and the other Auto variables.\nlibrary(ISLR2)\nAuto = na.omit(Auto)\nmpg01 = rep(0, dim(Auto)[1])\nmpg01[Auto$mpg > median(Auto$mpg)] = 1\nAuto = data.frame(Auto, mpg01)\nhead(Auto)\nor a easier way by using ifelse() function:\nlibrary(ISLR2)\nAuto = na.omit(Auto)\nAuto$mpg01 <- ifelse(Auto$mpg > median(Auto$mpg), 1, 0)\nhead(Auto)\nBefore we move to next step, we need to lable mpg01 and orgin as factor so that R could recognized them as quanlitative variables instead of quantitative variables.\nAuto$mpg01 = as.factor(Auto$mpg01)\nAuto$origin = as.factor(Auto$origin)\nExplore the data graphically in order to investigate the association between mpg01 and the other features. Which of the other features seem most likely to be useful in predicting mpg01? Scatterplots and boxplots may be useful tools to answer this question. Describe your findings.\npar(mfrow = c(2, 3))\nplot(Auto$mpg01, Auto$cylinders, xlab = \"mpg01\", ylab = \"Number of engine cylinders\")\nplot(Auto$mpg01, Auto$displacement, xlab = \"mpg01\", ylab = \"Engine displacement (cubic inches)\")\nplot(Auto$mpg01, Auto$horsepower, xlab = \"mpg01\", ylab = \"Horsepower\")\nplot(Auto$mpg01, Auto$weight, xlab = \"mpg01\", ylab = \"Weight (pounds)\")\nplot(Auto$mpg01, Auto$acceleration, xlab = \"mpg01\", ylab = \"Time to reach 60mpg (seconds)\")\nplot(Auto$mpg01, Auto$year, xlab = \"mpg01\", ylab = \"Manufacture year\")\nmtext(\"Boxplots for cars with above(1) and below(0) median mpg\", outer = TRUE, line = -3)\nBoxplots were plotted to compare the distributions for each of the quantitative variables between cars with above-median mpg and those with below median-mpg. If the distribution of a predictor significantly varies with the response variable, then it may contribute to the prediction of response variable. If the distribution of a predictor does not significantly differ between different values of the response variable, then it may not contribute to the prediction of response variable. The boxplots suggest that cylinders, displacement, horsepower, and weight might be the most useful in predicting mpg01. The function par() is used to change the layout of output plots.\nWe could try to use ggplot() function to creat some fancy plots which is a combination of boxplot and scatterplot:\nggplot(data = Auto, aes(acceleration, mpg01, colour = mpg01, fill = mpg01)) +\ngeom_boxplot(alpha = 0.125) +\ngeom_jitter(alpha = 0.5, size = 2)  \nTo visualise the association between mpg01 and year, scatterplot is used.\npar(mfrow = c(1, 1))\nplot(Auto$year, Auto$mpg)\nabline(h = median(Auto$mpg), lwd = 2, col = \"red\")\nThe above scatterplot of mpg vs year shows that the newer cars in the data set tend to be more fuel efficient. Therefore, while manufacture year might not be as useful as the other four quantitative variables, it still seems worth including.\nplot(Auto$origin, Auto$mpg, xlab = \"Origin\", ylab = \"mpg\")\nabline(h = median(Auto$mpg), lwd = 2, col = \"red\")\nLastly, when looking at a boxplot that compares the mpg values for each car, categorized by country of origin, we see that there is a clear difference between American cars, which tend to have below-median fuel efficiency, and European and Japanese cars, which tend to have above-median fuel efficiency. Thus, it seems that origin will also be useful in predicting mpg01.\nIn conclusion, all of the predictors except for acceleration and name will be used in fitting this classification model for trying to predict mpg01. Also, mpg will be excluded because that was directly used to create the classification label.\nSplit the data into a training set and a test set. Train set contains observations before 1979. Test set contains the rest of the observations.\nattach(Auto)\ntrain <- (year < 79)\nAuto_train <- Auto[train , ]\nAuto_test <- Auto[!train , ]\nPerform logistic regression on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in question 2. What is the test error of the model obtained?\nglm.fit = glm(mpg01 ~ cylinders + displacement + horsepower + weight + year + origin, data = Auto, subset = train, family = \"binomial\")\nsummary(glm.fit)\nglm.probs = predict(glm.fit, Auto_test, type = \"response\")\nglm.pred = rep(0, dim(Auto_test)[1])\nglm.pred[glm.probs > 0.5] = 1\ntable(glm.pred, Auto_test$mpg01, dnn = c(\"Predicted\", \"Actual\"))\nmean(glm.pred == Auto_test$mpg01)\n[1] 0.877193\nAs is shown, the test error of the model ontained is (1-0.877193) = 0.122807.\nPerform naive Bayes on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in question 2. What is the test error of the model obtained?\nlibrary (e1071)\nnb.fit = naiveBayes(mpg01 ~ cylinders + displacement + horsepower + weight + year + origin, data = Auto, subset = train)\nnb.fit\nnb.class <- predict(nb.fit, Auto_test)\ntable(nb.class, Auto_test$mpg01, dnn = c(\"Predicted\", \"Actual\"))\nmean(nb.class == Auto_test$mpg01)\nAs is shown, the test error of the model ontained is (1-0.877193) = 0.122807.\nWhich of these two methods appears to provide the best results on this data? Justify your choice.\nAfter comparing the test errors, these two classification models were equally good. To further compare the performance of these two model, we have to look at the confusion matrix and find these two classification models have identiical confusion matrix. It means that Precision, Recall, Accuracy and F-Score of these two models are all same. Therefore, we can conclude that these two classification models have equally good performace on this data set. One thing that should be cautious of is that the data set is imbalnced. Therefore, we cannot judge the performance of this data set based on test error. More detailed explaination about be found here: https://www.analyticsvidhya.com/blog/2021/06/5-techniques-to-handle-imbalanced-data-for-a-classification-problem/."
  },
  {
    "objectID": "weeks/week04/lecture.html",
    "href": "weeks/week04/lecture.html",
    "title": "👨‍🏫 Week 04 - Lecture",
    "section": "",
    "text": "References\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York NY: Springer. https://www.statlearning.com/."
  },
  {
    "objectID": "weeks/week05/checklist.html",
    "href": "weeks/week05/checklist.html",
    "title": "✅ Week 05 - Checklist",
    "section": "",
    "text": "Important\n\n\n\n\n\nKeep in mind that after the lecture at the end of this week, on Friday 28 October 2022, we will release the Summative Problem Set 01. This is the first summative assessment of this course and it is worth 20% of your final grade. You will have until 9 November 2022 — Wednesday of Week 07 — to submit your solutions via Moodle.\nHere is a suggestion of how to program your week in relation to this course:\nExtra:"
  },
  {
    "objectID": "weeks/week05/checklist.html#if-your-lab-is-on-monday",
    "href": "weeks/week05/checklist.html#if-your-lab-is-on-monday",
    "title": "✅ Week 05 - Checklist",
    "section": "If your lab is on Monday:",
    "text": "If your lab is on Monday:\n\n📥 Download: Before or once you arrive at the classroom, download the DS202_2022MT_w05_lab_rmark.Rmd file that contains the lab roadmap (under 🗓️ Week 05 - Non-linear algorithms/W04 Lab Files section on Moodle). Or browse the webpage version here.\n💻 Participate: Actively engage with the material in the lab. Ask your class teacher for help if anything is unclear. Work with others whenever possible and take notes of theoretical concepts or practical coding skill you might want to revisit later in the week.\n📙 Read: Find some time to read (James et al. 2021, chap. 5) and reinforce your theoretical understanding of resampling methods; the textbook is available online for free.\n\nIn the lecture/workshop last week, we only explored training/test splits and the idea of cross-validation; in the lab, you will have a chance to explore another technique called the bootstrap.\nAs you go through the text, try to connect what you read to the things you heard about in the lecture/workshop or the examples you explored in the lab.\n\n✍️ Solve: There is still time to submit your solutions to the Formative Problem Set 01, the deadline is Tuesday 25 October 23:59 UK time.\n🏫 Attend the lecture: This week, you will learn of two new algorithms: Support Vector Machine and Decision Tree. You will need to use these algorithms in the first summative that will be released on Friday."
  },
  {
    "objectID": "weeks/week05/checklist.html#if-your-lab-is-on-friday",
    "href": "weeks/week05/checklist.html#if-your-lab-is-on-friday",
    "title": "✅ Week 05 - Checklist",
    "section": "If your lab is on Friday:",
    "text": "If your lab is on Friday:\n\n✍️ Solve: There is still time to submit your solutions to the Formative Problem Set 01, the deadline is Tuesday 25 October 23:59 UK time.\n📙 Read: Find some time to read (James et al. 2021, chap. 5) and reinforce your theoretical understanding of resampling methods; the textbook is available online for free.\n\nIn the lecture/workshop last week, we only explored training/test splits and the idea of cross-validation; in the lab, you will have a chance to explore another technique called the bootstrap.\nAs you go through the text, try to connect what you read to the things you heard about in the lecture/workshop or the examples you explored in the lab.\n\n📥 Download: Before or once you arrive at the classroom, download the DS202_2022MT_w05_lab_rmark.Rmd file that contains the lab roadmap (under 🗓️ Week 05 - Non-linear algorithms/W04 Lab Files section on Moodle). Or browse the webpage version here.\n💻 Participate: Actively engage with the material in the lab. Ask your class teacher for help if anything is unclear. Work with others whenever possible and take notes of theoretical concepts or practical coding skill you might want to revisit later in the week.\n🏫 Attend the lecture: This week, you will learn of two new algorithms: Support Vector Machine and Decision Tree. You will need to use these algorithms in the first summative that will be released on Friday."
  },
  {
    "objectID": "weeks/week05/lab.html",
    "href": "weeks/week05/lab.html",
    "title": "💻 Week 05 - Lab Roadmap (90 min)",
    "section": "",
    "text": "Last week we learned how to do the Classification Methods in R. As data scientists, we need to know how to examine the results we generated and also to justify our results. So, we will focus on and practice the validation methods in the R language in this week’s lab.\nIf you feel confident at the current stage, free to explore more on your own. We have provided you with some supplementary online resources :\nR packages you will need:"
  },
  {
    "objectID": "weeks/week05/lab.html#step-1-the-validation-set-approach",
    "href": "weeks/week05/lab.html#step-1-the-validation-set-approach",
    "title": "💻 Week 05 - Lab Roadmap (90 min)",
    "section": "Step 1: The Validation Set Approach",
    "text": "Step 1: The Validation Set Approach\nIn this step, we will explore how to extract the subset of the whole dataset as a training dataset, and then estimate the test error rates of various linear models. The dataset is Auto from the ISLR2 package.\n\nStep 1.1: Training vs Testing splits\nSplit the Auto dataset into two halves, as randomly selecting 196 observations out of the original 392 observations. We performed splits using base R in the last lab. However, this can be done more easily using the rsample package. We first create a split object Auto.split using prop = 0.5 to specify a \\(50\\%/50\\%\\) train/test split. We also specify strata = mpg as we want our train/test split to have a similar mean/standard deviation for mpg. We then create dataframes using the training and testing functions.\n\nlibrary(ISLR2)\nlibrary(rsample)\n\nset.seed(1) # Just so you and I have the same \"random\" numbers.\n\nAuto.split <- initial_split(Auto, prop = 0.5, strata = mpg)\nAuto.train <- training(Auto.split)\nAuto.test <- testing(Auto.split)\n\n\n\n\n\n\n\nAbout set.seed()\n\n\n\n\n\nset.seed() is important here as it set a seed for the random number generator. Literally, the same results will be replicated in the following steps. Further information can be found in the official documentation.Seeding Random Variate Generators\n\n\n\n\n\nStep 1.2: How good is your model at predicting the test set?\nFit a linear regression model using the training dataset (Auto.train), making the mpg as the dependent variable(x) and horsepower as the independent variable(y). Then, using the fitted model to estimate the mpg from Auto.test. Finally, calculating the MSE of the 196 observations in the validation set.\n\n# use the lm() function to fit a linear regression model\nlm.fit <- lm(mpg ~ horsepower, data = Auto.train)\n\n# estimate the 'mpg' values by the lm.fit model\nlm.pred <- predict(lm.fit, Auto.test)\n\n# calculate MSE\nmean((Auto.test$mpg - lm.pred)^2)\n\n[1] 25.58657\n\n\nTherefore, we have estimated the test MSE for the linear regression model, \\(\\mathbf{25.59}\\). (Well Done! 💪 )\n\n\n\n\n\n\nHow to get help in R\n\n\n\n\n\nIf you are ever in doubt of what a particular R function do, you can consult the R documentation for it by using ?.\nFor example, you have been using lm and predict but you want to see what other arguments/parameters this function can take, just type ?lm or ?predict in your console.\n\n\n\n\n\n\n\n\n\nHow to access or create columns in R?\n\n\n\n\n\nIn base R, you generally use the $ sign to access columns or create new columns.\n\n#You can see all the values from the 'mpg' column.\nAuto$mpg\n\n  [1] 18.0 15.0 18.0 16.0 17.0 15.0 14.0 14.0 14.0 15.0 15.0 14.0 15.0 14.0 24.0\n [16] 22.0 18.0 21.0 27.0 26.0 25.0 24.0 25.0 26.0 21.0 10.0 10.0 11.0  9.0 27.0\n [31] 28.0 25.0 19.0 16.0 17.0 19.0 18.0 14.0 14.0 14.0 14.0 12.0 13.0 13.0 18.0\n [46] 22.0 19.0 18.0 23.0 28.0 30.0 30.0 31.0 35.0 27.0 26.0 24.0 25.0 23.0 20.0\n [61] 21.0 13.0 14.0 15.0 14.0 17.0 11.0 13.0 12.0 13.0 19.0 15.0 13.0 13.0 14.0\n [76] 18.0 22.0 21.0 26.0 22.0 28.0 23.0 28.0 27.0 13.0 14.0 13.0 14.0 15.0 12.0\n [91] 13.0 13.0 14.0 13.0 12.0 13.0 18.0 16.0 18.0 18.0 23.0 26.0 11.0 12.0 13.0\n[106] 12.0 18.0 20.0 21.0 22.0 18.0 19.0 21.0 26.0 15.0 16.0 29.0 24.0 20.0 19.0\n[121] 15.0 24.0 20.0 11.0 20.0 19.0 15.0 31.0 26.0 32.0 25.0 16.0 16.0 18.0 16.0\n[136] 13.0 14.0 14.0 14.0 29.0 26.0 26.0 31.0 32.0 28.0 24.0 26.0 24.0 26.0 31.0\n[151] 19.0 18.0 15.0 15.0 16.0 15.0 16.0 14.0 17.0 16.0 15.0 18.0 21.0 20.0 13.0\n[166] 29.0 23.0 20.0 23.0 24.0 25.0 24.0 18.0 29.0 19.0 23.0 23.0 22.0 25.0 33.0\n[181] 28.0 25.0 25.0 26.0 27.0 17.5 16.0 15.5 14.5 22.0 22.0 24.0 22.5 29.0 24.5\n[196] 29.0 33.0 20.0 18.0 18.5 17.5 29.5 32.0 28.0 26.5 20.0 13.0 19.0 19.0 16.5\n[211] 16.5 13.0 13.0 13.0 31.5 30.0 36.0 25.5 33.5 17.5 17.0 15.5 15.0 17.5 20.5\n[226] 19.0 18.5 16.0 15.5 15.5 16.0 29.0 24.5 26.0 25.5 30.5 33.5 30.0 30.5 22.0\n[241] 21.5 21.5 43.1 36.1 32.8 39.4 36.1 19.9 19.4 20.2 19.2 20.5 20.2 25.1 20.5\n[256] 19.4 20.6 20.8 18.6 18.1 19.2 17.7 18.1 17.5 30.0 27.5 27.2 30.9 21.1 23.2\n[271] 23.8 23.9 20.3 17.0 21.6 16.2 31.5 29.5 21.5 19.8 22.3 20.2 20.6 17.0 17.6\n[286] 16.5 18.2 16.9 15.5 19.2 18.5 31.9 34.1 35.7 27.4 25.4 23.0 27.2 23.9 34.2\n[301] 34.5 31.8 37.3 28.4 28.8 26.8 33.5 41.5 38.1 32.1 37.2 28.0 26.4 24.3 19.1\n[316] 34.3 29.8 31.3 37.0 32.2 46.6 27.9 40.8 44.3 43.4 36.4 30.0 44.6 33.8 29.8\n[331] 32.7 23.7 35.0 32.4 27.2 26.6 25.8 23.5 30.0 39.1 39.0 35.1 32.3 37.0 37.7\n[346] 34.1 34.7 34.4 29.9 33.0 33.7 32.4 32.9 31.6 28.1 30.7 25.4 24.2 22.4 26.6\n[361] 20.2 17.6 28.0 27.0 34.0 31.0 29.0 27.0 24.0 36.0 37.0 31.0 38.0 36.0 36.0\n[376] 36.0 34.0 38.0 32.0 38.0 25.0 38.0 26.0 22.0 32.0 36.0 27.0 27.0 44.0 32.0\n[391] 28.0 31.0\n\n\n\n\n\n\n\nStep 1.3: Does it get better if I modify the features using polynomial terms?\nRepeat the second part to estimate the test error for the quadratic and cubic regressions.\n\nlm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto.train)\n\n\nmean((Auto.test$mpg - predict(lm.fit2, Auto.test))^2)\n\n[1] 19.08374\n\n\n\nlm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto.train)\nmean((Auto.test$mpg - predict(lm.fit3, Auto.test))^2)\n\n[1] 19.00317\n\n\n💡 We can see a model that predicts mpg using a quadratic function of horsepower performs better than a model that involves only a linear function of horsepower. Furthermore, we see that adding a cubic function of horsepower actually increases MSE when compared to the quadratic function. Thus, the quadratic function of horsepower appears to perform the best out of all the functions considered.\n\n\nStep 1.4: Visualisation\nWant to see how well does the quadratic fit maps onto the raw data?\nLet’s create a scatter plot with horsepower on the x-axis and mpg on the y-axis. Now, we can predict mpg using lm.fit2 and specify interval = 'confidence' to get \\(95\\%\\) confidence intervals. Along with geom_line we can use geom_ribbon to plot the line of best fit and associated confidence intervals. Remember to specify alpha so that we can see the predicted value - otherwise the ribbon will not be translucent.\n\n    library(tidyverse)\n\n    sim.data <- data.frame(horsepower = 46:230)\n\n    sim.pred <- predict(lm.fit2, sim.data, interval = 'confidence') \n\n    sim.data <- cbind(sim.data, sim.pred)\n\n    ggplot(data = sim.data, aes(x = horsepower, y = fit)) +\n        geom_point(data = Auto, aes(x = horsepower, y = mpg)) +\n        geom_line() +\n        geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.125, fill = 'blue') +\n        theme_minimal() +\n        labs(x = 'Horsepower', y = 'MPG')"
  },
  {
    "objectID": "weeks/week05/lab.html#step-2-k-fold-cross-validation",
    "href": "weeks/week05/lab.html#step-2-k-fold-cross-validation",
    "title": "💻 Week 05 - Lab Roadmap (90 min)",
    "section": "Step 2: k-Fold Cross-Validation",
    "text": "Step 2: k-Fold Cross-Validation\nIt will be easy to follow the former procedure in Step 2, by the cv.glm to implement K-fold CV.\n\nStep 2.1: Using K=10 folds\nLet’s estimate Cross-Validation errors corresponding to the polynomial fits of orders one to ten using ten-fold cross-validation (via K = 10).\nThe cv.glm() function is part of the boot library. Meanwhile, you can explore the cv.err by yourself to see what call,K,delta and seed mean. This online webpage will be useful when interpreting the results.\n\nlibrary(boot)\n\n\nset.seed(17)\n\ncv.error.10 <- c() \n\nfor(i in 1:10){\n    glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)\n    cv.error.10[i] <- cv.glm(data = Auto, glmfit = glm.fit, K = 10)$delta[1]\n}\n\ncv.error.10\n\n [1] 24.27207 19.26909 19.34805 19.29496 19.03198 18.89781 19.12061 19.14666\n [9] 18.87013 20.95520\n\n\n\n# we can plot the results by passing a data frame to ggplot\n\ncv.data <- data.frame(poly = 1:10, cv.errs = cv.error.10)\n\nggplot(data = cv.data, aes(x = poly, y = cv.errs)) +\n    geom_point() +\n    geom_line(linetype = 'dashed') +\n    scale_x_continuous(breaks = 1:10) +\n    theme_minimal() +\n    labs(x = 'Degree of polynomial', y = 'Cross-validated MSE')\n\n\n\n\nNote that in the line of cv.error.10[i] <- cv.glm(data = Auto, glmfit = glm.fit, K=10)$delta[1], it will be very strict to K rather than k."
  },
  {
    "objectID": "weeks/week05/lab.html#step-3-the-bootstrap",
    "href": "weeks/week05/lab.html#step-3-the-bootstrap",
    "title": "💻 Week 05 - Lab Roadmap (90 min)",
    "section": "Step 3: The Bootstrap",
    "text": "Step 3: The Bootstrap\nWe have learnt the theoretical method regarding Bootstrap. I understand that it may be a bit difficult for beginners in statistics, but we will mainly focus on the coding implementation and visualisation here. Also, we will introduce how to create a function below.\nFunctions are “self contained” modules of code that accomplish a specific task. referenced from Functions and theri argumens\nWith the help of a function, you can reuse the same pattern codes with a simple function name. In fact, you work with functions all the time in R - perhaps without even realising it!\n\nStep 3.1 Estimating the Accuracy of a Linear Regression Model\nIn this step, we will use the boostrap approach to assess the variability of a coefficient estimate. For the sake of simplicity we will look at the relationship between weight and horsepower which appears to be linear.\nCreate a function as boot.fn() which\n\nboot.fn <- function(data, index) {\n\n    lm(horsepower ~ weight, data = data[index,])$coefficients\n\n}  \n\nboot.fn simply returns a vector of coefficient estimates. It takes two parameters: data and index. data is a placeholder for the dataframe used in the model. index is a placeholder for the sample used to subset the dataframe. Other than this, the body of the function should look familiar. We are estimating a linear model where we are looking to predict horsepower by weight, and then extracting the coefficients.\nNow, compare the results from bootstrap estimates and the standard estimates\n\n# bootstrap with 1000 times\n\nboot(Auto, boot.fn, 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Auto, statistic = boot.fn, R = 1000)\n\n\nBootstrap Statistics :\n        original        bias    std. error\nt1* -12.18348470  1.659517e-01 3.221766853\nt2*   0.03917702 -6.479214e-05 0.001251433\n\n\n\nsummary(lm(horsepower ~ weight, data = Auto))$coef\n\n                Estimate  Std. Error   t value      Pr(>|t|)\n(Intercept) -12.18348470 3.570431493 -3.412328  7.115312e-04\nweight        0.03917702 0.001153214 33.972031 1.364347e-118\n\n\nWe can find that in the bootstrap estimation process, \\(\\mathrm{SE}(\\hat{\\beta}_{0}) = 3.5704\\) and \\(\\mathrm{SE}(\\hat{\\beta}_{1}) = 0.0012\\) , while in the standard estimation process, \\(\\hat{\\beta}_{Intercept}=-12.1835\\) and \\(\\hat{\\beta}_{horsepower}=0.0392\\).\nTo get a better intuition of what the bootstrap algorithm does let’s create a ggplot. We can get the intercepts and slopes estimated and overlay them on a scatterplot (weight on x-axis, horsepower on y-axis). We will create 50 bootstrap resamples for ease of visualisation and use geom_abline to overlay all the lines of best fit.\n\nboot.model <- boot(Auto, boot.fn, 50)\n\nboot.df <- as.data.frame(boot.model$t)\nnames(boot.df) <- c('b0','b1')\n\nggplot(data = Auto, aes(x = weight, y = horsepower)) +\n    geom_point() +\n    geom_abline(data = boot.df,\n                aes(intercept = b0, slope = b1), \n                alpha = 0.1, colour = 'blue') +\n    theme_minimal() +\n    labs(x = 'Weight (lbs.)', y = 'Engine horsepower')"
  },
  {
    "objectID": "weeks/week05/lab.html#step-4-practical-exercises",
    "href": "weeks/week05/lab.html#step-4-practical-exercises",
    "title": "💻 Week 05 - Lab Roadmap (90 min)",
    "section": "Step 4: 🎯 Practical Exercises",
    "text": "Step 4: 🎯 Practical Exercises\nSince then, we have known and implemented the coding with Cross-validation and Bootstrap. In this practical case, we will use the new dataset Default and also Weekly from the ISRL package. Do not forget to set a random seed before beginning your analysis.\nSome questions are listed below. You are required to try to answer these questions in pairs using R commands. We will go over the solutions once everyone has finished these questions.\n\nQ1: Train vs test sets\nFor the Default dataset, please split the sample set into a training set and a test set, then fit a logistic regression model that uses income and balance to predict default:\n\nQ1.1: Use three different splits of the observations into a training set and a test set.\nQ1.2: Fit three multiple logistic regression models using only the training observations.\nQ1.3: Based on the three models, obtain a prediction of default status for each individual in the test set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than 0.5.\nQ1.4: Based on the three models, compute the test set error, which is the fraction of the observations in the test set that are misclassified.\n\n\n\nQ2: Bootstrap\nFor the Default dataset, We continue to consider the use of a logistic regression model to predict the probability of default using income and balance on the Default data set.\nIn particular, we will now compute estimates for the standard errors of the income and balance logistic regression coefficients in two different ways: 2.1. Using the bootstrap, 2.2. Using the standard formula for computing the standard errors in the glm() function. As following,\n\nUsing the summary() and glm() functions, determine the estimated standard errors for the coefficients associated with income and balance in a multiple logistic regression model that uses both predictors.\nWrite a function,boot.fn(),that takes as input the Default data set as well as an index of the observations, and that outputs the coefficient estimates for income and balance in the multiple logistic regression model.\nUse the boot() function together with your boot.fn() function to estimate the standard errors of the logistic regression coefficients for income and balance. Then, Create a histogram of the bootstrap parameter estimates with ggplot2, and also set the bins=20, title as 1,000 Bootstrap Parameter Estimates - 'balance' & 'income."
  },
  {
    "objectID": "weeks/week05/lab_solutions.html",
    "href": "weeks/week05/lab_solutions.html",
    "title": "✔️ Week 05 - Lab Solutions",
    "section": "",
    "text": "For the Default dataset, please split the sample set into a training set and a validation set, then fit a logistic regression model that uses income and balance to predict default.\nlibrary(ISLR2)\nlibrary(tidyverse)\n\n?Default\n\n\nUse three different splits of the observations into a training set and a test set.\nFirst generate the splits:\nlibrary(tidymodels)\n\nDefault.split1 <- initial_split(Default, prop = 0.5, strata = default)\nDefault.split2 <- initial_split(Default, prop = 0.5, strata = default)\nDefault.split3 <- initial_split(Default, prop = 0.5, strata = default)\nor maybe with different random proportions?\nDefault.split1 <- initial_split(Default, prop = 0.6, strata = default)\nDefault.split2 <- initial_split(Default, prop = 0.6, strata = default)\nDefault.split3 <- initial_split(Default, prop = 0.7, strata = default)\nThen, using those splits, create separate training and test sets from the original data:\nDefault.train1 <- training(Default.split1)\nDefault.test1 <- testing(Default.split1)\n\nDefault.train2 <- training(Default.split2)\nDefault.test2 <- testing(Default.split2)\n\nDefault.train3 <- training(Default.split3)\nDefault.test3 <- testing(Default.split3)\n\n\n\nFit three multiple logistic regression models using only the training observations.\nglm.fit.1 = glm(default ~ income + balance, data = Default.train1, family = \"binomial\")\n\nglm.fit.2 = glm(default ~ income + balance, data = Default.train2, family = \"binomial\")\n\nglm.fit.3 = glm(default ~ income + balance, data = Default.train3, family = \"binomial\")\n\nsummary(glm.fit.1)\n\n\n\nBased on the three models, obtain a prediction of default status for each individual in the test set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than 0.5.\n# Predict the test set\nglm.probs.1 <- predict(glm.fit.1, Default.test1, type = \"response\")\n\n# Make predictions using threshold=0.5\nglm.preds.1 = if_else(glm.probs.1 > 0.5, \"Yes\", \"No\")\n\n# do the same for the other datasets\n\n\n\nBased on the three models, compute the test set error, which is the fraction of the observations in the test set that are misclassified.\nmean(glm.preds.1 != Default.test1$default)\nmean(glm.preds.2 != Default.test2$default)\nmean(glm.preds.3 != Default.test3$default)\n\n\n\n\nFor the Default dataset, We continue to consider the use of a logistic regression model to predict the probability of default using income and balance on the Default data set.\nIn particular, we will now compute estimates for the standard errors of the income and balance logistic regression coefficients in two different ways: 2.1. Using the bootstrap, 2.2. Using the standard formula for computing the standard errors in the glm() function. As following,\n\n\nUsing the summary() and glm() functions, determine the estimated standard errors for the coefficients associated with income and balance in a multiple logistic regression model that uses both predictors.\n# multiple logistic regression model \nlog_def <- glm(default ~ income + balance, data = Default, family = \"binomial\")\n\nsummary(log_def)\nsummary(log_def)$coefficients[, 2]  ## standard errors\n\n\n\nWrite a function,boot.fn(),that takes as input the Default data set as well as an index of the observations, and that outputs the coefficient estimates for income and balance in the multiple logistic regression model.\n# This is a modified version of the original `boot.fn` function\nboot.fn <- function(data, index = 1:nrow(data)) {\n      coef(glm(default ~ income + balance, data = data, subset = index, family = \"binomial\"))[-1]\n    }\nTest it:\n## all data --without intercept\nboot.fn(Default)\n\n\n\nUse the boot() function together with your boot.fn() function to estimate the standard errors of the logistic regression coefficients for income and balance. Then, Create a histogram of the bootstrap parameter estimates with ggplot2, and also set the bins=20, title as 1,000 Bootstrap Parameter Estimates - 'balance' & 'income.\nboot_results <- boot(data = Default, statistic = boot.fn, R = 1000)\nboot_results"
  },
  {
    "objectID": "weeks/week05/lecture.html",
    "href": "weeks/week05/lecture.html",
    "title": "👨‍🏫 Week 05 - Lecture",
    "section": "",
    "text": "Tip\n\n\n\nFeel free to browse the slides before the lecture, but it is probably safer to wait until the time of the lecture to download/save them, as we might make small changes to slides before then."
  },
  {
    "objectID": "weeks/week05/lecture.html#part-i---non-linear-algorithms-decision-tree-45-50-min",
    "href": "weeks/week05/lecture.html#part-i---non-linear-algorithms-decision-tree-45-50-min",
    "title": "👨‍🏫 Week 05 - Lecture",
    "section": "Part I - Non-linear algorithms (Decision Tree) (45-50 min)",
    "text": "Part I - Non-linear algorithms (Decision Tree) (45-50 min)\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides. For a PDF copy, check Moodle."
  },
  {
    "objectID": "weeks/week05/lecture.html#coffee-break-10-min",
    "href": "weeks/week05/lecture.html#coffee-break-10-min",
    "title": "👨‍🏫 Week 05 - Lecture",
    "section": "☕ Coffee Break (10 min)",
    "text": "☕ Coffee Break (10 min)\nUse this time to chat, stretch, drink some coffee or just relax for a bit by yourself."
  },
  {
    "objectID": "weeks/week05/lecture.html#part-ii---non-linear-algorithms-support-vector-machines-45-50-min",
    "href": "weeks/week05/lecture.html#part-ii---non-linear-algorithms-support-vector-machines-45-50-min",
    "title": "👨‍🏫 Week 05 - Lecture",
    "section": "Part II - Non-linear algorithms (Support Vector Machines) (45-50 min)",
    "text": "Part II - Non-linear algorithms (Support Vector Machines) (45-50 min)\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides. For a PDF copy, check Moodle."
  },
  {
    "objectID": "weeks/week06/checklist.html",
    "href": "weeks/week06/checklist.html",
    "title": "✅ Week 06 - Checklist",
    "section": "",
    "text": "Here is a suggestion of how to program your week in relation to this course:"
  },
  {
    "objectID": "weeks/week06/checklist.html#useful-links-about-tidyverse",
    "href": "weeks/week06/checklist.html#useful-links-about-tidyverse",
    "title": "✅ Week 06 - Checklist",
    "section": "Useful Links about tidyverse",
    "text": "Useful Links about tidyverse\ntidyverse is a set of R packages that have several functions and facilities for working with data. I find tidyverse more intuitive than base R, and there’s an entire book available for free online (R for Data Science) that contains a lot of helpful tutorials about tidyverse. Let me point to a few specific chapters:\n\nYou might need or want to transform data when working on your problem sets. Check out Chapter 5 of R for Data Science online book.\nData visualization is another helpful skill. You can learn a bit more about ggplot, the tidyverse way of making plots, in Chapter 3 of R for Data Science online book.\nWhat should you be looking for when working with data? Check Chapter 7 to learn the basics of exploratory data analysis.\nDo you get confused about R Markdown, the idea of “knitting” a file? Then read Chapter 27.\n(More advanced) Do you want to learn how to reshape data or deal with more complex data manipulation? Then have a look at Chapter 12 - Tidy data and Chapter 13 - Iteration.\nAre you already familiar with tidyverse, but you constantly need to Google how to do things? Save these cheatsheets to your computer."
  },
  {
    "objectID": "weeks/week06/drop_in_jon.html",
    "href": "weeks/week06/drop_in_jon.html",
    "title": "DS202 2022MT W06 Drop In session",
    "section": "",
    "text": "OBJECTIVE: Support with R programming skills"
  },
  {
    "objectID": "weeks/week06/drop_in_jon.html#comparing-vectors",
    "href": "weeks/week06/drop_in_jon.html#comparing-vectors",
    "title": "DS202 2022MT W06 Drop In session",
    "section": "Comparing vectors",
    "text": "Comparing vectors\n\nSay I have two string vectors:\n\nThey have the same length\nElements in the same index represents the same “day”\n\n\nvector1 <- c(\"blue\", \"red\", \"green\", \"green\", \"blue\", \"red\")\nvector2 <- c(\"red\",  \"red\", \"blue\", \"blue\", \"blue\", \"green\")\n\n\n\nHow many blues do they have in common?\n\nvector1 == vector2\n\n[1] FALSE  TRUE FALSE FALSE  TRUE FALSE\n\n\n\nvector1 == \"blue\"\n\n[1]  TRUE FALSE FALSE FALSE  TRUE FALSE\n\n\nNow, put it all together:\n\noutput <- vector1 == \"blue\" & vector2 == \"blue\"\noutput\n\n[1] FALSE FALSE FALSE FALSE  TRUE FALSE\n\n\nKeyword: Logical Operators\n& stands for an AND operation\n| stands for an OR operation\n! stands for a NOT operation\nRead more about it here\n\nsum(output) # Count all occurences of TRUE in the vector\n\n[1] 1\n\n\nIf I wanted to do it all in a single line:\n\nsum(vector1 == \"blue\" & vector2 == \"blue\")\n\n[1] 1"
  },
  {
    "objectID": "weeks/week06/drop_in_jon.html#doing-the-same-with-dataframes",
    "href": "weeks/week06/drop_in_jon.html#doing-the-same-with-dataframes",
    "title": "DS202 2022MT W06 Drop In session",
    "section": "Doing the same with dataframes",
    "text": "Doing the same with dataframes\nIf I have the same info but now represented as a data frame, how would I count the number of blues in common?\n\n# A random dataframe\ndf <- data.frame(colourA=c(\"blue\", \"red\", \"green\", \"green\", \"blue\", \"red\"),\n                 colourB=c(\"red\",  \"red\", \"blue\", \"blue\", \"blue\", \"green\"))\ndf\n\n  colourA colourB\n1    blue     red\n2     red     red\n3   green    blue\n4   green    blue\n5    blue    blue\n6     red   green\n\n\nYou can access each column by using the $:\n\ndf$colourA\n\n[1] \"blue\"  \"red\"   \"green\" \"green\" \"blue\"  \"red\"  \n\n\n\nsum(df$colourA == \"blue\" & df$colourB == \"blue\")\n\n[1] 1"
  },
  {
    "objectID": "weeks/week06/drop_in_jon.html#now-lets-imagine-we-have-two-dataframes",
    "href": "weeks/week06/drop_in_jon.html#now-lets-imagine-we-have-two-dataframes",
    "title": "DS202 2022MT W06 Drop In session",
    "section": "Now let’s imagine we have two dataframes",
    "text": "Now let’s imagine we have two dataframes\n \n\n# A random dataframe\ndf1 <- data.frame(observation=c(1, 2, 3, 4, 5, 6),\n                  colour=c(\"blue\", \"red\", \"green\", \"green\", \"blue\", \"red\"))\ndf1\n\n  observation colour\n1           1   blue\n2           2    red\n3           3  green\n4           4  green\n5           5   blue\n6           6    red\n\n\n\n# A random dataframe\ndf2 <- data.frame(observation=c(1, 2, 3, 4, 5, 5,  6),\n                  colour=c(\"red\",  \"red\", \"blue\", \"blue\", \"red\", \"blue\", \"green\"))\ndf2\n\n  observation colour\n1           1    red\n2           2    red\n3           3   blue\n4           4   blue\n5           5    red\n6           5   blue\n7           6  green\n\n\nFirst, let’s calculate whether there was at least one “blue” in each observation.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nselect(df2, observation)\n\n  observation\n1           1\n2           2\n3           3\n4           4\n5           5\n6           5\n7           6\n\n\nThe pipe\nI could do exactly the same thing using the pipe %>%\n\ndf2 %>% select(observation) # no need to pass `df2` to function select\n\n  observation\n1           1\n2           2\n3           3\n4           4\n5           5\n6           5\n7           6\n\n\n\ntail(select(df2,observation),n=1)\n\n  observation\n7           6\n\ntail(\n  select(\n      df2, \n      observation), \n  n=1)\n\n  observation\n7           6\n\ndf2 %>% select(observation) %>% tail(n=1)\n\n  observation\n7           6"
  },
  {
    "objectID": "weeks/week06/drop_in_jon.html#summarise-duplicated-groupings",
    "href": "weeks/week06/drop_in_jon.html#summarise-duplicated-groupings",
    "title": "DS202 2022MT W06 Drop In session",
    "section": "Summarise duplicated groupings",
    "text": "Summarise duplicated groupings\nCheck the idea of group_by (a tidyverse feature)\nsummarise and n() only works with groupings (group_by).\n\n# How many colours are there, per observation?\ndf2 %>% group_by(observation) %>% summarise(count=n())\n\n# A tibble: 6 × 2\n  observation count\n        <dbl> <int>\n1           1     1\n2           2     1\n3           3     1\n4           4     1\n5           5     2\n6           6     1\n\n\n\nHow many observations of df2 have at least one colour “blue”?\n\n# How many colours are there, per observation?\ndf2 %>% group_by(observation) %>% summarise(has_blue=any(colour == \"blue\"))\n\n# A tibble: 6 × 2\n  observation has_blue\n        <dbl> <lgl>   \n1           1 FALSE   \n2           2 FALSE   \n3           3 TRUE    \n4           4 TRUE    \n5           5 TRUE    \n6           6 FALSE   \n\n\n\n\nHow many observations of df1 have at least one colour “blue”?\n\n# How many colours are there, per observation?\ndf1 %>% group_by(observation) %>% summarise(has_blue=any(colour == \"blue\"))\n\n# A tibble: 6 × 2\n  observation has_blue\n        <dbl> <lgl>   \n1           1 TRUE    \n2           2 FALSE   \n3           3 FALSE   \n4           4 FALSE   \n5           5 TRUE    \n6           6 FALSE   \n\n\n\n\nHow do I compare these two new dataframes?\n\n# Store those results into variables\ndf1_summary <- \n    df1 %>% group_by(observation) %>% summarise(has_blue=any(colour == \"blue\"))\n\ndf2_summary <- \n    df2 %>% group_by(observation) %>% summarise(has_blue=any(colour == \"blue\"))\n\nBoth dataframes now have the same number of rows, representing the same “observations” and both have a column called has_blue. I can compare both like this:\n\nsum(df1_summary$has_blue & df2_summary$has_blue)\n\n[1] 1\n\n\nKeyword: Logical Operators\n& stands for an AND operation\n| stands for an OR operation\n! stands for a NOT operation\nRead more about it here\n\n\nAnother way to compare two dataframes\nUseful if the two dataframes are not aligned\n\noutput <- merge(df1, df2, by=c(\"observation\", \"colour\"))\noutput\n\n  observation colour\n1           2    red\n2           5   blue\n\n\n\nsum(output$colour == \"blue\")\n\n[1] 1"
  },
  {
    "objectID": "weeks/week06/drop_in_jon.html#remember-to-save-your-mutates",
    "href": "weeks/week06/drop_in_jon.html#remember-to-save-your-mutates",
    "title": "DS202 2022MT W06 Drop In session",
    "section": "Remember to save your mutates!",
    "text": "Remember to save your mutates!\n\n# mutate adds a new column to a dataframe\ndf2 %>% mutate(is_blue=colour == \"blue\")\n\n  observation colour is_blue\n1           1    red   FALSE\n2           2    red   FALSE\n3           3   blue    TRUE\n4           4   blue    TRUE\n5           5    red   FALSE\n6           5   blue    TRUE\n7           6  green   FALSE\n\n\nNote: mutate will add a new column but it will NOT update the dataframe. If you want to re-use the new column, you have to save the new dataframe:\n\n# df2 does not have a `is_blue` column\ndf2\n\n  observation colour\n1           1    red\n2           2    red\n3           3   blue\n4           4   blue\n5           5    red\n6           5   blue\n7           6  green\n\n\nIf I want to updated it to the SAME dataframe, I have to reassign it (using <-)\n\ndf2 <- df2 %>% mutate(is_blue=colour==\"blue\")\ndf2\n\n  observation colour is_blue\n1           1    red   FALSE\n2           2    red   FALSE\n3           3   blue    TRUE\n4           4   blue    TRUE\n5           5    red   FALSE\n6           5   blue    TRUE\n7           6  green   FALSE"
  },
  {
    "objectID": "weeks/week06/drop_in_jon.html#general-comparison-of-dataframes",
    "href": "weeks/week06/drop_in_jon.html#general-comparison-of-dataframes",
    "title": "DS202 2022MT W06 Drop In session",
    "section": "General comparison of dataframes",
    "text": "General comparison of dataframes\nBy manual inspection:\n\ntable(df1$colour) \n\n\n blue green   red \n    2     2     2 \n\n\n\ntable(df2$colour)\n\n\n blue green   red \n    3     1     3"
  },
  {
    "objectID": "weeks/week06/drop_in_jon.html#ggplot2",
    "href": "weeks/week06/drop_in_jon.html#ggplot2",
    "title": "DS202 2022MT W06 Drop In session",
    "section": "ggplot2",
    "text": "ggplot2\n\n- Colour histograms according to a category\nI will use iris as an example:\n\nlibrary(datasets)\ndata(iris)\niris\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1            5.1         3.5          1.4         0.2     setosa\n2            4.9         3.0          1.4         0.2     setosa\n3            4.7         3.2          1.3         0.2     setosa\n4            4.6         3.1          1.5         0.2     setosa\n5            5.0         3.6          1.4         0.2     setosa\n6            5.4         3.9          1.7         0.4     setosa\n7            4.6         3.4          1.4         0.3     setosa\n8            5.0         3.4          1.5         0.2     setosa\n9            4.4         2.9          1.4         0.2     setosa\n10           4.9         3.1          1.5         0.1     setosa\n11           5.4         3.7          1.5         0.2     setosa\n12           4.8         3.4          1.6         0.2     setosa\n13           4.8         3.0          1.4         0.1     setosa\n14           4.3         3.0          1.1         0.1     setosa\n15           5.8         4.0          1.2         0.2     setosa\n16           5.7         4.4          1.5         0.4     setosa\n17           5.4         3.9          1.3         0.4     setosa\n18           5.1         3.5          1.4         0.3     setosa\n19           5.7         3.8          1.7         0.3     setosa\n20           5.1         3.8          1.5         0.3     setosa\n21           5.4         3.4          1.7         0.2     setosa\n22           5.1         3.7          1.5         0.4     setosa\n23           4.6         3.6          1.0         0.2     setosa\n24           5.1         3.3          1.7         0.5     setosa\n25           4.8         3.4          1.9         0.2     setosa\n26           5.0         3.0          1.6         0.2     setosa\n27           5.0         3.4          1.6         0.4     setosa\n28           5.2         3.5          1.5         0.2     setosa\n29           5.2         3.4          1.4         0.2     setosa\n30           4.7         3.2          1.6         0.2     setosa\n31           4.8         3.1          1.6         0.2     setosa\n32           5.4         3.4          1.5         0.4     setosa\n33           5.2         4.1          1.5         0.1     setosa\n34           5.5         4.2          1.4         0.2     setosa\n35           4.9         3.1          1.5         0.2     setosa\n36           5.0         3.2          1.2         0.2     setosa\n37           5.5         3.5          1.3         0.2     setosa\n38           4.9         3.6          1.4         0.1     setosa\n39           4.4         3.0          1.3         0.2     setosa\n40           5.1         3.4          1.5         0.2     setosa\n41           5.0         3.5          1.3         0.3     setosa\n42           4.5         2.3          1.3         0.3     setosa\n43           4.4         3.2          1.3         0.2     setosa\n44           5.0         3.5          1.6         0.6     setosa\n45           5.1         3.8          1.9         0.4     setosa\n46           4.8         3.0          1.4         0.3     setosa\n47           5.1         3.8          1.6         0.2     setosa\n48           4.6         3.2          1.4         0.2     setosa\n49           5.3         3.7          1.5         0.2     setosa\n50           5.0         3.3          1.4         0.2     setosa\n51           7.0         3.2          4.7         1.4 versicolor\n52           6.4         3.2          4.5         1.5 versicolor\n53           6.9         3.1          4.9         1.5 versicolor\n54           5.5         2.3          4.0         1.3 versicolor\n55           6.5         2.8          4.6         1.5 versicolor\n56           5.7         2.8          4.5         1.3 versicolor\n57           6.3         3.3          4.7         1.6 versicolor\n58           4.9         2.4          3.3         1.0 versicolor\n59           6.6         2.9          4.6         1.3 versicolor\n60           5.2         2.7          3.9         1.4 versicolor\n61           5.0         2.0          3.5         1.0 versicolor\n62           5.9         3.0          4.2         1.5 versicolor\n63           6.0         2.2          4.0         1.0 versicolor\n64           6.1         2.9          4.7         1.4 versicolor\n65           5.6         2.9          3.6         1.3 versicolor\n66           6.7         3.1          4.4         1.4 versicolor\n67           5.6         3.0          4.5         1.5 versicolor\n68           5.8         2.7          4.1         1.0 versicolor\n69           6.2         2.2          4.5         1.5 versicolor\n70           5.6         2.5          3.9         1.1 versicolor\n71           5.9         3.2          4.8         1.8 versicolor\n72           6.1         2.8          4.0         1.3 versicolor\n73           6.3         2.5          4.9         1.5 versicolor\n74           6.1         2.8          4.7         1.2 versicolor\n75           6.4         2.9          4.3         1.3 versicolor\n76           6.6         3.0          4.4         1.4 versicolor\n77           6.8         2.8          4.8         1.4 versicolor\n78           6.7         3.0          5.0         1.7 versicolor\n79           6.0         2.9          4.5         1.5 versicolor\n80           5.7         2.6          3.5         1.0 versicolor\n81           5.5         2.4          3.8         1.1 versicolor\n82           5.5         2.4          3.7         1.0 versicolor\n83           5.8         2.7          3.9         1.2 versicolor\n84           6.0         2.7          5.1         1.6 versicolor\n85           5.4         3.0          4.5         1.5 versicolor\n86           6.0         3.4          4.5         1.6 versicolor\n87           6.7         3.1          4.7         1.5 versicolor\n88           6.3         2.3          4.4         1.3 versicolor\n89           5.6         3.0          4.1         1.3 versicolor\n90           5.5         2.5          4.0         1.3 versicolor\n91           5.5         2.6          4.4         1.2 versicolor\n92           6.1         3.0          4.6         1.4 versicolor\n93           5.8         2.6          4.0         1.2 versicolor\n94           5.0         2.3          3.3         1.0 versicolor\n95           5.6         2.7          4.2         1.3 versicolor\n96           5.7         3.0          4.2         1.2 versicolor\n97           5.7         2.9          4.2         1.3 versicolor\n98           6.2         2.9          4.3         1.3 versicolor\n99           5.1         2.5          3.0         1.1 versicolor\n100          5.7         2.8          4.1         1.3 versicolor\n101          6.3         3.3          6.0         2.5  virginica\n102          5.8         2.7          5.1         1.9  virginica\n103          7.1         3.0          5.9         2.1  virginica\n104          6.3         2.9          5.6         1.8  virginica\n105          6.5         3.0          5.8         2.2  virginica\n106          7.6         3.0          6.6         2.1  virginica\n107          4.9         2.5          4.5         1.7  virginica\n108          7.3         2.9          6.3         1.8  virginica\n109          6.7         2.5          5.8         1.8  virginica\n110          7.2         3.6          6.1         2.5  virginica\n111          6.5         3.2          5.1         2.0  virginica\n112          6.4         2.7          5.3         1.9  virginica\n113          6.8         3.0          5.5         2.1  virginica\n114          5.7         2.5          5.0         2.0  virginica\n115          5.8         2.8          5.1         2.4  virginica\n116          6.4         3.2          5.3         2.3  virginica\n117          6.5         3.0          5.5         1.8  virginica\n118          7.7         3.8          6.7         2.2  virginica\n119          7.7         2.6          6.9         2.3  virginica\n120          6.0         2.2          5.0         1.5  virginica\n121          6.9         3.2          5.7         2.3  virginica\n122          5.6         2.8          4.9         2.0  virginica\n123          7.7         2.8          6.7         2.0  virginica\n124          6.3         2.7          4.9         1.8  virginica\n125          6.7         3.3          5.7         2.1  virginica\n126          7.2         3.2          6.0         1.8  virginica\n127          6.2         2.8          4.8         1.8  virginica\n128          6.1         3.0          4.9         1.8  virginica\n129          6.4         2.8          5.6         2.1  virginica\n130          7.2         3.0          5.8         1.6  virginica\n131          7.4         2.8          6.1         1.9  virginica\n132          7.9         3.8          6.4         2.0  virginica\n133          6.4         2.8          5.6         2.2  virginica\n134          6.3         2.8          5.1         1.5  virginica\n135          6.1         2.6          5.6         1.4  virginica\n136          7.7         3.0          6.1         2.3  virginica\n137          6.3         3.4          5.6         2.4  virginica\n138          6.4         3.1          5.5         1.8  virginica\n139          6.0         3.0          4.8         1.8  virginica\n140          6.9         3.1          5.4         2.1  virginica\n141          6.7         3.1          5.6         2.4  virginica\n142          6.9         3.1          5.1         2.3  virginica\n143          5.8         2.7          5.1         1.9  virginica\n144          6.8         3.2          5.9         2.3  virginica\n145          6.7         3.3          5.7         2.5  virginica\n146          6.7         3.0          5.2         2.3  virginica\n147          6.3         2.5          5.0         1.9  virginica\n148          6.5         3.0          5.2         2.0  virginica\n149          6.2         3.4          5.4         2.3  virginica\n150          5.9         3.0          5.1         1.8  virginica\n\n\nGenerate a histogram of Petal.Length:\n\ng <- (\n  ggplot(iris, aes(x=Petal.Length))\n  \n  + geom_histogram()\n  \n  # Customize\n  + theme_minimal()\n)\n\ng\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nHow do I colour the histogram according to the Species?\n\ng <- (\n  ggplot(iris, aes(x=Petal.Length, fill=Species))\n  \n  + geom_histogram()\n  \n  # Customize\n  + theme_minimal()\n)\n\ng\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nEach geom_ “listens” to a different set of aesthetics. (Check Chapter 3 of R for Data Science for more info)\nFor example, geom_point does not understand the fill :\n\ng <- (\n  ggplot(iris, aes(x=Petal.Length, y=Petal.Width, fill=Species))\n  \n  + geom_point()\n  \n  # Customize\n  + theme_minimal()\n)\n\ng\n\n\n\n\n\ng <- (\n  ggplot(iris, aes(x=Petal.Length, y=Petal.Width, colour=Species))\n  \n  + geom_point()\n  \n  # Customize\n  + theme_minimal()\n)\n\ng\n\n\n\n\n\n\nCustomizing the colours\n\nManually\n\nmy_favourite_colours = c(\"#5bc0de\", \"#d9534f\",  \"#ffbf00\")\n\nPlaces to find colours: https://www.color-hex.com/color-palettes/popular.php\n\ng <- (\n  ggplot(iris, aes(x=Petal.Length, y=Petal.Width, colour=Species))\n  \n  + geom_point()\n\n  \n  # Customize\n  + theme_minimal()\n  \n  + scale_colour_manual(values=my_favourite_colours) # this is where you customize it\n)\n\ng\n\n\n\n\n\n\nBuilt-in palettes\nYou can use built-in palettes, you just need to know their names/numbers.\n\nCheck the documentation https://ggplot2.tidyverse.org/reference/scale_brewer.html -> for the different settings\nTo understand which colour palettes are available, check: https://colorbrewer2.org/\n\n\ng <- (\n  ggplot(iris, aes(x=Petal.Length, y=Petal.Width, colour=Species))\n  \n  + geom_point()\n\n  \n  # Customize\n  + theme_minimal()\n  \n  + scale_colour_brewer(type=\"qual\", palette=2) # Bult-in palette of colours for the aesthetic `colour`, as given by the Colour Brewer\n)\n\ng\n\n\n\n\n\ndf1$source <- \"df1\"\ndf2$source <- \"df2\"\n\n\nggplot(bind_rows(df1, df2), aes(x=colour)) +\n  geom_bar() + facet_wrap(~ source)"
  },
  {
    "objectID": "weeks/week06/drop_in_jon.html#faceting",
    "href": "weeks/week06/drop_in_jon.html#faceting",
    "title": "DS202 2022MT W06 Drop In session",
    "section": "Faceting",
    "text": "Faceting\nUseful when you want to plot two charts in the same image.\nObservation: you might need to combine (append) the two dataframes first. Use the tidyverse function bind_rows (same as rbind)\n\ng <- (\n  ggplot(iris, aes(x=Petal.Length, fill=Species))\n  \n  + geom_histogram()\n\n  \n  # Customize\n  + theme_bw()\n  \n  + scale_colour_manual(values=my_favourite_colours) \n  \n  + facet_grid(Species ~ .) #This is what does it\n)\n\ng\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "weeks/week06/drop_in_jon.html#about-tidyverse",
    "href": "weeks/week06/drop_in_jon.html#about-tidyverse",
    "title": "DS202 2022MT W06 Drop In session",
    "section": "About tidyverse",
    "text": "About tidyverse\ntidyverse is a set of R packages that have several functions and facilities for working with data. I find tidyverse more intuitive than base R, and there’s an entire book available for free online (R for Data Science) that contains a lot of helpful tutorials about tidyverse. Let me point to a few specific chapters:\n\nYou might need or want to transform data when working on your problem sets. Check out Chapter 5 of R for Data Science online book.\nData visualization is another helpful skill. You can learn a bit more about ggplot — the tidyverse way of making plots — in Chapter 3 of R for Data Science online book.\nWhat should you be looking for when working with data? Check Chapter 7 to learn the basics of exploratory data analysis.\nDo you get confused about R Markdown, the idea of “knitting” a file? Then read Chapter 27.\n(More advanced) Do you want to learn how to reshape data or deal with more complex data manipulation? Then have a look at Chapter 12 - Tidy data and Chapter 13 - Iteration.\nAre you already familiar with tidyverse, but you constantly need to Google how to do things? Save these cheatsheets to your computer."
  },
  {
    "objectID": "weeks/week07/checklist.html",
    "href": "weeks/week07/checklist.html",
    "title": "✅ Week 07 - Checklist",
    "section": "",
    "text": "Sadly, I didn’t have the time this week to suggest a checklist for you!"
  },
  {
    "objectID": "weeks/week07/lab.html",
    "href": "weeks/week07/lab.html",
    "title": "💻 Week 07 - Lab Roadmap (90 min)",
    "section": "",
    "text": "📥 Download the RMarkdown version of this roadmap from Moodle.\nThis lab session draws on 🗓️ Week 05 lecture content and on feedback given by the course representatives about the main struggles you are facing with R (tidyverse).\nIf you are already very familiar with tidyverse, you can skip to Step 2.\nR packages you will need:\nYou might have already installed some of these packages. Some were used in 🗓️ Week 05 slides, others in previous labs. If you see an error like “package not found”, then install the package using install.packages(\"<package name>\")."
  },
  {
    "objectID": "weeks/week07/lab.html#step-1.1-the-tibble-5-min",
    "href": "weeks/week07/lab.html#step-1.1-the-tibble-5-min",
    "title": "💻 Week 07 - Lab Roadmap (90 min)",
    "section": "Step 1.1: The tibble (5 min)",
    "text": "Step 1.1: The tibble (5 min)\nSo far, we have been working with data frames. We will introduce you to a different kind of data frame called a tibble. Tibbles are like data frames, only they have a more descriptive print function, and you can perform more advanced tasks like looping over lists (without needing to specify a for loop).\nLet’s start by converting our now familiar Boston data to a tibble using as_tibble:\n\nboston <- as_tibble(Boston)\nboston\n\nInstead of printing a lot of rows, we only get to see the first ten rows.\nWe can also see the dimensions of the data 506 x 13 and the class of each variable.\nSo with one command, we can get a lot more useful information on our data without the need for multiple commands."
  },
  {
    "objectID": "weeks/week07/lab.html#step-1.2-basic-dplyr-verbs-10-minutes",
    "href": "weeks/week07/lab.html#step-1.2-basic-dplyr-verbs-10-minutes",
    "title": "💻 Week 07 - Lab Roadmap (90 min)",
    "section": "Step 1.2: Basic dplyr verbs (10 minutes)",
    "text": "Step 1.2: Basic dplyr verbs (10 minutes)\nIn previous labs, we have been using base R to subset data based on the rows and columns, to create new variables, and to create summary statistics. However, there is a more verbal way of performing these tasks using the tidyverse. We will introduce you to several key verbs, namely filter, select, mutate and summarise.\nAuto cylinders\nSuppose we wanted to select only the rows of the Auto data for cars with 4 cylinders. We can achieve this using the following base R command:\n\nAuto[Auto$cylinders == 4, ]\n\nNow here is the tidyverse solution using filter.\n\nfilter(Auto, cylinders == 4)\n\nBoston columns\nNext, let’s only include lstat and medv from Boston:\n\nBoston[, c('medv','lstat')]\n\nNow here is the tidyverse solution using select:\n\nselect(Boston, medv, lstat)\n\nCarseats median sales\nNow that we can subset variables, let’s create some new ones. Let’s create a dummy variable SalesGTMedian for each car seat in Carseats:\n\nCarseats$SalesGTMedian <- if_else(Carseats$Sales > median(Carseats$Sales), TRUE, FALSE)\n\nNow here is the tidyverse solution using mutate:\n\nmutate(Carseats, SalesGTMedian = if_else(Sales > median(Sales), TRUE, FALSE))\n\nMissing data in the Hitters dataset\nFinally, suppose we wanted to find the average Salary in Hitters (a basketball data set). We specify na.rm = TRUE to get R to ignore all the missing values.\n\nmean(Hitters$Salary, na.rm = TRUE)\n\nNow here is the tidyverse solution using summarise.\n\nsummarise(Hitters, mean_salary = mean(Salary, na.rm = TRUE))\n\nCategorical variables in Default dataset\nSome of our variables will be categories, so let’s find out the distribution of defaults in Default:\n\ntable(Default$default)\n\nNow here is the tidyverse solution using count:\n\ncount(Default, default)\n\n💡 REFLECTION TIME\nLet’s pause for a minute to see the advantages of these commands:\n\nthe commands themselves give a better indication of what it is we are trying to do. This is highly advantageous when it comes to communicating our code with others.\nwhen working with variables in data frames, we do not need to use $. Instead, we can just reference the variable on its own, provided we pass the command the data frame.\nevery time we use these verbs, a new data frame is created - meaning we can use the output to create ggplots!"
  },
  {
    "objectID": "weeks/week07/lab.html#step-1.3-the-pipe-10-minutes",
    "href": "weeks/week07/lab.html#step-1.3-the-pipe-10-minutes",
    "title": "💻 Week 07 - Lab Roadmap (90 min)",
    "section": "Step 1.3 The pipe (10 minutes)",
    "text": "Step 1.3 The pipe (10 minutes)\nYou may have seen %>% in some of our code. This is known as the pipe operator, and it enables us to chain together multiple verbs into one fluid sequence of steps. To get this quickly you can use ctrl+shift+m (for Windows users) or command+shift+m (for Mac users).\nSuppose we want to find out what proportion of American cars had mpg above the global average. We can find this out by using the following sequence of commands:\n\nAuto %>% \n  as_tibble() %>% \n  select(mpg, origin) %>% \n  mutate(mpg_gt_gavg = if_else(mpg > mean(mpg), TRUE, FALSE)) %>% \n  filter(origin == 1) %>% \n  summarise(prop_mpg_gt_gavg = mean(mpg_gt_gavg))\n\n# A tibble: 1 × 1\n  prop_mpg_gt_gavg\n             <dbl>\n1            0.269\n\n\nLet’s walk through what we just did:\n\nWe converted Auto to a tibble.\nWe selected only the variables of interest mpg and origin. (If you are working with larger data sets, removing superfluous columns can be an advantage.)\nWe create a new variable mpg_gt_gaverage which finds out whether an automobile has an MPG greater than the global average.\nWe filter all rows to only include American cars.\nWe summarised the data calculate, which helped us find that only 27 percent of American-made cars had MPGs greater than the global average.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nThink of the pipe as “take this data and do something with it”\n\n\n\n\n\n\n\n\n\nHow would I do this in base R?\n\n\n\n\n\nLet’s look at how to recreate this using base R.\n\nAuto_cleaned <- Auto[,c('mpg','origin')]\n\nAuto_cleaned$mpg_gt_gavg <- if_else(Auto_cleaned$mpg > mean(Auto_cleaned$mpg), TRUE, FALSE) \n\nAuto_cleaned <- Auto_cleaned[Auto_cleaned$origin == 1, ]\n\ndata.frame(prop_mpg_gt_gavg = mean(Auto_cleaned$mpg_gt_gavg))\n\n  prop_mpg_gt_gavg\n1        0.2693878\n\n\nWhile this code is technically correct, notice a few things. We need to keep updating the same object to save our results. Our code is disjointed and difficult to understand. The final product is also less satisfactory: we needed to convert it from a vector to a data frame, which displays no information on the class of prop_mpg_gt_gavg. That is, perhaps, why R can appear so confusing for first-time learners."
  },
  {
    "objectID": "weeks/week07/lab.html#step-1.4-practice-15-min",
    "href": "weeks/week07/lab.html#step-1.4-practice-15-min",
    "title": "💻 Week 07 - Lab Roadmap (90 min)",
    "section": "Step 1.4 PRACTICE! (15 min)",
    "text": "Step 1.4 PRACTICE! (15 min)\nNow let’s use some of these skills to classify Chinstrap penguins. This is data from the package palmerpenguins 1.\n\n# Importing this library will make `penguins` data available\nlibrary(palmerpenguins) \n\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  <fct>   <fct>              <dbl>         <dbl>       <int>   <int> <fct> <int>\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n2 Adelie  Torgersen           39.5          17.4         186    3800 fema…  2007\n3 Adelie  Torgersen           40.3          18           195    3250 fema…  2007\n4 Adelie  Torgersen           NA            NA            NA      NA <NA>   2007\n5 Adelie  Torgersen           36.7          19.3         193    3450 fema…  2007\n6 Adelie  Torgersen           39.3          20.6         190    3650 male   2007\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n\n🎯 ACTION POINTS\nUse the verbs you learned above to modify the original data set and create a new one, call it penguins_cleaned, according to the following steps:\n\nRemove any observations with missing data. 💡You can pipe na.omit() into you sequence of commands to achieve this.\nNext, create a binary variable, call it chinstrap that stores TRUE if the penguin is a Chinstrap, FALSE otherwise.\nNow, filter the dataset and keep only the following variables:\n\nis_chinstrap (our outcome of interest)\nbill_length_mm\nbill_depth_mm\n\n\nCan you do it without looking at the solution?\n\n\n\n\n\n\nClick here to view the solution\n\n\n\n\n\n\npenguins_cleaned <-\n  penguins %>% \n  na.omit() %>% \n  mutate(is_chinstrap = if_else(species == 'Chinstrap', TRUE, FALSE)) %>% \n  select(is_chinstrap, bill_length_mm, bill_depth_mm)\n\n\n\n\nIf you did it right, you should be able to run the code below and get same plot as shown here in the page:\n\npenguins_cleaned %>% \n  ggplot(aes(bill_length_mm, bill_depth_mm, colour=is_chinstrap)) +\n  geom_point(size=2.5, stroke=1.4, alpha=0.8, shape=21) +\n\n  # (Optional) customizing the plot\n  theme_bw()\n\n\n\n\nWe can see that Chinstrap penguins tend to have above average bill length and depth whereas the other two species of penguins tend to either have shallow yet long or deep yet short bills.\n\n\n\n\n\n\n“What if I struggle to understand the pipe and these”verbs” above?“\n\n\n\n\n\nThe R for Data Science book is a great resource to learn more about the tidyverse. For more guided tips related to our course, check section Useful Links about tidyverse of the ✅ Week 06 - Checklist."
  },
  {
    "objectID": "weeks/week07/lab.html#step-2.1-train-and-visualise-a-decision-tree-10-mins",
    "href": "weeks/week07/lab.html#step-2.1-train-and-visualise-a-decision-tree-10-mins",
    "title": "💻 Week 07 - Lab Roadmap (90 min)",
    "section": "Step 2.1: Train and visualise a Decision Tree (10 mins)",
    "text": "Step 2.1: Train and visualise a Decision Tree (10 mins)\nSince we know the data we are modelling has a nonlinear relationship, let’s train a Decision Tree to classify Chinstrap penguins. R does not come with Decision Trees installed, so we need to import it from a library. Here we will use the function rpart from the rpart package and rpart.plot from the rpart.plot package.\nHere are a few things to know about the rpart function:\n\nThe rpart command is largely similar to other commands such as lm and glm in that the first parameter is a formula and the second is the data set.\nWe have to add method = class to tell the algorithm that we are performing a classification task.\n\n\ntree.model <- rpart(is_chinstrap ~ ., data = penguins_cleaned, method = 'class')\n\nrpart.plot(tree.model)\n\n\n\n\n🤝 WORKING TOGETHER In pairs, discuss what you see in the plot.\n\nWhat do each node represents?\nWhat are the numbers inside the nodes?\nWhat do the percentages represent?\n\nThe video below explains how to read the numbers inside the nodes of the decision tree\n\n\n\n\nAlternative visualisation with parttree package\nAlternatively, you could visualise a decision tree as a partition tree plot. For this, you will need to have the parttree package installed (check instructions at the top of the page).\n\nggplot() +\n  geom_point(data = penguins_cleaned, \n             aes(bill_length_mm, bill_depth_mm, colour = is_chinstrap)) +\n  geom_parttree(data = tree.model, aes(fill = factor(is_chinstrap)), alpha = 0.25) +\n  theme_minimal() +\n  theme(panel.grid = element_blank(),\n        legend.position = 'bottom') +\n  scale_colour_lancet() +\n  scale_fill_lancet() +\n  labs(x = 'Bill length (mm)', y = 'Bill depth (mm)',\n       colour = 'Is actually chinstrap?', \n       fill = 'Is predicted chinstrap?')\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\nDon’t understand what the code above does? You might want to read Chapter 3 of R for Data Science to review ggplot."
  },
  {
    "objectID": "weeks/week07/lab.html#step-2.2-goodness-of-fit-of-the-decision-tree-10-mins",
    "href": "weeks/week07/lab.html#step-2.2-goodness-of-fit-of-the-decision-tree-10-mins",
    "title": "💻 Week 07 - Lab Roadmap (90 min)",
    "section": "Step 2.2: Goodness-of-Fit of the Decision Tree (10 mins)",
    "text": "Step 2.2: Goodness-of-Fit of the Decision Tree (10 mins)\nLet’s investigate how well our model fits the data. Let’s reuse the model we trained (tree.model) and predict the same samples we used to train it. To avoid modifying our original dataframe, let’s save the output of the prediction in an auxiliary df (plot_df):\n\nplot_df <- \n    penguins_cleaned %>% \n    mutate(class_pred = predict(tree.model, newdata = ., type=\"class\"),\n           correct    = class_pred == is_chinstrap)\nplot_df\n\n🤝 WORKING TOGETHER In pairs, discuss the following:\n\nExplain what you see in the output of the chunk of code above.\nWhat does the code above do?\n\nSimple confusion matrix:\n\nconfusion_matrix <- \n    table(expected=plot_df$is_chinstrap, class_pred=plot_df$class_pred)\nprint(confusion_matrix)\n\nNicer looking confusion matrix:\n\nlibrary(cvms)\n\nplot_confusion_matrix(as_tibble(confusion_matrix), \n                      target_col = \"expected\", \n                      prediction_col = \"class_pred\",\n                      \n                      # Customizing the plot\n                      add_normalized = TRUE,\n                      add_col_percentages = FALSE,\n                      add_row_percentages = FALSE,\n                      counts_col = \"n\",\n                      )"
  },
  {
    "objectID": "weeks/week07/lab.html#step-2.3-control-the-parameters-15-min",
    "href": "weeks/week07/lab.html#step-2.3-control-the-parameters-15-min",
    "title": "💻 Week 07 - Lab Roadmap (90 min)",
    "section": "Step 2.3 Control the parameters! (15 min)",
    "text": "Step 2.3 Control the parameters! (15 min)\nWe can tweak how the tree is built by controlling for certain parameters of the algorithm. To take a look at all possible parameters you can control, open the R console, type the command below and hit ENTER:\n?rpart.control\nFor example, let’s reduce the minbucket parameter:\n\ntree.model <- rpart(is_chinstrap ~ ., data = penguins_cleaned, method = 'class', control=list(minbucket=1))\n\nrpart.plot(tree.model)\n\n\n\n\n🤝 WORKING TOGETHER In pairs, discuss the following:\n\nExplain what is different in this model.\nVisualize the new tree using the parttree package (reuse from Step 2.1)"
  },
  {
    "objectID": "weeks/week07/lab.html#step-2.4-practice-15-min",
    "href": "weeks/week07/lab.html#step-2.4-practice-15-min",
    "title": "💻 Week 07 - Lab Roadmap (90 min)",
    "section": "Step 2.4 PRACTICE! (15 min)",
    "text": "Step 2.4 PRACTICE! (15 min)\n🎯 ACTION POINTS\n\nBuild new tree models, this time trying out different control parameters.\nWhich parameters led to different trees?\nWhich parameters change the tree the most?\n\n\n🔜 Next week, we will continue our journey of supervised learning by exploring the Support Vector Machine algorithm. We will also visit once again the topic of resampling (cross-validation) to select the optimal parameters for classifiers and regressors."
  },
  {
    "objectID": "weeks/week07/lab.html#q1-selecting-columns",
    "href": "weeks/week07/lab.html#q1-selecting-columns",
    "title": "💻 Week 07 - Lab Roadmap (90 min)",
    "section": "Q1: Selecting columns",
    "text": "Q1: Selecting columns\nUsing the Bikeshare data set present in ISLR2 subset the data to only include mnth, holiday, and bikers columns.\n\n# Your code goes here"
  },
  {
    "objectID": "weeks/week07/lab.html#q2-a-new-tidyverse-verb",
    "href": "weeks/week07/lab.html#q2-a-new-tidyverse-verb",
    "title": "💻 Week 07 - Lab Roadmap (90 min)",
    "section": "Q2: A new tidyverse verb",
    "text": "Q2: A new tidyverse verb\nUsing the Bikeshare data set present in ISLR2, replicate Step 1.3, only this time replace filter(origin == 1) with group_by(origin). How have the results changed?\n\n# Your code goes here\n\n\nYour text goes here"
  },
  {
    "objectID": "weeks/week07/lab.html#q3-exploratory-data-analysis-part-i",
    "href": "weeks/week07/lab.html#q3-exploratory-data-analysis-part-i",
    "title": "💻 Week 07 - Lab Roadmap (90 min)",
    "section": "Q3: Exploratory Data Analysis (Part I)",
    "text": "Q3: Exploratory Data Analysis (Part I)\nCalculate the average daily number of bikers in March, in the Bikeshare data set.\n\n# Your code goes here"
  },
  {
    "objectID": "weeks/week07/lab.html#q4-exploratory-data-analysis-part-ii0",
    "href": "weeks/week07/lab.html#q4-exploratory-data-analysis-part-ii0",
    "title": "💻 Week 07 - Lab Roadmap (90 min)",
    "section": "Q4: Exploratory Data Analysis (Part II0",
    "text": "Q4: Exploratory Data Analysis (Part II0\nDo people bike more during holiday seasons?\n\n# Your code goes here\n\n\nYour text goes here"
  },
  {
    "objectID": "weeks/week07/lab.html#q5-back-to-penguins",
    "href": "weeks/week07/lab.html#q5-back-to-penguins",
    "title": "💻 Week 07 - Lab Roadmap (90 min)",
    "section": "Q5: Back to penguins…",
    "text": "Q5: Back to penguins…\nLet’s go back to the penguins data set. Create a new dataframe that omits missing values (NAs), remove the island and year columns but keep the rest of the dataset intact (don’t create new columns).\n\n# Your code goes here"
  },
  {
    "objectID": "weeks/week07/lab.html#q6.-predict-penguin-species",
    "href": "weeks/week07/lab.html#q6.-predict-penguin-species",
    "title": "💻 Week 07 - Lab Roadmap (90 min)",
    "section": "Q6. Predict Penguin species",
    "text": "Q6. Predict Penguin species\nBuild a Decision Tree model to predict the species of penguins. Note that the outcome will not be a binary variable this time.\n\n# Your code goes here"
  },
  {
    "objectID": "weeks/week07/lab.html#q7.-control-parameters",
    "href": "weeks/week07/lab.html#q7.-control-parameters",
    "title": "💻 Week 07 - Lab Roadmap (90 min)",
    "section": "Q7. Control parameters",
    "text": "Q7. Control parameters\nThe decision tree algorithm we are using sets the cost complexity parameter (cp) by default as control = list(cp = 0.01). Build a new model with a smaller cp value, say control = list(cp = 0.001). Does this increase or reduce the complexity of the tree?\n\n# Your code goes here\n\n\nYour text goes here"
  },
  {
    "objectID": "weeks/week07/lab_solutions.html",
    "href": "weeks/week07/lab_solutions.html",
    "title": "✔️ Week 07 - Lab Solutions",
    "section": "",
    "text": "This lab session draws on 🗓️ Week 05 lecture content and on feedback given by the course representatives about the main struggles you are facing with R (tidyverse).\nIf you are already very familiar with tidyverse, you can skip to Step 2.\nR packages you will need:\nYou might have already installed some of these packages. Some were used in 🗓️ Week 05 slides, others in previous labs. If you see an error like “package not found”, then install the package using install.packages(\"<package name>\").\nInstallation instructions for parttree\nThe package parttree cannot be installed by install.packages. Instead, we have to follow the instructions set by the developers of the package:"
  },
  {
    "objectID": "weeks/week07/lab_solutions.html#q1-selecting-columns",
    "href": "weeks/week07/lab_solutions.html#q1-selecting-columns",
    "title": "✔️ Week 07 - Lab Solutions",
    "section": "Q1: Selecting columns",
    "text": "Q1: Selecting columns\nUsing the Bikeshare data set present in ISLR2 subset the data to only include mnth, holiday, and bikers columns.\n\nBikeshare %>% select(mnth, holiday, bikers) %>% head()\n\n  mnth holiday bikers\n1  Jan       0     16\n2  Jan       0     40\n3  Jan       0     32\n4  Jan       0     13\n5  Jan       0      1\n6  Jan       0      1"
  },
  {
    "objectID": "weeks/week07/lab_solutions.html#q2-a-new-tidyverse-verb",
    "href": "weeks/week07/lab_solutions.html#q2-a-new-tidyverse-verb",
    "title": "✔️ Week 07 - Lab Solutions",
    "section": "Q2: A new tidyverse verb",
    "text": "Q2: A new tidyverse verb\nUsing the Auto data set present in ISLR2, replicate Step 1.3, only this time replace filter(origin == 1) with group_by(origin). How have the results changed?\n\nAuto %>% \n  as_tibble() %>% \n  select(mpg, origin) %>% \n  mutate(mpg_gt_gavg = if_else(mpg > mean(mpg), TRUE, FALSE)) %>% \n  group_by(origin) %>% \n  summarise(prop_mpg_gt_gavg = mean(mpg_gt_gavg))\n\n# A tibble: 3 × 2\n  origin prop_mpg_gt_gavg\n   <int>            <dbl>\n1      1            0.269\n2      2            0.75 \n3      3            0.873\n\n\n\nA: Instead of looking just at the origin=1, we have now summarized the data and computed prop_mpg_gt_gavg (Proportion of samples for which mpg is greater than average) for each unique value of origin."
  },
  {
    "objectID": "weeks/week07/lab_solutions.html#q3-exploratory-data-analysis-part-i",
    "href": "weeks/week07/lab_solutions.html#q3-exploratory-data-analysis-part-i",
    "title": "✔️ Week 07 - Lab Solutions",
    "section": "Q3: Exploratory Data Analysis (Part I)",
    "text": "Q3: Exploratory Data Analysis (Part I)\nCalculate the average daily number of bikers in March, in the Bikeshare data set.\n\nBikeshare %>% \n  filter(mnth == \"March\") %>% \n  group_by(day) %>% \n  summarise(daily_bikers=n()) %>% \n  summarise(avg_daily_bikers=mean(daily_bikers))\n\n# A tibble: 1 × 1\n  avg_daily_bikers\n             <dbl>\n1             23.5"
  },
  {
    "objectID": "weeks/week07/lab_solutions.html#q4-exploratory-data-analysis-part-ii0",
    "href": "weeks/week07/lab_solutions.html#q4-exploratory-data-analysis-part-ii0",
    "title": "✔️ Week 07 - Lab Solutions",
    "section": "Q4: Exploratory Data Analysis (Part II0",
    "text": "Q4: Exploratory Data Analysis (Part II0\nDo people bike more during holiday seasons?\n\nBikeshare %>% \n  group_by(holiday, day) %>% \n  summarise(daily_bikers=n()) %>%\n  summarise(mean_daily_bikers=mean(daily_bikers),\n            std_daily_bikers=sd(daily_bikers))\n\n# A tibble: 2 × 3\n  holiday mean_daily_bikers std_daily_bikers\n    <dbl>             <dbl>            <dbl>\n1       0              23.7            1.32 \n2       1              23.9            0.316\n\n\n\nJust by summarising the data, it’s not really possible to say. On average, the mean number of daily bikers seems to be about the same, just with a smaller standard deviation."
  },
  {
    "objectID": "weeks/week07/lab_solutions.html#q5-back-to-penguins",
    "href": "weeks/week07/lab_solutions.html#q5-back-to-penguins",
    "title": "✔️ Week 07 - Lab Solutions",
    "section": "Q5: Back to penguins…",
    "text": "Q5: Back to penguins…\nLet’s go back to the penguins data set. Create a new dataframe that omits missing values (NAs), remove the island and year columns but keep the rest of the dataset intact (don’t create new columns).\n\nnew_penguins <- \n  penguins %>% \n  select(-c(island, year))\nnew_penguins\n\n# A tibble: 344 × 6\n   species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   <fct>            <dbl>         <dbl>             <int>       <int> <fct> \n 1 Adelie            39.1          18.7               181        3750 male  \n 2 Adelie            39.5          17.4               186        3800 female\n 3 Adelie            40.3          18                 195        3250 female\n 4 Adelie            NA            NA                  NA          NA <NA>  \n 5 Adelie            36.7          19.3               193        3450 female\n 6 Adelie            39.3          20.6               190        3650 male  \n 7 Adelie            38.9          17.8               181        3625 female\n 8 Adelie            39.2          19.6               195        4675 male  \n 9 Adelie            34.1          18.1               193        3475 <NA>  \n10 Adelie            42            20.2               190        4250 <NA>  \n# … with 334 more rows"
  },
  {
    "objectID": "weeks/week07/lab_solutions.html#q6.-predict-penguin-species",
    "href": "weeks/week07/lab_solutions.html#q6.-predict-penguin-species",
    "title": "✔️ Week 07 - Lab Solutions",
    "section": "Q6. Predict Penguin species",
    "text": "Q6. Predict Penguin species\nBuild a Decision Tree model to predict the species of penguins. Note that the outcome will not be a binary variable this time.\n\nset.seed(1)\n\ntree.model <- rpart(species ~ ., data = new_penguins, method = 'class')\n\nrpart.plot(tree.model)\n\n\n\n\n\nNote that we three different proportions in the middle of the nodes now (instead of just one). Each represent the proportion of penguins of a specific specie.\nFor example, the left-most leaf node shows: 0.97 0.03 0.00. That means: 97% of the samples that fall into that part of the decision tree are of the Adelie species, 3% are of the Chinstrap species and 0% are of the Gentoo species.\nHow do we know the order of the species represented by these numbers? Check the levels of the column species (which is a column of the type factor). For more on factors read: https://r4ds.had.co.nz/factors.html\n\n\nlevels(new_penguins$species)\n\n[1] \"Adelie\"    \"Chinstrap\" \"Gentoo\"   \n\n\nThe video below explains how to read the numbers inside the nodes of the decision tree"
  },
  {
    "objectID": "weeks/week07/lab_solutions.html#q7.-control-parameters",
    "href": "weeks/week07/lab_solutions.html#q7.-control-parameters",
    "title": "✔️ Week 07 - Lab Solutions",
    "section": "Q7. Control parameters",
    "text": "Q7. Control parameters\nThe decision tree algorithm we are using sets the cost complexity parameter (cp) by default as control = list(cp = 0.01). Build a new model with a smaller cp value, say control = list(cp = 0.001). Does this increase or reduce the complexity of the tree?\n\nA smaller cp parameter would force the decision tree to try to find a more complex model (more branches) if that leads to a better overall model. (If you want to understand how the rpart method chooses to make the splits and branches of the tree, type ?rpart in the console and read the section about method).\n\nIt turns out that nothing will make this model more complex! Even if you set a very low cp, the tree stays the same:\n\n\nset.seed(1)\n\ntree.model <- rpart(species ~ ., data = new_penguins, method = 'class', control=list(cp=0.00000000000001))\n\nrpart.plot(tree.model)\n\n\n\n\n\nOn the other hand, it is possible to simplify the tree (fewer branches) if we increase cp parameter:\n\n\nset.seed(1)\n\ntree.model <- rpart(species ~ ., data = new_penguins, method = 'class', control=list(cp=0.4))\n\nrpart.plot(tree.model)\n\n\n\n\n\nIt all depends on what we care about and which metrics we will use to assess this model and we plan to use the model later on. But it looks like the above will underfit the data: one of the species is not even represented there."
  },
  {
    "objectID": "weeks/week07/lecture.html",
    "href": "weeks/week07/lecture.html",
    "title": "👨‍🏫 Week 07 - Lecture",
    "section": "",
    "text": "Either click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides. For a PDF copy, check Moodle."
  },
  {
    "objectID": "weeks/week07/lecture.html#coffee-break-10-min",
    "href": "weeks/week07/lecture.html#coffee-break-10-min",
    "title": "👨‍🏫 Week 07 - Lecture",
    "section": "☕ Coffee Break (10 min)",
    "text": "☕ Coffee Break (10 min)\nUse this time to chat, stretch, drink some coffee or just relax for a bit by yourself."
  },
  {
    "objectID": "weeks/week07/lecture.html#part-ii---unsupervised-learning-clustering-45-50-min",
    "href": "weeks/week07/lecture.html#part-ii---unsupervised-learning-clustering-45-50-min",
    "title": "👨‍🏫 Week 07 - Lecture",
    "section": "Part II - Unsupervised Learning (Clustering) (45-50 min)",
    "text": "Part II - Unsupervised Learning (Clustering) (45-50 min)\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides. For a PDF copy, check Moodle."
  },
  {
    "objectID": "weeks/week08/lab.html",
    "href": "weeks/week08/lab.html",
    "title": "💻 Week 08 - Lab Roadmap (90 min)",
    "section": "",
    "text": "Context\nThis lab session draws on 🗓️ Week 04 lecture/workshop and 🗓️ Week 05 lecture content.\nIt also reuses elements of tidymodels introduced in previous labs where we used functions from library(broom) or library(rsample). These packages are part of tidymodels. For the record, tidymodel’s ‘Get Started’ tutorials are really good.\nTopics\nMore specifically, these are the things we will explore or revisit:"
  },
  {
    "objectID": "weeks/week08/lab.html#setup",
    "href": "weeks/week08/lab.html#setup",
    "title": "💻 Week 08 - Lab Roadmap (90 min)",
    "section": "Setup",
    "text": "Setup\n💡 Some of you have mentioned that your R version cannot handle tidymodels. We recommend you update R to version 4.2.0 or above.\nPackages you will need\n\nlibrary('ISLR2')       # for the data\nlibrary('tidyverse')   # to use things like the pipe (%>%)\nlibrary('e1071')       # for SVM model\nlibrary('tidymodels')  # for model tuning, cross-validation etc.\n\n# Vanity packages:\nlibrary('GGally')      # for pretty correlation plot\nlibrary('ggsci')       # for pretty plot colours\nlibrary('cvms')        # for pretty confusion matrix plots"
  },
  {
    "objectID": "weeks/week08/lab.html#the-data-10-min",
    "href": "weeks/week08/lab.html#the-data-10-min",
    "title": "💻 Week 08 - Lab Roadmap (90 min)",
    "section": "The Data (10 min)",
    "text": "The Data (10 min)\n\n🍊Orange Juice\nThis week we will use a different ISLR2 dataset: OJ . We will perform a classification task with the goal to predict the Purchase column.\n\nThe data contains 1070 purchases where the customer either purchased Citrus Hill or Minute Maid Orange Juice. A number of characteristics of the customer and product are recorded.\n\n\nISLR2::OJ %>% head()\n\n  Purchase WeekofPurchase StoreID PriceCH PriceMM DiscCH DiscMM SpecialCH\n1       CH            237       1    1.75    1.99   0.00    0.0         0\n2       CH            239       1    1.75    1.99   0.00    0.3         0\n3       CH            245       1    1.86    2.09   0.17    0.0         0\n4       MM            227       1    1.69    1.69   0.00    0.0         0\n5       CH            228       7    1.69    1.69   0.00    0.0         0\n6       CH            230       7    1.69    1.99   0.00    0.0         0\n  SpecialMM  LoyalCH SalePriceMM SalePriceCH PriceDiff Store7 PctDiscMM\n1         0 0.500000        1.99        1.75      0.24     No  0.000000\n2         1 0.600000        1.69        1.75     -0.06     No  0.150754\n3         0 0.680000        2.09        1.69      0.40     No  0.000000\n4         0 0.400000        1.69        1.69      0.00     No  0.000000\n5         0 0.956535        1.69        1.69      0.00    Yes  0.000000\n6         1 0.965228        1.99        1.69      0.30    Yes  0.000000\n  PctDiscCH ListPriceDiff STORE\n1  0.000000          0.24     1\n2  0.000000          0.24     1\n3  0.091398          0.23     1\n4  0.000000          0.00     1\n5  0.000000          0.00     0\n6  0.000000          0.30     0\n\n\nTo understand what each variable represent, open the R Console, type the following and hit ENTER:\n?ISLR2::OJ\nWhich variables can help us distinguish the two different brands?\nTo simplify our plots later on, let’s focus on just two predictors:\n\nplot_df <- ISLR2::OJ %>% select(Purchase, LoyalCH, PriceDiff)\n\ng = (\n  ggpairs(plot_df, aes(colour=Purchase))\n  \n  # Customizing the plot\n  + scale_colour_startrek()\n  + scale_fill_startrek()\n  + theme_bw()\n)\ng"
  },
  {
    "objectID": "weeks/week08/lab.html#stock-market",
    "href": "weeks/week08/lab.html#stock-market",
    "title": "💻 Week 08 - Lab Roadmap (90 min)",
    "section": "📈 Stock Market",
    "text": "📈 Stock Market\nWe will also use the Smarket dataset from the ISLR2 package. We will perform a regression task with the goal to predict the percentage of return of the S&P 500 stock index on any given day, as represented by the Today column.\n\nDaily percentage returns for the S&P 500 stock index between 2001 and 2005.\n\n\nISLR2::Smarket %>% head()\n\n  Year   Lag1   Lag2   Lag3   Lag4   Lag5 Volume  Today Direction\n1 2001  0.381 -0.192 -2.624 -1.055  5.010 1.1913  0.959        Up\n2 2001  0.959  0.381 -0.192 -2.624 -1.055 1.2965  1.032        Up\n3 2001  1.032  0.959  0.381 -0.192 -2.624 1.4112 -0.623      Down\n4 2001 -0.623  1.032  0.959  0.381 -0.192 1.2760  0.614        Up\n5 2001  0.614 -0.623  1.032  0.959  0.381 1.2057  0.213        Up\n6 2001  0.213  0.614 -0.623  1.032  0.959 1.3491  1.392        Up\n\n\n\nplot_df <- ISLR2::Smarket %>% select(Today, Volume, Lag1, Lag2)\n\ng = (\n  ggpairs(plot_df)\n  \n  # Customizing the plot\n  + scale_colour_startrek()\n  + scale_fill_startrek()\n  + theme_bw()\n)\ng \n\n\n\n\nTo understand what each variable represent, open the R Console, type the following and hit ENTER:\n?ISLR2::Smarket"
  },
  {
    "objectID": "weeks/week08/lab.html#step-1-svm-models-for-classification-30-min",
    "href": "weeks/week08/lab.html#step-1-svm-models-for-classification-30-min",
    "title": "💻 Week 08 - Lab Roadmap (90 min)",
    "section": "Step 1: SVM models for classification (30 min)",
    "text": "Step 1: SVM models for classification (30 min)\nSVM stands for Support Vector Machines. Revisit 🗓️ Week 05 lecture or Chapter 9 of our textbook to understand more about this algorithm.\nR does not come with SVM, so we need to import it from a library. Let’s start with the function svm we used in the Week 05 lecture, imported from the e1071 package. Here are a few things to know about the svm function:\n\nThe svm command is largely similar to other commands such as lm and glm in that the first parameter is an R formula and the second is the data set.\nThere are a few other options, but we will focus on specifying a radial kernel using kernel = 'radial'.\nWe can specify the type of machine learning task we are performing. Since we are doing classification, we use the option type = 'C-classification'.\n\n\nStep 1.1: Train a SVM model\n\nfiltered_data <- ISLR2::OJ %>% select(Purchase, LoyalCH, PriceDiff)\n\norange_svm_model <- svm(Purchase ~ .,\n                        data=filtered_data,\n                        kernel='radial', \n                        type='C-classification')\norange_svm_model\n\n\nCall:\nsvm(formula = Purchase ~ ., data = filtered_data, kernel = \"radial\", \n    type = \"C-classification\")\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  1 \n\nNumber of Support Vectors:  427\n\n\n🎯 ACTION POINT: What does the ‘Number of Support Vectors’ represent?\n\nyour text here\n\n🎯 ACTION POINT: What would happened if we changed kernel to kernel=\"linear\"?\n\nyour text here\n\n\n\nStep 1.2: Goodness-of-Fit of the SVM\nLet’s investigate how well our model fits the data. Let’s reuse the model we trained (orange_svm_model) and predict the same samples we used to train it. To avoid modifying our original dataframe, let’s save the output of the prediction in an auxiliary df (plot_df):\n\nplot_df <- \n    filtered_data %>% \n    mutate(class_pred = predict(orange_svm_model, newdata = .))\nhead(plot_df)\n\nAdd a is_correct column to indicate whether the prediction was correct or not:\n\nplot_df <- plot_df %>% mutate(is_correct = class_pred == Purchase)\nhead(plot_df)\n\n  Purchase  LoyalCH PriceDiff class_pred is_correct\n1       CH 0.500000      0.24         CH       TRUE\n2       CH 0.600000     -0.06         CH       TRUE\n3       CH 0.680000      0.40         CH       TRUE\n4       MM 0.400000      0.00         MM       TRUE\n5       CH 0.956535      0.00         CH       TRUE\n6       CH 0.965228      0.30         CH       TRUE\n\n\nRemember what we did in the Decision Tree example last week!\n\nSimple confusion matrix:\n\nconfusion_matrix <- \n    table(expected=plot_df$Purchase, class_pred=plot_df$class_pred)\nprint(confusion_matrix)\n\n\n\nNicer looking confusion matrix:\n\nplot_confusion_matrix(as_tibble(confusion_matrix), \n                      target_col = \"expected\", \n                      prediction_col = \"class_pred\",\n                      \n                      # Customizing the plot\n                      add_normalized = TRUE,\n                      add_col_percentages = FALSE,\n                      add_row_percentages = FALSE,\n                      counts_col = \"n\",\n                      )\n\n🎯 ACTION POINT: How well does the model fit the data? What is your opinion?\n\n\nMeasure: Precision\nRemember from Week 04 Lecture/Workshop (notebook can be found under📥W04 Lecture Files on Moodle):\n\nPRECISION: Given all predictions for a specific class, how many were True Positives? In other words, Precision = True Positives/(True Positives + False Positives).\n\nLet’s calculate the score for the CH class. That is, as if CH=“Yes” and MM=“No”.\n\n# expected == CH & predicted == CH\ntotal_correct_CH   <- confusion_matrix[\"CH\", \"CH\"]  \n\n# sum of samples predicted == CH\ntotal_predicted_CH <- sum(confusion_matrix[, \"CH\"]) \n\nprecision          <- total_correct_CH/total_predicted_CH\ncat(sprintf(\"%.2f%%\", 100*precision))\n\n85.27%\n\n\n\n\nMeasure: Recall\n\nAlso called True Positive Rate = True Positive/(True Positives + False Negatives)\n\n\n# number of samples of brand CH\ntotal_real_CH      <- sum(confusion_matrix[\"CH\", ]) \n\nrecall <- total_correct_CH/total_real_CH\ncat(sprintf(\"%.2f%%\", 100*recall))\n\n87.75%\n\n\n\n\nMeasure: F1-SCORE\n\nF1-SCORE: A combination of Precision and Recall\n\\[\n\\operatorname{F1-score} = \\frac{2 \\times \\operatorname{Precision} \\times \\operatorname{Recall}}{(\\operatorname{Precision} + \\operatorname{Recall})}\n\\]\n\n\nf1_score <- (2*precision*recall)/(precision + recall)\n\ncat(f1_score)\n\n0.8649057\n\n\n\n\n\nStep 1.3: Visualize the SVM decision space\nHere we will demonstrate how you could simulate some data to cover the entire feature space of the data we are modelling. What do we mean by that? By inspecting LoyalCH and PriceDiff, we see the range values these variables can assume:\n\nfiltered_data %>% \n    select(c(LoyalCH, PriceDiff)) %>%\n    summary()\n\n    LoyalCH           PriceDiff      \n Min.   :0.000011   Min.   :-0.6700  \n 1st Qu.:0.325257   1st Qu.: 0.0000  \n Median :0.600000   Median : 0.2300  \n Mean   :0.565782   Mean   : 0.1465  \n 3rd Qu.:0.850873   3rd Qu.: 0.3200  \n Max.   :0.999947   Max.   : 0.6400  \n\n\n💡 We can simulate data to account for all possible combinations of LoyalCH and PriceDiff. We achieve this using crossing, another tidyverse function:\n\nWe feed crossing a sequence of numbers that range from the minimal and maximal values of both variables, incremented by 0.1 values.\nWe then create a new variable class_pred which uses the SVM model object to predict the brand of orange juice purchased by the customer\nNote that we say newdata = . to indicate that we simply want to use the data set created with crossing as our new data.\n\n\nsim.data <- \n  crossing(LoyalCH   = seq(0,1,0.05),\n           PriceDiff = seq(-1,1,0.1)) %>% \n  mutate(class_pred = predict(orange_svm_model, newdata = .))\nhead(sim.data)\n\n# A tibble: 6 × 3\n  LoyalCH PriceDiff class_pred\n    <dbl>     <dbl> <fct>     \n1       0      -1   MM        \n2       0      -0.9 MM        \n3       0      -0.8 MM        \n4       0      -0.7 MM        \n5       0      -0.6 MM        \n6       0      -0.5 MM        \n\n\nThe data above is all synthetic (“fake”)! But it is very useful to colour the background of our plot.\nWe use geom_tile to show the area the SVM model identifies as Chinstrap penguins. We then use geom_point to overlay the actual data. Red dots in blue areas and vice versa indicate cases where the SVM model makes errors.\n\ng <- (\n  plot_df %>%   \n    ggplot()\n  \n    # Tile the background of the plot with SVM predictions\n    + geom_tile(data = sim.data, aes(x=LoyalCH, y=PriceDiff, fill = class_pred), alpha = 0.25)\n  \n    # Actual data\n    + geom_point(aes(x=LoyalCH, y=PriceDiff, colour = Purchase, shape = is_correct), size=2.5, stroke=0.95, alpha=0.7)\n  \n    # Define X and Os\n    + scale_shape_manual(values = c(4, 1))\n    \n    # (OPTIONAL) Customizing the colours and theme of the plot\n    + scale_x_continuous(labels=scales::percent)\n    + scale_colour_startrek()\n    + scale_fill_startrek()\n    + theme_minimal()\n    + theme(panel.grid = element_blank(), legend.position = 'bottom', plot.title = element_text(hjust = 0.5))\n    + labs(x = 'Customer brand loyalty for CH', y = 'Sale price of MM less sale price of CH', fill = 'Brand', colour = 'Brand', shape = 'Correct prediction?', title = sprintf('Overall Training Accuracy = %.2f %%', 100*(sum(plot_df$correct)/nrow(plot_df))))\n)\n\ng\n\n\n\n\n🤝 WORKING TOGETHER In pairs, discuss what you see in the plot:\n\nWhat do the shape of dots represent? The X and Os?\n\n\nyour text here\n\n\nWhat do the colours of the dots represent?\n\n\nyour text here\n\n\nWhat do the background colour in the plot represent?\n\n\nyour text here\n\n\nCan you point in the plot roughly which dots you would expect to be the support vectors?"
  },
  {
    "objectID": "weeks/week08/lab.html#step-2-doing-the-same-with-tidymodels-20-min",
    "href": "weeks/week08/lab.html#step-2-doing-the-same-with-tidymodels-20-min",
    "title": "💻 Week 08 - Lab Roadmap (90 min)",
    "section": "Step 2: Doing the same with tidymodels (20 min)",
    "text": "Step 2: Doing the same with tidymodels (20 min)\nThe function svm from library(e1071) package is not the only way to run SVM in R. The package parsnip also have its own SVM functions. The functionality is roughly the same but there are differences in how you write the code.\nparsnip already comes installed in tidymodels, so we do not need to import or install anything else.\n\nStep 2.1 Training the SVM model\nWe specify a radial basis function SVM (see the part about kernels in the 🗓️ Week 05 lecture) with the function svm_rbf.\nIn the spirit of tidyverse, we pipe the SVM algorithm into the fit function, where we can define the R formula like we have been doing with other algorithms:\n\norange_tidymodel <-\n  svm_rbf() %>% \n  set_mode('classification') %>% \n  fit(Purchase ~ ., data = filtered_data)\n\norange_tidymodel\n\nparsnip model object\n\nSupport Vector Machine object of class \"ksvm\" \n\nSV type: C-svc  (classification) \n parameter : cost C = 1 \n\nGaussian Radial Basis kernel function. \n Hyperparameter : sigma =  1.69158164951656 \n\nNumber of Support Vectors : 434 \n\nObjective Function Value : -386.6126 \nTraining error : 0.170093 \nProbability model included. \n\n\n🎯 ACTION POINT: Compare the output above to that of another colleague. Why don’t you get the exact same output?\n\nyour personal notes go here\n\n💡 If you want to try different kernels, you will need to replace svm_rbf() by svm_linear() or svm_poly().\n\n\nStep 2.2: Goodness-of-Fit of the SVM\nLet’s investigate how well our model fits the data. Let’s reuse the model we trained (orange_tidymodel) and predict the same samples we used to train it.\nFunction augment(<model>, <df>) of tidymodels applies a model to a dataframe and return the same data plus a few columns:\n\nplot_df <- augment(orange_tidymodel, filtered_data)\nhead(plot_df)\n\n🎯 ACTION POINT: How is the plot_df data frame above different to the first plot_df we created in Step 1.2?\n\nyour notes go here\n\nAdd a is_correct column to indicate whether the prediction was correct or not:\n\nplot_df <- plot_df %>% mutate(is_correct = .pred_class == Purchase)\nhead(plot_df)\n\n# A tibble: 6 × 7\n  Purchase LoyalCH PriceDiff .pred_class .pred_CH .pred_MM is_correct\n  <fct>      <dbl>     <dbl> <fct>          <dbl>    <dbl> <lgl>     \n1 CH         0.5        0.24 CH             0.859    0.141 TRUE      \n2 CH         0.6       -0.06 CH             0.715    0.285 TRUE      \n3 CH         0.68       0.4  CH             0.872    0.128 TRUE      \n4 MM         0.4        0    MM             0.175    0.825 TRUE      \n5 CH         0.957      0    CH             0.875    0.125 TRUE      \n6 CH         0.965      0.3  CH             0.872    0.128 TRUE      \n\n\n\nMeasure: Precision\nYou don’t need to calculate precision by hand, just use the precision() function from tidymodels:\n\nplot_df %>% precision(Purchase, .pred_class) %>% head()\n\n# A tibble: 1 × 3\n  .metric   .estimator .estimate\n  <chr>     <chr>          <dbl>\n1 precision binary         0.856\n\n\n\n\nMeasure: Recall\nYou don’t need to calculate recall by hand, just use the recall() function from tidymodels:\n\nplot_df %>% recall(Purchase, .pred_class) %>% head()\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 recall  binary         0.867\n\n\n\n\nMeasure: F1-score\nYou don’t need to calculate F1-score by hand, just use the f_meas() function from tidymodels:\n\nplot_df %>% f_meas(Purchase, .pred_class) %>% head()\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 f_meas  binary         0.861\n\n\n\n\n(Optional) ROC curve\nPlot the ROC curve for class Purchase==\"CH\" :\n\nplot_df %>% \n  roc_curve(Purchase, .pred_CH) %>% \n  autoplot\n\n\n\n\n\n\n🏠 Take-home exercise Q1:\nEdit the cell below modifying event_level from \"second\" to \"first\". Why do you get different results? What do you think is going on?\nplot_df %>% f_meas(Purchase, .pred_class, event_level=...)\n💡Tip: Read the documentation of f_meas to understand what event_level represents. (Type ?f_meas)\n💡 Gold Tip: note the Levels of the factor variable called Purchase:\nplot_df$Purchase\n\n\n🏠 Take-home exercise Q2:\nCreate a plot of the confusion matrix for the orange_tidymodel like we did in Step 1.\n\n## your code goes here\n\n\n\n🏠 Take-home exercise Q3:\nCreate a plot of SVM decision space for the orange_tidymodel like we did in Step 1.\n\n## your code goes here\n\n🎯 ACTION POINT: If you were to run the SVM algorithm by yourself in another dataset, which version would you prefer, the one in Step 1 or the one in Step 2?\n\nyour notes go here"
  },
  {
    "objectID": "weeks/week08/lab.html#step-3-what-about-regression-30-min",
    "href": "weeks/week08/lab.html#step-3-what-about-regression-30-min",
    "title": "💻 Week 08 - Lab Roadmap (90 min)",
    "section": "Step 3: What about regression? (30 min)",
    "text": "Step 3: What about regression? (30 min)\nHere we will be using the 📈 SMarket data.\n\nStep 3.1: Train the model\nLet’s select just the predictors Volume and Lag1 and fit a regression model to predict Today:\n\n# Remove Direction, otherwise we would be \"cheating\" \nfiltered_data <- ISLR2::Smarket %>% select(Today, Volume, Lag1)\n\nsmarket_tidymodel <-\n  svm_rbf() %>% \n  set_mode('regression') %>% \n  fit(Today ~ ., data = filtered_data)\n\nsmarket_tidymodel\n\nparsnip model object\n\nSupport Vector Machine object of class \"ksvm\" \n\nSV type: eps-svr  (regression) \n parameter : epsilon = 0.1  cost C = 1 \n\nGaussian Radial Basis kernel function. \n Hyperparameter : sigma =  1.60989422870261 \n\nNumber of Support Vectors : 1119 \n\nObjective Function Value : -762.849 \nTraining error : 0.931991 \n\n\n\n\nStep 3.2: Goodness-of-Fit of the SVM\nSince the target variable is continuous, not discrete, we cannot plot confusion matrix nor anything like that. We will have to go back to the idea of residuals (🗓️ Week 02 Lecture & 💻 Week 03 - Lab).\n\nplot_df <- \n  augment(smarket_tidymodel, filtered_data) %>% \n  mutate(row_number=row_number()) # adding this here just to make our plot easier\nplot_df %>% head()\n\n# A tibble: 6 × 6\n   Today Volume   Lag1   .pred .resid row_number\n   <dbl>  <dbl>  <dbl>   <dbl>  <dbl>      <int>\n1  0.959   1.19  0.381 -0.0467  1.01           1\n2  1.03    1.30  0.959 -0.239   1.27           2\n3 -0.623   1.41  1.03  -0.251  -0.372          3\n4  0.614   1.28 -0.623  0.238   0.376          4\n5  0.213   1.21  0.614 -0.146   0.359          5\n6  1.39    1.35  0.213 -0.155   1.55           6\n\n\n🎯 ACTION POINT: What do the different columns mean?\n\nyour text go here\n\nNow, let’s look at the distribution of residuals and let’s mark the absolute residuals above 2 to flag the worst predictions (2 was an arbitrary choice, it all depends on the context):\n\ng <- (\n  ggplot(plot_df, aes(x=row_number, y=.resid))\n  + geom_point(alpha=0.6)\n  \n  + theme_bw()\n  + geom_hline(yintercept = c(-2,2), color=\"red\", linetype=\"dashed\")\n  + labs(title=\"Distribution of residuals (the closer to zero the better)\")\n)\n\ng\n\n\n\n\n\nMeasure: Mean Absolute Error (MAE)\n\\[\nMAE = \\frac{\\sum_{i=1}^{n}{|y_i - \\hat{y}_i|}}{n}\n\\]\n\nplot_df %>% mae(Today, .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 mae     standard       0.791\n\n\n\n\nMeasure: Root Mean Squared Error (RMSE)\n\\[\nRMSE = \\frac{\\sum_{i=1}^{n}{(y_i - \\hat{y}_i)^2}}{n}\n\\]\n\nplot_df %>% rmse(Today, .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.10\n\n\n🎯 ACTION POINT: Would a better model have a larger or smaller value of MAE/RMSE?\n\nyour text goes here\n\n\n\n\nStep 3.3 Visualize the SVM decision space (Regression)\nNow, let’s replicate what we did in Step 1.3 only this time for the Smarket data and using predictions from the smarket_tidymodel.\nWe start by summarising the data. We want to find out the minimum and maximal values that the columns Volumn and Lag1 reach:\n\nfiltered_data %>% \n    select(c(Volume, Lag1)) %>%\n    summary()\n\n     Volume            Lag1          \n Min.   :0.3561   Min.   :-4.922000  \n 1st Qu.:1.2574   1st Qu.:-0.639500  \n Median :1.4229   Median : 0.039000  \n Mean   :1.4783   Mean   : 0.003834  \n 3rd Qu.:1.6417   3rd Qu.: 0.596750  \n Max.   :3.1525   Max.   : 5.733000  \n\n\nThen, we create a simulated dataset with combinations of the Volume and Lag1 columns:\n\nsim.data <- \n  crossing(Volume   = seq(0,3.5,0.1),\n           Lag1 = seq(-5,6,0.2))\nsim.data <- augment(smarket_tidymodel, sim.data)\nhead(sim.data)\n\n# A tibble: 6 × 3\n  Volume  Lag1   .pred\n   <dbl> <dbl>   <dbl>\n1      0  -5   -0.0135\n2      0  -4.8 -0.0135\n3      0  -4.6 -0.0135\n4      0  -4.4 -0.0135\n5      0  -4.2 -0.0135\n6      0  -4   -0.0135\n\n\n💡 Look at the entire dataset with View(sim.data)\nLooking back at the plot of residuals, let’s flag the worst predictions, those with an absolute residual above 2.\n\nplot_df$residual_above_2 <- (plot_df$.resid) > 2\n\n\ng <- (\n  plot_df %>%   \n    ggplot()\n  \n    ## Tile the background of the plot with SVM predictions\n    + geom_tile(data = sim.data, aes(x=Volume, y=Lag1, fill = .pred), alpha = 0.45)\n  \n    ## Actual data\n    + geom_point(aes(x=Volume, y=Lag1, colour = residual_above_2, shape = residual_above_2, alpha=residual_above_2), size=2.5, stroke=0.95)\n  \n    ## Define X and Os\n    + scale_shape_manual(values = c(4, 1))\n    + scale_fill_viridis_c()\n    + scale_color_manual(values=c(\"black\", \"red\"))\n    + scale_alpha_manual(values=c(0.1, 0.7))\n    \n    ## (OPTIONAL) Customizing the colours and theme of the plot\n    + theme_minimal()\n    + theme(panel.grid = element_blank(), legend.position = 'bottom', plot.title = element_text(hjust = 0.5))\n    + labs(x = 'Volume', y = 'Lag 1', fill = \"Today's Prediction\", colour = 'Residual above 2?', shape = 'Residual above 2?', alpha='Residual above 2?', title='Worst predictions are marked as red circles')\n)\n\ng\n\n\n\n\n💡 The plot above might not be as easy to understand as the one for classification.\n\n\nStep 3.3: Understand the parameters of svm_rbf\nThe svm_rbf function has three parameters you can tune: - cost - rbf_sigma - margin\n🎯 ACTION POINT: Train your abilities to interact with code documentation. Type ?svm_rbf and hit ENTER. What do these parameters represent?\n💡 Tip: At the bottom of the help page, you will find a link to kernlab engine details that has more useful info about SVM RBF.\n\nyour text goes here\n\n\n\nStep 3.4: Tweak the parameters\n🤝 WORKING TOGETHER In pairs, change the values of cost, rbf_sigma and margin in the chunk below and run the other two chunks of code to look at the distribution of residuals and summary metrics.\nDiscuss your findings. Can you find any combination of values that makes the model better? Or any that makes it worse?\n\nalternative_smarket_model <-\n  svm_rbf(cost=1, rbf_sigma=10, margin=0.9) %>% \n  set_mode('regression') %>% \n  fit(Today ~ ., data = filtered_data)\n\nalternative_smarket_model\n\nparsnip model object\n\nSupport Vector Machine object of class \"ksvm\" \n\nSV type: eps-svr  (regression) \n parameter : epsilon = 0.9  cost C = 1 \n\nGaussian Radial Basis kernel function. \n Hyperparameter : sigma =  10 \n\nNumber of Support Vectors : 401 \n\nObjective Function Value : -195.7568 \nTraining error : 0.832623 \n\n\nResiduals plot\n\nplot_df <- \n  augment(alternative_smarket_model, filtered_data) %>% \n  mutate(row_number=row_number()) ## adding this here just to make our plot easier\n\ng <- (\n  ggplot(plot_df, aes(x=row_number, y=.resid))\n  + geom_point(alpha=0.6)\n  \n  + theme_bw()\n  + geom_hline(yintercept = c(-2,2), color=\"red\")\n)\n\ng\n\n\n\n\nMetrics\n\n## Use the vectorized version of MAE and RMSE functions\nplot_df %>% summarise(mae=mae_vec(Today, .pred),\n                      rmse=rmse_vec(Today, .pred))\n\n# A tibble: 1 × 2\n    mae  rmse\n  <dbl> <dbl>\n1 0.787  1.04\n\n\n\n🏠 Take-home exercise Q4:\n\nRetrain the model in Step 3 for Smarket, this time using svm_linear instead of svm_rbf.\nReuse the code in Step 3.4 to replicate the plots and metric calculations for this new model.\nWhich model fits the target variable (Today) better?\n\n\n# your code goes here\n\n\nyour text goes here"
  },
  {
    "objectID": "weeks/week08/lab.html#want-to-take-it-to-next-level-optional",
    "href": "weeks/week08/lab.html#want-to-take-it-to-next-level-optional",
    "title": "💻 Week 08 - Lab Roadmap (90 min)",
    "section": "Want to take it to next level? (Optional)",
    "text": "Want to take it to next level? (Optional)\nIn a separate, more advanced, bonus lab roadmap we show how to perform k-fold cross-validation using tidymodels to tune the parameters of SVM automatically.\nIt is optional, you don’t need to read it, but it might be the best way to solidify your knowledge of SVM and its parameters."
  },
  {
    "objectID": "weeks/week08/lab_advanced.html",
    "href": "weeks/week08/lab_advanced.html",
    "title": "✨ Bonus Lab (Optional) - Cross-validation",
    "section": "",
    "text": "Originally, we intended it to be part of W08 lab but decided against it as it would make the lab too cluttered.\nAlthough optional, we think the exercises in here might be a great way to solidify your knowledge of SVM and its parameters.\n\n\nHere we show an alternative way to perform k-fold cross-validation using tidymodels instead of the cv.glm we saw in W05 lab."
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#packages-you-will-need",
    "href": "weeks/week08/lab_advanced.html#packages-you-will-need",
    "title": "✨ Bonus Lab (Optional) - Cross-validation",
    "section": "Packages you will need",
    "text": "Packages you will need\n\nlibrary('ISLR2')       # for the data\nlibrary('tidyverse')   # to use things like the pipe (%>%)\nlibrary('e1071')       # for SVM model\nlibrary('tidymodels')  # for model tuning, cross-validation etc.\n\n# Vanity packages:\nlibrary('GGally')      # for pretty correlation plot\nlibrary('ggsci')       # for pretty plot colours\nlibrary('cvms')        # for pretty confusion matrix plots"
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#orange-juice",
    "href": "weeks/week08/lab_advanced.html#orange-juice",
    "title": "✨ Bonus Lab (Optional) - Cross-validation",
    "section": "🍊Orange Juice",
    "text": "🍊Orange Juice\nThis week we will use a different ISLR2 dataset: OJ . We will perform a classification task with the goal to predict the Purchase column.\n\nThe data contains 1070 purchases where the customer either purchased Citrus Hill or Minute Maid Orange Juice. A number of characteristics of the customer and product are recorded.\n\n\nISLR2::OJ %>% head()\n\n  Purchase WeekofPurchase StoreID PriceCH PriceMM DiscCH DiscMM SpecialCH\n1       CH            237       1    1.75    1.99   0.00    0.0         0\n2       CH            239       1    1.75    1.99   0.00    0.3         0\n3       CH            245       1    1.86    2.09   0.17    0.0         0\n4       MM            227       1    1.69    1.69   0.00    0.0         0\n5       CH            228       7    1.69    1.69   0.00    0.0         0\n6       CH            230       7    1.69    1.99   0.00    0.0         0\n  SpecialMM  LoyalCH SalePriceMM SalePriceCH PriceDiff Store7 PctDiscMM\n1         0 0.500000        1.99        1.75      0.24     No  0.000000\n2         1 0.600000        1.69        1.75     -0.06     No  0.150754\n3         0 0.680000        2.09        1.69      0.40     No  0.000000\n4         0 0.400000        1.69        1.69      0.00     No  0.000000\n5         0 0.956535        1.69        1.69      0.00    Yes  0.000000\n6         1 0.965228        1.99        1.69      0.30    Yes  0.000000\n  PctDiscCH ListPriceDiff STORE\n1  0.000000          0.24     1\n2  0.000000          0.24     1\n3  0.091398          0.23     1\n4  0.000000          0.00     1\n5  0.000000          0.00     0\n6  0.000000          0.30     0\n\n\nTo understand what each variable represent, open the R Console, type the following and hit ENTER:\n?ISLR2::OJ\n\nWhich variables can help us distinguish the two different brands?\nTo simplify our plots later on, let’s focus on just two predictors:\n\nplot_df <- ISLR2::OJ %>% select(Purchase, LoyalCH, PriceDiff)\n\ng = (\n  ggpairs(plot_df, aes(colour=Purchase))\n  \n  # Customizing the plot\n  + scale_colour_startrek()\n  + scale_fill_startrek()\n  + theme_bw()\n)\ng"
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#stock-market",
    "href": "weeks/week08/lab_advanced.html#stock-market",
    "title": "✨ Bonus Lab (Optional) - Cross-validation",
    "section": "📈 Stock Market",
    "text": "📈 Stock Market\nWe will also use the Smarket dataset from the ISLR2 package. We will perform a regression task with the goal to predict the percentage of return of the S&P 500 stock index on any given day, as represented by the Today column.\n\nDaily percentage returns for the S&P 500 stock index between 2001 and 2005.\n\n\nISLR2::Smarket %>% head()\n\n  Year   Lag1   Lag2   Lag3   Lag4   Lag5 Volume  Today Direction\n1 2001  0.381 -0.192 -2.624 -1.055  5.010 1.1913  0.959        Up\n2 2001  0.959  0.381 -0.192 -2.624 -1.055 1.2965  1.032        Up\n3 2001  1.032  0.959  0.381 -0.192 -2.624 1.4112 -0.623      Down\n4 2001 -0.623  1.032  0.959  0.381 -0.192 1.2760  0.614        Up\n5 2001  0.614 -0.623  1.032  0.959  0.381 1.2057  0.213        Up\n6 2001  0.213  0.614 -0.623  1.032  0.959 1.3491  1.392        Up\n\n\n\nplot_df <- ISLR2::Smarket %>% select(Today, Volume, Lag1, Lag2)\n\ng = (\n  ggpairs(plot_df)\n  \n  # Customizing the plot\n  + scale_colour_startrek()\n  + scale_fill_startrek()\n  + theme_bw()\n)\ng \n\n\n\n\nTo understand what each variable represent, open the R Console, type the following and hit ENTER:\n?ISLR2::Smarket"
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#step-4.1-create-training-test-split",
    "href": "weeks/week08/lab_advanced.html#step-4.1-create-training-test-split",
    "title": "✨ Bonus Lab (Optional) - Cross-validation",
    "section": "Step 4.1: Create training / test split",
    "text": "Step 4.1: Create training / test split\nWe start by creating a training / test split using the functions initial_split packages for 📈 SMarket data.\n\nset.seed(123)\n\n# Remove Direction, otherwise we would be \"cheating\" \nfiltered_data <- ISLR2::Smarket %>% select(Today, Volume, Lag1)\n\ndefault_split <- initial_split(filtered_data, prop = 0.75, strata = Today)\n\ninternal_validation_set <- training(default_split)\n\nexternal_validation_set <- testing(default_split)\n\nHow many samples are in the internal validation set?\n\nnrow(internal_validation_set)\n\n[1] 936\n\n\nHow many samples were left in the external validation set?\n\nnrow(external_validation_set)\n\n[1] 314\n\n\nThe external validation set will only be used at the end"
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#step-4.2-create-resampling-folds-for-cross-validation",
    "href": "weeks/week08/lab_advanced.html#step-4.2-create-resampling-folds-for-cross-validation",
    "title": "✨ Bonus Lab (Optional) - Cross-validation",
    "section": "Step 4.2: Create resampling folds for cross validation",
    "text": "Step 4.2: Create resampling folds for cross validation\nNext, let’s create 10-fold cross-validation data using internal_validation_set. We can achieve this by using the vfold_cv command, specifying v = 10.\n\nk_folds <- vfold_cv(internal_validation_set, v = 10)\nk_folds\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits           id    \n   <list>           <chr> \n 1 <split [842/94]> Fold01\n 2 <split [842/94]> Fold02\n 3 <split [842/94]> Fold03\n 4 <split [842/94]> Fold04\n 5 <split [842/94]> Fold05\n 6 <split [842/94]> Fold06\n 7 <split [843/93]> Fold07\n 8 <split [843/93]> Fold08\n 9 <split [843/93]> Fold09\n10 <split [843/93]> Fold10\n\n\nNotice anything odd? The output is a tibble but the first column splits is a series of lists. This is another thing that makes tibbles different to data frames - you can nest lists within tibbles but not data frames. We will use this more explicitly in the next lab when we build k-means clustering models."
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#step-4.3-specifying-a-recipe",
    "href": "weeks/week08/lab_advanced.html#step-4.3-specifying-a-recipe",
    "title": "✨ Bonus Lab (Optional) - Cross-validation",
    "section": "Step 4.3: 🍳Specifying a recipe",
    "text": "Step 4.3: 🍳Specifying a recipe\nThe next step is to create a recipe. Luckily, recipe function takes the same values as the lm model! We first create a formula default ~ . and then use data = internal_validation_set. Printing smarket_recipe, we have one outcome and three predictors.\n\nsmarket_recipe <- recipe(Today ~ ., data = internal_validation_set)\nsmarket_recipe\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          2"
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#step-4.4-specify-a-model",
    "href": "weeks/week08/lab_advanced.html#step-4.4-specify-a-model",
    "title": "✨ Bonus Lab (Optional) - Cross-validation",
    "section": "Step 4.4: Specify a model",
    "text": "Step 4.4: Specify a model\nNext, we will specify a support vector machine model. Here’s where things get a bit more involved.\nWe specify a radial basis function SVM. svm_rbf takes two hyperparameters: cost and rbf_sigma. Instead of specifying a single value for each, we will instead set them equal to tune(). This indicates that we want to try a range of different values.\n\nsvm_regressor <-\n  svm_rbf(cost = tune(), rbf_sigma = tune()) %>% \n  set_mode('regression')"
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#step-4.5-create-a-hyperparameter-grid",
    "href": "weeks/week08/lab_advanced.html#step-4.5-create-a-hyperparameter-grid",
    "title": "✨ Bonus Lab (Optional) - Cross-validation",
    "section": "Step 4.5: Create a hyperparameter grid",
    "text": "Step 4.5: Create a hyperparameter grid\nWhich values for cost and rbf_sigma should we choose? It is often hard to tell, so instead we can experiment with different values.\nWe can use grid_regular to create a tibble of different hyperparameter combinations. levels = 5 indicates that we want to try out five different values for each hyperparameter.\n\nset.seed(234)\n\nsvm_grid <- grid_regular(cost(), rbf_sigma(), levels = 5)\nsvm_grid\n\n# A tibble: 25 × 2\n        cost    rbf_sigma\n       <dbl>        <dbl>\n 1  0.000977 0.0000000001\n 2  0.0131   0.0000000001\n 3  0.177    0.0000000001\n 4  2.38     0.0000000001\n 5 32        0.0000000001\n 6  0.000977 0.0000000316\n 7  0.0131   0.0000000316\n 8  0.177    0.0000000316\n 9  2.38     0.0000000316\n10 32        0.0000000316\n# … with 15 more rows"
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#step-4.6-perform-cross-validation",
    "href": "weeks/week08/lab_advanced.html#step-4.6-perform-cross-validation",
    "title": "✨ Bonus Lab (Optional) - Cross-validation",
    "section": "Step 4.6: Perform cross-validation",
    "text": "Step 4.6: Perform cross-validation\nWe now have all we need to run cross-validation, and the function tune_grid puts everything together. Let’s think intuitively what this command is doing.\n\nWe are telling tune_grid that we want to run a classification model on a recipe using different resampling folds.\nInstead of specifying hyperparameter values we want to run a combination of different values.\nAfter this, we want to choose a metric to evaluate different combinations.\nWe opt for rmse but we can specify several metrics however using the metric_set command.\n\n\nsmarket_tuned <-\n  tune_grid(object = svm_regressor,\n            preprocessor = smarket_recipe,\n            resamples = k_folds,\n            grid = svm_grid,\n            metrics = metric_set(rmse),\n            control = control_grid(save_pred=TRUE))"
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#step-4.7-which-combination-of-hyperparameters-works-best",
    "href": "weeks/week08/lab_advanced.html#step-4.7-which-combination-of-hyperparameters-works-best",
    "title": "✨ Bonus Lab (Optional) - Cross-validation",
    "section": "Step 4.7: Which combination of hyperparameters works best?",
    "text": "Step 4.7: Which combination of hyperparameters works best?\nNow we have tuned our models, let’s find out which combination of hyperparameters work best. We can create a ggplot easily using the autoplot command.\n\nsmarket_tuned %>% \n  autoplot() +\n  theme_minimal() +\n  ggsci::scale_color_jco() +\n  labs(y = 'RMSE', colour = 'Sigma')\n🎯 ACTION POINT: Can you explain what we see in the plot above?\n\nyour text go here\n\nWe can use the function select_best to identify the hyperparameter combination that leads to the highest precision.\n\nselect_best(smarket_tuned)"
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#q5-re-run-step3-regression",
    "href": "weeks/week08/lab_advanced.html#q5-re-run-step3-regression",
    "title": "✨ Bonus Lab (Optional) - Cross-validation",
    "section": "Q5: Re-run Step3 (Regression)",
    "text": "Q5: Re-run Step3 (Regression)\nBuild a standalone SVM model (tidymodels version) on the same data we used in Step 3 of W08 lab, only this time set the parameters of the SVM to the optimal parameters identified in Step 4.\n\n# your code goes here"
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#q6-predict-the-external-validation-set",
    "href": "weeks/week08/lab_advanced.html#q6-predict-the-external-validation-set",
    "title": "✨ Bonus Lab (Optional) - Cross-validation",
    "section": "Q6: Predict the external validation set",
    "text": "Q6: Predict the external validation set\nUse the model you built in Q5 and make predictions on the external validation set. How does the RMSE of these predictions compare to the RMSE of the internal validation set?\n\n# your code goes here"
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#q7-svm-decision-space-regression",
    "href": "weeks/week08/lab_advanced.html#q7-svm-decision-space-regression",
    "title": "✨ Bonus Lab (Optional) - Cross-validation",
    "section": "Q7: SVM Decision Space (Regression)",
    "text": "Q7: SVM Decision Space (Regression)\nReplicate the plot from Step 3.3 of W08 lab roadmap, only this time using the model from Q5.\n\n# your code goes here"
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#q8-compare-svm-decision-space-plots",
    "href": "weeks/week08/lab_advanced.html#q8-compare-svm-decision-space-plots",
    "title": "✨ Bonus Lab (Optional) - Cross-validation",
    "section": "Q8: Compare SVM Decision Space plots",
    "text": "Q8: Compare SVM Decision Space plots\nExplain if and how the decision space you obtained in Q8 differs from the one in Step 3.3.\n\nyour text goes here"
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#q9-grid-search-for-orange-juice-classification",
    "href": "weeks/week08/lab_advanced.html#q9-grid-search-for-orange-juice-classification",
    "title": "✨ Bonus Lab (Optional) - Cross-validation",
    "section": "Q9: Grid search for Orange Juice (Classification)",
    "text": "Q9: Grid search for Orange Juice (Classification)\n\nFor this task, you will use the Orange Juice data set (ISLR2::OJ) with ALL the predictors included\nReplicate the entire procedure of Step 4, making the necessary adjustments to predict Purchase\nUse F1 score as the optimisation metric.\n\n💡You will have two tweak at least two main things: metric_set and set_mode\n\n# your code goes here"
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#q10-challenge",
    "href": "weeks/week08/lab_advanced.html#q10-challenge",
    "title": "✨ Bonus Lab (Optional) - Cross-validation",
    "section": "Q10: Challenge",
    "text": "Q10: Challenge\nReplicate the same steps as in Q9 for different SVM kernels (linear, polynomial, etc.). Is it possible to fit a model that is better than the radial kernel,in terms of F1-score?\n\n# your code goes here"
  },
  {
    "objectID": "slides/week01_slides_part1.html#the-data-science-institute",
    "href": "slides/week01_slides_part1.html#the-data-science-institute",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "The Data Science Institute",
    "text": "The Data Science Institute\n\n\n\n\n\nThis course is offered by the LSE Data Science Institute (DSI).\nDSI is the hub for LSE’s interdisciplinary collaboration in data science\n\n\n\n\nSign up for DSI events at lse.ac.uk/DSI/Events"
  },
  {
    "objectID": "slides/week01_slides_part1.html#the-data-science-institute-1",
    "href": "slides/week01_slides_part1.html#the-data-science-institute-1",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "The Data Science Institute",
    "text": "The Data Science Institute\n\n\n\n\nActivities of interest to you:\n\nCIVICA Seminar Series\nCareers in Data Science\nSocial events\nIndustry “field trips”\nSummer projects\n\n\n\n\nSign up for DSI events at lse.ac.uk/DSI/Events"
  },
  {
    "objectID": "slides/week01_slides_part1.html#our-courses",
    "href": "slides/week01_slides_part1.html#our-courses",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Our courses",
    "text": "Our courses\nDSI offer accessible introductions to Data Science:\n\n\nDS101\nFundamentals of  Data Science\n🎯 Focus:  theoretical concepts of data science\n📂 How:  reflections through reading and writing\n\nDS105\nData for  Data Scientists\n🎯 Focus: collection and handling of real data\n📂 How: hands-on coding exercises and a group project\n\nDS202\nData Science for  Social Scientists\n🎯 Focus: fundamental machine learning algorithms\n📂 How: practical use of ML techniques and metrics"
  },
  {
    "objectID": "slides/week01_slides_part1.html#your-lecturer",
    "href": "slides/week01_slides_part1.html#your-lecturer",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Your lecturer",
    "text": "Your lecturer\n\n\n\n\n\n \nDr. Jonathan Cardoso-Silva\n\nPhD in Computer Science\nBackground: Engineering, Bio & Health Informatics\nFormer Lead Data Scientist\nResearch:\n\nNetworks\nOptimisation\nMachine Learning applications\nData Science Workflow"
  },
  {
    "objectID": "slides/week01_slides_part1.html#teaching-assistants",
    "href": "slides/week01_slides_part1.html#teaching-assistants",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Teaching Assistants",
    "text": "Teaching Assistants\n\n\n\nDr. Stuart Bramwell  ESRC Postdoctoral Fellow  Department of Methodology PhD in Politics (Oxford)\n\n\nYijun Wang  Guest Teacher at the DSI PhD cand. in Health Informatics (KCL)  MSc in Data Science (KCL)\n\n\nMustafa Can Ozkan  Guest Teacher at the DSI PhD cand. in the Spacetime Lab (UCL)  MSc in Transport (Imperial/UCL)\n\n\n\n\n\n\n\nXiaowei Gao  Guest Teacher at the DSI PhD cand. in the Spacetime Lab (UCL)  MSc in Data Science (KCL)\n\n\nAnton Boichenko  Guest Teacher at the DSI Product Developer at Decoded  MSc in Applied Social Data Science (LSE)"
  },
  {
    "objectID": "slides/week01_slides_part1.html#who-are-you",
    "href": "slides/week01_slides_part1.html#who-are-you",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Who are you",
    "text": "Who are you\n\n\n\n\n\n\n\n\n\n\nProgramme\nFreq\n\n\n\n\nBSc in Economics\n34\n\n\nBSc in Pyschological and Behavioural Science\n32\n\n\nGeneral Course\n11\n\n\nBSc in Politics and Economics\n4\n\n\nLLB in Laws\n3\n\n\nBSc in International Relations\n2\n\n\nBSc in Philosophy and Economics\n2\n\n\nBSc in Philosophy, Politics and Economics\n2\n\n\nBSc in Economic History and Geography\n1\n\n\nBSc in Economics and Economic History\n1\n\n\nBSc in Geography with Economics\n1\n\n\nBSc in International Relations and History\n1\n\n\nBSc in Mathematics, Statistics and Business\n1\n\n\nBSc in Philosophy, Logic and Scientific Method\n1\n\n\nErasmus Reciprocal Programme of Study\n1\n\n\nExchange Programme for Students from University of California, Berkeley\n1\n\n\n\n\n\n\n\nSource: LSE For You. Last Updated: 30 September 2022"
  },
  {
    "objectID": "slides/week01_slides_part1.html#learning-objectives-cont.",
    "href": "slides/week01_slides_part1.html#learning-objectives-cont.",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Learning Objectives (cont.)",
    "text": "Learning Objectives (cont.)\n\n\nKnow how to evaluate and compare fitted models, and to improve model performance.\nUse applied computer programming, including the hands-on use of programming through course exercises.\nApply the methods learned to real data through hands-on exercises.\nIntegrate the insights from data analytics into knowledge generation and decision-making;"
  },
  {
    "objectID": "slides/week01_slides_part1.html#learning-objectives-cont.-1",
    "href": "slides/week01_slides_part1.html#learning-objectives-cont.-1",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Learning Objectives (cont.)",
    "text": "Learning Objectives (cont.)\n\nUnderstand an introductory framework for working with natural language (text) data using techniques of machine learning.\n\n\n\nLearn how data science methods have been applied to a particular domain of study (applications)."
  },
  {
    "objectID": "slides/week01_slides_part1.html#philosophy-of-this-course-cont.",
    "href": "slides/week01_slides_part1.html#philosophy-of-this-course-cont.",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Philosophy of this course (cont.)",
    "text": "Philosophy of this course (cont.)\n\nThis is an exciting research area, having important applications in science, industry and policy.\nMachine learning is a fundamental ingredient in the training of a modern data scientist.\n\n\nContent borrowed from ME314 Day 1"
  },
  {
    "objectID": "slides/week01_slides_part1.html#the-basics-of-statistics",
    "href": "slides/week01_slides_part1.html#the-basics-of-statistics",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "The basics of statistics",
    "text": "The basics of statistics\nBasic concepts of Statistics you might want to recap:\n\n\nExpected value, mean, median, variance, standard deviation\nProbabilities and simple probability distributions\nTypes of data\n\ndiscrete vs continuous\ncategorical vs numerical vs ordinal"
  },
  {
    "objectID": "slides/week01_slides_part1.html#resources-stats",
    "href": "slides/week01_slides_part1.html#resources-stats",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Resources (Stats)",
    "text": "Resources (Stats)\nA few references that might be useful to read or skim through:\n\n(Warne 2018, chaps. 1-3,5,6,11-12)\n(Gelman, Hill, and Vehtari 2020, chaps. 1–4)\nIf you are a PBS student, you can revisit the content of PB130 (MT3, MT4, MT8-MT11)"
  },
  {
    "objectID": "slides/week01_slides_part1.html#the-basics-of-r-programming",
    "href": "slides/week01_slides_part1.html#the-basics-of-r-programming",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "The basics of R programming",
    "text": "The basics of R programming\nBasic concepts of programming in R to recap:\n\n\ndata structures (vectors, matrices, data frames)\nhow to manipulate data (filter, subset, select)\nread/write data files (for example: CSV, JSON, TXT)\n(optional but encouraged) some knowledge tidyverse can give you a productive boost\n\nthe official website (tidyverse.org) has some good tutorials."
  },
  {
    "objectID": "slides/week01_slides_part1.html#resources-r",
    "href": "slides/week01_slides_part1.html#resources-r",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Resources (R)",
    "text": "Resources (R)\n\nCheck out “R for Data Science” (Wickham and Grolemund 2016, chaps. 1–21). The online version is free.\n“Statistical inference via data science” (Ismay and Kim 2020, chaps. 4–6) is another great free resource"
  },
  {
    "objectID": "slides/week01_slides_part1.html#what-if-i-struggle-with-r",
    "href": "slides/week01_slides_part1.html#what-if-i-struggle-with-r",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "‘What if I struggle with R’?",
    "text": "‘What if I struggle with R’?\n➡️ Our first lab (Week 02) is a recap of some basic R commands, plus some ggplot2.\n\n\nIf you are not confident with your R skills, I strongly encourage you invest in studying the basics in the next couple of weeks.\nContact LSE Digital Skills Lab to attend in-person workshops or self-paced online R courses."
  },
  {
    "objectID": "slides/week01_slides_part1.html#any-questions",
    "href": "slides/week01_slides_part1.html#any-questions",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Any questions?",
    "text": "Any questions?\n\n\n\n\n\n\nImage created with the DALL·E algorithm using the prompt: ‘35mm macro photography of a robot holding a question mark card, white background’"
  },
  {
    "objectID": "slides/week01_slides_part1.html#syllabus",
    "href": "slides/week01_slides_part1.html#syllabus",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Syllabus",
    "text": "Syllabus\n\n\n\n\n\n\n\nIntro\n\n\n\n\n\n    Introduction, Context & Key Concepts\nWeek 01\n\n\nSupervised Learning\n\n\n\n    Simple and Multiple Linear Regression      Classifiers (Logistic Regression & Naive Bayes)      Resampling methods       Non-linear algorithms (SVM & tree-based models)\nWeek 02  Week 03  Week 04  Week 05\n\n\nUnsupervised Learning\n\n\n\n    Unsupervised Learning: Clustering     Unsupervised Learning: PCA         \nWeek 07  Week 08\n\n\nApplications\n\n\n\n    Applications: Predictive Modelling on Tabular Data    Applications: Text as Data & Topic Modelling     Applications: Social Media Data\nWeek 09  Week 10  Week 11"
  },
  {
    "objectID": "slides/week01_slides_part1.html#structure-of-lectures",
    "href": "slides/week01_slides_part1.html#structure-of-lectures",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Structure of lectures 👨🏻‍🏫",
    "text": "Structure of lectures 👨🏻‍🏫\nOur lectures will be split in two parts:\n\n\nPart I (~ 50 min): Traditional exposition of theoretical content\nbreak (~ 10 min): Grab coffee ☕ or relax 🧘\nPart II (~ 50 min): Live demo\n\nTypically, an exploratory analysis or application of an algorithm\nFeel free to follow along in your own laptops."
  },
  {
    "objectID": "slides/week01_slides_part1.html#structure-of-classes",
    "href": "slides/week01_slides_part1.html#structure-of-classes",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Structure of classes 👩‍💻",
    "text": "Structure of classes 👩‍💻\n\n\nStudents will work on weekly, structured problem sets in the staff-led class sessions.\nTips to get the most of classes:\n\nBring your own laptops 💻 (most tablets are not suitable for programming)\nRead the recommended reading prior to the class\nSkim through the problem set before class"
  },
  {
    "objectID": "slides/week01_slides_part1.html#class-groups",
    "href": "slides/week01_slides_part1.html#class-groups",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Class groups",
    "text": "Class groups\n\n\nGroup 01\n\n📆 Mondays\n⌚ 09:00 — 10:30\n📍 PAN.1.03\n\n\nGroup 02\n\n📆 Mondays\n⌚ 10:30 — 12:00\n📍 PAN.1.03\n\n\nGroup 03\n\n📆 Mondays\n⌚ 13:00 — 14:30\n📍 MAR.1.09\n\n\nGroup 04\n\n📆 Fridays\n⌚ 16:00 — 17:30\n📍 NAB.1.04\n\n\nGroup 05\n\n📆 Mondays\n⌚ 09:00 — 10:30\n📍 32L.LG.11\n\n\nGroup 06\n\n📆 Mondays\n⌚ 10:30 — 12:00\n📍 32L.LG.11\n\n\nGroup 07\n\n📆 Fridays\n⌚ 09:30 — 11:00\n📍 CBG.2.06\n\n\n\n\n🗺️ Check LSE campus map"
  },
  {
    "objectID": "slides/week01_slides_part1.html#your-background-knowledge",
    "href": "slides/week01_slides_part1.html#your-background-knowledge",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Your background knowledge",
    "text": "Your background knowledge\n\nPlease, help our teaching team understand your needs as we prepare for the first labs next week.\nFind the link to the survey on our Slack group or point your phone to the QR code below"
  },
  {
    "objectID": "slides/week01_slides_part1.html#assessments",
    "href": "slides/week01_slides_part1.html#assessments",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Assessments 📔",
    "text": "Assessments 📔\nThe breakdown of assessment for this class will be as follows:"
  },
  {
    "objectID": "slides/week01_slides_part1.html#assessments-1",
    "href": "slides/week01_slides_part1.html#assessments-1",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Assessments 📔",
    "text": "Assessments 📔\n\nProblem sets (60%)\n\n\nSummative problem sets released on Weeks 5, 8 & 11.\nThese will have a similar style to the formative problem sets, a mix of R tasks and your written interpretation of the analyses.\nYou will have 4-6 days to submit your solutions.\nEach of the three summative problem sets is worth 20% of the final mark, and will be graded on a 100 point scale."
  },
  {
    "objectID": "slides/week01_slides_part1.html#assessments-2",
    "href": "slides/week01_slides_part1.html#assessments-2",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Assessments 📔",
    "text": "Assessments 📔\n\nTake-home exam (40%)\n\n\nAn open-book take-home exam, taken during the January exams period.\nExam questions will be comparable in style to the problem sets.\nThe exam questions will be released on Moodle on 5 January 2023. (tentative)\nThe exam is due on 11 January at 4pm (tentative)\n⚠️ Update 11/10/2022: Last year, DS202 exam was performed entirely online due to COVID-19 mitigation procedures. We want to run it online via our own Moodle page again this academic term, we just need to understand LSE regulations about exams for this year. We will update you on this very soon (hopefully by the end of W04)."
  },
  {
    "objectID": "slides/week01_slides_part1.html#office-hours",
    "href": "slides/week01_slides_part1.html#office-hours",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Office hours",
    "text": "Office hours\n\n\nIt is probably a good idea to book office hours if:\n\nyou struggled with a technical or theoretical aspect of a problem set in the previous week,\nyou have queries about careers in data science,\nyou want guidance in how to apply data science to other things you are studying outside this course.\n\nCome prepared. You only have 15 minutes.\nAsk for help sooner rather than later.\nBook slots via StudentHub up to 12 hours in advance."
  },
  {
    "objectID": "slides/week01_slides_part1.html#communication",
    "href": "slides/week01_slides_part1.html#communication",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Communication",
    "text": "Communication\n\n\nJoin our Slack group (more info here).\nUse the public Slack channels to talk to share links, content (or memes) with your colleagues.\nOur teaching team will dedicate some time during the week to answer questions or other interactions on Slack.\nReserve 📧 e-mail for formal requests: extensions, deferrals, etc.\n\nNo need to e-mail to inform you will skip a class, for example."
  },
  {
    "objectID": "slides/week01_slides_part1.html#any-questions-1",
    "href": "slides/week01_slides_part1.html#any-questions-1",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Any questions?",
    "text": "Any questions?\n\n\n\n\n\n\nImage created with the DALL·E algorithm using the prompt: ‘35mm macro photography of a robot holding a question mark card, white background’"
  },
  {
    "objectID": "slides/week01_slides_part1.html#we-changed-how-we-consume-music",
    "href": "slides/week01_slides_part1.html#we-changed-how-we-consume-music",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "We changed how we consume music 🎧",
    "text": "We changed how we consume music 🎧\n\n\n\n\n\n\nTo interact with this plot, check reference (Fischer-Baum 2017) at the end of this presentation."
  },
  {
    "objectID": "slides/week01_slides_part1.html#we-changed-how-we-consume-video",
    "href": "slides/week01_slides_part1.html#we-changed-how-we-consume-video",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "We changed how we consume video 🎞️",
    "text": "We changed how we consume video 🎞️\n\n\n\n\n\n\nTo interact with this plot, check reference (Fischer-Baum 2017) at the end of this presentation."
  },
  {
    "objectID": "slides/week01_slides_part1.html#smartphones-are-a-very-recent-thing",
    "href": "slides/week01_slides_part1.html#smartphones-are-a-very-recent-thing",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Smartphones 📱 are a very recent thing",
    "text": "Smartphones 📱 are a very recent thing\n\n\n\n\n\n\nTo interact with this plot, check reference (Fischer-Baum 2017) at the end of this presentation."
  },
  {
    "objectID": "slides/week01_slides_part1.html#we-spend-a-lot-more-time-connected",
    "href": "slides/week01_slides_part1.html#we-spend-a-lot-more-time-connected",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "We spend a lot more time connected",
    "text": "We spend a lot more time connected"
  },
  {
    "objectID": "slides/week01_slides_part1.html#references",
    "href": "slides/week01_slides_part1.html#references",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "References",
    "text": "References\n\n\nFischer-Baum, Reuben. 2017. “What ‘Tech World’ Did You Grow up In?” Washington Post. https://www.washingtonpost.com/graphics/2017/entertainment/tech-generations/.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. 1st ed. Cambridge University Press. https://doi.org/10.1017/9781139161879.\n\n\nIsmay, Chester, and Albert Young-Sun Kim. 2020. Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. Chapman & Hall/CRC the R Series. Boca Raton: CRC Press / Taylor & Francis Group. https://moderndive.com/.\n\n\nKolawole, Emi. 2013. “About Those 2005 and 2013 Photos of the Crowds in St. Peter’s Square.” Washington Post. http://wapo.st/WKKTMh.\n\n\nWarne, Russell T. 2018. Statistics for the Social Sciences: A General Linear Model Approach. https://www.cambridge.org/highereducation/books/statistics-for-the-social-sciences/716FF25785A6154CC6822D067A959445.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. First edition. Sebastopol, CA: O’Reilly. https://r4ds.had.co.nz/.\n\n\n\n\n\nDS202 - Data Science for Social Scientists 🤖 🤹"
  },
  {
    "objectID": "slides/week01_slides_part2.html#data-science-is",
    "href": "slides/week01_slides_part2.html#data-science-is",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Data science is…",
    "text": "Data science is…\n\n\n“[…] a field of study and practice that involves the collection, storage, and processing of data in order to derive important 💡 insights into a problem or a phenomenon.\n\n\n\n\nSuch data may be generated by humans (surveys, logs, etc.) or machines (weather data, road vision, etc.),\n\n\n\n\nand could be in different formats (text, audio, video, augmented or virtual reality, etc.).”\n\n\n\n\n(Shah 2020, chap. 1) - Emphasis and emojis are of my own making."
  },
  {
    "objectID": "slides/week01_slides_part2.html#the-mythical-unicorn",
    "href": "slides/week01_slides_part2.html#the-mythical-unicorn",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "The mythical unicorn 🦄",
    "text": "The mythical unicorn 🦄\n\n\nknows everything about statistics\n\n\nable to communicate insights perfectly\n\n\nfully understands businesses like no one\n\n\nis a fluent computer programmer\n\n\n\nOf course, such a person does not exist!\n\n\nSee (Davenport 2020) for a more in-depth discussion about this"
  },
  {
    "objectID": "slides/week01_slides_part2.html#in-reality",
    "href": "slides/week01_slides_part2.html#in-reality",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "In reality…",
    "text": "In reality…\n\n\nWe are all jugglers 🤹\n\n\nEveryone brings a different skill set.\nWe need multi-disciplinary teams.\nGood data scientists know a bit of everything.\n\nNot fluent in all things\nUnderstands their strenghts and weaknessess\nThey know when and where to interface with others\n\n\n\n\n\n\n\n\n\nSee (Schutt and O’Neil 2013, chap. 1) for more on this."
  },
  {
    "objectID": "slides/week01_slides_part2.html#the-data-science-workflow-1",
    "href": "slides/week01_slides_part2.html#the-data-science-workflow-1",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "The Data Science Workflow",
    "text": "The Data Science Workflow\n\n\n\n\n\n   \n\nstart\n\n Start   \n\ngather\n\nGather data     \n\nstart->gather\n\n    \n\nstore\n\nStore it          somewhere   \n\ngather->store\n\n           \n\nclean\n\nClean &         pre-process   \n\nstore->clean\n\n           \n\nbuild\n\nBuild a  dataset   \n\nclean->build\n\n           \n\neda\n\nExploratory     data analysis   \n\nbuild->eda\n\n    \n\nml\n\nMachine learning   \n\neda->ml\n\n           \n\ninsight\n\nObtain    insights   \n\nml->insight\n\n           \n\ncommunicate\n\nCommunicate results            \n\ninsight->communicate\n\n           \n\nend\n\n End   \n\ncommunicate->end\n\n   \n\n\n\n\n\n\n\n⚠️ Note that this is a simplified version of what happens in a data science project.  In practice, the process is not linear and there are many feedback loops."
  },
  {
    "objectID": "slides/week01_slides_part2.html#the-data-science-workflow-2",
    "href": "slides/week01_slides_part2.html#the-data-science-workflow-2",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "The Data Science Workflow",
    "text": "The Data Science Workflow\n\n\n\n\n\n   \n\nstart\n\n Start   \n\ngather\n\n Gather data     \n\nstart->gather\n\n    \n\nend\n\n End   \n\nstore\n\n Store it          somewhere   \n\ngather->store\n\n           \n\nclean\n\n Clean &         pre-process   \n\nstore->clean\n\n           \n\nbuild\n\n Build a  dataset   \n\nclean->build\n\n           \n\neda\n\n Exploratory     data analysis   \n\nbuild->eda\n\n    \n\nml\n\n Machine learning   \n\neda->ml\n\n           \n\ninsight\n\n Obtain    insights   \n\nml->insight\n\n           \n\ncommunicate\n\n Communicate results            \n\ninsight->communicate\n\n           \n\ncommunicate->end\n\n   \n\n\n\n\n\nIt is often said that 80% of the time and effort spent on a data science project goes to the tasks highlighted above."
  },
  {
    "objectID": "slides/week01_slides_part2.html#the-data-science-workflow-3",
    "href": "slides/week01_slides_part2.html#the-data-science-workflow-3",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "The Data Science Workflow",
    "text": "The Data Science Workflow\n\n\n\n\n\n   \n\nstart\n\n Start   \n\ngather\n\n Gather data     \n\nstart->gather\n\n    \n\nend\n\n End   \n\nstore\n\n Store it          somewhere   \n\ngather->store\n\n           \n\nclean\n\n Clean &         pre-process   \n\nstore->clean\n\n           \n\nbuild\n\n Build a  dataset   \n\nclean->build\n\n           \n\neda\n\n Exploratory     data analysis   \n\nbuild->eda\n\n    \n\nml\n\n Machine learning   \n\neda->ml\n\n           \n\ninsight\n\n Obtain    insights   \n\nml->insight\n\n           \n\ncommunicate\n\n Communicate results            \n\ninsight->communicate\n\n           \n\ncommunicate->end\n\n   \n\n\n\n\n\nThis course is about Machine Learning. So, in most examples and tutorials, we will assume that we already have good quality data."
  },
  {
    "objectID": "slides/week01_slides_part2.html#data-science-and-social-science",
    "href": "slides/week01_slides_part2.html#data-science-and-social-science",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Data Science and Social Science",
    "text": "Data Science and Social Science\n\nIn reality, data scientists work as a multidisciplinary group, collaborating towards a common goal.\nContent borrowed from ME314 Day 1\n\n\n\nSocial science: The goal is typically explanation\nData science: The goal is frequently prediction, or data exploration\nMany of the same methods are used for both objectives\n\n\n\n\nCheck (Shmueli 2010) for a discussion about this topic."
  },
  {
    "objectID": "slides/week01_slides_part2.html#what-does-it-mean-to-learn-something",
    "href": "slides/week01_slides_part2.html#what-does-it-mean-to-learn-something",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "What does it mean to learn something?",
    "text": "What does it mean to learn something?\n\n\n\n\n\n\nImage created with the DALL·E algorithm using the prompt: ‘35mm macro photography of a robot holding a question mark card, white background’"
  },
  {
    "objectID": "slides/week01_slides_part2.html#predicting-a-sequence-intuitively",
    "href": "slides/week01_slides_part2.html#predicting-a-sequence-intuitively",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Predicting a sequence intuitively",
    "text": "Predicting a sequence intuitively\n\n\nSay our data is the following simple sequence:  \\(6, 9, 12, 15, 18, 21, 24, ...\\) \nWhat number do you expect to come next? Why?\nIt is very likely that you guessed that \\(\\operatorname{next number}=27\\)\nWe spot that the sequence follows a pattern\nFrom this, we notice — we learn — that the sequence is governed by: \\(\\operatorname{next number} = \\operatorname{previous number} + 3\\)"
  },
  {
    "objectID": "slides/week01_slides_part2.html#predicting-a-sequence-formula",
    "href": "slides/week01_slides_part2.html#predicting-a-sequence-formula",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Predicting a sequence (formula)",
    "text": "Predicting a sequence (formula)\nThe next number is a function of the previous one:\n \\[\n\\operatorname{next number} = f(\\operatorname{previous number})\n\\]"
  },
  {
    "objectID": "slides/week01_slides_part2.html#predicting-a-sequence-generic-formula",
    "href": "slides/week01_slides_part2.html#predicting-a-sequence-generic-formula",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Predicting a sequence (generic formula)",
    "text": "Predicting a sequence (generic formula)\nIn general terms, we can represented it as:\n \\[\n\\operatorname{Y} = f(\\operatorname{X})\n\\] \n\nwhere:\n\n\\(Y\\): a quantitative response.  It goes by many names: dependent variable, response, target, outcome\n\\(X\\): a set of predictors,  also called inputs, regressors, covariates, features, independent variables.\n\\(f\\): the systematic information that \\(X\\) provides about \\(Y\\)"
  },
  {
    "objectID": "slides/week01_slides_part2.html#predicting-a-sequence-generic-formula-1",
    "href": "slides/week01_slides_part2.html#predicting-a-sequence-generic-formula-1",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Predicting a sequence (generic formula)",
    "text": "Predicting a sequence (generic formula)\nIn general terms, we can represented it as:\n\n\\[\n\\operatorname{Y} = f(\\operatorname{X}) + \\epsilon\n\\]\n\nwhere:\n\n\\(Y\\): the output\n\\(X\\): a set of inputs\n\\(f\\): the systematic information that \\(X\\) provides about \\(Y\\)\n\\(\\epsilon~~\\): a random error term\n\n\nIn reality, there is some error \\(\\epsilon\\) that cannot be reduced."
  },
  {
    "objectID": "slides/week01_slides_part2.html#approximating-f",
    "href": "slides/week01_slides_part2.html#approximating-f",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Approximating \\(f\\)",
    "text": "Approximating \\(f\\)\n\n\n\\(f\\) is almost always unknown\nWe aim to find an approximation (a model). Let’s call it \\(\\hat{f}\\)\nthat can then use it to predict values of \\(Y\\) for whatever \\(X\\).\nThat is: \\(\\hat{Y} = \\hat{f}(X)\\)"
  },
  {
    "objectID": "slides/week01_slides_part2.html#what-is-machine-learning",
    "href": "slides/week01_slides_part2.html#what-is-machine-learning",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\n\n\nStatistical learning, or Machine learning, refers to a set of approaches for estimating \\(f\\).\nEach algorithm you will learn on this course has its own way to determine \\(\\hat{f}\\) given data"
  },
  {
    "objectID": "slides/week01_slides_part2.html#types-of-learning",
    "href": "slides/week01_slides_part2.html#types-of-learning",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Types of learning",
    "text": "Types of learning\n\nIn general terms, there are two main ways to learn from data:\n\n\nSupervised Learning\n\nEach observation (\\(x_i\\)) has an outcome associated with it (\\(y_i\\)).\nYour goal is to find a \\(\\hat{f}\\) that produces \\(\\hat{Y}\\) value close to the true \\(Y\\) values.\nOur focus on 🗓️ Weeks 2, 3, 4 & 5.\n\n\nUnsupervised Learning\n\nYou have observations (\\(x_i\\)) but there is no response variable.\nYour goal is to find a \\(\\hat{f}\\), focused only on \\(X\\) that best represents the patterns in the data.\nOur focus on 🗓️ Weeks 7 & 8."
  },
  {
    "objectID": "slides/week01_slides_part2.html#data-structure",
    "href": "slides/week01_slides_part2.html#data-structure",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Data Structure",
    "text": "Data Structure\nLet’s go back to our example:\n\n\n\nOur simple sequence:\n \\(6, 9, 12, 15, 18, 21, 24\\) \n\n\n\nBecomes:\n\n\n\n\\(X\\)\n\\(Y\\)\n\n\n\n\n6\n9\n\n\n9\n12\n\n\n12\n15\n\n\n15\n18\n\n\n18\n21\n\n\n21\n24\n\n\n\n\n\n\nAnd for prediction:\n\n\n\n\n\n\n\n\\(X\\)\n\\(\\hat{Y}\\)\n\n\n\n\n24\n?\n\n\n\nwe present the \\(X\\) values and ask the fitted model to give us \\(\\hat{Y}\\).\n\n\n\n\nSame data but now in tabular format\na few other terms: - training data/test data - fitted model"
  },
  {
    "objectID": "slides/week01_slides_part2.html#the-ground-truth",
    "href": "slides/week01_slides_part2.html#the-ground-truth",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "The ground truth",
    "text": "The ground truth\nLet’s create a dataframe to illustrate the process of training an algorithm:\n\nlibrary(tidyverse)\n\ndf = tibble(X=as.integer(seq(6, 21, 3)),\n            Y=as.integer(seq(6+3, 21+3, 3)))\nprint(df)\n\n\n\n# A tibble: 6 × 2\n      X     Y\n  <int> <int>\n1     6     9\n2     9    12\n3    12    15\n4    15    18\n5    18    21\n6    21    24"
  },
  {
    "objectID": "slides/week01_slides_part2.html#adding-noise",
    "href": "slides/week01_slides_part2.html#adding-noise",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Adding noise",
    "text": "Adding noise\nLet’s simulate the introduction of some random error:\n\n# Let's simulate some noise\ngaussian_noise = rnorm(n=nrow(df), mean=0, sd=1.5)\n\n# Call it \"observed Y\"\ndf$obsY = df$Y + gaussian_noise\nprint(df)\n\n\n\n# A tibble: 6 × 3\n      X     Y  obsY\n  <int> <int> <dbl>\n1     6     9  7.95\n2     9    12 12.1 \n3    12    15 15.9 \n4    15    18 19.4 \n5    18    21 21.7 \n6    21    24 24.4"
  },
  {
    "objectID": "slides/week01_slides_part2.html#visualizing-the-data",
    "href": "slides/week01_slides_part2.html#visualizing-the-data",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Visualizing the data",
    "text": "Visualizing the data"
  },
  {
    "objectID": "slides/week01_slides_part2.html#visualizing-the-data-w-noise",
    "href": "slides/week01_slides_part2.html#visualizing-the-data-w-noise",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Visualizing the data (w/ noise)",
    "text": "Visualizing the data (w/ noise)"
  },
  {
    "objectID": "slides/week01_slides_part2.html#visualizing-the-data-w-noise-1",
    "href": "slides/week01_slides_part2.html#visualizing-the-data-w-noise-1",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Visualizing the data (w/ noise)",
    "text": "Visualizing the data (w/ noise)\n\n\n\n\n\n\n\nWhich line is closer to the “truth”?"
  },
  {
    "objectID": "slides/week01_slides_part2.html#visualizing-the-data-w-noise-2",
    "href": "slides/week01_slides_part2.html#visualizing-the-data-w-noise-2",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Visualizing the data (w/ noise)",
    "text": "Visualizing the data (w/ noise)\n\n\n\n\n\n\n\nHow much error can we accept?"
  },
  {
    "objectID": "slides/week01_slides_part2.html#assessing-error",
    "href": "slides/week01_slides_part2.html#assessing-error",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Assessing error",
    "text": "Assessing error\nHow much error was introduced by \\(\\epsilon\\) per sample?\n\ndf$error    <- df$Y - df$obsY  # Calculate the error\ndf$absError <- abs(df$error)   # Ignore the sign of error\ndf\n\n\n\n# A tibble: 6 × 5\n      X     Y  obsY  error absError\n  <int> <int> <dbl>  <dbl>    <dbl>\n1     6     9  7.95  1.05     1.05 \n2     9    12 12.1  -0.141    0.141\n3    12    15 15.9  -0.882    0.882\n4    15    18 19.4  -1.39     1.39 \n5    18    21 21.7  -0.714    0.714\n6    21    24 24.4  -0.377    0.377\n\n\n\nOn average, what is the error?\n\nmean(df$absError)\n\n[1] 0.758862\n\n\n\n\nThis measure is called the Mean Absolute Error."
  },
  {
    "objectID": "slides/week01_slides_part2.html#measures-of-error",
    "href": "slides/week01_slides_part2.html#measures-of-error",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Measures of error",
    "text": "Measures of error\nThis is what we computed:\n\\[\n\\operatorname{MAE} = \\frac{\\sum_{i=1}^n{|(y_i + \\epsilon) - y_i|}}{n}\n\\]\n\n\nWe were able to compute this error because we knew what the ground truth \\(Y\\), we knew what its real value was.\nIt was only possible because it was a simulation, not real data.\nIn practice, we will almost never be able to assess the impact of \\(\\epsilon\\).\nWe will use this same way of thinking to assess how good and accurate our models are. 🔜"
  },
  {
    "objectID": "slides/week01_slides_part2.html#references",
    "href": "slides/week01_slides_part2.html#references",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "References",
    "text": "References\n\n\nDavenport, Thomas. 2020. “Beyond Unicorns: Educating, Classifying, and Certifying Business Data Scientists.” Harvard Data Science Review 2 (2). https://doi.org/10.1162/99608f92.55546b4a.\n\n\nSchutt, Rachel, and Cathy O’Neil. 2013. Doing Data Science. First edition. Beijing ; Sebastopol: O’Reilly Media. https://ebookcentral.proquest.com/lib/londonschoolecons/detail.action?docID=1465965.\n\n\nShah, Chirag. 2020. A Hands-on Introduction to Data Science. Cambridge, United Kingdom ; New York, NY, USA: Cambridge University Press. https://librarysearch.lse.ac.uk/permalink/f/1n2k4al/TN_cdi_askewsholts_vlebooks_9781108673907.\n\n\nShmueli, Galit. 2010. “To Explain or to Predict?” Statistical Science 25 (3). https://doi.org/10.1214/10-STS330.\n\n\n\n\n\nDS202 - Data Science for Social Scientists 🤖 🤹"
  },
  {
    "objectID": "slides/week02_slides_part1.html#the-basic-models",
    "href": "slides/week02_slides_part1.html#the-basic-models",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "The basic models",
    "text": "The basic models\n\nLinear regression is a simple approach to supervised learning.\n\n\n\n\nThe generic supervised model:\n\\[\nY = \\operatorname{f}(X) + \\epsilon\n\\]\nis defined more explicitly as follows ➡️\n\n\n\n\nSimple linear regression\n\n\\[\n\\begin{align}\nY = \\beta_0 +& \\beta_1 X + \\epsilon, \\\\\n\\\\\n\\\\\n\\end{align}\n\\] \nwhen we use a single predictor, \\(X\\).\n\n\n\n\nMultiple linear regression\n\n\\[\n\\begin{align}\nY = \\beta_0 &+ \\beta_1 X_1 + \\beta_2 X_2 \\\\\n   &+ \\dots \\\\\n   &+ \\beta_p X_p + \\epsilon\n\\end{align}\n\\]\n\nwhen there are multiple predictors, \\(X_p\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nTrue regression functions are never linear!\nAlthough it may seem overly simplistic, linear regression is extremely useful both conceptually and practically.\n\n\n\n\n\n\n\n\n\nSee 📺 Regression: Crash Course Statistics on YouTube for inspiration on how to present linear regression to students.  \nWe will talk about both types of models, how we can estimate the values of all \\(\\beta\\) and assess how good our models are."
  },
  {
    "objectID": "slides/week02_slides_part1.html#linear-regression-with-a-single-predictor",
    "href": "slides/week02_slides_part1.html#linear-regression-with-a-single-predictor",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "Linear Regression with a single predictor",
    "text": "Linear Regression with a single predictor\n\n\nWe assume a model:\n\n\\[\nY = \\beta_0 + \\beta_1 X + \\epsilon ,\n\\]\n\n\n\n\n\n\n\n\n\n\nwhere:\n\n\\(\\beta_0\\): an unknown constant that represents the intercept of the line.\n\\(\\beta_1\\): an unknown constant that represents the slope of the line\n\\(\\epsilon\\): the random error term (irreducible)\n\n\n\n\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are also known as coefficients or parameters of the model."
  },
  {
    "objectID": "slides/week02_slides_part1.html#linear-regression-with-a-single-predictor-1",
    "href": "slides/week02_slides_part1.html#linear-regression-with-a-single-predictor-1",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "Linear Regression with a single predictor",
    "text": "Linear Regression with a single predictor\n\n\nWe want to estimate:\n\\[\n\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} x\n\\]\n\n\n\n\n\n\n\n\n\nwhere:\n\n\\(\\hat{y}\\): is a prediction of \\(Y\\) on the basis of \\(X = x\\).\n\\(\\hat{\\beta_0}\\): is an estimate of the “true” \\(\\beta_0\\).\n\\(\\hat{\\beta_1}\\): is an estimate of the “true” \\(\\beta_1\\).\n\n\n\n\n\nThe hat symbol denotes an estimated value."
  },
  {
    "objectID": "slides/week02_slides_part1.html#different-estimators-different-equations",
    "href": "slides/week02_slides_part1.html#different-estimators-different-equations",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "Different estimators, different equations",
    "text": "Different estimators, different equations\n\n\n\nThere are multiple ways to estimate the coefficients.\n\nIf you use different techniques, you might get different equations\nThe most common algorithm is called  Ordinary Least Squares (OLS)\nJust to name a few other estimators (Karafiath 2009):\n\nLeast Absolute Deviation (LAD)\nWeighted Least Squares (WLS)\nGeneralized Least Squares (GLS)\nHeteroskedastic-Consistent (HC) variants\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will only cover OLS in this course."
  },
  {
    "objectID": "slides/week02_slides_part1.html#the-concept-of-residuals",
    "href": "slides/week02_slides_part1.html#the-concept-of-residuals",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "The concept of residuals",
    "text": "The concept of residuals\nSuppose you came across some data:\n\n\n\n\n\n\n\nFirst, let’s think of the concept of residuals…\n\n\nAnd you suspect there is a linear relationship between X and Y."
  },
  {
    "objectID": "slides/week02_slides_part1.html#the-concept-of-residuals-1",
    "href": "slides/week02_slides_part1.html#the-concept-of-residuals-1",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "The concept of residuals",
    "text": "The concept of residuals\nSo, you decide to fit a line to it.\n\n\n\n\n\n\n\nA line that goes right through the middle of the cloud of data."
  },
  {
    "objectID": "slides/week02_slides_part1.html#the-concept-of-residuals-2",
    "href": "slides/week02_slides_part1.html#the-concept-of-residuals-2",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "The concept of residuals",
    "text": "The concept of residuals\n\nResiduals are the distances from each data point to this line. \n\n\n\n\n\n\n\n\\(e_i\\)\\(=y_i-\\hat{y}_i\\) represents the \\(i\\)th residual"
  },
  {
    "objectID": "slides/week02_slides_part1.html#residual-sum-of-squares-rss",
    "href": "slides/week02_slides_part1.html#residual-sum-of-squares-rss",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "Residual Sum of Squares (RSS)",
    "text": "Residual Sum of Squares (RSS)\nFrom this, we can define the  Residual Sum of Squares  (RSS) as\n\\[\n\\mathrm{RSS}= e_1^2 + e_2^2 + \\dots + e_n^2,\n\\]\n\nor equivalently as\n\\[\n\\mathrm{RSS}= (y_1 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_1)^2 + (y_2 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_2)^2 + \\dots + (y_n - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_n)^2.\n\\]\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe (ordinary) least squares approach chooses \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) to minimize the RSS.\n\n\n\n\n\nThat is how it does its job."
  },
  {
    "objectID": "slides/week02_slides_part1.html#a-question-for-you",
    "href": "slides/week02_slides_part1.html#a-question-for-you",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "A question for you",
    "text": "A question for you\n\n\n\n\nWhy the squares and not, say, just the sum of residuals?\n\nImage created with the DALL·E algorithm using the prompt: ‘35mm macro photography of a robot holding a question mark card, white background’\n\n\n\n\nExplain that the sum penalizes individual large errors a lot more Consider adding a visualisation to illustrate this point."
  },
  {
    "objectID": "slides/week02_slides_part1.html#the-objective-function",
    "href": "slides/week02_slides_part1.html#the-objective-function",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "The objective function",
    "text": "The objective function\nWe treat this as an optimisation problem. We want to minimize RSS: \\[\n\\begin{align}\n\\min \\mathrm{RSS} =& \\sum_i^n{e_i^2} \\\\\n             =& \\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2} \\\\\n             =& \\sum_i^n{\\left(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i\\right)^2}\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/week02_slides_part1.html#estimating-hatbeta_0",
    "href": "slides/week02_slides_part1.html#estimating-hatbeta_0",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "Estimating \\(\\hat{\\beta}_0\\)",
    "text": "Estimating \\(\\hat{\\beta}_0\\)\nTo find \\(\\hat{\\beta}_0\\), we have to solve the following partial derivative:\n\\[\n\\frac{\\partial ~\\mathrm{RSS}}{\\partial \\hat{\\beta}_0}{\\sum_i^n{(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2}} = 0\n\\]\n\n… which will lead you to:\n\n\n\\[\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x},\n\\]\n\n\nwhere we made use of the sample means:\n\n\\(\\bar{y} \\equiv \\frac{1}{n} \\sum_{i=1}^n y_i\\)\n\\(\\bar{x} \\equiv \\frac{1}{n} \\sum_{i=1}^n x_i\\)\n\n\n\n\nFull derivation if needed:\n\\[\n\\begin{align}\n0 &= \\frac{\\partial ~\\mathrm{RSS}}{\\partial \\hat{\\beta}_0}{\\sum_i^n{(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2}} & (\\text{chain rule})\\\\\n0 &= \\sum_i^n{-2 (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)} & (\\text{take $-2$ out})\\\\\n0 &= -2 \\sum_i^n{ (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)}  & (\\div -2) \\\\\n0 &=\\sum_i^n{ (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)}  & (\\text{sep. sums}) \\\\\n0 &=\\sum_i^n{y_i} - \\sum_i^n{\\hat{\\beta}_0} - \\sum_i^n{\\hat{\\beta}_1 x_i}  & (\\text{simplify}) \\\\\n0 &=\\sum_i^n{y_i} - n\\hat{\\beta}_0 - \\hat{\\beta}_1\\sum_i^n{ x_i}  & (+ n\\hat{\\beta}_0) \\\\\nn\\hat{\\beta}_0 &= \\sum_i^n{y_i} - \\hat{\\beta}_1\\sum_i^n{ x_i} & (\\text{isolate }\\hat{\\beta}_0 ) \\\\\n\\hat{\\beta}_0 &= \\frac{\\sum_i^n{y_i} - \\hat{\\beta}_1\\sum_i^n{ x_i}}{n} & (\\text{rearranging}) \\\\\n\\hat{\\beta}_0 &= \\frac{\\sum_i^n{y_i}}{n} - \\hat{\\beta}_1\\frac{\\sum_i^n{x_i}}{n} & (\\text{or simply}) \\\\\n\\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x} & \\blacksquare\n\\end{align}\n\\]\n\n\n\n📝 Give it a go! Pretend \\(\\hat{\\beta}_1\\) is constant and use the power rule to solve the equation and reach the same result."
  },
  {
    "objectID": "slides/week02_slides_part1.html#estimating-hatbeta_1",
    "href": "slides/week02_slides_part1.html#estimating-hatbeta_1",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "Estimating \\(\\hat{\\beta}_1\\)",
    "text": "Estimating \\(\\hat{\\beta}_1\\)\nSimilarly, to find \\(\\hat{\\beta}_1\\) we solve:\n\\[\n\\frac{\\partial ~\\mathrm{RSS}}{\\partial \\hat{\\beta}_1}{[\\sum_i^n{y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i}]} = 0\n\\]\n\n… which will lead you to:\n\n\n\\[\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})^2}\n\\]\n\n\n\nFull derivation if needed:\n\\[\n\\begin{align}\n0 &= \\frac{\\partial ~\\mathrm{RSS}}{\\partial \\hat{\\beta}_1}{\\sum_i^n{(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2}} & (\\text{chain rule})\\\\\n0 &= \\sum_i^n{\\left(-2x_i~ (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\\right)} & (\\text{take $-2$ out})\\\\\n0 &= -2\\sum_i^n{\\left( x_i~ (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\\right)} & (\\div -2) \\\\\n0 &= \\sum_i^n{\\left(x_i~ (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\\right)} & (\\text{distribute } x_i) \\\\\n0 &= \\sum_i^n{\\left(y_ix_i - \\hat{\\beta}_0x_i - \\hat{\\beta}_1 x_i^2\\right)} & (\\text{replace } \\hat{\\beta}_0) \\\\\n0 &= \\sum_i^n{\\left(y_ix_i - (\\bar{y} - \\hat{\\beta}_1 \\bar{x})x_i - \\hat{\\beta}_1 x_i^2\\right)} & (\\text{rearrange}) \\\\\n0 &= \\sum_i^n{\\left(y_ix_i - \\bar{y}x_i + \\hat{\\beta}_1 \\bar{x}x_i - \\hat{\\beta}_1 x_i^2\\right)} & (\\text{separate sums}) \\\\\n0 &= \\sum_i^n{\\left(y_ix_i - \\bar{y}x_i\\right)} + \\sum_i^n{\\left(\\hat{\\beta}_1 \\bar{x}x_i - \\hat{\\beta}_1 x_i^2\\right)} & (\\text{take $\\hat{\\beta}_1$ out}) \\\\\n0 &= \\sum_i^n{\\left(y_ix_i - \\bar{y}x_i\\right)} + \\hat{\\beta}_1\\sum_i^n{\\left(\\bar{x}x_i - x_i^2\\right)} & (\\text{isolate}) \\\\\n\\hat{\\beta}_1 &= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})^2}\n\\end{align}\n\\]\n\n\n\n📝 Give it a go! Use the same method as before to solve the equation and isolate \\(\\hat{\\beta}_1\\). Tip: Use the previous formula to substitute \\(\\hat{\\beta}_0\\)."
  },
  {
    "objectID": "slides/week02_slides_part1.html#parameter-estimation-ols",
    "href": "slides/week02_slides_part1.html#parameter-estimation-ols",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "Parameter Estimation (OLS)",
    "text": "Parameter Estimation (OLS)\nAnd that is how OLS works!\n\\[\n\\begin{align}\n\\hat{\\beta}_1 &= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})^2} \\\\\n\\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/week02_slides_part1.html#estimates-for-multiple-regression",
    "href": "slides/week02_slides_part1.html#estimates-for-multiple-regression",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "Estimates for Multiple Regression",
    "text": "Estimates for Multiple Regression\n\nThe process of estimation is similar when we have more than one predictor. To estimate:\n\\[\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\dots + \\hat{\\beta}_p x_p.\n\\]\n\n\nWe aim to minimize Residual Sum of Squares as before:\n\\[\n\\min \\mathrm{RSS} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_{i1} - \\hat{\\beta}_2 x_{i2} - \\dots - \\hat{\\beta}_p x_{ip})^2.\n\\]\nThis is done using standard statistical software — you need a good linear algebra solver.\n\n\nThe values \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_p\\) that minimize RSS are the multiple least squares regression coefficient estimates."
  },
  {
    "objectID": "slides/week02_slides_part1.html#example-advertising-data",
    "href": "slides/week02_slides_part1.html#example-advertising-data",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "Example: Advertising data",
    "text": "Example: Advertising data\n\n\nA sample of the data:\n\nlibrary(tidyverse)\n\nfile = \"https://www.statlearning.com/s/Advertising.csv\"\nadvertising <- read_csv(file) %>% select(-1)\nhead(advertising, 11)\n\n# A tibble: 11 × 4\n      TV radio newspaper sales\n   <dbl> <dbl>     <dbl> <dbl>\n 1 230.   37.8      69.2  22.1\n 2  44.5  39.3      45.1  10.4\n 3  17.2  45.9      69.3   9.3\n 4 152.   41.3      58.5  18.5\n 5 181.   10.8      58.4  12.9\n 6   8.7  48.9      75     7.2\n 7  57.5  32.8      23.5  11.8\n 8 120.   19.6      11.6  13.2\n 9   8.6   2.1       1     4.8\n10 200.    2.6      21.2  10.6\n11  66.1   5.8      24.2   8.6\n\n\n\nHow the data is spread:\n\nsummary(advertising$TV)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.70   74.38  149.75  147.04  218.82  296.40 \n\n\n\nsummary(advertising$radio)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   9.975  22.900  23.264  36.525  49.600 \n\n\n\nsummary(advertising$newspaper)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.30   12.75   25.75   30.55   45.10  114.00 \n\n\n\nsummary(advertising$sales)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.60   10.38   12.90   14.02   17.40   27.00"
  },
  {
    "objectID": "slides/week02_slides_part1.html#simple-linear-regression-models",
    "href": "slides/week02_slides_part1.html#simple-linear-regression-models",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "Simple linear regression models",
    "text": "Simple linear regression models\n\n\n\nTV 📺\n\n\ntv_model <- lm(sales ~ TV, data=advertising)\ncat(sprintf(\"Sales (1k units) = %.4f %+.4f TV ($ 1k)\\n\", \n            tv_model$coefficients[\"(Intercept)\"], \n            tv_model$coefficients[\"TV\"]))\n\nSales (1k units) = 7.0326 +0.0475 TV ($ 1k)\n\n\n\nRadio 📻\n\n\nradio_model <- lm(sales ~ radio, data=advertising)\ncat(sprintf(\"Sales (1k units) = %.4f %+.4f Radio ($ 1k)\\n\", \n            radio_model$coefficients[\"(Intercept)\"], \n            radio_model$coefficients[\"radio\"]))\n\nSales (1k units) = 9.3116 +0.2025 Radio ($ 1k)\n\n\n\n\nNewspaper 📰\n\n\nnewspaper_model <- lm(sales ~ newspaper, data=advertising)\ncat(sprintf(\"Sales (1k units) = %.4f %+.4f Newspaper ($ 1k)\\n\", \n            newspaper_model$coefficients[\"(Intercept)\"], \n            newspaper_model$coefficients[\"newspaper\"]))\n\nSales (1k units) = 12.3514 +0.0547 Newspaper ($ 1k)\n\n\n\n🗨️ How should we interpret these models?\n\n\n\n\n\nGather answers from students.\nFor every 1k dollars spent in advertising on a particular media channel, we expect more \\(\\hat{\\beta}_1\\) thousand units of the product to be sold.\nWhy don’t the models agree about the intercept?"
  },
  {
    "objectID": "slides/week02_slides_part1.html#confidence-interval-of-coefficients",
    "href": "slides/week02_slides_part1.html#confidence-interval-of-coefficients",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "Confidence Interval of coefficients",
    "text": "Confidence Interval of coefficients\n\n\n\nThe confidence interval of an estimate has the form: \\[\n\\hat{\\beta}_1 \\pm 2 \\times \\mathrm{SE}(\\hat{\\beta}_1).\n\\] where \\(SE\\) is the standard error and reflects how the estimate varies under repeated sampling.\n\n\n\n\nThat is, there is approximately a 95% chance that the interval \\[\n\\biggl[ \\hat{\\beta}_1 - 2 \\times \\mathrm{SE}(\\hat{\\beta}_1), \\hat{\\beta}_1 + 2 \\times \\mathrm{SE}(\\hat{\\beta}_1) \\biggr]\n\\] will contain the true value of \\(\\beta_1\\).\n\n\n\nHow SE differs from STD? See (Altman and Bland 2005)"
  },
  {
    "objectID": "slides/week02_slides_part1.html#standard-errors",
    "href": "slides/week02_slides_part1.html#standard-errors",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "Standard Errors",
    "text": "Standard Errors\n\n\nThe standard error of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) is shown below:\n\\[\n\\begin{align}\n  \\mathrm{SE}(\\hat{\\beta}_1)^2 &= \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}, \\\\\n  \\mathrm{SE}(\\hat{\\beta}_0)^2 &= \\sigma^2 \\biggl[ \\frac{1}{n} +  \\frac{\\bar{x}^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\biggr],\n\\end{align}\n\\]\nwhere \\(\\sigma^2 = \\operatorname{Var}(\\epsilon)\\).\n\n\n\nBut, wait, we don’t know \\(\\epsilon\\)! How would we compute \\(\\sigma^2\\)?\nIn practice, we aproximate \\(\\sigma^2 \\approx \\mathrm{RSE} = \\sqrt{\\mathrm{RSS}/(n-2)}\\).\n\n\n\n\n\n\n\n\n\nImportant\n\n\n💡 Standard errors are a type of standard deviation but are not the same! See (Altman and Bland 2005) for more on this.\n\n\n\n\n\n\nThese formulas are only valid if we assume the errors \\(\\epsilon_i\\) have common variance \\(\\sigma^2\\) and are uncorrelated.\nRSE makes a comeback in Section 3.1.3"
  },
  {
    "objectID": "slides/week02_slides_part1.html#back-to-our-advertising-linear-models",
    "href": "slides/week02_slides_part1.html#back-to-our-advertising-linear-models",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "Back to our Advertising linear models",
    "text": "Back to our Advertising linear models\nWhat are the confidence intervals of our independent linear models?\n\n\n\nTV 📺\n\n\nconfint(tv_model)\n\n                 2.5 %     97.5 %\n(Intercept) 6.12971927 7.93546783\nTV          0.04223072 0.05284256\n\n\n\nRadio 📻\n\n\nconfint(radio_model)\n\n                2.5 %     97.5 %\n(Intercept) 8.2015885 10.4216877\nradio       0.1622443  0.2427472\n\n\n\n\nNewspaper 📰\n\n\nconfint(newspaper_model)\n\n                  2.5 %      97.5 %\n(Intercept) 11.12595560 13.57685854\nnewspaper    0.02200549  0.08738071\n\n\n\n🗨️ What does it mean?\n\n\n\n\n\n\nFor every additional $1000 invested in Radio, we can expect an increase in sales of between 162 and 242 units.\n\n\n\nUse the function confint to compute confidence intervals in R."
  },
  {
    "objectID": "slides/week02_slides_part1.html#p-values",
    "href": "slides/week02_slides_part1.html#p-values",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "p-values",
    "text": "p-values\n\nTo test the null hypothesis, we compute a t-statistic, given by \\[\nt = \\frac{\\hat{\\beta}_1 - 0}{\\mathrm{SE}(\\hat{\\beta}_1)},\n\\]\nThis will have a t-distribution1 with \\(n - 2\\) degrees of freedom, assuming \\(\\beta_1 = 0\\).\nUsing statistical software, it is easy to compute the probability of observing any value equal to \\(\\mid t \\mid\\) or larger.\nWe call this probability the p-value.\n\n🤔 How are the t-distribution and the Normal distribution related? Check this link to find out."
  },
  {
    "objectID": "slides/week02_slides_part1.html#back-to-our-advertising-linear-models-1",
    "href": "slides/week02_slides_part1.html#back-to-our-advertising-linear-models-1",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "Back to our Advertising linear models",
    "text": "Back to our Advertising linear models\nHow significant are the linear models?\n\n\n\nTV 📺\n\n\nout <- capture.output(summary(tv_model))\ncat(paste(out[9:15]), sep=\"\\n\")\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 7.032594   0.457843   15.36   <2e-16 ***\nTV          0.047537   0.002691   17.67   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nRadio 📻\n\n\nout <- capture.output(summary(radio_model))\ncat(paste(out[9:15]), sep=\"\\n\")\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  9.31164    0.56290  16.542   <2e-16 ***\nradio        0.20250    0.02041   9.921   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nNewspaper 📰\n\n\nout <- capture.output(summary(newspaper_model))\ncat(paste(out[9:15]), sep=\"\\n\")\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 12.35141    0.62142   19.88  < 2e-16 ***\nnewspaper    0.05469    0.01658    3.30  0.00115 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n🗨️ What does it mean?"
  },
  {
    "objectID": "slides/week02_slides_part1.html#references",
    "href": "slides/week02_slides_part1.html#references",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "References",
    "text": "References\n\n\nAltman, Douglas G, and J Martin Bland. 2005. “Standard Deviations and Standard Errors.” BMJ 331 (7521): 903. https://doi.org/10.1136/bmj.331.7521.903.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York NY: Springer. https://www.statlearning.com/.\n\n\nKarafiath, Imre. 2009. “Is There a Viable Alternative to Ordinary Least Squares Regression When Security Abnormal Returns Are the Dependent Variable?” Review of Quantitative Finance and Accounting 32 (1): 17–31. https://doi.org/10.1007/s11156-007-0079-y.\n\n\n\n\n\nDS202 - Data Science for Social Scientists 🤖 🤹"
  },
  {
    "objectID": "slides/week02_slides_part2.html#residual-standard-errors-rse",
    "href": "slides/week02_slides_part2.html#residual-standard-errors-rse",
    "title": "🗓️ Week 02:Multiple Linear Regression",
    "section": "Residual Standard Errors (RSE)",
    "text": "Residual Standard Errors (RSE)\n\n\nRecall the “true model”: \\(Y = f(X) + \\epsilon\\)\nEven if we knew the true values of \\(\\beta_0\\) and \\(\\beta_1\\) — not just the estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) — our predictions of sales might still be off.\nBy how much?"
  },
  {
    "objectID": "slides/week02_slides_part2.html#residual-standard-errors-rse-1",
    "href": "slides/week02_slides_part2.html#residual-standard-errors-rse-1",
    "title": "🗓️ Week 02:Multiple Linear Regression",
    "section": "Residual Standard Errors (RSE)",
    "text": "Residual Standard Errors (RSE)\n\n\nThis can be estimated by the variance of errors: \\(\\sigma^2 = \\operatorname{Var}(\\epsilon)\\).\nAs said earlier, this quantity can be approximated, for the simple linear regression case, by the Residual Standard Errors (\\(\\mathrm{RSE}\\)) formula below:\n\n\n\n\\[\n\\sigma^2 \\approx \\mathrm{RSE} = \\sqrt{\\frac{\\mathrm{RSS}}{(n-\\mathrm{df})}}\n\\]\nwhere \\(\\mathrm{RSS} = \\sum_i^n{(y_i - \\hat{y}_i)^2}\\) represents the residual sum of squares and \\(\\mathrm{df}\\) represents the degrees of freedom in our model.\n\n\n\n\n➡️ It turns out that \\(\\mathrm{RSE}\\) is a good way to assess the goodness-of-fit of a model."
  },
  {
    "objectID": "slides/week02_slides_part2.html#back-to-our-advertising-linear-models",
    "href": "slides/week02_slides_part2.html#back-to-our-advertising-linear-models",
    "title": "🗓️ Week 02:Multiple Linear Regression",
    "section": "Back to our Advertising linear models",
    "text": "Back to our Advertising linear models\nLet’s compare the linear models we fitted earlier:\n\n\n\n\n\n\nTV 📺\n\n\nout <- capture.output(summary(tv_model))\ncat(paste(out[16:16]), sep=\"\\n\")\n\nResidual standard error: 3.259 on 198 degrees of freedom\n\n\n\nRadio 📻\n\n\nout <- capture.output(summary(radio_model))\ncat(paste(out[16:16]), sep=\"\\n\")\n\nResidual standard error: 4.275 on 198 degrees of freedom\n\n\n\n\nNewspaper 📰\n\n\nout <- capture.output(summary(newspaper_model))\ncat(paste(out[16:16]), sep=\"\\n\")\n\nResidual standard error: 5.092 on 198 degrees of freedom\n\n\n\n🗨️ What does it mean?"
  },
  {
    "objectID": "slides/week02_slides_part2.html#the-r2-statistic",
    "href": "slides/week02_slides_part2.html#the-r2-statistic",
    "title": "🗓️ Week 02:Multiple Linear Regression",
    "section": "The \\(R^2\\) statistic",
    "text": "The \\(R^2\\) statistic\n\nR-squared or fraction of variance explained is defined as:\n\n\\[\nR^2 = \\frac{\\mathrm{TSS - RSS}}{\\mathrm{TSS}} = 1 - \\frac{\\mathrm{RSS}}{\\mathrm{TSS}}\n\\]\nwhere TSS = \\(\\sum_{i=1}^n (y_i - \\bar{y})^2\\) is the total sum of squares. \n\n\n\n\n\n\n\nTip\n\n\nIntuitively, \\(R^2\\) measures the proportion of variability in \\(Y\\) that can be explained using \\(X\\).\n\n\\(R^2\\) close to 1 means that a large proportion of the variance in \\(Y\\) is explained by the regression.\n\\(R^2\\) close to 0 means that the regression does not explain much of the variability in \\(Y\\)."
  },
  {
    "objectID": "slides/week02_slides_part2.html#sample-correlation-coefficient",
    "href": "slides/week02_slides_part2.html#sample-correlation-coefficient",
    "title": "🗓️ Week 02:Multiple Linear Regression",
    "section": "Sample correlation coefficient",
    "text": "Sample correlation coefficient\nBy the way, in the simple linear regression setting, it can be shown that \\(R^2 = (\\operatorname{Cor}(X, Y))^2\\), where \\(\\operatorname{Cor}(X, Y)\\) is the correlation between \\(X\\) and \\(Y\\):\n\\[\n\\operatorname{Cor}(X, Y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}}.\n\\]\n\n\n📝 Give it a go! Play around with the definition of \\(R^2\\) shown in the previous slide and verify that \\(R^2 = (\\operatorname{Cor}(X, Y))^2\\)."
  },
  {
    "objectID": "slides/week02_slides_part2.html#f-statistic",
    "href": "slides/week02_slides_part2.html#f-statistic",
    "title": "🗓️ Week 02:Multiple Linear Regression",
    "section": "F-statistic",
    "text": "F-statistic\nWe used t-statistic to compute p-values for the coefficients (\\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\)).Now how do I test whether the model, as a whole, makes sense?\n\n\n\n\nFor this, we perform the hypothesis test: \\[\n\\begin{align}\n&~~~~H_0:&\\beta_1 = \\beta_2 = \\ldots = \\beta_j = 0 \\\\\n&\\text{vs} \\\\\n&~~~~H_A:& \\text{at least one } \\beta_j \\neq 0.\n\\end{align}\n\\]\n\n\n\n\nwhich is performed by computing the F-statistic: \\[\nF = \\frac{(TSS - RSS) / p}{RSS/(n - p - 1)} \\sim F_{p, n-p-1}\n\\]\n\n\n\n\n\nIf F is close to 1, there is no relationship between the response and the predictor(s).\nIf \\(H_A\\) is true, then we expect \\(F\\) to be greater than 1.\nCheck (James et al. 2021, 75–77) for an in-depth explanation of this test.\n\n\n\n\n\n\n\n\nNote that the F-statistic applies to both simple and multiple linear regression models.\nCheck this link if you want to understand the difference between the t-test and the the F-test."
  },
  {
    "objectID": "slides/week02_slides_part2.html#back-to-our-advertising-linear-models-1",
    "href": "slides/week02_slides_part2.html#back-to-our-advertising-linear-models-1",
    "title": "🗓️ Week 02:Multiple Linear Regression",
    "section": "Back to our Advertising linear models",
    "text": "Back to our Advertising linear models\nHow well do our models explain the variability of the response?\n\n\n\nTV 📺\n\n\nout <- capture.output(summary(tv_model))\ncat(paste(out[17:18]), sep=\"\\n\")\n\nMultiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: < 2.2e-16\n\n\n\nRadio 📻\n\n\nout <- capture.output(summary(radio_model))\ncat(paste(out[17:18]), sep=\"\\n\")\n\nMultiple R-squared:  0.332, Adjusted R-squared:  0.3287 \nF-statistic: 98.42 on 1 and 198 DF,  p-value: < 2.2e-16\n\n\n\n\nNewspaper 📰\n\n\nout <- capture.output(summary(newspaper_model))\ncat(paste(out[17:18]), sep=\"\\n\")\n\nMultiple R-squared:  0.05212,   Adjusted R-squared:  0.04733 \nF-statistic: 10.89 on 1 and 198 DF,  p-value: 0.001148\n\n\n\n🗨️ What does it mean?"
  },
  {
    "objectID": "slides/week02_slides_part2.html#a-multiple-linear-regression-to-advertising",
    "href": "slides/week02_slides_part2.html#a-multiple-linear-regression-to-advertising",
    "title": "🗓️ Week 02:Multiple Linear Regression",
    "section": "A multiple linear regression to Advertising",
    "text": "A multiple linear regression to Advertising\n\n\nWhen you run a linear model in R, you can call the summary function to see and check all of these statistics we’ve covered so far.\nBy now, you should be able to understand its full output\n\n\nFitting all predictors:\n\n\nTV 📺 + Radio 📻 + Newspaper 📰\n\nfull_model <- lm(sales ~ ., data=advertising)\nsummary(full_model)\n\n\nCall:\nlm(formula = sales ~ ., data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.938889   0.311908   9.422   <2e-16 ***\nTV           0.045765   0.001395  32.809   <2e-16 ***\nradio        0.188530   0.008611  21.893   <2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: < 2.2e-16\n\n\n\nConfidence Intervals\n\nconfint(full_model)\n\n                  2.5 %     97.5 %\n(Intercept)  2.32376228 3.55401646\nTV           0.04301371 0.04851558\nradio        0.17154745 0.20551259\nnewspaper   -0.01261595 0.01054097"
  },
  {
    "objectID": "slides/week02_slides_part2.html#interpreting-the-coefficients",
    "href": "slides/week02_slides_part2.html#interpreting-the-coefficients",
    "title": "🗓️ Week 02:Multiple Linear Regression",
    "section": "Interpreting the coefficients",
    "text": "Interpreting the coefficients\n\nRecall the multiple regression model:\n\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p + \\epsilon ,\n\\]\n\n\nWe interpret \\(\\beta_j\\) as the average effect on \\(Y\\) of a one unit increase in \\(X_j\\), holding all other predictors fixed. In the advertising example, the model becomes\n\n\\[\n\\mathrm{sales} = \\beta_0 + \\beta_1 \\times \\mathrm{TV} + \\beta_2 \\times \\mathrm{radio} + \\beta_3 \\times \\mathrm{newspaper} + \\epsilon .\n\\]"
  },
  {
    "objectID": "slides/week02_slides_part2.html#interpreting-the-coefficients-1",
    "href": "slides/week02_slides_part2.html#interpreting-the-coefficients-1",
    "title": "🗓️ Week 02:Multiple Linear Regression",
    "section": "Interpreting the coefficients",
    "text": "Interpreting the coefficients\n\n\nThe ideal scenario is when the predictors are uncorrelated – a balanced design:\n\nEach coefficient can be estimated and tested separately.\nInterpretations such as “a unit change in \\(X_j\\) is associated with a \\(\\beta_j\\) change in \\(Y\\), while all the other variables stay fixed”, are possible.\n\nCorrelations amongst predictors cause problems:\n\nThe variance of all coefficients tends to increase, sometimes dramatically\nInterpretations become hazardous – when \\(X_j\\) changes, everything else changes.\n\nClaims of causality should be avoided for observational data."
  },
  {
    "objectID": "slides/week02_slides_part2.html#references",
    "href": "slides/week02_slides_part2.html#references",
    "title": "🗓️ Week 02:Multiple Linear Regression",
    "section": "References",
    "text": "References\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York NY: Springer. https://www.statlearning.com/.\n\n\n\n\n\nDS202 - Data Science for Social Scientists 🤖 🤹"
  },
  {
    "objectID": "slides/week03_slides_part1.html#classification",
    "href": "slides/week03_slides_part1.html#classification",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Classification",
    "text": "Classification\n\n\nWe have so far only modelled quantitative responses.\nToday, we focus on predicting categorical, or qualitative, responses.\n\n\n\n\n\nThe generic supervised model:\n\\[\nY = \\operatorname{f}(X) + \\epsilon\n\\]\nstill applies, only this time \\(Y\\) is categorical. ➡️\n\n\n\nOur categorical variables of interest take values in an unordered set \\(\\mathcal{C}\\), such as:\n\n\\(\\text{eye color} \\in \\mathcal{C} = \\{\\color{brown}{brown},\\color{blue}{blue},\\color{green}{green}\\}\\)\n\\(\\text{email} \\in \\mathcal{C} = \\{spam, ham\\}\\)\n\\(\\text{football results} \\in \\mathcal{C} \\{away\\ win,draw,home\\ win\\}\\)\n\n\n\n\n\nOpening slides - Unordered here is an important distinction. - We can also call it a class"
  },
  {
    "objectID": "slides/week03_slides_part1.html#why-cant-i-use-linear-regression",
    "href": "slides/week03_slides_part1.html#why-cant-i-use-linear-regression",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Why can’t I use linear regression?",
    "text": "Why can’t I use linear regression?\n\n\nWhat if I just coded each category as a number?\n\\[\nY =\n    \\begin{cases}\n        1 &\\text{if}~\\color{brown}{brown},\\\\\n        2 &\\text{if}~\\color{blue}{blue},\\\\\n        3 &\\text{if}~\\color{green}{green}.\n    \\end{cases}\n\\]\n\nWhat could go wrong?\n\n\n\n\n\n\nHow would you interpret a particular prediction if your model returned:\n\n\\(\\hat{y} = ~~1.5\\) or\n\\(\\hat{y} = ~~0.1\\) or\n\\(\\hat{y} = 20.0\\)?\n\n\n\n\n\n\n\nKey takeaway: Regression is not suitable for all problems. - regression cannot accommodate a qualitative response with more than two classes - regression will not provide meaningful estaimtes of Pr(Y|X)"
  },
  {
    "objectID": "slides/week03_slides_part1.html#more-on-classification",
    "href": "slides/week03_slides_part1.html#more-on-classification",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "More on Classification",
    "text": "More on Classification\n\n\nOften we are more interested in estimating the probabilities that \\(X\\) belongs to each category in \\(\\mathcal{C}\\).\n\n\n\n\nFor example, it is sometimes more valuable to have an estimate of the probability that an insurance claim is fraudulent, than a classification fraudulent or not.\n\n\n\n\n\n\n\nA successful gambling strategy, for instance, requires placing bets on outcomes to which you believe the bookmakers have assigned incorrect probabilities. Knowing the most likely outcome is not enough!\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nStatistical models for ordinal response, when sets are discrete but have an order, are outside the scope of this course. Should you need to create models for ordinal variables, consult “ordinal logistic regression”. A good reference about this is (Agresti 2019, chap. 6).\n\n\n\n\n\nKey takeaway: normally, we estimate"
  },
  {
    "objectID": "slides/week03_slides_part1.html#speaking-of-probabilities",
    "href": "slides/week03_slides_part1.html#speaking-of-probabilities",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Speaking of Probabilities…",
    "text": "Speaking of Probabilities…\nLet’s talk about three possible interpretations of probability:\n\n\n\n\nClassical\n\n\n\n\nFrequentist\n\n\n\n\nBayesian\n\n\n\n\n\nEvents of the same kind can be reduced to a certain number of equally possible cases.\nExample: coin tosses lead to either heads or tails \\(1/2\\) of the time ( \\(50\\%/50\\%\\))\n\n\n\n\nWhat would be the outcome if I repeat the process many times?\nExample: if I toss a coin \\(1,000,000\\) times, I expect \\(\\approx 50\\%\\) heads and \\(\\approx 50\\%\\) tails outcome.\n\n\n\n\nWhat is your judgement of the likelihood of the outcome? Based on previous information.\nExample: if I know that this coin has symmetric weight, I expect a \\(50\\%/50\\%\\) outcome.\n\n\n\n\n\n\nSource: (DeGroot and Schervish 2003)"
  },
  {
    "objectID": "slides/week03_slides_part1.html#speaking-of-probabilities-1",
    "href": "slides/week03_slides_part1.html#speaking-of-probabilities-1",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Speaking of Probabilities…",
    "text": "Speaking of Probabilities…\nFor our purposes:\n\nProbabilities are numbers between 0 and 1\nThe sum of all possible outcomes of an event must sum to 1.\nIt is useful to think of things as probabilities\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n💡 Although there is no such thing as “a probability of \\(120\\%\\)” or “a probability of \\(-23\\%\\)”, you could still use this language to refer to increase or decrease in an outcome."
  },
  {
    "objectID": "slides/week03_slides_part1.html#the-logistic-regression-model",
    "href": "slides/week03_slides_part1.html#the-logistic-regression-model",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "The Logistic Regression model",
    "text": "The Logistic Regression model\n\nConsider a binary response:\n\\[\nY = \\begin{cases}\n0 \\\\\n1\n\\end{cases}\n\\]\n\n\nWe model the probability that \\(Y = 1\\) using the logistic function (aka. sigmoid curve):\n\\[\nPr(Y = 1|X) = p(X) = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1 X}}\n\\]\n\n\n\n\n\nSource of illustration: TIBCO\n\n\n\nThis is how this function looks like"
  },
  {
    "objectID": "slides/week03_slides_part1.html#the-logistic-function",
    "href": "slides/week03_slides_part1.html#the-logistic-function",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "The Logistic function",
    "text": "The Logistic function\n\nChanging \\(\\beta_0\\) while keeping \\(\\beta_1 = 1\\):"
  },
  {
    "objectID": "slides/week03_slides_part1.html#the-logistic-function-cont.",
    "href": "slides/week03_slides_part1.html#the-logistic-function-cont.",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "The Logistic function (cont.)",
    "text": "The Logistic function (cont.)\n\nKeep \\(\\beta_0 = 0\\) but vary \\(\\beta_1\\):"
  },
  {
    "objectID": "slides/week03_slides_part1.html#maximum-likelihood-estimate",
    "href": "slides/week03_slides_part1.html#maximum-likelihood-estimate",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Maximum likelihood estimate",
    "text": "Maximum likelihood estimate\n\nAs with linear regression, the coefficients are unknown and need to be estimated from training data:\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1X}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}}\n\\]\n\n\nWe estimate these by maximising the likelihood function:\n\\[\n\\max \\ell(\\beta_0, \\beta_1) = \\prod_{i:y_i=1}{p(x_i)} \\prod_{i':y_{i'}=0} (1 - p(x_{i'})),\n\\]\nand we call this method the Maximum Likelihood Estimate (MLE).\n\n\n➡️ As usual, there are multiple ways to solve this equation!\n\n\n\nKey takeaway of this slide: MLE (logistic regression) is analogous to OLS (linear regression).\nIntuition: What are the values for \\(\\alpha\\) and \\(\\beta\\) that generate predicted probabilities, \\(\\hat{Y}_i\\) for each training observation that are as close as possible to the realised outcomes, \\(Y_i\\)?\n\n\n\n💡 If you want to read on how exactly this is solved check this link"
  },
  {
    "objectID": "slides/week03_slides_part1.html#solutions-to-mle",
    "href": "slides/week03_slides_part1.html#solutions-to-mle",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Solutions to MLE",
    "text": "Solutions to MLE\n\n\nMLE is much more difficult to solve than the least squares formulations.\nMost solutions rely on a variant of the Hill Climbing algorithm\n\n\n\n\n\n\n\n\n\nHow do you find the latitude and longitude of a mountain peak if you can’t see very far?\n\n\n\nStart somewhere.\nLook around for the best way to go up.\nGo a small distance in that direction.\nLook around for the best way to go up.\nGo a small distance in that direction.\n\\(\\cdots\\)\n\n\n\n\n\n\n\nAdvanced: If for whatever random reason, you find yourself enamored with the Maximum Likelihood Estimate, check (Agresti 2019) for a recent take on the statistical properties of this method."
  },
  {
    "objectID": "slides/week03_slides_part1.html#the-concept-of-odds",
    "href": "slides/week03_slides_part1.html#the-concept-of-odds",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "The concept of odds",
    "text": "The concept of odds\nThe quantity below is called the odds:\n\\[\n\\frac{p(X)}{1 - p(X)} = e^{\\beta_0 + \\beta_1 X}\n\\]\n\n\n\n\n\n\nExample\n\n\nIf the odds are 9, then \\(\\frac{p(X)}{(1-p(X))} = 9 \\Rightarrow p(X) = 0.9\\).\nThis means that 9 out of 10 people will default.\n\n\n\n\n\n\n\n\n\nTip\n\n\nHow to interpret \\(\\beta_1\\)\nIf X increases one unit then the odds increase by a factor of \\(e^{\\beta_1}\\)\n\n\n\n\n\n📝 Give it a go! Using algebra, can you re-arrange the equation for \\(p(X)\\) presented in the Logistic regression model slides to arrive at the odds quantity shown above?"
  },
  {
    "objectID": "slides/week03_slides_part1.html#log-odds-or-logit",
    "href": "slides/week03_slides_part1.html#log-odds-or-logit",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Log odds or logit",
    "text": "Log odds or logit\n\nIt is also useful to think of the odds in log terms.\n\n\\[\nlog\\left(\\frac{p(X)}{1 - p(X)}\\right) = \\beta_0 + \\beta_1 X\n\\]\n\nWe call the quantity above the log odds or logit\n\n\n\n\n\n\n\nTip\n\n\nHow to interpret \\(\\beta_1\\)\nIf X increases one unit then the log odds increase by \\(\\beta_1\\)"
  },
  {
    "objectID": "slides/week03_slides_part1.html#example-default-data",
    "href": "slides/week03_slides_part1.html#example-default-data",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Example: Default data",
    "text": "Example: Default data\n\n\nA sample of the data:\n\nlibrary(ISLR2)\n\nhead(ISLR2::Default, n=15)\n\n   default student   balance    income\n1       No      No  729.5265 44361.625\n2       No     Yes  817.1804 12106.135\n3       No      No 1073.5492 31767.139\n4       No      No  529.2506 35704.494\n5       No      No  785.6559 38463.496\n6       No     Yes  919.5885  7491.559\n7       No      No  825.5133 24905.227\n8       No     Yes  808.6675 17600.451\n9       No      No 1161.0579 37468.529\n10      No      No    0.0000 29275.268\n11      No     Yes    0.0000 21871.073\n12      No     Yes 1220.5838 13268.562\n13      No      No  237.0451 28251.695\n14      No      No  606.7423 44994.556\n15      No      No 1112.9684 23810.174\n\n\n\nHow the data is spread:\n\nsummary(ISLR2::Default$default)\n\n  No  Yes \n9667  333 \n\n\n\nsummary(ISLR2::Default$balance)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    0.0   481.7   823.6   835.4  1166.3  2654.3 \n\n\n\nsummary(ISLR2::Default$income)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    772   21340   34553   33517   43808   73554 \n\n\n\nsummary(ISLR2::Default$student)\n\n  No  Yes \n7056 2944"
  },
  {
    "objectID": "slides/week03_slides_part1.html#simple-logistic-regression-models",
    "href": "slides/week03_slides_part1.html#simple-logistic-regression-models",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Simple logistic regression models",
    "text": "Simple logistic regression models\n\n\n\nIncome 💰\n\n\nincome_model <- \n  glm(default ~ income, data=ISLR2::Default, family=binomial)\ncat(sprintf(\"beta_0 = %.5f | beta_1 = %e\",\n            income_model$coefficients[\"(Intercept)\"],\n            income_model$coefficients[\"income\"]))\n\nbeta_0 = -3.09415 | beta_1 = -8.352575e-06\n\n\n\nBalance 💸\n\n\nbalance_model <- \n  glm(default ~ balance, data=ISLR2::Default, family=binomial)\ncat(sprintf(\"beta_0 = %.5f | beta_1 = %.4f\",\n            balance_model$coefficients[\"(Intercept)\"],\n            balance_model$coefficients[\"balance\"]))\n\nbeta_0 = -10.65133 | beta_1 = 0.0055\n\n\n\n\nStudent 🧑‍🎓\n\n\nstudent_model <- \n  glm(default ~ student, data=ISLR2::Default, family=binomial)\ncat(sprintf(\"beta_0 = %.5f | beta_1 = %.4f\",\n            student_model$coefficients[\"(Intercept)\"],\n            student_model$coefficients[\"studentYes\"]))\n\nbeta_0 = -3.50413 | beta_1 = 0.4049\n\n\n\n\n\n\n\n\n\nNote\n\n\nLogistic regression coefficients are a bit trickier to interpret when compared to those of linear regression. Let’s look at how it works ➡️\n\n\n\n\n\n\n\n\nGather answers from students."
  },
  {
    "objectID": "slides/week03_slides_part1.html#example-default-vs-balance",
    "href": "slides/week03_slides_part1.html#example-default-vs-balance",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Example: Default vs Balance",
    "text": "Example: Default vs Balance\n\n\n\nModel: Default vs Balance 💸\n\n\\[\n\\hat{y} = \\frac{e^{-10.65133 + 0.005498917X}}{1 + e^{-10.65133 + 0.005498917X}}\n\\]\n\n  That is:\n\\[\n\\begin{align}\n\\hat{\\beta}_0 &= -10.65133\\\\\n\\hat{\\beta}_1 &= 0.005498917\n\\end{align}\n\\]\n\n\n\n\n\nInterpreting \\(\\hat{\\beta}_0\\):\n\nIn the absence of balance information:\n\nLog odds: \\(-10.65133\\)\nOdds : \\(e^{-10.65133} = 2.366933 \\times 10^{-5}\\) \\[\n\\begin{align}\np(\\text{default}=\\text{Yes}) &= \\frac{\\text{odds}}{(1 + \\text{odds})} \\\\\n                           &= 2.366877 \\times 10^{-5}\\end{align}\n\\]\n\n\n\n\n\nInterpreting \\(\\hat{\\beta}_1\\):\n\nWith balance information:\n\nLog odds: \\(0.005498917\\)\nOdds : \\(e^{0.005498917} = 1.005514\\)\nThat is, for every \\(\\$1\\) increase in balance, the probability of default increases \n\n\n\n\n\n\n\nNote that the increase is cumulative, not linear. It depends on where X is."
  },
  {
    "objectID": "slides/week03_slides_part1.html#example-default-vs-income",
    "href": "slides/week03_slides_part1.html#example-default-vs-income",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Example: Default vs Income",
    "text": "Example: Default vs Income\n\n\n\nModel: Default vs Income 💰\n\n\\[\n\\hat{y} = \\frac{e^{-3.094149 - 8.352575 \\times 10^{-6} X}}{1 + e^{-3.094149 - 8.352575 \\times 10^{-6} X}}\n\\]\n\n  That is:\n\\[\n\\begin{align}\n\\hat{\\beta}_0 &= - 3.094149\\\\\n\\hat{\\beta}_1 &= - 8.352575 \\times 10^{-6}\n\\end{align}\n\\]\n\n\n\n\n\nInterpreting \\(\\hat{\\beta}_0\\):\n\nIn the absence of balance information:\n\nLog odds: \\(- 3.094149\\)\nOdds : \\(e^{- 3.094149} = 0.04531355\\) \\[\n\\begin{align}\np(\\text{default}=\\text{Yes}) &= \\frac{\\text{odds}}{(1 + \\text{odds})} \\\\\n                           &= 0.04334924 = 4.33\\%\\end{align}\n\\]\n\n\n\n\n\nInterpreting \\(\\hat{\\beta}_1\\):\n\nWith balance information:\n\nLog odds: \\(- 8.352575\\)\nOdds : \\(e^{- 8.352575} = 0.9999916\\)\nThat is, for every \\(\\$1\\) increase in income, the probability of default decreases \n\n\n\n\n\n\n\nNote that the increase is cumulative, not linear. It depends on where X is."
  },
  {
    "objectID": "slides/week03_slides_part1.html#example-default-vs-is-student",
    "href": "slides/week03_slides_part1.html#example-default-vs-is-student",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Example: Default vs Is Student?",
    "text": "Example: Default vs Is Student?\n\n\n\nModel: Default vs Student 🧑‍🎓\n\n\\[\n\\hat{y} = \\frac{e^{-3.504128 + 0.4048871 X}}{1 + e^{-3.504128 + 0.4048871 X}}\n\\]\n\n  That is:\n\\[\n\\begin{align}\n\\hat{\\beta}_0 &= -3.504128\\\\\n\\hat{\\beta}_1 &= +0.4048871\n\\end{align}\n\\]\n\n\n\n\n\nInterpreting \\(\\hat{\\beta}_0\\):\n\nIn the absence of balance information:\n\nLog odds: \\(-3.504128\\)\nOdds : \\(e^{- 3.504128} = 0.03007299\\) \\[\n\\begin{align}\np(\\text{default}=\\text{Yes}) &= \\frac{\\text{odds}}{(1 + \\text{odds})} \\\\\n                           &= 0.02919501 \\approx 2.92\\%\\end{align}\n\\]\n\n\n\n\n\nInterpreting \\(\\hat{\\beta}_1\\):\n\nWith balance information:\n\nLog odds: \\(0.4048871\\)\nOdds : \\(e^{0.4048871} = 1.499133\\)\n\nIf person is a student, then the probability of default increases"
  },
  {
    "objectID": "slides/week03_slides_part1.html#model-info",
    "href": "slides/week03_slides_part1.html#model-info",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Model info",
    "text": "Model info\nThe output of summary is similar to that of linear regression:\n\n\n\nModel: Default vs Balance 💸\n\n\nsummary(balance_model)\n\n\nCall:\nglm(formula = default ~ balance, family = binomial, data = ISLR2::Default)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.2697  -0.1465  -0.0589  -0.0221   3.7589  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.065e+01  3.612e-01  -29.49   <2e-16 ***\nbalance      5.499e-03  2.204e-04   24.95   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1596.5  on 9998  degrees of freedom\nAIC: 1600.5\n\nNumber of Fisher Scoring iterations: 8\n\n\n\nConfidence Intervals\n\nconfint(balance_model)\n\n                    2.5 %       97.5 %\n(Intercept) -11.383288936 -9.966565064\nbalance       0.005078926  0.005943365"
  },
  {
    "objectID": "slides/week03_slides_part1.html#wraping-up-on-coefficients",
    "href": "slides/week03_slides_part1.html#wraping-up-on-coefficients",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Wraping up on coefficients:",
    "text": "Wraping up on coefficients:\n\n\n\nPay attention to the sign of the coefficient. The sign of the coefficients indicate the direction of the association.\nIf the value of a predictor increases, we look at the sign of its coefficient:\n\nIf it is a ➕ positive coefficient, we predict an increase in the probability of the class\nIf it is a ➖ negative coefficient, we predict a decrease in the probability of the class"
  },
  {
    "objectID": "slides/week03_slides_part1.html#multiple-logistic-regression-1",
    "href": "slides/week03_slides_part1.html#multiple-logistic-regression-1",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Multiple Logistic Regression",
    "text": "Multiple Logistic Regression\n\nIt is straightforward to extend the logistic model to include multiple predictors:\n\n\\[\nlog \\left( \\frac{p(X)}{1-p(X)} \\right)=\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p\n\\]\n\\[\np(X) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p}}\n\\]\n\nMost things are still available (hypothesis test, confidence intervals, etc.)\nLet’s explore the output and summary of the full model ⏭️"
  },
  {
    "objectID": "slides/week03_slides_part1.html#fitting-all-predictors-of-default",
    "href": "slides/week03_slides_part1.html#fitting-all-predictors-of-default",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Fitting all predictors of Default",
    "text": "Fitting all predictors of Default\n\n\nFull Model\n\nfull_model <- glm(default ~ ., data=ISLR2::Default, family=binomial)\nsummary(full_model)\n\n\nCall:\nglm(formula = default ~ ., family = binomial, data = ISLR2::Default)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.4691  -0.1418  -0.0557  -0.0203   3.7383  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.087e+01  4.923e-01 -22.080  < 2e-16 ***\nstudentYes  -6.468e-01  2.363e-01  -2.738  0.00619 ** \nbalance      5.737e-03  2.319e-04  24.738  < 2e-16 ***\nincome       3.033e-06  8.203e-06   0.370  0.71152    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1571.5  on 9996  degrees of freedom\nAIC: 1579.5\n\nNumber of Fisher Scoring iterations: 8\n\n\n\nConfidence Intervals\n\nconfint(full_model)\n\n                    2.5 %        97.5 %\n(Intercept) -1.185902e+01 -9.928174e+00\nstudentYes  -1.109018e+00 -1.822147e-01\nbalance      5.294898e-03  6.204587e-03\nincome      -1.304712e-05  1.912447e-05"
  },
  {
    "objectID": "slides/week03_slides_part1.html#references",
    "href": "slides/week03_slides_part1.html#references",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "References",
    "text": "References\n\n\nAgresti, Alan. 2019. An Introduction to Categorical Data Analysis. Third edition. Wiley Series in Probability and Statistics. Hoboken, NJ: John Wiley & Sons.\n\n\nDeGroot, Morris H., and Mark J. Schervish. 2003. Probability and Statistics. 3. ed., international edition. Boston Munich: Addison-Wesley.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York NY: Springer. https://www.statlearning.com/.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDS202 - Data Science for Social Scientists 🤖 🤹"
  },
  {
    "objectID": "slides/week03_slides_part2.html#bayes-theorem",
    "href": "slides/week03_slides_part2.html#bayes-theorem",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\n\nBefore we go on to explain what Naive Bayes is about, we need to understand the formula below.\n\n\\[\nP(\\mathbf{Y} = k | \\mathbf{X} = x) = \\frac{P(k)P(\\mathbf{X}|\\mathbf{Y}=k)}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}\n\\]\n\n\nLet’s look at it step-by-step ⏭️"
  },
  {
    "objectID": "slides/week03_slides_part2.html#bayes-theorem-1",
    "href": "slides/week03_slides_part2.html#bayes-theorem-1",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\n\\[\nP(\\mathbf{Y} = k | \\mathbf{X} = x) = \\frac{P(k)P(\\mathbf{X}|\\mathbf{Y}=k)}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}\n\\]\nNew variables\n\n\n\\(K \\Rightarrow\\) is the set of classes. In the binary case, \\(K = \\{0, 1\\}\\).\n\\(P(k) \\Rightarrow\\) is the probability that a random sample belongs to class \\(k\\).\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe textbook uses a slightly different notation."
  },
  {
    "objectID": "slides/week03_slides_part2.html#bayes-theorem-2",
    "href": "slides/week03_slides_part2.html#bayes-theorem-2",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\n\\[\n\\color{blue}{P(\\mathbf{Y} = k | \\mathbf{X} = x)} \\color{Gainsboro}{= \\frac{P(k)P(\\mathbf{X}|\\mathbf{Y}=k)}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}}\n\\]\n\n\nThe quantity above (in blue) is called the posterior distribution\nIt is what we are interested in when making inferences/predictions\n\n\n\nRead it as:\n\nWhat is the probability that the class is \\(k\\) given that the sample is \\(x\\)?"
  },
  {
    "objectID": "slides/week03_slides_part2.html#bayes-theorem-3",
    "href": "slides/week03_slides_part2.html#bayes-theorem-3",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\n\\[\n\\color{Gainsboro}{P(\\mathbf{Y} = k | \\mathbf{X} = x) =} \\frac{\\color{blue}{P(k)}\\color{Gainsboro}{P(\\mathbf{X}|\\mathbf{Y}=k)}}{\\color{Gainsboro}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}}\n\\]\n\n\nThe quantity above (in blue) is called the prior distribution\nIt represents the proportion of samples of class \\(k\\) we believe (estimate) we would find if sampling at random.\n\n\n\nRead it as:\n\nWhat is the probability that the class is \\(k\\) given a random sample?"
  },
  {
    "objectID": "slides/week03_slides_part2.html#bayes-theorem-4",
    "href": "slides/week03_slides_part2.html#bayes-theorem-4",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\n\\[\n\\color{Gainsboro}{P(\\mathbf{Y} = k | \\mathbf{X} = x) =} \\frac{\\color{Gainsboro}{P(k)}\\color{blue}{P(\\mathbf{X}|\\mathbf{Y}=k)}}{\\color{Gainsboro}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}}\n\\]\n\n\nThe quantity above (in blue) is often called the likelihood\nIt represents the density function of \\(\\mathbf{X}\\) for samples of class \\(k\\).\n\n\n\nThink of it as:\n\nWhat values would I expect \\(X\\) to take when the class is \\(\\mathbf{Y} = k\\)?"
  },
  {
    "objectID": "slides/week03_slides_part2.html#bayes-theorem-5",
    "href": "slides/week03_slides_part2.html#bayes-theorem-5",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\n\\[\n\\color{Gainsboro}{P(\\mathbf{Y} = k | \\mathbf{X} = x) =} \\frac{\\color{Gainsboro}{P(k)}\\color{Gainsboro}{P(\\mathbf{X}|\\mathbf{Y}=k)}}{\\color{blue}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}}\n\\]\n\n\nThe quantity above (in blue) represents the density function of \\(\\mathbf{X}\\) regardless of the class\nIt is often called the marginal probability of \\(\\mathbf{X}\\).\n\nNote that \\(\\color{blue}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}} = P(\\mathbf{X})\\)\n\n\n\n\nThink of it as:\n\nWhat values would I expect \\(X\\) if ignored the class?"
  },
  {
    "objectID": "slides/week03_slides_part2.html#bayes-theorem-6",
    "href": "slides/week03_slides_part2.html#bayes-theorem-6",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\n\\[\nP(\\mathbf{Y} = k | \\mathbf{X} = x) = \\frac{P(k)P(\\mathbf{X}|\\mathbf{Y}=k)}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}\n\\]\n\nLet’s look at how different algorithms explore this rule ⏭️"
  },
  {
    "objectID": "slides/week03_slides_part2.html#linear-discriminant-analysis-lda",
    "href": "slides/week03_slides_part2.html#linear-discriminant-analysis-lda",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Linear Discriminant Analysis (LDA)",
    "text": "Linear Discriminant Analysis (LDA)\n\n\nAssumptions:\n\nLikelihood follows a Gaussian distribution\n\nEach class has its own mean, \\(\\mu_k\\)\nAll classes have the same standard deviation\n\nThat is, \\(\\sigma^2_1 = \\sigma^2_2 = \\ldots = \\sigma^2_K\\), or simply \\(\\sigma^2\\)\n\n\nWe denote this as: \\(P(\\mathbf{X}|\\mathbf{Y}=k) \\sim N(\\mu_k, \\sigma^2)\\)"
  },
  {
    "objectID": "slides/week03_slides_part2.html#lda---estimates",
    "href": "slides/week03_slides_part2.html#lda---estimates",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "LDA - Estimates",
    "text": "LDA - Estimates\n\nWe estimate the mean per class and the shared standard deviation as follows:\n\n\n\\[\n\\begin{align}\n\\hat{\\mu}_k &= \\frac{1}{n_k}\\sum_{i:y_i=k}{x_i}\\\\\n\\hat{\\sigma}^2 &= \\frac{1}{n - K}\\sum_{k=1}^K{\\sum_{i:y_i=k}{\\left(x_i - \\hat{\\mu}_k\\right)^2}} \\\\\n\\hat{P}(k) &= \\frac{n_k}{n}\n\\end{align}\n\\]\n\n\n\nwhere:\n\n\\(n\\) is the total number of training observations\n\\(n_k\\) is the number of training observations in the \\(k\\)th class\n\n\n\n\n\nRead (James et al. 2021, sec. 4.4) to understand why these estimates are the way they are.\n\n\n\n\nMention that priors could come from prior knowledge"
  },
  {
    "objectID": "slides/week03_slides_part2.html#naive-bayes-classifier-1",
    "href": "slides/week03_slides_part2.html#naive-bayes-classifier-1",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Naive Bayes Classifier",
    "text": "Naive Bayes Classifier\n\n\nMain Assumption:\n\n\nWithin the \\(k\\)th class, the \\(p\\) predictors are independent\n\n\n\n\nAssuming features are not associated (not correlated), the likelihood becomes: \\[\nP(\\mathbf{X}|\\mathbf{Y}=k) = \\underbrace{P(x_1 |\\mathbf{Y}=k)}_{1\\text{st} \\text{ predictor}} \\times \\underbrace{P(x_2 |\\mathbf{Y}=k)}_{2\\text{nd} \\text{ predictor}} \\times \\ldots \\times \\underbrace{P(x_p |\\mathbf{Y}=k)}_{p\\text{-th} \\text{ predictor}}\n\\]\n\n\n\n\nThis means the posterior is given by: \\[\nP(\\mathbf{Y} = k| \\mathbf{X} = x) = \\frac{\\quad\\quad P(k) \\times P(x_1 |\\mathbf{Y}=k) \\times P(x_2 |\\mathbf{Y}=k) \\times \\ldots \\times P(x_p |\\mathbf{Y}=k)}{\\sum_{l=1}^K{P(l) \\times P(x_1 |\\mathbf{Y}=l) \\times P(x_2 |\\mathbf{Y}=l) \\times \\ldots \\times P(x_p |\\mathbf{Y}=l)}}\n\\]"
  },
  {
    "objectID": "slides/week03_slides_part2.html#a-naive-approach-indeed",
    "href": "slides/week03_slides_part2.html#a-naive-approach-indeed",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "A naive approach indeed",
    "text": "A naive approach indeed\n\n\nThis may all look very complicated but it is actually quite simple\n\n\n\n\nIf data is discrete (categorical), you just count the proportion of each category.\n\nExample:\n\\[\nP(\\mathbf{Y} = k| \\mathbf{X}_j = x_j) =\n\\begin{cases}\n0.32 & \\text{if } x_j = 1\\\\\n0.55 & \\text{if } x_j = 2\\\\\n0.13 & \\text{if } x_j = 3\n\\end{cases}\n\\]\n\n\n\nIf data is continuous, use a histogram as an estimate for the true density of \\(x_p\\)\n\nAlternatively, use a kernel density estimator"
  },
  {
    "objectID": "slides/week03_slides_part2.html#default-yes-or-no",
    "href": "slides/week03_slides_part2.html#default-yes-or-no",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Default: Yes or No?",
    "text": "Default: Yes or No?\n\n\nWe have looked at how the probabilities (risk of default) change according to the value of predictors\nBut in practice we need to decide whether the risk is too high or tolerable\nIn our example, we might want to ask:\n\n\n\n\n“Will this person default on their credit card? YES or NO?”"
  },
  {
    "objectID": "slides/week03_slides_part2.html#default-yes-or-no-1",
    "href": "slides/week03_slides_part2.html#default-yes-or-no-1",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Default: Yes or No?",
    "text": "Default: Yes or No?\n\n\nHow would you classify the following customers?\n\n\nCode\nlibrary(tidyverse)\n\nfull_model <- \n  glm(default ~ ., data=ISLR2::Default, family=binomial)\n\nset.seed(40)\nsample_customers <- \n  ISLR2::Default %>% \n  slice(9986, 9908, 6848, 9762, 9979, 7438)\npred <- predict(full_model, sample_customers, type=\"response\")\n# Format it as percentage\nsample_customers$prediction <- \n  sapply(pred, function(x){sprintf(\"%.2f %%\", 100*x)})\nsample_customers\n\n\n  default student   balance   income prediction\n1      No      No  842.9494 39957.13     0.27 %\n2      No      No 1500.5721 39891.86    10.53 %\n3     Yes     Yes 1957.1203 18805.95    44.23 %\n4      No      No 1902.1499 35008.67    53.71 %\n5     Yes      No 2202.4624 47287.26    87.09 %\n6     Yes     Yes 2461.5070 11878.56    93.34 %\n\n\n\n\n\n\n\n\n\nImage created with the DALL·E algorithm using the prompt: ‘35mm macro photography of a robot holding a question mark card, white background’\n\n\nFull model expression: \\[\n\\hat{y} \\approxeq \\frac{e^{-10.87 - 0.65\\times\\text{student[Yes]} + 5.74 \\times 10^{-3}\\times\\text{balance} + 3\\times 10^{-6}\\times\\text{income}}}{1 + e^{-10.87 - 0.65\\times\\text{student[Yes]} + 5.74 \\times 10^{-3}\\times\\text{balance} + 3\\times 10^{-6}\\times\\text{income}}}\n\\]"
  },
  {
    "objectID": "slides/week03_slides_part2.html#default-yes-or-no-2",
    "href": "slides/week03_slides_part2.html#default-yes-or-no-2",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Default: Yes or No?",
    "text": "Default: Yes or No?\n\n\nHow would you classify the following customers?\n\n\nCode\nlibrary(tidyverse)\n\nfull_model <- \n  glm(default ~ ., data=ISLR2::Default, family=binomial)\n\nset.seed(40)\nsample_customers <- \n  ISLR2::Default %>% \n  slice(9986, 9908, 6848, 9762, 9979, 7438)\npred <- predict(full_model, sample_customers, type=\"response\")\n# Format it as percentage\nsample_customers$prediction <- \n  sapply(pred, function(x){sprintf(\"%.2f %%\", 100*x)})\nsample_customers\n\n\n  default student   balance   income prediction\n1      No      No  842.9494 39957.13     0.27 %\n2      No      No 1500.5721 39891.86    10.53 %\n3     Yes     Yes 1957.1203 18805.95    44.23 %\n4      No      No 1902.1499 35008.67    53.71 %\n5     Yes      No 2202.4624 47287.26    87.09 %\n6     Yes     Yes 2461.5070 11878.56    93.34 %\n\n\n\n\nIf we set our threshold \\(= 50\\%\\), we get the following confusion matrix:\n\n\n\n\n\nActual\n\n\n\n\n\nPredicted\nNo\nYes\n\n\nNo\n2\n1\n\n\nYes\n1\n2\n\n\n\n\n\n\nIf we set our threshold \\(= 40\\%\\), we get the following confusion matrix:\n\n\n\n\n\nActual\n\n\n\n\n\nPredicted\nNo\nYes\n\n\nNo\n2\n0\n\n\nYes\n1\n3\n\n\n\n\n\n\n\n\n\nWhich of the two is more accurate?"
  },
  {
    "objectID": "slides/week03_slides_part2.html#thresholds",
    "href": "slides/week03_slides_part2.html#thresholds",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Thresholds",
    "text": "Thresholds\n\nWhen making predictions about classes, we always have to make decisions.\nThresholds, applied to the predicted probability scores, are a way to decide whether to favour a particular class over another\n⏭️ Next, we will explore several metrics that can help us decide whether our classification model is good or bad."
  },
  {
    "objectID": "slides/week03_slides_part2.html#confusion-matrix",
    "href": "slides/week03_slides_part2.html#confusion-matrix",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\nLet’s take another look at the confusion matrix. We can think of the numbers in each cell as the following: \n\n\n\n\n\n\n\n\n\n\nActual\n\n\n\n\n\nPredicted\nNo\nYes\n\n\nNo\nTrue Negative (TN)\nFalse Negative (FN)\n\n\nYes\nFalse Positive (FP)\nTrue Positive (TP)\n\n\n\n\n\nIdeally, we would have no False Negatives and no False Positives but, of course, that is never the case."
  },
  {
    "objectID": "slides/week03_slides_part2.html#classification-metrics-1",
    "href": "slides/week03_slides_part2.html#classification-metrics-1",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Classification metrics",
    "text": "Classification metrics\n\n\nIt is convenient to aggregate those quantities into a few other metrics\nTwo of the most common ones are called sensitivity and specificity\n\n\n\n\\[\n\\begin{align}\n\\text{Sensitivity} &= \\text{True Positive Rate (TPR)} = \\frac{TP}{P} \\\\\n\\text{Specificity} &= \\text{True Negative Rate (TNR)} = \\frac{TN}{N}\n\\end{align}\n\\]\n\n\n\nAnother common one is accuracy:\n\n\\[\n\\text{Accuracy} = \\frac{TP + TN}{P + N}\n\\]\n\n\nA good model has high sensitivity and high specificity and high accuracy.\n\n\n\nThere are many other ways to assess the results of a classification model"
  },
  {
    "objectID": "slides/week03_slides_part2.html#which-threshold-is-better",
    "href": "slides/week03_slides_part2.html#which-threshold-is-better",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Which threshold is better?",
    "text": "Which threshold is better?\n\n\n\n📝 Now, looking at the logistic regression model we built for the entire dataset, work out the sensitivity, specificity and accuracy of the following confusion matrices:\n\n\n\n\n\n\n\nPractice\n\n\n\n⏲️ 5 min to work out the math\n🗳️ Vote on your preferred threshold (on  Slack)\n\n\n\n\n\n\\(\\text{Threshold} = 50\\%\\):\n\n\n\n\nActual\n\n\n\n\n\nPredicted\nNo\nYes\n\n\nNo\n9627\n228\n\n\nYes\n40\n105\n\n\n\n\n\\(\\text{Threshold} = 40\\%\\):\n\n\n\n\nActual\n\n\n\n\n\nPredicted\nNo\nYes\n\n\nNo\n9588\n199\n\n\nYes\n79\n134"
  },
  {
    "objectID": "slides/week03_slides_part2.html#meet-the-roc-curve",
    "href": "slides/week03_slides_part2.html#meet-the-roc-curve",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Meet the ROC curve",
    "text": "Meet the ROC curve\n\n\n\nThe Receiver Operating Characteristic (ROC) curve is another way to assess the model.\nIt shows how sensitivity and specificity change as we vary the threshold from 0 to 1 (threshold not shown).\n\n\n\n\n\n\n\n\nAsk: how would an ideal curve look like?"
  },
  {
    "objectID": "slides/week03_slides_part2.html#generalisation-problems",
    "href": "slides/week03_slides_part2.html#generalisation-problems",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Generalisation problems",
    "text": "Generalisation problems\n\n\nThe data used to train algorithms is called training data\nOften, we want to use the fitted models to make predictions on new previously unseen data\n\n\n\n\n\n\n\n\n\nImportant\n\n\n⚠️ A model that performs well on training data will not necessarily perform well on new data ⚠️\n\n\n\n\n\n\nTo make a robust assessment of our model, we have to split the data in two:\n\nthe training data and\nthe test data\n\nWe do NOT use the test data to fit the model\nWe will come back to this next week, this is the topic of 🗓️ Week 04."
  },
  {
    "objectID": "slides/week03_slides_part2.html#inappropriate-reliance-on-metrics",
    "href": "slides/week03_slides_part2.html#inappropriate-reliance-on-metrics",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Inappropriate reliance on metrics",
    "text": "Inappropriate reliance on metrics\n\n\nAccuracy can be very misleading when classes are imbalanced\nConsider the following model: \\(\\hat{y} = \\text{Yes}\\) (always)\n\nOnly \\(3\\%\\) of customers default on their credit cards\nTherefore, this model would have a \\(97\\%\\) accuracy!\nIt is correct ninety-seven percent of times. But is it a good model?\n\n🙅‍♂️ NO!\n\n\nSimilarly, you have to ask yourself about the usefulness of any other metric\n\nIs True Positive Rate more or less important than True Negative Rate for the classification problem at hand?\nWhy? Why not?\n\nUltimately, it boils down to how you plan to use this model afterwards."
  },
  {
    "objectID": "slides/week03_slides_part2.html#references",
    "href": "slides/week03_slides_part2.html#references",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "References",
    "text": "References\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York NY: Springer. https://www.statlearning.com/.\n\n\n\n\n\nDS202 - Data Science for Social Scientists 🤖 🤹"
  },
  {
    "objectID": "slides/week05_slides_part1.html#regression-analysis-in-real-life",
    "href": "slides/week05_slides_part1.html#regression-analysis-in-real-life",
    "title": "🗓️ Week 05: Decision Trees",
    "section": "Regression analysis in real life",
    "text": "Regression analysis in real life\n\n\nFollowing current trends, the next PM will be in office for approximately minus 200 days pic.twitter.com/avLQE9i1yy\n\n— Rob Sansom (@Sansom_Rob) October 20, 2022"
  },
  {
    "objectID": "slides/week05_slides_part1.html#the-limits-of-classic-regression-models-1",
    "href": "slides/week05_slides_part1.html#the-limits-of-classic-regression-models-1",
    "title": "🗓️ Week 05: Decision Trees",
    "section": "The limits of classic regression models",
    "text": "The limits of classic regression models\nLinear and logistic regression are a good first shot for building ML models\n\n\nEasy-to-interpret coefficients\nIntuitive (ish) ways to assess variable importance\nOften good out-of-the-box predictions"
  },
  {
    "objectID": "slides/week05_slides_part1.html#however",
    "href": "slides/week05_slides_part1.html#however",
    "title": "🗓️ Week 05: Decision Trees",
    "section": "However…",
    "text": "However…\n\n\nAssumption that the predictors are linearly related to the outcome is restrictive\nWe have seen, for instance, that accounting for higher order polynomial relationships can produce better model fit"
  },
  {
    "objectID": "slides/week05_slides_part1.html#enter-non-linear-methods",
    "href": "slides/week05_slides_part1.html#enter-non-linear-methods",
    "title": "🗓️ Week 05: Decision Trees",
    "section": "Enter non-linear methods",
    "text": "Enter non-linear methods\n\n\nThese algorithms do not make (strong) statistical assumptions about the data\nThe focus is more on predictive rather than explanatory power"
  },
  {
    "objectID": "slides/week05_slides_part1.html#decision-tree-for-a-regression-task",
    "href": "slides/week05_slides_part1.html#decision-tree-for-a-regression-task",
    "title": "🗓️ Week 05: Decision Trees",
    "section": "Decision Tree for a Regression task",
    "text": "Decision Tree for a Regression task\nUsing the Auto dataset, predict mpg with a tree-based model using weight and year as features."
  },
  {
    "objectID": "slides/week05_slides_part1.html#decision-tree-for-a-classification-task",
    "href": "slides/week05_slides_part1.html#decision-tree-for-a-classification-task",
    "title": "🗓️ Week 05: Decision Trees",
    "section": "Decision Tree for a Classification task",
    "text": "Decision Tree for a Classification task\nUsing the Boston dataset, predict whether medv is above the median using crim and tax:"
  },
  {
    "objectID": "slides/week05_slides_part1.html#whats-going-on-behind-the-scenes",
    "href": "slides/week05_slides_part1.html#whats-going-on-behind-the-scenes",
    "title": "🗓️ Week 05: Decision Trees",
    "section": "What’s going on behind the scenes?",
    "text": "What’s going on behind the scenes?\nHow decision trees work:\n\n\nDivide the predictor space into \\(\\mathbf{J}\\) distinct regions \\(R_1\\), \\(R_2\\),…,\\(R_j\\).\nTake the mean of the response values in each region\n\n\n\nHere’s how the regions were created in our regression/classification examples ⏭️"
  },
  {
    "objectID": "slides/week05_slides_part1.html#how-are-regions-created",
    "href": "slides/week05_slides_part1.html#how-are-regions-created",
    "title": "🗓️ Week 05: Decision Trees",
    "section": "How are regions created?",
    "text": "How are regions created?\nRecursive binary splitting\n\n\nTop down\n\nStart from the top of the tree\nThen perform splits at a current level of depth\n\n\nGreedy\n\nSplits are “local” not global\nOnly cares about data in the current branch"
  },
  {
    "objectID": "slides/week05_slides_part1.html#when-trees-run-amock",
    "href": "slides/week05_slides_part1.html#when-trees-run-amock",
    "title": "🗓️ Week 05: Decision Trees",
    "section": "When trees run amock",
    "text": "When trees run amock\n\n\nTrees can become too complex if we are not careful\nIt can lead to something called overfitting\n\nHigh training set predictive power\nLow test set predictive power\n\nLet’s see one example ⏭️"
  },
  {
    "objectID": "slides/week05_slides_part1.html#pruning-the-tree",
    "href": "slides/week05_slides_part1.html#pruning-the-tree",
    "title": "🗓️ Week 05: Decision Trees",
    "section": "Pruning the tree",
    "text": "Pruning the tree\n\n\nHyperparameters are model-specific dials that we can tune\n\nThings like max tree depth, or min samples per leaf\n\nAs with model selection, there is no one one-size-fits-all approach to hyperparameter tuning.\nInstead, we experiment with resampling\n\nMost frequently, k-fold cross-validation\n\n\n\n\n\n\n\n\n\n\nk-fold cross-validation\n\n\n\nWe experimented with k-fold CV in 🗓️ Week 04’s lecture/workshop\nWe will revisit this topic in 🗓️ Week 07’s lab\nNot compulsory for ✍️ Summmative Problem Set (01) | W05-W07"
  },
  {
    "objectID": "slides/week05_slides_part1.html#cost-complexity",
    "href": "slides/week05_slides_part1.html#cost-complexity",
    "title": "🗓️ Week 05: Decision Trees",
    "section": "Cost Complexity",
    "text": "Cost Complexity\n\nWe apply \\(\\alpha\\) which is a non-negative value to prune the tree.\nFor example, when \\(\\alpha = 0.02\\) we can create a less complex tree."
  },
  {
    "objectID": "slides/week05_slides_part2.html#support-vector-machines-for-classification",
    "href": "slides/week05_slides_part2.html#support-vector-machines-for-classification",
    "title": "🗓️ Week 05:Support Vector Machines (SVM)",
    "section": "Support Vector Machines for Classification",
    "text": "Support Vector Machines for Classification\n\nConsidered one of the best out-of-the-box classifiers (ISLR)\nAble to accommodate non-linear decision boundaries"
  },
  {
    "objectID": "slides/week05_slides_part2.html#building-intuition-the-hyperplane",
    "href": "slides/week05_slides_part2.html#building-intuition-the-hyperplane",
    "title": "🗓️ Week 05:Support Vector Machines (SVM)",
    "section": "Building Intuition: the Hyperplane",
    "text": "Building Intuition: the Hyperplane"
  },
  {
    "objectID": "slides/week05_slides_part2.html#building-intuition-the-hyperplane-cont.",
    "href": "slides/week05_slides_part2.html#building-intuition-the-hyperplane-cont.",
    "title": "🗓️ Week 05:Support Vector Machines (SVM)",
    "section": "Building Intuition: the Hyperplane (cont.)",
    "text": "Building Intuition: the Hyperplane (cont.)\nWhen \\(1 + 2x_1 + 3x_2 < 0\\)"
  },
  {
    "objectID": "slides/week05_slides_part2.html#building-intuition-the-hyperplane-cont.-1",
    "href": "slides/week05_slides_part2.html#building-intuition-the-hyperplane-cont.-1",
    "title": "🗓️ Week 05:Support Vector Machines (SVM)",
    "section": "Building Intuition: the Hyperplane (cont.)",
    "text": "Building Intuition: the Hyperplane (cont.)\nWhen \\(1 + 2x_1 + 3x_2 > 0\\)"
  },
  {
    "objectID": "slides/week05_slides_part2.html#building-intuition-the-hyperplane-cont.-2",
    "href": "slides/week05_slides_part2.html#building-intuition-the-hyperplane-cont.-2",
    "title": "🗓️ Week 05:Support Vector Machines (SVM)",
    "section": "Building Intuition: the Hyperplane (cont.)",
    "text": "Building Intuition: the Hyperplane (cont.)\nWhen \\(1 + 2x_1 + 3x_2 = 0\\)"
  },
  {
    "objectID": "slides/week05_slides_part2.html#the-maximal-marginal-classifier",
    "href": "slides/week05_slides_part2.html#the-maximal-marginal-classifier",
    "title": "🗓️ Week 05:Support Vector Machines (SVM)",
    "section": "The Maximal Marginal Classifier",
    "text": "The Maximal Marginal Classifier\nThe linearly separable case:"
  },
  {
    "objectID": "slides/week05_slides_part2.html#identifying-support-vectors",
    "href": "slides/week05_slides_part2.html#identifying-support-vectors",
    "title": "🗓️ Week 05:Support Vector Machines (SVM)",
    "section": "Identifying support vectors",
    "text": "Identifying support vectors\nThe SV’s represent the so-called support vectors:"
  },
  {
    "objectID": "slides/week05_slides_part2.html#when-data-is-not-linearly-separable",
    "href": "slides/week05_slides_part2.html#when-data-is-not-linearly-separable",
    "title": "🗓️ Week 05:Support Vector Machines (SVM)",
    "section": "When data is not linearly separable",
    "text": "When data is not linearly separable\nSuppose we have a case that is not linearly separable like this. We have two classes but class 1 is “sandwiched” in between class 2."
  },
  {
    "objectID": "slides/week05_slides_part2.html#enter-support-vector-machines",
    "href": "slides/week05_slides_part2.html#enter-support-vector-machines",
    "title": "🗓️ Week 05:Support Vector Machines (SVM)",
    "section": "Enter support vector machines",
    "text": "Enter support vector machines\nLet’s start with a (linear) support vector classifier function\n\\[\nf(x) = \\beta_0 + \\sum_{i\\in\\mathcal{S}}^{} \\alpha_i\\left\\langle x_i, x_{i'} \\right\\rangle\n\\]\nA couple of points:\n\n\\(\\left\\langle x_i, x_{i'} \\right\\rangle\\) is the inner product 1 of two observations\n\\(\\alpha_i\\) is a parameter fitted for all pairs of training observations\n\\(i\\in\\mathcal{S}\\) indicates observations that are support vectors (all other observations are ignored by setting all \\(\\alpha_i \\notin \\mathcal{S}\\) to zero.)\n\nRead about inner products"
  },
  {
    "objectID": "slides/week05_slides_part2.html#enter-support-vector-machines-cont.",
    "href": "slides/week05_slides_part2.html#enter-support-vector-machines-cont.",
    "title": "🗓️ Week 05:Support Vector Machines (SVM)",
    "section": "Enter support vector machines (cont.)",
    "text": "Enter support vector machines (cont.)\n\n\nWe can replace \\(\\left\\langle x_i, x_{i'} \\right\\rangle\\) with a of the form \\(K(x_i, x_{i'})\\) where \\(K\\) is a kernel.\n\n\n\nTwo well-known kernels:\n\nPolynomial \\(K(x_i, x_{i'}) = (1 + \\sum_{j = 1}^{p} x_{ij}x_{i'j})^d\\) where \\(d > 1\\).\nRadial or “Gaussian” \\(K(x_i, x_{i'}) = \\text{exp}(-\\gamma\\sum_{j=1}^{p}(x_{ij}-x_{i'j})^2)\\) where \\(\\gamma\\) is a positive constant."
  },
  {
    "objectID": "slides/week05_slides_part2.html#svm-with-linear-kernel",
    "href": "slides/week05_slides_part2.html#svm-with-linear-kernel",
    "title": "🗓️ Week 05:Support Vector Machines (SVM)",
    "section": "SVM with Linear Kernel",
    "text": "SVM with Linear Kernel"
  },
  {
    "objectID": "slides/week05_slides_part2.html#svm-with-polynomial-kernel",
    "href": "slides/week05_slides_part2.html#svm-with-polynomial-kernel",
    "title": "🗓️ Week 05:Support Vector Machines (SVM)",
    "section": "SVM with Polynomial Kernel",
    "text": "SVM with Polynomial Kernel"
  },
  {
    "objectID": "slides/week05_slides_part2.html#svm-with-sigmoid-kernel",
    "href": "slides/week05_slides_part2.html#svm-with-sigmoid-kernel",
    "title": "🗓️ Week 05:Support Vector Machines (SVM)",
    "section": "SVM with Sigmoid Kernel",
    "text": "SVM with Sigmoid Kernel"
  },
  {
    "objectID": "slides/week05_slides_part2.html#svm-with-radial-kernel",
    "href": "slides/week05_slides_part2.html#svm-with-radial-kernel",
    "title": "🗓️ Week 05:Support Vector Machines (SVM)",
    "section": "SVM with Radial Kernel",
    "text": "SVM with Radial Kernel"
  },
  {
    "objectID": "slides/week05_slides_part2.html#source-code",
    "href": "slides/week05_slides_part2.html#source-code",
    "title": "🗓️ Week 05:Support Vector Machines (SVM)",
    "section": "Source Code",
    "text": "Source Code\n\n\n\n\n\n\nTip\n\n\n\nUse the code below to replicate the plot from the previous slide.\nFound a bug? Report it on Slack.\n\n\n\n\nlibrary(datasets)   # to load the iris  data\nlibrary(tidyverse)  # to use things like the pipe (%>%), mutate and if_else\nlibrary(ggsci)      # just for pretty colours! It enables functions scale_fill_lancet() and scale_colour_lancet().\nlibrary(e1071)      # to load the SVM algorithm\ndata(iris)          # load the dataset `iris`\n\n# Train the model! change the parameter `kernel`. It accepts 'linear', 'polynomial', 'radial' and 'sigmoid'\nmodel <- svm(Species ~ Sepal.Length + Sepal.Width, data = iris, kernel = 'linear')\n\n# Generate all possible combinations of Sepal.Length and Sepal.Width\nkernel.points <- crossing(Sepal.Length = seq(4, 8, 0.1), Sepal.Width = seq(2, 5, 0.1)) %>%  mutate(pred = predict(model, .))\n\n# Create a dataframe just for plotting (with predictions)\nplot_df <- iris %>% mutate(pred=predict(model, iris), correct = if_else(pred == Species, TRUE, FALSE))\n\nplot_df %>%   \n  ggplot() +\n  geom_tile(data = kernel.points, aes(x=Sepal.Length, y=Sepal.Width, fill = pred), alpha = 0.25) +\n  geom_point(aes(x=Sepal.Length, y=Sepal.Width, colour = Species, shape = correct), size = 4) +\n  scale_shape_manual(values = c(4, 1)) +\n  scale_colour_lancet() +\n  scale_fill_lancet() +\n  theme_minimal() +\n  theme(panel.grid = element_blank(), legend.position = 'bottom', plot.title = element_text(hjust = 0.5)) +\n  labs(x = 'Sepal.Length', y = 'Sepal.Width', fill = 'Species', colour = 'Species', shape = 'Correct prediction?',\n       title = sprintf('Overall Training Accuracy = %.2f %%', 100*(sum(plot_df$correct)/nrow(plot_df))))"
  },
  {
    "objectID": "slides/week05_slides_part2.html#overfitting-illustrated",
    "href": "slides/week05_slides_part2.html#overfitting-illustrated",
    "title": "🗓️ Week 05:Support Vector Machines (SVM)",
    "section": "Overfitting illustrated",
    "text": "Overfitting illustrated\nSimple models can sometimes be better than complex models."
  },
  {
    "objectID": "slides/week05_slides_part2.html#simple-model-on-training-set",
    "href": "slides/week05_slides_part2.html#simple-model-on-training-set",
    "title": "🗓️ Week 05:Support Vector Machines (SVM)",
    "section": "Simple model on training set",
    "text": "Simple model on training set"
  },
  {
    "objectID": "slides/week05_slides_part2.html#complex-model-on-training-set",
    "href": "slides/week05_slides_part2.html#complex-model-on-training-set",
    "title": "🗓️ Week 05:Support Vector Machines (SVM)",
    "section": "Complex model on training set",
    "text": "Complex model on training set"
  },
  {
    "objectID": "slides/week05_slides_part2.html#now-lets-create-some-testing-data",
    "href": "slides/week05_slides_part2.html#now-lets-create-some-testing-data",
    "title": "🗓️ Week 05:Support Vector Machines (SVM)",
    "section": "Now let’s create some testing data",
    "text": "Now let’s create some testing data\nLook what happens when I set a different seed (nothing else changes) to construct a test set."
  },
  {
    "objectID": "slides/week05_slides_part2.html#simple-model-on-test-set",
    "href": "slides/week05_slides_part2.html#simple-model-on-test-set",
    "title": "🗓️ Week 05:Support Vector Machines (SVM)",
    "section": "Simple model on test set",
    "text": "Simple model on test set"
  },
  {
    "objectID": "slides/week05_slides_part2.html#complex-model-on-test-set",
    "href": "slides/week05_slides_part2.html#complex-model-on-test-set",
    "title": "🗓️ Week 05:Support Vector Machines (SVM)",
    "section": "Complex model on test set",
    "text": "Complex model on test set"
  },
  {
    "objectID": "slides/week05_slides_part2.html#back-to-iris",
    "href": "slides/week05_slides_part2.html#back-to-iris",
    "title": "🗓️ Week 05:Support Vector Machines (SVM)",
    "section": "Back to iris",
    "text": "Back to iris\nSVM with Radial Kernel but tweaking parameters, namely cost and gamma:"
  },
  {
    "objectID": "slides/week05_slides_part2.html#source-code-1",
    "href": "slides/week05_slides_part2.html#source-code-1",
    "title": "🗓️ Week 05:Support Vector Machines (SVM)",
    "section": "Source Code",
    "text": "Source Code\n\n\n\n\n\n\nTip\n\n\n\nThe only difference is that we added gamma and cost:\n\n\n\n\nlibrary(datasets)   # to load the iris  data\nlibrary(tidyverse)  # to use things like the pipe (%>%), mutate and if_else\nlibrary(ggsci)      # just for pretty colours! It enables functions scale_fill_lancet() and scale_colour_lancet().\nlibrary(e1071)      # to load the SVM algorithm\ndata(iris)          # load the dataset `iris`\n\n# Train the model! change the parameter `kernel`. It accepts 'linear', 'polynomial', 'radial' and 'sigmoid'\nmodel <- svm(Species ~ Sepal.Length + Sepal.Width, data = iris, kernel = 'radial', gamma = 10^2, cost = 10^4)\n\n# Generate all possible combinations of Sepal.Length and Sepal.Width\nkernel.points <- crossing(Sepal.Length = seq(4, 8, 0.1), Sepal.Width = seq(2, 5, 0.1)) %>%  mutate(pred = predict(model, .))\n\n# Create a dataframe just for plotting (with predictions)\nplot_df <- iris %>% mutate(pred=predict(model, iris), correct = if_else(pred == Species, TRUE, FALSE))\n\nplot_df %>%   \n  ggplot() +\n  geom_tile(data = kernel.points, aes(x=Sepal.Length, y=Sepal.Width, fill = pred), alpha = 0.25) +\n  geom_point(aes(x=Sepal.Length, y=Sepal.Width, colour = Species, shape = correct), size = 4) +\n  scale_shape_manual(values = c(4, 1)) +\n  scale_colour_lancet() +\n  scale_fill_lancet() +\n  theme_minimal() +\n  theme(panel.grid = element_blank(), legend.position = 'bottom', plot.title = element_text(hjust = 0.5)) +\n  labs(x = 'Sepal.Length', y = 'Sepal.Width', fill = 'Species', colour = 'Species', shape = 'Correct prediction?',\n       title = sprintf('Overall Training Accuracy = %.2f %%', 100*(sum(plot_df$correct)/nrow(plot_df))))"
  },
  {
    "objectID": "slides/week05_slides_part2.html#example",
    "href": "slides/week05_slides_part2.html#example",
    "title": "🗓️ Week 05:Support Vector Machines (SVM)",
    "section": "Example",
    "text": "Example\n\nA 5-fold cross-validation:\n\n\n\nSplit 1:\n\n\nTest\n\n\nTrain\n\n\nTrain\n\n\nTrain\n\n\nTrain\n\n\n\n\nSplit 2:\n\n\nTrain\n\n\nTest\n\n\nTrain\n\n\nTrain\n\n\nTrain\n\n\n\n\nSplit 3:\n\n\nTrain\n\n\nTrain\n\n\nTest\n\n\nTrain\n\n\nTrain\n\n\n\n\nSplit 4:\n\n\nTrain\n\n\nTrain\n\n\nTrain\n\n\nTest\n\n\nTrain\n\n\n\n\nSplit 5:\n\n\nTrain\n\n\nTrain\n\n\nTrain\n\n\nTrain\n\n\nTest\n\n\n\nWe experimented with k-fold CV in 🗓️ Week 04’s lecture/workshop"
  },
  {
    "objectID": "slides/week05_slides_part2.html#recommendations",
    "href": "slides/week05_slides_part2.html#recommendations",
    "title": "🗓️ Week 05:Support Vector Machines (SVM)",
    "section": "Recommendations",
    "text": "Recommendations\n\nReadings - Decision Trees: (James et al. 2021, sec. 8.1) (ignore section 8.2) - SVMs: (James et al. 2021, chap. 9) - Read it all once just to retain main concepts - Focus your attention on the concepts on the margins of the book\nPractice the code that is contained in these slides! Several times! - There are many datasets in the ISLR2 package (link) - Load any dataset and explore it with these new algorithms"
  },
  {
    "objectID": "slides/week07_slides_part1.html#the-rationale",
    "href": "slides/week07_slides_part1.html#the-rationale",
    "title": "🗓️ Week 07: Data Transformation & Iterations",
    "section": "The rationale",
    "text": "The rationale\nOur expectations for this problem set:\n\n\nwe knew it would require somewhat considerable time effort\nencourage people to collaborate and work together\nput what you learned from the R pre-sessionals to practice\nwe wanted to allow for some freedom"
  },
  {
    "objectID": "slides/week07_slides_part1.html#the-r-pre-sessional-course",
    "href": "slides/week07_slides_part1.html#the-r-pre-sessional-course",
    "title": "🗓️ Week 07: Data Transformation & Iterations",
    "section": "The R pre-sessional course",
    "text": "The R pre-sessional course"
  },
  {
    "objectID": "slides/week07_slides_part1.html#our-blind-spots",
    "href": "slides/week07_slides_part1.html#our-blind-spots",
    "title": "🗓️ Week 07: Data Transformation & Iterations",
    "section": "Our blind spots",
    "text": "Our blind spots\nWhat we could have done better:\n\n\nwe assumed the pre-sessional chapter above would have prepared you even for the challenging questions\n\ndata types & data frames\nif-else statements\nfor-loops\ncreation of functions\nvectorized functions like apply() and sapply\n\nwe could have provided a cheatsheet for common R tasks\nwe could have given better marking criteria for questions where more freedom was allowed"
  },
  {
    "objectID": "slides/week07_slides_part1.html#next-summative",
    "href": "slides/week07_slides_part1.html#next-summative",
    "title": "🗓️ Week 07: Data Transformation & Iterations",
    "section": "Next summative",
    "text": "Next summative\nOperational changes we plan to introduce:\n\n\nLess code writing and more code reading\nReduce ambiguity.\n\nWhere freedom/creativity is allowed, explain if and how this is rewarded\n\nElements of randomness. Each student will be assigned a unique combination of:\n\nselected variables and\nselected metrics to consider"
  },
  {
    "objectID": "slides/week07_slides_part1.html#next-summative-1",
    "href": "slides/week07_slides_part1.html#next-summative-1",
    "title": "🗓️ Week 07: Data Transformation & Iterations",
    "section": "Next summative",
    "text": "Next summative\nTopics of Summative W08-W10:\n\nRegression & Classification\nDecision Tree\nSupport Vector Machine\nk-fold Cross-Validation"
  },
  {
    "objectID": "slides/week07_slides_part1.html#explore",
    "href": "slides/week07_slides_part1.html#explore",
    "title": "🗓️ Week 07: Data Transformation & Iterations",
    "section": "Explore",
    "text": "Explore\nLet’s explore together using content from two sources:\n\n\nR for Data Science Book\n\n\nTidy Data Tutor\n\n\n\n\n\nLinks:\n\nR for Data Science\nTidy Data Tutor"
  },
  {
    "objectID": "slides/week07_slides_part2.html#example-of-supervised-models",
    "href": "slides/week07_slides_part2.html#example-of-supervised-models",
    "title": "🗓️ Week 07:Unsupervised Learning",
    "section": "Example of Supervised Models",
    "text": "Example of Supervised Models\nFrom 🗓️ Week 02:\n\n\nThe generic supervised model:\n\\[\nY = \\operatorname{f}(X) + \\epsilon\n\\]\nis defined more explicitly according to each algorithm ➡️\n\n\n\nMultiple Linear Regression\n\\[\n\\begin{align}\nY = \\beta_0 &+ \\beta_1 X_1 + \\beta_2 X_2 \\\\\n   &+ \\dots \\\\\n   &+ \\beta_p X_p + \\epsilon\n\\end{align}\n\\]\nwhen there are multiple predictors, \\(X_p\\).\n\n\n\n\nMultiple Logistic Regression\n\\[\n\\begin{align}\nY \\propto p(X) &= \\\\\n               &= \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p}}\n\\end{align}\n\\]\nwhen there are multiple predictors, \\(X_p\\)."
  },
  {
    "objectID": "slides/week07_slides_part2.html#how-is-it-different",
    "href": "slides/week07_slides_part2.html#how-is-it-different",
    "title": "🗓️ Week 07:Unsupervised Learning",
    "section": "How is it different?",
    "text": "How is it different?\n\n\nSupervised Learning\n\n\nOur main goal is to predict future values of \\(Y\\)\n\n\n\n\nWe have historic \\(X\\) and \\(Y\\) data\n\n\n\n\nAlgorithms fit the data and supervise themselves objectively (e.g.: residuals)\n\n\n\n\nWe can validate how well the model fits training data and how it generalises beyond that.\n\n\n\nUnsupervised Learning\n\n\nThe main goal is to observe (dis-)similarities in \\(X\\)\n\n\n\n\nWe only have \\(X\\) data\n\n\n\n\nThere is no \\(Y\\) variable to “supervise” how models should fit the data\n\n\n\n\nValidation is a lot more subjective. There is no objective way to check our work."
  },
  {
    "objectID": "slides/week07_slides_part2.html#why-clustering",
    "href": "slides/week07_slides_part2.html#why-clustering",
    "title": "🗓️ Week 07:Unsupervised Learning",
    "section": "Why clustering?",
    "text": "Why clustering?\n\n\nAre there clusters (subgroups) in my data?\nWhich samples are most similar to each other?\nAre there samples that do not fall in any subgroup?"
  },
  {
    "objectID": "slides/week07_slides_part2.html#each-algorithm-is-different",
    "href": "slides/week07_slides_part2.html#each-algorithm-is-different",
    "title": "🗓️ Week 07:Unsupervised Learning",
    "section": "Each algorithm is different",
    "text": "Each algorithm is different\n\n\nSource: Scikit Learn | Comparing different clustering algorithms on toy datasets"
  },
  {
    "objectID": "slides/week07_slides_part2.html#penguins-data",
    "href": "slides/week07_slides_part2.html#penguins-data",
    "title": "🗓️ Week 07:Unsupervised Learning",
    "section": "🐧 penguins data",
    "text": "🐧 penguins data\n\n\nCode\npenguins_cleaned <-\n  penguins %>% \n  na.omit() %>% \n  select(species, where(is.numeric), - year)\nhead(penguins_cleaned, 15)\n\n\n# A tibble: 15 × 5\n   species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   <fct>            <dbl>         <dbl>             <int>       <int>\n 1 Adelie            39.1          18.7               181        3750\n 2 Adelie            39.5          17.4               186        3800\n 3 Adelie            40.3          18                 195        3250\n 4 Adelie            36.7          19.3               193        3450\n 5 Adelie            39.3          20.6               190        3650\n 6 Adelie            38.9          17.8               181        3625\n 7 Adelie            39.2          19.6               195        4675\n 8 Adelie            41.1          17.6               182        3200\n 9 Adelie            38.6          21.2               191        3800\n10 Adelie            34.6          21.1               198        4400\n11 Adelie            36.6          17.8               185        3700\n12 Adelie            38.7          19                 195        3450\n13 Adelie            42.5          20.7               197        4500\n14 Adelie            34.4          18.4               184        3325\n15 Adelie            46            21.5               194        4200\n\n\n\n\n\nData collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network."
  },
  {
    "objectID": "slides/week07_slides_part2.html#correlation-plot",
    "href": "slides/week07_slides_part2.html#correlation-plot",
    "title": "🗓️ Week 07:Unsupervised Learning",
    "section": "Correlation plot",
    "text": "Correlation plot\nIt is easy to distinguish the species using colour because we have a species column:"
  },
  {
    "objectID": "slides/week07_slides_part2.html#source-code",
    "href": "slides/week07_slides_part2.html#source-code",
    "title": "🗓️ Week 07:Unsupervised Learning",
    "section": "Source Code",
    "text": "Source Code\n\n\n\n\n\n\nTip\n\n\n\nUse the code below to replicate the plot from the previous slide.\nFound a bug? Report it on Slack.\n\n\n\n\nlibrary(GGally)         # pretty correlation plots\nlibrary(tidyverse)      # to use things like the pipe (%>%), mutate and if_else\nlibrary(palmerpenguins) # for penguin data \n\npenguins_cleaned <-\n  penguins %>% \n  na.omit() %>%  \n  select(species, where(is.numeric), - year) \n\n# View(penguins_cleaned) or do head(penguins_cleaned)\n\ng <-\n  ggpairs(penguins_cleaned, \n        aes(colour = species, fill = species, alpha = 0.875),\n        columns = 2:5, \n        upper = list(continuous = 'blankDiag')) +\n  theme_minimal() +\n  theme(panel.grid.minor = element_blank()) +\n  scale_colour_viridis_d() +\n  scale_fill_viridis_d() +\n  labs(colour = 'Species', fill = 'Species')\n\ng"
  },
  {
    "objectID": "slides/week07_slides_part2.html#penguins-data-without-the-species",
    "href": "slides/week07_slides_part2.html#penguins-data-without-the-species",
    "title": "🗓️ Week 07:Unsupervised Learning",
    "section": "Penguins data without the species",
    "text": "Penguins data without the species\nWhat if we did not have the species column?\n\n\nCode\npenguins %>% \n  na.omit() %>% \n  select(species, where(is.numeric), - year, -species) %>%\n  head(15)\n\n\n# A tibble: 15 × 4\n   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n            <dbl>         <dbl>             <int>       <int>\n 1           39.1          18.7               181        3750\n 2           39.5          17.4               186        3800\n 3           40.3          18                 195        3250\n 4           36.7          19.3               193        3450\n 5           39.3          20.6               190        3650\n 6           38.9          17.8               181        3625\n 7           39.2          19.6               195        4675\n 8           41.1          17.6               182        3200\n 9           38.6          21.2               191        3800\n10           34.6          21.1               198        4400\n11           36.6          17.8               185        3700\n12           38.7          19                 195        3450\n13           42.5          20.7               197        4500\n14           34.4          18.4               184        3325\n15           46            21.5               194        4200"
  },
  {
    "objectID": "slides/week07_slides_part2.html#objective-function",
    "href": "slides/week07_slides_part2.html#objective-function",
    "title": "🗓️ Week 07:Unsupervised Learning",
    "section": "Objective function:",
    "text": "Objective function:\nFind a partition \\(C_1 \\cup C_2 \\cup C_3 \\cup \\ldots \\cup C_K = \\{1, \\ldots, n\\}\\) for the data such that: \\[\n\\text{minimize}_{C_1, \\ldots, C_K} \\left\\{ \\sum_{k=1}^{K}{\\frac{1}{|C_k|} \\sum_{i,i' \\in C_k}{\\sum_{j=1}^p{(x_{ij} - x_{i'j})^2}} }\\right\\}\n\\]"
  },
  {
    "objectID": "slides/week07_slides_part2.html#k-means-algorithm",
    "href": "slides/week07_slides_part2.html#k-means-algorithm",
    "title": "🗓️ Week 07:Unsupervised Learning",
    "section": "K-Means algorithm",
    "text": "K-Means algorithm\nStep by step of one clustering algorithm:\n\n\nYou have to inform \\(K\\), the number of clusters you wish to recover\nThe algorithm randomly assign each observation to a random cluster\nIterate until cluster assignments stop changing\n\nFor each of the \\(K\\) clusters, compute the cluster centroid\nRe-assign samples to their closest centroid (euclidean distance)\n\n\n\n\n\nIndicative reading: (James et al. 2021, sec. 12.4.1)"
  },
  {
    "objectID": "slides/week07_slides_part2.html#clustering-penguins",
    "href": "slides/week07_slides_part2.html#clustering-penguins",
    "title": "🗓️ Week 07:Unsupervised Learning",
    "section": "Clustering Penguins",
    "text": "Clustering Penguins\n\n\nCode\nset.seed(1)\n\nK=3\n\nkmeans_model <- kmeans(penguins_cleaned %>% select(-species), K)\nkmeans_model\n\n\nK-means clustering with 3 clusters of sizes 113, 80, 140\n\nCluster means:\n  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n1       44.24336      17.44779          201.5487    4310.619\n2       48.66250      15.39750          219.9875    5365.938\n3       41.12214      17.94643          189.6286    3461.250\n\nClustering vector:\n  [1] 3 3 3 3 3 3 1 3 3 1 3 3 1 3 1 3 3 3 1 3 3 3 3 3 1 3 1 3 1 3 1 1 3 3 1 3 1\n [38] 3 1 3 1 3 3 1 3 1 3 1 3 3 3 3 3 3 3 1 3 1 3 1 3 1 3 1 3 1 3 1 3 1 3 1 3 1\n [75] 3 1 3 1 3 3 3 3 1 3 3 1 3 1 3 1 3 1 3 1 3 1 3 1 3 3 3 1 3 1 3 1 3 1 1 1 3\n[112] 3 3 3 3 3 3 3 3 1 3 1 3 1 3 3 3 1 3 1 3 1 3 1 3 3 3 3 3 3 1 3 3 3 3 1 1 2\n[149] 1 2 2 1 1 2 1 2 1 2 1 2 1 2 1 2 1 2 2 2 1 2 2 2 2 1 2 2 1 2 2 2 2 2 2 1 2\n[186] 1 2 1 1 2 2 1 2 2 2 2 2 1 2 2 2 1 2 1 2 1 2 1 2 1 2 2 1 2 1 2 2 2 1 2 1 2\n[223] 1 2 1 2 1 2 1 2 1 2 2 2 2 2 1 2 2 2 2 2 1 2 2 2 2 2 2 1 2 1 2 2 2 1 2 1 2\n[260] 2 2 2 2 2 2 3 1 3 3 3 1 3 3 1 3 3 3 3 1 3 1 3 3 3 1 3 3 3 3 3 1 3 3 3 1 3\n[297] 1 3 1 3 1 3 1 3 1 1 3 3 3 3 1 3 1 3 3 3 1 3 1 3 3 3 1 3 3 1 3 3 1 3 3 1 3\n\nWithin cluster sum of squares by cluster:\n[1] 9318036 9718829 9724809\n (between_SS / total_SS =  86.6 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\""
  },
  {
    "objectID": "slides/week07_slides_part2.html#inspect-the-centroids",
    "href": "slides/week07_slides_part2.html#inspect-the-centroids",
    "title": "🗓️ Week 07:Unsupervised Learning",
    "section": "Inspect the centroids",
    "text": "Inspect the centroids\n\n\nCode\nknitr::kable(kmeans_model$centers)\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\n44.24336\n17.44779\n201.5487\n4310.619\n\n\n48.66250\n15.39750\n219.9875\n5365.938\n\n\n41.12214\n17.94643\n189.6286\n3461.250"
  },
  {
    "objectID": "slides/week07_slides_part2.html#how-does-it-compare-to-the-real-one",
    "href": "slides/week07_slides_part2.html#how-does-it-compare-to-the-real-one",
    "title": "🗓️ Week 07:Unsupervised Learning",
    "section": "How does it compare to the real one?",
    "text": "How does it compare to the real one?\nWe can cross-tabulate species and cluster membership:\n\n\nCode\nplot_df <- penguins_cleaned\nplot_df$cluster <- factor(kmeans_model$cluster)\nlevels(plot_df$cluster) <- c(\"Cluster 1\", \"Cluster 2\", \"Cluster 3\")\n\nknitr::kable(table(plot_df$species, plot_df$cluster))\n\n\n\n\n\n\nCluster 1\nCluster 2\nCluster 3\n\n\n\n\nAdelie\n52\n0\n94\n\n\nChinstrap\n22\n0\n46\n\n\nGentoo\n39\n80\n0"
  },
  {
    "objectID": "slides/week07_slides_part2.html#visual-comparison",
    "href": "slides/week07_slides_part2.html#visual-comparison",
    "title": "🗓️ Week 07:Unsupervised Learning",
    "section": "Visual comparison",
    "text": "Visual comparison"
  },
  {
    "objectID": "slides/week07_slides_part2.html#visual-comparison-1",
    "href": "slides/week07_slides_part2.html#visual-comparison-1",
    "title": "🗓️ Week 07:Unsupervised Learning",
    "section": "Visual comparison",
    "text": "Visual comparison"
  },
  {
    "objectID": "slides/week07_slides_part2.html#whats-next",
    "href": "slides/week07_slides_part2.html#whats-next",
    "title": "🗓️ Week 07:Unsupervised Learning",
    "section": "What’s Next",
    "text": "What’s Next\n\nNext week, Week 08, the labs will be about SVM & cross-validation\n\nRevisit 🗓️ Week 05 lecture\nRead “The cross-validation setup” section in Week 04 page\n\nYou will explore k-means clustering in Week 09 labs"
  },
  {
    "objectID": "slides/week07_slides_part2.html#references",
    "href": "slides/week07_slides_part2.html#references",
    "title": "🗓️ Week 07:Unsupervised Learning",
    "section": "References",
    "text": "References\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York NY: Springer. https://www.statlearning.com/.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDS202 - Data Science for Social Scientists 🤖 🤹"
  }
]