[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LSE DS202 (2022/2023)",
    "section": "",
    "text": "🎯 Learning Objectives\n\nUnderstand the fundamentals of the data science approach, with an emphasis on social scientific analysis and the study of the social, political, and economic worlds;\nUnderstand how classical methods such as regression analysis or principal components analysis can be treated as machine learning approaches for prediction or for data mining.\nKnow how to fit and apply supervised machine learning models for classification and prediction.\nKnow how to evaluate and compare fitted models, and to improve model performance.\nUse applied computer programming, including the hands-on use of programming through course exercises.\nApply the methods learned to real data through hands-on exercises.\nIntegrate the insights from data analytics into knowledge generation and decision-making.\nUnderstand an introductory framework for working with natural language (text) data using techniques of machine learning.\nLearn how data science methods have been applied to a particular domain of study (applications).\n\n\n\n🧑🏻‍🏫 Our Team\n\nTeacher ResponsibleTeaching StaffAdministrative Support\n\n\n\nDr. Jonathan Cardoso-Silva  Assistant Professorial Lecturer  LSE Data Science Institute 📧 J.Cardoso-Silva at lse dot ac dot uk\nOffice Hours:\n\n15-min slots, on Wednesdays 13:00 – 15:00 during Term Time\nRoom: PEL 9.01c (check out the 🗺️ campus map)\nBook via Student Hub up to 12 hours in advance\n\n\n\n\nDr. Stuart Bramwell  ESRC Postdoctoral Fellow  Department of Methodology PhD in Politics (Oxford)\n\nYijun Wang  Guest Teacher at the LSE Data Science Institute PhD candidate in Health Informatics (KCL)  MSc in Data Science (KCL)\n\nMustafa Can Ozkan  Guest Teacher at the LSE Data Science Institute PhD candidate in the Spacetime Lab (UCL)  MSc in Transport (Imperial/UCL)\n\nXiaowei Gao  Guest Teacher at the LSE Data Science Institute PhD candidate in the Spacetime Lab (UCL)  MSc in Data Science (KCL)\n\nAnton Boichenko  Guest Teacher at the LSE Data Science Institute Product Developer at Decoded  MSc in Applied Social Data Science (LSE)\n\n\n\nNathaniel Ocquaye  Teaching Support and Events Officer Office: PEL 9.01 Email: DSI.UG at lse dot ac dot uk\n\nJill Beattie  Institute Coordinator Office: PEL 9.01E Tel: +44 (0) 20 7955 7759 Email: DSI.Admin at lse dot ac dot uk\n\n\n\n\n\nClass Groups\n\nGroup 01\n\n📆 Mondays\n⌚ 09:00 — 10:30\n📍 PAN.1.03\n🧑‍🏫 Xiaowei\n\n\n\nGroup 02\n\n📆 Mondays\n⌚ 10:30 — 12:00\n📍 PAN.1.03\n🧑‍🏫 Xiaowei\n\n\n\nGroup 03\n\n📆 Mondays\n⌚ 13:00 — 14:30\n📍 MAR.1.09\n🧑‍🏫 Stuart/Jon\n\n\n\nGroup 04\n\n📆 Fridays\n⌚ 16:00 — 17:30\n📍 NAB.1.04\n🧑‍🏫 Stuart/Yijun\n\n\n\nGroup 05\n\n📆 Mondays\n⌚ 09:00 — 10:30\n📍 32L.LG.11\n🧑‍🏫 Mustafa\n\n\n\nGroup 06\n\n📆 Mondays\n⌚ 10:30 — 12:00\n📍 32L.LG.11\n🧑‍🏫 Mustafa\n\n\n\nGroup 07\n\n📆 Fridays\n⌚ 09:30 — 11:00\n📍 CBG.2.06\n🧑‍🏫 Stuart/Yijun"
  },
  {
    "objectID": "main/assessments.html",
    "href": "main/assessments.html",
    "title": "Assessments",
    "section": "",
    "text": "This course is assessed by a mix of problem sets and final exam. You can find the details below:"
  },
  {
    "objectID": "main/assessments.html#problem-sets-60",
    "href": "main/assessments.html#problem-sets-60",
    "title": "Assessments",
    "section": "📝 Problem Sets (60%)",
    "text": "📝 Problem Sets (60%)\n\nSummative problem sets released on Weeks 5, 8 & 11.\nThese will have a similar style to the formative problem sets, a mix of R tasks and your written interpretation of the analyses.\nTypically, you will have 4-6 days to submit your solutions.\nEach of the three summative problem sets is worth 20% of the final mark, and will be graded on a 100 point scale."
  },
  {
    "objectID": "main/assessments.html#exam-40",
    "href": "main/assessments.html#exam-40",
    "title": "Assessments",
    "section": "✍️ Exam (40%)",
    "text": "✍️ Exam (40%)\n\nAn open-book take-home exam, taken during the January exams period.\nExam questions will be comparable in style to the problem sets.\nThe exam questions will be released on Moodle\n\n\n\n\n\n\n\n⚠️ Import Update 11/10/2022\n\n\n\nLast year, DS202 exam was performed entirely online due to COVID-19 mitigation procedures. We want to run it online via our own Moodle page again this academic term, we just need to understand LSE regulations about exams for this year.\nWe will update you on this very soon (hopefully by the end of W04)."
  },
  {
    "objectID": "main/communication.html",
    "href": "main/communication.html",
    "title": "LSE DS202",
    "section": "",
    "text": "Find out how to reach out to your peers, teaching and administrative staff during this course.\n\n\n\nMost of our “informal” communication and interactions will happen through Slack.\nSlack is a platform used by many companies and institutions where teams can collaborate and communicate about a specific project.\nThere will be channels dedicated to discussing each week’s content, a channel for sharing useful links and events, plus a random channel to share random stuff about data science.\n\nYou will receive an invitation to join our Slack group via e-mail. Send an e-mail to Jon (J.Cardoso-Silva at lse dot ac dot uk) if you have not received an invite by the time of the first lecture.\nCheck out the following links to understand more about this tool:\n\nSlack tutorials\nCollaborate effectively in channels\n\n\n\n\n\nIt is probably a good idea to book office hours if:\n\nyou struggled with a technical or theoretical aspect of a problem set in the previous week,\nyou have queries about careers in data science,\nyou want guidance in how to apply data science to other things you are studying outside this course.\n\nCome prepared. You only have 15 minutes.\nAsk for help sooner rather than later.\nBook slots via StudentHub up to 12 hours in advance.\n\n⚠️ Reserve 📧 e-mail for formal requests: extensions, deferrals, etc. No need to e-mail to inform you will skip a class, for example."
  },
  {
    "objectID": "main/courserep.html",
    "href": "main/courserep.html",
    "title": "Course Representative",
    "section": "",
    "text": "The Data Science Institute (DSI) is excited to announce that the elections for Student Academic Representatives have now gone live! You can nominate yourself if you are taking DS105M or DS202 this Term, and all students get to to cast their votes anonymously on the candidates. The nomination and voting process will be conducted via Slack.\nAs a Student Academic Representative (Course Rep), you’ll work with staff to ensure your peers’ feedback is seen and acted on, and that student voices are represented in institutional decision-making. It is also a fantastic way to develop new skills, get to know the Institute better, and bolster your CV.\nYou can see the positions available in the table below:"
  },
  {
    "objectID": "main/courserep.html#engagement",
    "href": "main/courserep.html#engagement",
    "title": "Course Representative",
    "section": "Engagement",
    "text": "Engagement\n\nYou will have a direct channel of communication to voice suggestions, concerns and recommendations to the teaching and administrative staff at the DSI.\nYou will be allowed to use the DSI space on certain periods during the week.\nYou will have direct contact to PhD students and researchers who are visiting the DSI.\nYou will be the first to know about internal research projects available to undergraduate students."
  },
  {
    "objectID": "main/courserep.html#teaching-committees",
    "href": "main/courserep.html#teaching-committees",
    "title": "Course Representative",
    "section": "Teaching Committees",
    "text": "Teaching Committees\nAs a course rep, you will be invited to attend two Teaching Committees at the DSI during Michaelmas Term:\n\nthe first on Week 07\nthe second on Week 11\n\n\n\n\n\n\n\nWhat are Teaching Committees?\n\n\n\n\n\nTeaching Committees are official meetings where academic and professional service staff at the DSI discuss how the courses are going, devise adjustments for what is not going as planned, and plan new ways to boost recognition of innovative work of students enrolled in DSI courses. These meetings typically involve Lecturers, the Teaching Support Officer, the Communications Officer, the Institute Manager, and the Director of the DSI."
  },
  {
    "objectID": "main/courserep.html#nominations",
    "href": "main/courserep.html#nominations",
    "title": "Course Representative",
    "section": "1. Nominations",
    "text": "1. Nominations\n\nWrite down a short paragraph (max. ~50 words) explaining why you feel you would be great for the role!\nHead to your Slack group and post your short paragraph on the #student-rep channel.\nYou can nominate yourself anytime but no later than Monday 10 October 12 p.m.."
  },
  {
    "objectID": "main/courserep.html#voting",
    "href": "main/courserep.html#voting",
    "title": "Course Representative",
    "section": "2. Voting",
    "text": "2. Voting\n\nWe will set up an (anonymous) poll on the #student-rep channel using the app Polly\nNo one will be able to see who is winning/losing. Only when the poll closes will we know the results.\nAll students will be able to cast their votes anonymously on their favourite candidates from Tuesday 11 October – Thursday 13 October (end of Week 03).\nResults will be announced on the #student-rep channel."
  },
  {
    "objectID": "main/syllabus.html",
    "href": "main/syllabus.html",
    "title": "LSE DS202",
    "section": "",
    "text": "References\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York NY: Springer. https://www.statlearning.com/.\n\n\nSumner, Chris, Alison Byers, Rachel Boochever, and Gregory J. Park. 2012. “Predicting Dark Triad Personality Traits from Twitter Usage and a Linguistic Analysis of Tweets.” In 2012 11th International Conference on Machine Learning and Applications, 2:386–93. https://doi.org/10.1109/ICMLA.2012.218."
  },
  {
    "objectID": "weeks/week01.html",
    "href": "weeks/week01.html",
    "title": "🗓️ Week 01 - Introduction, Context & Key Concepts",
    "section": "",
    "text": "In this first week, we will cover what you can expect to learn from this course and the course logistics: all you need to know about the structure of the lectures, classes, assessments and how we will interact throughout this course.\nWe will also cover some of the basics: what do we mean by data science & machine learning, and what do you need to do to get the most of this course?\nJoin the lecture on 30 September 2022 at 2pm at NAB LG.01 (Wolfson Theatre)."
  },
  {
    "objectID": "weeks/week01.html#links",
    "href": "weeks/week01.html#links",
    "title": "🗓️ Week 01 - Introduction, Context & Key Concepts",
    "section": "Links",
    "text": "Links\n\n👨‍🏫 Lecture slides\n📒 Preparing for next week’s lab\n🔖 Appendix"
  },
  {
    "objectID": "weeks/week02.html",
    "href": "weeks/week02.html",
    "title": "🗓️ Week 02 - Simple and Multiple Linear Regression",
    "section": "",
    "text": "If you have already taken stats level at university, it is likely that you have met the linear regression method.\nIt is the most fundamental algorithm for regression and if you truly master it, you will be way ahead of the curve when it comes to the most advanced Machine Learning algorithms.\nJoin the lecture 7 October 2022 2pm at NAB LG.01 (Wolfson Theatre).."
  },
  {
    "objectID": "weeks/week02.html#links",
    "href": "weeks/week02.html#links",
    "title": "🗓️ Week 02 - Simple and Multiple Linear Regression",
    "section": "Links",
    "text": "Links\n\n👨‍🏫 Lecture slides\n✅ Take a look at this week’s checklist\n💻 This week’s lab"
  },
  {
    "objectID": "weeks/week03.html",
    "href": "weeks/week03.html",
    "title": "🗓️ Week 03 - Classifiers",
    "section": "",
    "text": "On 🗓️ Week 02, we learned how to make predictions about numerical variables. But what if you wanted to predict whether someone will perform an action (a Yes or No question)? Or, say, you were interested in assessing how the risk of fraud increases depending on the behaviour of a customer? These problems can be modelled using classifiers, a type of supervised learning.\nThis week, we will explore two classifier algorithms: the Logistic Regression and the Naive Bayes classifiers. We will learn how those methods relate (or not) to linear regression, and how to interpret its results. You will also meet a few new metrics and will learn of new ways to assess the ‘accuracy’ of models. These metrics will be incrediblly important on Week 04!\nJoin the lecture 14 October 2022 2pm at NAB LG.01 (Wolfson Theatre)."
  },
  {
    "objectID": "weeks/week03.html#links",
    "href": "weeks/week03.html#links",
    "title": "🗓️ Week 03 - Classifiers",
    "section": "Links",
    "text": "Links\n\n👨‍🏫 Lecture slides\n✅ Take a look at this week’s checklist\n💻 This week’s lab"
  },
  {
    "objectID": "weeks/week06.html",
    "href": "weeks/week06.html",
    "title": "🗓️ Week 06 - Reading Week",
    "section": "",
    "text": "Check out this blog post from Students@LSE blog.\nI would definitely recommend you take a break during Week 06. After all, rest and sleep can help you consolidate memories and improve your learning process.\nHowever, if you feel like revisiting concepts you have learned so far, check out the Recommended Learning sections present in each week’s sections here on our Moodle Page.\nWe will return with our lectures and classes on Week 07."
  },
  {
    "objectID": "weeks/week01/appendix.html",
    "href": "weeks/week01/appendix.html",
    "title": "🔖 Week 01 - Appendix",
    "section": "",
    "text": "This week’s indicative reading: (James et al. 2021, chaps. 2, 2.1–2.2)\n\n\nNeed to recap probability and statistics concepts? Check the suggested readings below:\n\n(Warne 2018, chaps. 1-3,5,6,11-12)\n(Gelman, Hill, and Vehtari 2020, chaps. 1–4)\nIf you are a PBS student, you can revisit the content of PB130 (MT3, MT4, MT8-MT11)"
  },
  {
    "objectID": "weeks/week01/appendix.html#recommended-additional-reading",
    "href": "weeks/week01/appendix.html#recommended-additional-reading",
    "title": "🔖 Week 01 - Appendix",
    "section": "Recommended (additional) reading",
    "text": "Recommended (additional) reading\nWhat are different ways one can approach a modelling problem?\nCheckout the upcoming book ‘Modeling Mindsets’ (Molnar 2022, chaps. 2–3) (it’s free to read online) to learn about the traditional frequentist statistics vs Bayesian statistics vs Machine Learning approaches.\nThe following twitter thread also summarises the main points of these different paradigms:\n\n\nIn a perfect world, you could effortlessly switch between modeling mindsets (statistics, machine learning, causal inference, …).Realistically, you only have time to master a few mindsets.So what to do? A thread 🧵\n\n— Christoph Molnar (@ChristophMolnar) August 30, 2022"
  },
  {
    "objectID": "weeks/week01/appendix.html#lse-digital-skills-lab",
    "href": "weeks/week01/appendix.html#lse-digital-skills-lab",
    "title": "🔖 Week 01 - Appendix",
    "section": "LSE Digital Skills Lab",
    "text": "LSE Digital Skills Lab\nLSE Digital Skills Lab offers R and python workshops during Term time and they will also give DSI students access to self-paced programming courses via Dataquest.\nFollow the links below to take the pre-sessional self-paced courses:\n\nR for Data Science Pre-sessional Course 22/23\n\nAlso, keep an eye on the following pages for news of the in-person workshops:\n\nR workshops"
  },
  {
    "objectID": "weeks/week01/appendix.html#other-resources",
    "href": "weeks/week01/appendix.html#other-resources",
    "title": "🔖 Week 01 - Appendix",
    "section": "Other resources",
    "text": "Other resources\n\nCheckout this summer’s LSE Careers Skill Accelerator programme. Some of the self-paced courses will remain open to LSE students until the end of the year.\n\n\n\nReady to develop the key skills employers are looking for in 2022?Join this summer's LSE Careers Skills Accelerator programme to expand your skillset!⭐Apply on CareerHub by 11.59pm, Wed 15 June for your chance to join the programme⭐https://t.co/J5sI1NRaMA\n\n— LSE Careers (@LSECareers) June 9, 2022\n\n\n\nThe book R for Data Science is free to read online and is a great resource to advance your R skills."
  },
  {
    "objectID": "weeks/week01/lecture.html",
    "href": "weeks/week01/lecture.html",
    "title": "👨‍🏫 Week 01 - Slides",
    "section": "",
    "text": "Either click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides."
  },
  {
    "objectID": "weeks/week01/lecture.html#coffee-break-10-min",
    "href": "weeks/week01/lecture.html#coffee-break-10-min",
    "title": "👨‍🏫 Week 01 - Slides",
    "section": "☕ Coffee Break (10 min)",
    "text": "☕ Coffee Break (10 min)\nUse this time to chat, stretch, drink some coffee or just relax for a bit by yourself."
  },
  {
    "objectID": "weeks/week01/lecture.html#part-ii---key-concepts-45-50-min",
    "href": "weeks/week01/lecture.html#part-ii---key-concepts-45-50-min",
    "title": "👨‍🏫 Week 01 - Slides",
    "section": "Part II - Key Concepts (45-50 min)",
    "text": "Part II - Key Concepts (45-50 min)\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides."
  },
  {
    "objectID": "weeks/week01/prep.html",
    "href": "weeks/week01/prep.html",
    "title": "📒 Week 01 - Preparing for next week’s lab",
    "section": "",
    "text": "We do not have classes this week. Instead, we recommend you use this time to learn or revisit the basics of the programming language R.\nSee sections below for advice on how to do that."
  },
  {
    "objectID": "weeks/week01/prep.html#preparing-for-the-next-week",
    "href": "weeks/week01/prep.html#preparing-for-the-next-week",
    "title": "📒 Week 01 - Preparing for next week’s lab",
    "section": "📒 Preparing for the next week",
    "text": "📒 Preparing for the next week\n\nJoin the Slack group of this course (more info)\nUse this time to learn or revisit basic R programming skills.\nWrite down your R questions. Next week’s lab, we have a roadmap for a revision of basic commands but we can tailor it to address any questions you might have with R."
  },
  {
    "objectID": "weeks/week01/prep.html#recommended-reading-other-resources",
    "href": "weeks/week01/prep.html#recommended-reading-other-resources",
    "title": "📒 Week 01 - Preparing for next week’s lab",
    "section": "🔖 Recommended reading & other resources",
    "text": "🔖 Recommended reading & other resources\nCheck out the Appendix page."
  },
  {
    "objectID": "weeks/week02/checklist.html",
    "href": "weeks/week02/checklist.html",
    "title": "✅ Week 02 - Checklist",
    "section": "",
    "text": "Your Checklist:\nYour Checklist:\n\n🖥️ Before you come to the class, skim through the W02 lab roadmap page to have an idea of what we are going to do.\n👨‍💻 New to R? Or perhaps you are in one of the Monday sessions and struggled to follow the lab? There is still time to take the R pre-sessional course. (Read the Getting access and using Dataquest session carefully)\n📚 Before you attend the lecture on Friday, try to catch up on the recommended reading of last week.\n💻 Assess your own understanding: did you understand all the exercises of the lab?\n📝 Take note of anything that is still not clear to you.\n📟 Share your conceptual or programming-related questions on #week02 channel on Slack.\n📤 Have anything else to share? If came across an interesting resource for R beginners, or curious articles about exploratory data analysis, feel free to share it on /week02 or /random channels in our Slack group.\n\nFollowing this will keep you well prepared for the Linear Regression lab of Week 03."
  },
  {
    "objectID": "weeks/week02/lab.html",
    "href": "weeks/week02/lab.html",
    "title": "💻 Week 02 - Lab Roadmap (90 min)",
    "section": "",
    "text": "This week, we will recap some basic R commands for social data science and then apply these commands to a practical case. We will learn about data structures and some simple data visualisation skills in R .\nIt is expected that R has been downloaded locally. We recommend that you run R within an integrated development environment (IDE) such as RStudio, which can be freely downloaded."
  },
  {
    "objectID": "weeks/week02/lab.html#step-1-basic-commands",
    "href": "weeks/week02/lab.html#step-1-basic-commands",
    "title": "💻 Week 02 - Lab Roadmap (90 min)",
    "section": "Step 1: Basic commands",
    "text": "Step 1: Basic commands\nWe will follow the instructions below step by step together while answering whatever questions you might encounter along the way.\n\nOpen R or RStudio. You can either run the folllowing commands in a R script or in the console window.\nCreate a vetor of numbers with the function c() and name it x. When we type x, it gives us back the vector:\n> x <- c(1, 3, 2, 5)\n> x\n[1] 1 3 2 5\nNote that the > is not part of the command; rather, it is printed by R to indicate that it is ready for another command to be entered. We can also save things using = rather than <-:\n> x = c(1, 3, 2, 5)\nCheck the length of vector x using the length() function:\n> length(x)\n[1] 4\nCreate a matrix of numbers with the function matrix() and name it y. When we type y, it gives us back the matrix:\n> y <- matrix(data = c(1:16), nrow = 4, ncol = 4)\n> y\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\nIf you want to learn about the meaning of some arguments like nrow or ncol:\n> ?matrix\nSelect one element in the matrix y:\n> y[2,3]\n[1] 10\nThe first number after the open-bracket symbol [ always refers to the row, and the second number always refers to the column\nSelect multiple rows and column at a time in the matrix y:\n> y[c(1, 3), c(2, 4)]\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n> y[1:3, 2:4]\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n> y[1:2, ]\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n> y[-c(1, 3), ]\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\nNo index for the columns or the rows indicates that R should include all columns or all rows, respectively. The use of a negative sign - in the index tells R to keep all rows or columns except those indicated in the index.\nCheck the number of rows and columns in a matrix:\n> dim(y)\n[1] 4 4\nGenerate a vector of random normal variables:\n> set.seed(1303)\n> x <- rnorm(50)\n> y <- x + rnorm(50, mean = 50, sd = .1)\n> cor(x, y)\n[1] 0.9942128\nBy default, rnorm() creates standard normal random variables with a mean of 0 and a standard deviation of 1. However, the mean and standard deviation can be altered as illustrated above.\nEach time we call the function rnorm(), we will get a different answer. However, sometimes we want our code to reproduce the exact same set of random numbers; we can use the set.seed() function to do this. We use set.seed() throughout the labs whenever we perform calculations involving random quantities.\nLet’s check some descriptive statistics of these vectors:\n> mean(y)\n[1] 50.18446\n> var(y)\n[1] 0.8002002\n> sqrt ( var (y))\n[1] 0.8945391\n> sd(y)\n[1] 0.8945391\n> cor (x, y)\n[1] 0.9942128\nThe mean() and var() functions can be used to compute the mean and variance of a vector of numbers. Applying sqrt() to the output of var() will give the standard deviation. Or we can simply use the sd() function. The cor() function is to compute the correlation between vector x and y."
  },
  {
    "objectID": "weeks/week02/lab.html#step-2-graphics",
    "href": "weeks/week02/lab.html#step-2-graphics",
    "title": "💻 Week 02 - Lab Roadmap (90 min)",
    "section": "Step 2: Graphics",
    "text": "Step 2: Graphics\nWe will plot and save plots in R.\n\nProduce a scatterplot between two vectors of numbers using the function plot():\n> set.seed(1303)\n> x <- rnorm(100)\n> y <- rnorm(100)\n> plot(x,y)\n> plot(x, y, xlab = \" this is the x- axis \",\n       ylab = \" this is the y- axis \",\n       main = \" Plot of X vs Y\")\nBy default, the output plot will show in Plots window in the lower right cornor.\nSave the scatterplot in a pdf or a jpeg file:\n> pdf(\"Figure.pdf\")\n> plot(x, y, col = \"green\")\n> dev.off()\nnull device\n        1    \nTo create a jpeg, we use the function jpeg() instaed of pdf(). The function dev.off() indicates to R that we are done creating the plot.\nProduce a contour plot (like a topographical map) to represent 3-Dimentional data using the function contour():\n> x <- seq(1, 10)\n> y <- x\n> f <- outer(x, y, function (x, y) cos(y) / (1 + x^2))\n> contour(x, y, f)\n> contour(x, y, f, nlevels = 45, add = T)\n> fa <- (f - t(f)) / 2\n> contour(x, y, fa, nlevels = 15)\nThe image() function works the same way as contour(). Explore it if you are interested.\nUsing ggplot2 package for graphic:\nIn R, the data is stored in a structure called dataframe. Dataframe can be seen as a 2-dimensional table consisting of rows and columns and their values. These values might be in different types such as numeric, character or logical. However, each column should have the exactly same data type.\nWe can use the open-source data visualization package - ggplot2 to construct aesthetic mappings based on our data.\n\nSince tidyverse library includes ggplot2, if you install tidyverse you will have access to ggplot2; installation can be done;\n\n> install.packages(\"tidyverse\")\n\nAlternatively, ggplot2 package can be installed\n\n> install.packages(\"ggplot2\")\nAfter the installation is completed, it should be called in R environment:\n> library(ggplot2)\nThere are some ready datasets to play with in the package ggplot2. Let’s explore and plot a dataset called diamonds showing the prices and some features of over 50000 diamonds. You can explore the meanings of the variables with ?diamonds command.\nPlease type:\n> View(diamonds)\nthe View() function can be used to view it in a spreadsheet-like window.\nwe can plot this dataset with desired variables.\n > ggplot(diamonds[0:50,], aes(x=carat, y=price)) +\ngeom_point() + \ngeom_text(label=diamonds[0:50,]$cut)\nx and y in aes shows the axis which are the carat and the price info each diamond. diamonds is the dataframe used in the plot and We used only the first 50 lines for clear visualisation. geom_point defines the shape of data to be plot and geom_text adds the labels. With $ sign, you can access a column in your dataset.\nWe can also plot a histogram showing price\n > ggplot(diamonds,aes(x=price)) + geom_histogram(binwidth=100)\nThis time all dataset is used for the visualisation.. For more detailed information and some examples you can use ?ggplot and ?aes\n\n\n\n\n\n\n\nFurther Study - Heatmap Example\n\n\n\n\n\nCreating a heatmap with ggplot2 package:\nThis time we will create a dummy dataframe with country names, a time period and random GDP for each country.\ncountries <- c(\"Canada\", \"France\",\"Greece\",\"Libya\",\"Malta\")\nyears <- c(2012:2021)\nLet’s gather them together and see what our dataframe looks like:\ndata <- expand.grid(Country=countries, Year=years)\ndata\nexpand.grid creates a dataframe from all combinations of the supplied vectors.\nto create random GPD for each country and for each year, and to add these values into our dataframe as GDP column::\ngdps  <- runif(50, min=20000, max=500000)\ndata$GDP = gdps\nrunif generates a certain number of random values between min and maximum values with a uniform distribution. Since we have 5 countries and 10 year, we generated 50 random GPD value.\nTo check the data and type of the variable data:\nView(data)\nclass(data)\nWe can plot now a very basic heatmap\nggplot(data, aes(Year, Country, fill= GDP)) + geom_tile()\nTo create a heatmap, our dataframe should look like a tabular dataset with three columns. aes defines X,Y axis and the values filling these pairs in the heatmap. geom_tile creates a heatmap with rectangulars with different options. For detailed information ?geom_tile"
  },
  {
    "objectID": "weeks/week02/lab.html#step-3-loading-data",
    "href": "weeks/week02/lab.html#step-3-loading-data",
    "title": "💻 Week 02 - Lab Roadmap (90 min)",
    "section": "Step 3: Loading data",
    "text": "Step 3: Loading data\nNow, we will learn how to import a data set into R and explore the data set. For this lab session, we will use a ready-to-use dataset AUTO in the book “Introduction to Statistical Learning, with Applications in R”. With the package ISLR2, we can use all the datasets in the book.\n\nFirst, we need to install ISLR2 into our R environment for future use.\n> install.packages(\"ISLR2\")\nTo use ISLR2 package and the datasets in our analyses, we need to call it in each R session with;\n> library(ISLR2)\nThat’s it! We now can use all datasets by calling them by their names. The package includes numerous datasets and you can explore them with R.\nAUTO dataset is ready to be used in the analyse. You can explore the dataset by using:\n> View(Auto)\n> head(Auto)\nThe head() function can also be used to view the first few rows of the data\nYou may want to save this dataset on a local computer, which is useful for your future analyses while doing some changes on it. To save a dataset as a csv file:\n> write.csv(DataFrameName, file=\"Path to save the DataFrame//File Name.csv\", row.names = FALSE)\nThe option row.names = FALSE deletes the row names when you are saving the dataset. In this case, it will remove basic incremental indexes such as 1,2,… from the data. A detailed explanation of write.csv and its options could be found by typing ?write.csv\n\n\n\n\n\n\nExample\n\n\n\nYou need to include the path where you would like to save the dataset on your computer. For example, if you work in a folder called Test in your desktop in a Windows machine. The code:\n> write.csv(Auto, \"C:Users//LSE//Desktop//Test//autodataset.csv\", row.names = FALSE )\n\n\nTo use the dataset in the future, you need to load it into a dataframe by importing the csv file.\nWe will load this dataset in a dataframe called Auto. Dataframe name is changable, however we would like to use words understandable and readable.\n> Auto <- read.csv(\"C://Users//LSE//Desktop//Test//autodataset.csv\", na.strings = \"?\")\nUsing the option na.strings tells R that any time it sees a particular character or set of characters (such as a question mark), it should be treated as a missing element of the data matrix.\nYou can check the dataset:\n> View(Auto)\n> head(Auto)\nDeal with the missing data by removing rows with missing observations:\n> Auto <- na.omit(Auto)\n> dim(Auto)\n[1] 392 9\nThe function dim() is to check the size of the data frame.\nProduce a numerical summary of each variable in the particular data frame:\n> summary(Auto)"
  },
  {
    "objectID": "weeks/week02/lab.html#step-4-practical-exercises-in-pairs",
    "href": "weeks/week02/lab.html#step-4-practical-exercises-in-pairs",
    "title": "💻 Week 02 - Lab Roadmap (90 min)",
    "section": "Step 4: Practical exercises (in pairs)",
    "text": "Step 4: Practical exercises (in pairs)\nSo far, we have learnt some basic commands in R. In this practical case, we will continues with the data set Auto studied in Step 3. Make sure that the missing values have been removed from the data.\nSix questions are listed below. You are required to try to answer these questions in pair using R commands. We will go over the solutions once everyone has finished these questions.\n🎯 Questions\n\nWhich of the predictors are quantitative, and which are qualitative?\nWhat is the range of each quantitative predictor? (hint: You can answer this using the range() function)\nWhat is the mean and standard deviation of each quantitative predictor?\nNow remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?\nUsing the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings.\nSuppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer."
  },
  {
    "objectID": "weeks/week02/lab_solutions.html",
    "href": "weeks/week02/lab_solutions.html",
    "title": "💻 Week 02 - Lab Roadmap (90 min)",
    "section": "",
    "text": "Use the function View()to identify the type of a variable (quantitative or qualitative):\nView(Auto)\nVariables mpg, cylinders, horsepower, weight, accelation, year are quantitative variable. Variables origin, name are qualitative varibale.\nUse the function range() to check the range of each quantitative predictor:\nrange(Auto$mpg)\n[1]  9.0 46.6\nTo refer to a variable, we must type the data set and the variable name joined with a $ symbol.\nUsing summary() to have an overall look at all variables and statistical features (like mean and standard deviation) are included in the outputs:\nsummary(Auto)\nor\nmean(Auto$mpg)\nsd(Auto$mpg)\nRemove the 10th through 85th observations from the original data frame and store it as another new data frame:\nAuto_tmp = Auto[-c(10:85), ]\nsummary(Auto_tmp)\nmean(Auto_tmp$mpg)\nsd(Auto_tmp$mpg)\nCreate a scatterplot matrix using the function pairs():\npairs( ~ mpg + displacement + horsepower + weight + \n        acceleration + year + origin + cylinders, \n        data = Auto)\nNotice the linear or non-linear trends in the scatterplots.Then create a histogram of the variable mpg:\nhist (Auto$mpg , col = 2, breaks = 15)\nUse the hist() function to produce some histograms with differing numbers of bins for a few of the quantitative variables. You may find the command par(mfrow = c(2, 2)) useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways.\nAfter observing the first row of the scatterplot matrix which indicates the relationship between gas mileage (mpg) and other variables, you will find evident linear or non-linear trends exist in the scatterplots with variables displacement, horsepower, weight, year and origin. Therefore, these varibles might be useful in predicting mpg.\n\nIf you want to achieve ststistical robust when exploring the relationship between variables, you need to culculate some statistics (like the correlation using the function cor()) and conduct statistical tests. This will be further illustrated in Week 03."
  },
  {
    "objectID": "weeks/week02/lecture.html",
    "href": "weeks/week02/lecture.html",
    "title": "👨‍🏫 Week 02 - Slides",
    "section": "",
    "text": "Either click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides. For a PDF copy, check Moodle."
  },
  {
    "objectID": "weeks/week02/lecture.html#coffee-break-10-min",
    "href": "weeks/week02/lecture.html#coffee-break-10-min",
    "title": "👨‍🏫 Week 02 - Slides",
    "section": "☕ Coffee Break (10 min)",
    "text": "☕ Coffee Break (10 min)\nUse this time to chat, stretch, drink some coffee or just relax for a bit by yourself."
  },
  {
    "objectID": "weeks/week02/lecture.html#part-ii---multiple-linear-regression-45-50-min",
    "href": "weeks/week02/lecture.html#part-ii---multiple-linear-regression-45-50-min",
    "title": "👨‍🏫 Week 02 - Slides",
    "section": "Part II - Multiple Linear Regression (45-50 min)",
    "text": "Part II - Multiple Linear Regression (45-50 min)\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides. For a PDF copy, check Moodle."
  },
  {
    "objectID": "weeks/week03/checklist.html",
    "href": "weeks/week03/checklist.html",
    "title": "✅ Week 03 - Checklist",
    "section": "",
    "text": "Your Checklist:\n\n📙 Read (James et al. 2021, chap. 3) to reinforce your theoretical knowledge of Linear Regression. The textbook is available online for free.\n🧑‍💻 If you already know linear regression from previous courses you have taken, why not take this knowledge to next level?\n\nTry to find a dataset online that contains a numerical variable you could predict by fitting a linear regression to it. I will be curious to see what you find. Share your findings on the #week03 channel in our Slack.\n\n🖥️ Before you come to the class, skim the W03 lab roadmap page to have an idea of what we are going to do.\n\nThis week, instead of just typing things in the terminal, we will use R Markdown. You can read about it here. This is also how you will be submitting solutions to formative and summative assignments in the future.\nI will post solutions to the practical exercises at the end of the week.\n\n💻 Assess yourself: did you understand all the exercises in the lab?\n\nIf you are new to linear regression and you are enrolled in the Monday sessions, it is likely that you will struggle a bit in the lab. During the week, reserve some time to read about Linear Regression and then practice the exercises again.\n\n📟 Struggling with something? Don’t know what a particular R command do? Share your questions on the #week03 channel in our Slack.\n\nI will also be posting follow up questions on Slack during the week.\n\n📝 Keep in mind that: after the lecture on Friday, 14 October 2022, we will post the first formative assignment on Moodle.\n\nYou will have until Thursday of the following week (20 October 2022) to submit your solutions.\nThis assignment is not marked, it doesn’t count towards your final grade, but you will receive feedback if you submit.\nThe assignment will have a similar format as the questions we explore in the lab.\n\n👨‍🏫 Attend the lecture. It will help you remember concepts more easily when revising later.\n\n\n\n\n\n\nReferences\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York NY: Springer. https://www.statlearning.com/."
  },
  {
    "objectID": "weeks/week03/formative.html",
    "href": "weeks/week03/formative.html",
    "title": "📝 Week 03 - Formative homework",
    "section": "",
    "text": "Use the Carseats data set in the ISLR2 package to answer the following questions:\n\nFit a multiple linear regression model to predict Sales using Price, Urban, and US. Show the summary output.\nProvide an interpretation of each coefficient in the model. Be careful—some of the variables in the model are qualitative!\nWrite the model in equation form, carefully handling the qualitative variables properly.\nFor which of the predictors can you reject the null hypothesis \\(H_0: \\beta_j = 0\\)?\nBased on your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of an association with the outcome. Justify your choices.\nHow well do the models in Questions 1 & 5 fit the data?\nUsing the model from question 5, obtain 95% confidence intervals for the coefficient(s).\nUse the * and : symbols to fit linear regression models with interaction effects. Could you find any model with interactions that fit better than the models you built in Questions 1 & 5? Justify your answer."
  },
  {
    "objectID": "weeks/week03/formative_solutions.html",
    "href": "weeks/week03/formative_solutions.html",
    "title": "📝 Week 03 - Formative homework",
    "section": "",
    "text": "Use the Carseats data set in the ISLR2 package to answer the following questions:\n\nFit a multiple linear regression model to predict Sales using Price, Urban, and US. Show the summary output.\n> library(ISLR2)\n> lm.fit <- lm(Sales ~ Price + Urban + US, data = Carseats)\n> summary(lm.fit)\nProvide an interpretation of each coefficient in the model. Be careful—some of the variables in the model are qualitative!\nβ1: Holding Urban and US fixed, the Sales decrease 54.459 units on average when the Price company charges for car seats at each site increases 1000 units.\nβ2: Holding Price and US fixed, the Sales decrease 0.021916 units on average when the store is in urban area.\nβ3: Holding Price and Urban fixed, the Sales increase 1.200573 units on average when the store is in US.\nWrite the model in equation form, carefully handling the qualitative variables properly.\nSales = β0 + β1 x Price + β2 + β3, when the store is in urban and in US\nSales = β0 + β1 x Price + β3, when the store is not in urban and but in US\nSales = β0 + β1 x Price + β2, when the store is in urban and but not in US\nSales = β0 + β1 x Price, when the store is not in urban and not in US\nFor which of the predictors can you reject the null hypothesis H0: βj = 0?\nFrom the p-values of t-test, we could reject the null hypothesis H0: β1 = 0 for the predictor Price and the null hypothesis H0: β3 = 0 for the predictor US.\nBased on your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of an association with the outcome. Justify your choices.\n> lm.fit1 <- lm(Sales ~ Price + US, data = Carseats)\n> summary(lm.fit1)\nHow well do the models in Questions 1 & 5 fit the data?\nNeither model fits the data well according to the small value of R2 and Adjusted R2. It means that the predictors included in both models can only interpret a small part of the change pattern of the response.\nUsing the model from question 5, obtain 95 % confidence intervals for the coefficient(s).\n> confint(lm.fit1)\n    2.5 %      97.5 %\n(Intercept) 11.79032020 14.27126531\nPrice       -0.06475984 -0.04419543\nUSYes        0.69151957  1.70776632\n95% confidence interval for β0 = [11.79032020, 14.27126531]; 95% confidence interval for β1 = [-0.06475984 -0.04419543]; 95% confidence interval for β2 = [0.69151957 1.70776632].\nUse the * and : symbols to fit linear regression models with interaction effects. Could you find any model with interactions that fit better than the models you built in Questions 1 & 5? Justify your answer.\n> lm.fit2 <- lm(Sales ~ Price * US, data = Carseats)\n> summary(lm.fit2)\n> lm.fit3 <- lm(Sales ~ Price * Urban, data = Carseats)\n> summary(lm.fit3)\n> lm.fit4 <- lm(Sales ~ Price * US + Urban, data = Carseats)\n> summary(lm.fit4)\n> lm.fit5 <- lm(Sales ~ Price * Urban + US, data = Carseats)\n> summary(lm.fit5)\n> lm.fit6 <- lm(Sales ~ Price + US + Urban + Price:US + Price:Urban, data = Carseats)\n> summary(lm.fit6)\n> lm.fit7 <- lm(Sales ~ Price + US + Urban + Price:US + Price:Urban + Urban:US, data = Carseats)\n> summary(lm.fit7)\nDifferent multiple linear regression models with interactions have been built. However, after comparision there is almost no difference between the values of the \\(R^2\\) and adjusted \\(R^2\\), which means the goodness of fit is not significantly improved. The reason is that there is no interaction effects between Price and US, and between Price and Urban, and between US and Urban which is supported by the significance of these coeficients’ t-tests."
  },
  {
    "objectID": "weeks/week03/lab.html",
    "href": "weeks/week03/lab.html",
    "title": "💻 Week 03 - Lab Roadmap (90 min)",
    "section": "",
    "text": "This week, we will fit simple and multiple linear regression models in R and learn to interpret the R output. We will apply this method to practical cases and deal with problems that commonly arise during this process."
  },
  {
    "objectID": "weeks/week03/lab.html#step-1-simple-linear-regression",
    "href": "weeks/week03/lab.html#step-1-simple-linear-regression",
    "title": "💻 Week 03 - Lab Roadmap (90 min)",
    "section": "Step 1: Simple linear regression",
    "text": "Step 1: Simple linear regression\nWe will follow the instructions below step by step together while answering whatever questions you might encounter along the way.\n\nInstall and load the ISLR2 package, which contains a large collection of data sets and functions.\ninstall.packages(\"ISLR2\").\nlibrary (ISLR2)\nThe function install.packages() is used to download packages that don’t come with R. This installation only needs to be done the first time you use a package. However, the library() function must be called within each R session to load packages.\nUse the Boston data set in the ISLR2 library. It records medv (median house value) for 506 census tracts in Boston. Have a look at the first few rows of the Boston data set:\nhead (Boston)\n    crim zn indus chas   nox    rm  age    dis rad tax ptratio lstat medv\n1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3  4.98 24.0\n2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8  9.14 21.6\n3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8  4.03 34.7\n4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7  2.94 33.4\n5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7  5.33 36.2\n6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7  5.21 28.7\nWe want to predict medv using the available predictors, such as rm (average number of rooms per house), age (average age of houses), and lstat (percentage of households with low socioeconomic status). To find out more about the data set, we can type ?Boston.\nFit a simple linear regression lm() model, with medv as the response and lstat as the predictor:\n> lm.fit <- lm(medv ~ lstat , data = Boston)\nThe basic syntax is lm(y ∼ x, data), where y is the response, x is the predictor, and data is the data set in which we keep these two variables.\nUse the tidy function to create a dataframe with columns for the estimate, standard error, f-statistic (estimate/standard error), p-values, and 95 percent confidence intervals:\ninstalling/loading broom:\ninstall.packages(\"broom\")\nlibrary(broom)\n> tidy(lm.fit, conf.int = TRUE)\n\n\n# A tibble: 2 × 7\nterm        estimate std.error statistic   p.value conf.low conf.high\n<chr>          <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)   34.6      0.563       61.4 3.74e-236    33.4     35.7  \n2 lstat         -0.950    0.0387     -24.5 5.08e- 88    -1.03    -0.874\nBecause lm.fit is a simple linear regression model, there are only two coefficients: \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). The goodness-of-fit of the model can be measured by the \\(R^2\\) in the output, which can be obtained (along with other model statistics) using the glance function.\n> glance(lm.fit)$r.squared\n[1] 0.5441463\nPlot medv and lstat along with the least squares regression line using the geom_point() and geom_abline() functions.::\nlibrary(tidyverse)\n\n> ggplot(data = Boston, aes(x = lstat, y = medv)) +\ngeom_point() + \ngeom_abline(intercept = lm.fit$coefficients[1], slope = lm.fit$coefficients[2])"
  },
  {
    "objectID": "weeks/week03/lab.html#step-2-multiple-linear-regression",
    "href": "weeks/week03/lab.html#step-2-multiple-linear-regression",
    "title": "💻 Week 03 - Lab Roadmap (90 min)",
    "section": "Step 2: Multiple linear regression",
    "text": "Step 2: Multiple linear regression\nWe will still use the Boston data set to fit multiple linear regression. The fitting process is similar to simple linear regression.\n\nFit a multiple linear regression lm() model, with medv as the response, lstat and age as the predictors:\n> lm.fit <- lm(medv ~ lstat + age , data = Boston)\n> tidy(lm.fit, conf.int = TRUE)\n\n# A tibble: 3 × 7\nterm        estimate std.error statistic   p.value conf.low conf.high\n<chr>          <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)  33.2       0.731      45.5  2.94e-180  31.8      34.7   \n2 lstat        -1.03      0.0482    -21.4  8.42e- 73  -1.13     -0.937 \n3 age           0.0345    0.0122      2.83 4.91e-  3   0.0105    0.0586\nThe syntax lm(y ~ x1 + x2 + x3) is used to fit a model with three predictors, x1, x2, and x3. The tidy() function now outputs the regression coefficients for all the predictors.\nFit a multiple linear regression lm() model, with medv as the response, all rest variables as the predictors:\n> lm.fit <- lm(medv ~ ., data = Boston)\n> tidy(lm.fit, conf.int = TRUE)\n\n# A tibble: 13 × 7\nterm         estimate std.error statistic  p.value conf.low conf.high\n<chr>           <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)  41.6       4.94        8.43  3.79e-16  31.9     51.3    \n2 crim         -0.121     0.0330     -3.68  2.61e- 4  -0.186   -0.0565 \n3 zn            0.0470    0.0139      3.38  7.72e- 4   0.0197   0.0742 \n4 indus         0.0135    0.0621      0.217 8.29e- 1  -0.109    0.136  \n5 chas          2.84      0.870       3.26  1.17e- 3   1.13     4.55   \n6 nox         -18.8       3.85       -4.87  1.50e- 6 -26.3    -11.2    \n7 rm            3.66      0.420       8.70  4.81e-17   2.83     4.48   \n8 age           0.00361   0.0133      0.271 7.87e- 1  -0.0226   0.0298 \n9 dis          -1.49      0.202      -7.39  6.17e-13  -1.89    -1.09   \n10 rad           0.289     0.0669      4.33  1.84e- 5   0.158    0.421  \n11 tax          -0.0127    0.00380    -3.34  9.12e- 4  -0.0202  -0.00521\n12 ptratio      -0.938     0.132      -7.09  4.63e-12  -1.20    -0.678  \n13 lstat        -0.552     0.0507    -10.9   6.39e-25  -0.652   -0.452  \n\nWe can access the individual components of a summary object by name (type ?glance to see what is available). Hence glance(lm.fit)$r.squared gives us the \\(R^2\\).\n\nSelect variables:\nIn these two multiple linear regression models, the t-tests and F-test results suggest that many of the predictors are significant for the response variable. However, some do not achieve statistical significance. Can you see which variables these are?\nWe call the process of determining which predictors are associated with the response as variable selection.\nIf the number of predictors is very small, we could perform the variable selection by trying out a lot of different models, each containing a different subset of the predictors. We can then select the best model out of all of the models we have considered.\nUsing the template below, try figuring out the model which produces the highest adjusted \\(R^2\\). The adjusted \\(R^2\\) has a similar interpretation to \\(R^2\\), only it is an advantage here as it penalises models that include insignificant parameters.\nlm.fit <- lm(medv ~ ., data = Boston)\nglance(lm.fit)$adj.r.squared\n[1] 0.7278399\nWe found that if you remove indus and age, the adjusted \\(R^2\\) becomes slightly larger compared to including all predictors.\nlm.fit <- lm(medv ~ ., data = Boston[,-c(3,7)])\nglance(lm.fit)$adj.r.squared\n[1] 0.7288734"
  },
  {
    "objectID": "weeks/week03/lab.html#step-3-some-potential-problems",
    "href": "weeks/week03/lab.html#step-3-some-potential-problems",
    "title": "💻 Week 03 - Lab Roadmap (90 min)",
    "section": "Step 3: Some potential problems",
    "text": "Step 3: Some potential problems\nMany problems may occur when we fit a linear regression model to a particular data set. These problems will lead to inaccurate estimation. In this step, we will identify and overcome potential problems such as outliers, collinearity and interaction effects.\nWe present a few of the many methods available, but those interested can explore more after class.\n\nHandle interaction terms:\nIn regression, an interaction effect exists when the effect of an independent variable on the response variable changes, depending on the values of one or more independent variables. When you believe there is an interaction effect, it is easy to include interaction terms in a linear model using the lm() function.\n> tidy(lm(medv ~ lstat * age , data = Boston))\n\n# A tibble: 4 × 5\nterm         estimate std.error statistic  p.value\n<chr>           <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept) 36.1        1.47      24.6    4.91e-88\n2 lstat       -1.39       0.167     -8.31   8.78e-16\n3 age         -0.000721   0.0199    -0.0363 9.71e- 1\n4 lstat:age    0.00416    0.00185    2.24   2.52e- 2\nThe syntax lstat:age tells R to include an interaction term between lstat and age. The syntax lstat*age simultaneously includes lstat, age, and the interaction term lstat×age as predictors; it is a shorthand for lstat+age+lstat:age.\nIdentify outliers through residual plots:\nAn outlier is a point for which \\(\\hat{y}_i\\) is far from the value predicted by the model. Outliers can arise for a variety of reasons, such as incorrect recording of observation during data collection. Outliers could be identified through residual plots:\n> par(mfrow = c(2, 2))\n> plot(lm.fit)\nThe plot function automatically produces four diagnostic plots when you pass the output from lm(). Plots on the left column are residual plots, indicating the relationship between residuals and fitted values.\nIn practice, it can be difficult to decide how large a residual needs to be before we consider the point to be an outlier. Instead of plotting the residuals, we can address this problem by plotting the studentized residuals. These are computed by dividing each residual ei by its estimated standard studentized residual error. Observations with studentized residuals greater than 3 in absolute value are possible outliers. Using the plot() function to plot the studentized residuals:\n> plot(predict(lm.fit), rstudent(lm.fit)\nHandle outliers:\nIf we believe an outlier is due to an error in data collection or recording, then one solution is to simply remove the observation. However, care should be taken, as an outlier may instead signal a deficiency with our model, such as a missing predictor.\nDetect multicollinearity using the correlation matrix:\nMulticollinearity refers to the situation in which two or more predictor variables are highly correlated to one another. It can be detected through the correlation matrix:\ncor(Boston)\nIgnoring the last row and the last column in the matrix, which indicate the relationship with response variable medv, an element of this matrix that is large in absolute value indicates a pair of highly correlated variables, and therefore a collinearity problem in the data.\nWe can detect multicollinearity quantitatively using vif() function in the `car’ package:\ninstall.packages(\"car\"))\nlibrary(car)\n> vif(lm.fit)\nInstead of inspecting the correlation matrix, a better way to assess multicollinearity is to compute the variance inflation factor (VIF). As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity.\n\n\n\n\n\n\n\nRead more about VIF\n\n\n\n\n\nCheck out our textbook (James et al. 2021, 99–103) for a description of the Variance Inflation Factor (VIF).\n\n\n\n\nHandle collinearity:\nWhen faced with the problem of multicollinearity, there are two simple solutions.\n-The first is to drop one of the problematic variables from the regression.\n-The second solution is to combine the collinear variables into a single predictor, where such combination makes theoretical sense."
  },
  {
    "objectID": "weeks/week03/lab.html#step-4-practical-exercises-in-pairs",
    "href": "weeks/week03/lab.html#step-4-practical-exercises-in-pairs",
    "title": "💻 Week 03 - Lab Roadmap (90 min)",
    "section": "Step 4: Practical exercises (in pairs)",
    "text": "Step 4: Practical exercises (in pairs)\nSo far, we have learnt to fit simple and multiple linear regression models in R. In this practical case, we will continue to use the data set Auto studied in the last lab. Make sure that the missing values have been removed from the data.\nEight questions are listed below. You are required to try to answer these questions in pairs using R commands. We will go over the solutions once everyone has finished these questions.\n🎯 Questions\n\nUse the lm() function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the tidy() function to print the results. Comment on the output. For example:\n\nIs there a relationship between the predictor and the response?\nHow strong is the relationship between the predictor and the response?\nIs the relationship between the predictor and the response positive or negative?\nWhat is the predicted mpg associated with a horsepower of 98? What are the associated 95 % confidence intervals?\n\nPlot the response and the predictor. Use the geom_abline() function to display the least squares regression line.\nUse the plot() function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.\nProduce a scatterplot matrix that includes all the variables in the data set.\nCompute the matrix of correlations between the variables using the function cor(). You will need to exclude the name and origin variable, which are qualitative.\nUse the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the tidy() function to print the results. Comment on the output. For instance:\n\nIs there a relationship between the predictors and the response?\nWhich predictors appear to have a statistically significant relationship to the response?\nWhat does the coefficient for the year variable suggest?\n\nUse the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers?\nUse the * and : symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?\n\n\n\n\n\n\n\nTip\n\n\n\nIf you could not finish all eight questions during the lab, take that as a home exercise.\nUse the #week03 channel on Slack if you have any questions."
  },
  {
    "objectID": "weeks/week03/lab_solutions.html",
    "href": "weeks/week03/lab_solutions.html",
    "title": "💻 Week 03 - Lab Roadmap (90 min)",
    "section": "",
    "text": "Use the lm() function to perform a simple linear regression，and use the summary() function to print the results:\n> library(ISLR2)\n> Auto <- na.omit(Auto)\n> lm.fit <- lm(mpg ~ horsepower, data = Auto)\n> summary(lm.fit)\n\nCall:\nlm(formula = mpg ~ horsepower, data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5710  -3.2592  -0.3435   2.7630  16.9240 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 39.935861   0.717499   55.66   <2e-16 ***\nhorsepower  -0.157845   0.006446  -24.49   <2e-16 ***\n---\n Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 4.906 on 390 degrees of freedom\nMultiple R-squared:  0.6059,    Adjusted R-squared:  0.6049 \nF-statistic: 599.7 on 1 and 390 DF,  p-value: < 2.2e-16\nRegarding to the p-values of t-test and F-test, there is a strong relationship between the predictor horsepower and the reponse mpg. From the sign of coefficients, the relationship between the predicator and the response is negative. Using the function predict() to predict the value of reponse and the confidence interval, we get:\n> predict(lm.fit, data.frame(horsepower = 98), interval = \"confidence\")\n       fit      lwr      upr\n1 24.46708 23.97308 24.96108\nTherefore, the predicted mpg associated with a horsepower of 98 is 24.47, and the associated 95 % confidence interval is [23.97308, 24.96108].\nUse the function plot() and abline():\n> attach(Auto)\n> plot(mpg, horsepower, ylim = c(0, 250))\n> abline (lm.fit, lwd = 3, col = \"red\")\n> dev.off()\nUsing plot() function to produce diagnostic plots:\n> par(mfrow = c(2, 2))\n> plot (lm.fit)\n> dev.off()\nBy observing four diagnostic plots, we could find non-linear patttern in residual plots. The quadratic trend of the residuals could be a problem. Then we plot studentized residuals to identify outliers:\n> plot(predict(lm.fit), rstudent(lm.fit))\n> dev.off()\nThere are possible outliers as seen in the plot of studentized residuals because there are data with a value greater than 3.\nUse the pairs() function to produce a scatterplot matrix:\n> pairs(Auto)\n> dev.off()\nUse the cor() function to compute the matrix of correlations between the variables while excluding the name variable:\n> cor(subset(Auto, select = -name))\nUse the lm() function to perform a multiple linear regression，and use the summary() function to print the results:\n> lm.fit1 <- lm(mpg ~ . -name, data = Auto)\n> summary (lm.fit1)\n\nCall:\nlm(formula = mpg ~ . - name, data = Auto)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5903 -2.1565 -0.1169  1.8690 13.0604 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -17.218435   4.644294  -3.707  0.00024 ***\ncylinders     -0.493376   0.323282  -1.526  0.12780    \ndisplacement   0.019896   0.007515   2.647  0.00844 ** \nhorsepower    -0.016951   0.013787  -1.230  0.21963    \nweight        -0.006474   0.000652  -9.929  < 2e-16 ***\nacceleration   0.080576   0.098845   0.815  0.41548    \nyear           0.750773   0.050973  14.729  < 2e-16 ***\norigin         1.426141   0.278136   5.127 4.67e-07 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 3.328 on 384 degrees of freedom\nMultiple R-squared:  0.8215,    Adjusted R-squared:  0.8182 \nF-statistic: 252.4 on 7 and 384 DF,  p-value: < 2.2e-16\n\nYes, there is a relationship between the predictors and the response by testing the null hypothesis of whether all the regression coefficients are zero. The F-statistic is far from 1 (with a small p-value), indicating evidence against the null hypothesis.\nObverving the p-values associated with each predictor’s t-statistic, we see that displacement, weight, year, and origin have a statistically significant relationship, while cylinders, horsepower and acceleration do not.\nThe regression coefficient for year is 0.75. This suggests that, considering all other predictors fixed, mpg increases by additional 0.75 unit. In other words, cars become more fuel efficient every year by almost 1 mpg/year.\n\nUse the plot() function to produce diagnostic plots:\n> par(mfrow = c(2, 2))\n> plot (lm.fit1)\n> dev.off()\nFrom the leverage plot, we see that point 14 appears to have a high leverage, although not a high magnitude residual. Besides, the quadratic trend of the residuals could be a problem. Maybe linear regression is not the best fit for this prediction.\nWe plot studentized residuals to identify outliers:\n> plot(predict(lm.fit1), rstudent(lm.fit1))\n> dev.off()\nThere are possible outliers as seen in the plot of studentized residuals because there are data with a value greater than 3.\nUse the * and : symbols to fit linear regression models with interaction effects:\n> lm.fit2 <-  lm(mpg ~ cylinders * displacement + displacement * weight, data = Auto)\n> summary(lm.fit2)\nInteraction between displacement and weight is statistically signifcant, while the interaction between cylinders and displacement is not."
  },
  {
    "objectID": "weeks/week03/lecture.html",
    "href": "weeks/week03/lecture.html",
    "title": "👨‍🏫 Week 03 - Slides",
    "section": "",
    "text": "Tip\n\n\n\nFeel free to browse the slides before the lecture, but it is probably safer to wait until the time of the lecture to download/save them, as we might make small changes to slides before then."
  },
  {
    "objectID": "weeks/week03/lecture.html#part-i---classifiers-logistic-regression-45-50-min",
    "href": "weeks/week03/lecture.html#part-i---classifiers-logistic-regression-45-50-min",
    "title": "👨‍🏫 Week 03 - Slides",
    "section": "Part I - Classifiers (Logistic Regression) (45-50 min)",
    "text": "Part I - Classifiers (Logistic Regression) (45-50 min)\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides. For a PDF copy, check Moodle."
  },
  {
    "objectID": "weeks/week03/lecture.html#coffee-break-10-min",
    "href": "weeks/week03/lecture.html#coffee-break-10-min",
    "title": "👨‍🏫 Week 03 - Slides",
    "section": "☕ Coffee Break (10 min)",
    "text": "☕ Coffee Break (10 min)\nUse this time to chat, stretch, drink some coffee or just relax for a bit by yourself."
  },
  {
    "objectID": "weeks/week03/lecture.html#part-ii---classifiers-naive-bayes-45-50-min",
    "href": "weeks/week03/lecture.html#part-ii---classifiers-naive-bayes-45-50-min",
    "title": "👨‍🏫 Week 03 - Slides",
    "section": "Part II - Classifiers (Naive Bayes) (45-50 min)",
    "text": "Part II - Classifiers (Naive Bayes) (45-50 min)\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides. For a PDF copy, check Moodle."
  },
  {
    "objectID": "slides/week01_slides_part1.html#the-data-science-institute",
    "href": "slides/week01_slides_part1.html#the-data-science-institute",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "The Data Science Institute",
    "text": "The Data Science Institute\n\n\n\n\n\nThis course is offered by the LSE Data Science Institute (DSI).\nDSI is the hub for LSE’s interdisciplinary collaboration in data science\n\n\n\n\nSign up for DSI events at lse.ac.uk/DSI/Events"
  },
  {
    "objectID": "slides/week01_slides_part1.html#the-data-science-institute-1",
    "href": "slides/week01_slides_part1.html#the-data-science-institute-1",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "The Data Science Institute",
    "text": "The Data Science Institute\n\n\n\n\nActivities of interest to you:\n\nCIVICA Seminar Series\nCareers in Data Science\nSocial events\nIndustry “field trips”\nSummer projects\n\n\n\n\nSign up for DSI events at lse.ac.uk/DSI/Events"
  },
  {
    "objectID": "slides/week01_slides_part1.html#our-courses",
    "href": "slides/week01_slides_part1.html#our-courses",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Our courses",
    "text": "Our courses\nDSI offer accessible introductions to Data Science:\n\n\nDS101\nFundamentals of  Data Science\n🎯 Focus:  theoretical concepts of data science\n📂 How:  reflections through reading and writing\n\nDS105\nData for  Data Scientists\n🎯 Focus: collection and handling of real data\n📂 How: hands-on coding exercises and a group project\n\nDS202\nData Science for  Social Scientists\n🎯 Focus: fundamental machine learning algorithms\n📂 How: practical use of ML techniques and metrics"
  },
  {
    "objectID": "slides/week01_slides_part1.html#your-lecturer",
    "href": "slides/week01_slides_part1.html#your-lecturer",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Your lecturer",
    "text": "Your lecturer\n\n\n\n\n\n \nDr. Jonathan Cardoso-Silva\n\nPhD in Computer Science\nBackground: Engineering, Bio & Health Informatics\nFormer Lead Data Scientist\nResearch:\n\nNetworks\nOptimisation\nMachine Learning applications\nData Science Workflow"
  },
  {
    "objectID": "slides/week01_slides_part1.html#teaching-assistants",
    "href": "slides/week01_slides_part1.html#teaching-assistants",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Teaching Assistants",
    "text": "Teaching Assistants\n\n\n\nDr. Stuart Bramwell  ESRC Postdoctoral Fellow  Department of Methodology PhD in Politics (Oxford)\n\n\nYijun Wang  Guest Teacher at the DSI PhD cand. in Health Informatics (KCL)  MSc in Data Science (KCL)\n\n\nMustafa Can Ozkan  Guest Teacher at the DSI PhD cand. in the Spacetime Lab (UCL)  MSc in Transport (Imperial/UCL)\n\n\n\n\n\n\n\nXiaowei Gao  Guest Teacher at the DSI PhD cand. in the Spacetime Lab (UCL)  MSc in Data Science (KCL)\n\n\nAnton Boichenko  Guest Teacher at the DSI Product Developer at Decoded  MSc in Applied Social Data Science (LSE)"
  },
  {
    "objectID": "slides/week01_slides_part1.html#who-are-you",
    "href": "slides/week01_slides_part1.html#who-are-you",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Who are you",
    "text": "Who are you\n\n\n\n\n\n\n\n\n\n\nProgramme\nFreq\n\n\n\n\nBSc in Economics\n34\n\n\nBSc in Pyschological and Behavioural Science\n32\n\n\nGeneral Course\n11\n\n\nBSc in Politics and Economics\n4\n\n\nLLB in Laws\n3\n\n\nBSc in International Relations\n2\n\n\nBSc in Philosophy and Economics\n2\n\n\nBSc in Philosophy, Politics and Economics\n2\n\n\nBSc in Economic History and Geography\n1\n\n\nBSc in Economics and Economic History\n1\n\n\nBSc in Geography with Economics\n1\n\n\nBSc in International Relations and History\n1\n\n\nBSc in Mathematics, Statistics and Business\n1\n\n\nBSc in Philosophy, Logic and Scientific Method\n1\n\n\nErasmus Reciprocal Programme of Study\n1\n\n\nExchange Programme for Students from University of California, Berkeley\n1\n\n\n\n\n\n\n\nSource: LSE For You. Last Updated: 30 September 2022"
  },
  {
    "objectID": "slides/week01_slides_part1.html#learning-objectives-cont.",
    "href": "slides/week01_slides_part1.html#learning-objectives-cont.",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Learning Objectives (cont.)",
    "text": "Learning Objectives (cont.)\n\n\nKnow how to evaluate and compare fitted models, and to improve model performance.\nUse applied computer programming, including the hands-on use of programming through course exercises.\nApply the methods learned to real data through hands-on exercises.\nIntegrate the insights from data analytics into knowledge generation and decision-making;"
  },
  {
    "objectID": "slides/week01_slides_part1.html#learning-objectives-cont.-1",
    "href": "slides/week01_slides_part1.html#learning-objectives-cont.-1",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Learning Objectives (cont.)",
    "text": "Learning Objectives (cont.)\n\nUnderstand an introductory framework for working with natural language (text) data using techniques of machine learning.\n\n\n\nLearn how data science methods have been applied to a particular domain of study (applications)."
  },
  {
    "objectID": "slides/week01_slides_part1.html#philosophy-of-this-course-cont.",
    "href": "slides/week01_slides_part1.html#philosophy-of-this-course-cont.",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Philosophy of this course (cont.)",
    "text": "Philosophy of this course (cont.)\n\nThis is an exciting research area, having important applications in science, industry and policy.\nMachine learning is a fundamental ingredient in the training of a modern data scientist.\n\n\nContent borrowed from ME314 Day 1"
  },
  {
    "objectID": "slides/week01_slides_part1.html#the-basics-of-statistics",
    "href": "slides/week01_slides_part1.html#the-basics-of-statistics",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "The basics of statistics",
    "text": "The basics of statistics\nBasic concepts of Statistics you might want to recap:\n\n\nExpected value, mean, median, variance, standard deviation\nProbabilities and simple probability distributions\nTypes of data\n\ndiscrete vs continuous\ncategorical vs numerical vs ordinal"
  },
  {
    "objectID": "slides/week01_slides_part1.html#resources-stats",
    "href": "slides/week01_slides_part1.html#resources-stats",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Resources (Stats)",
    "text": "Resources (Stats)\nA few references that might be useful to read or skim through:\n\n(Warne 2018, chaps. 1-3,5,6,11-12)\n(Gelman, Hill, and Vehtari 2020, chaps. 1–4)\nIf you are a PBS student, you can revisit the content of PB130 (MT3, MT4, MT8-MT11)"
  },
  {
    "objectID": "slides/week01_slides_part1.html#the-basics-of-r-programming",
    "href": "slides/week01_slides_part1.html#the-basics-of-r-programming",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "The basics of R programming",
    "text": "The basics of R programming\nBasic concepts of programming in R to recap:\n\n\ndata structures (vectors, matrices, data frames)\nhow to manipulate data (filter, subset, select)\nread/write data files (for example: CSV, JSON, TXT)\n(optional but encouraged) some knowledge tidyverse can give you a productive boost\n\nthe official website (tidyverse.org) has some good tutorials."
  },
  {
    "objectID": "slides/week01_slides_part1.html#resources-r",
    "href": "slides/week01_slides_part1.html#resources-r",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Resources (R)",
    "text": "Resources (R)\n\nCheck out “R for Data Science” (Wickham and Grolemund 2016, chaps. 1–21). The online version is free.\n“Statistical inference via data science” (Ismay and Kim 2020, chaps. 4–6) is another great free resource"
  },
  {
    "objectID": "slides/week01_slides_part1.html#what-if-i-struggle-with-r",
    "href": "slides/week01_slides_part1.html#what-if-i-struggle-with-r",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "‘What if I struggle with R’?",
    "text": "‘What if I struggle with R’?\n➡️ Our first lab (Week 02) is a recap of some basic R commands, plus some ggplot2.\n\n\nIf you are not confident with your R skills, I strongly encourage you invest in studying the basics in the next couple of weeks.\nContact LSE Digital Skills Lab to attend in-person workshops or self-paced online R courses."
  },
  {
    "objectID": "slides/week01_slides_part1.html#any-questions",
    "href": "slides/week01_slides_part1.html#any-questions",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Any questions?",
    "text": "Any questions?\n\n\n\n\n\n\nImage created with the DALL·E algorithm using the prompt: ‘35mm macro photography of a robot holding a question mark card, white background’"
  },
  {
    "objectID": "slides/week01_slides_part1.html#syllabus",
    "href": "slides/week01_slides_part1.html#syllabus",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Syllabus",
    "text": "Syllabus\n\n\n\n\n\n\n\nIntro\n\n\n\n\n\n    Introduction, Context & Key Concepts\nWeek 01\n\n\nSupervised Learning\n\n\n\n    Simple and Multiple Linear Regression      Classifiers (Logistic Regression & Naive Bayes)      Resampling methods       Non-linear algorithms (SVM & tree-based models)\nWeek 02  Week 03  Week 04  Week 05\n\n\nUnsupervised Learning\n\n\n\n    Unsupervised Learning: Clustering     Unsupervised Learning: PCA         \nWeek 07  Week 08\n\n\nApplications\n\n\n\n    Applications: Predictive Modelling on Tabular Data    Applications: Text as Data & Topic Modelling     Applications: Social Media Data\nWeek 09  Week 10  Week 11"
  },
  {
    "objectID": "slides/week01_slides_part1.html#structure-of-lectures",
    "href": "slides/week01_slides_part1.html#structure-of-lectures",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Structure of lectures 👨🏻‍🏫",
    "text": "Structure of lectures 👨🏻‍🏫\nOur lectures will be split in two parts:\n\n\nPart I (~ 50 min): Traditional exposition of theoretical content\nbreak (~ 10 min): Grab coffee ☕ or relax 🧘\nPart II (~ 50 min): Live demo\n\nTypically, an exploratory analysis or application of an algorithm\nFeel free to follow along in your own laptops."
  },
  {
    "objectID": "slides/week01_slides_part1.html#structure-of-classes",
    "href": "slides/week01_slides_part1.html#structure-of-classes",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Structure of classes 👩‍💻",
    "text": "Structure of classes 👩‍💻\n\n\nStudents will work on weekly, structured problem sets in the staff-led class sessions.\nTips to get the most of classes:\n\nBring your own laptops 💻 (most tablets are not suitable for programming)\nRead the recommended reading prior to the class\nSkim through the problem set before class"
  },
  {
    "objectID": "slides/week01_slides_part1.html#class-groups",
    "href": "slides/week01_slides_part1.html#class-groups",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Class groups",
    "text": "Class groups\n\n\nGroup 01\n\n📆 Mondays\n⌚ 09:00 — 10:30\n📍 PAN.1.03\n\n\nGroup 02\n\n📆 Mondays\n⌚ 10:30 — 12:00\n📍 PAN.1.03\n\n\nGroup 03\n\n📆 Mondays\n⌚ 13:00 — 14:30\n📍 MAR.1.09\n\n\nGroup 04\n\n📆 Fridays\n⌚ 16:00 — 17:30\n📍 NAB.1.04\n\n\nGroup 05\n\n📆 Mondays\n⌚ 09:00 — 10:30\n📍 32L.LG.11\n\n\nGroup 06\n\n📆 Mondays\n⌚ 10:30 — 12:00\n📍 32L.LG.11\n\n\nGroup 07\n\n📆 Fridays\n⌚ 09:30 — 11:00\n📍 CBG.2.06\n\n\n\n\n🗺️ Check LSE campus map"
  },
  {
    "objectID": "slides/week01_slides_part1.html#your-background-knowledge",
    "href": "slides/week01_slides_part1.html#your-background-knowledge",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Your background knowledge",
    "text": "Your background knowledge\n\nPlease, help our teaching team understand your needs as we prepare for the first labs next week.\nFind the link to the survey on our Slack group or point your phone to the QR code below"
  },
  {
    "objectID": "slides/week01_slides_part1.html#assessments",
    "href": "slides/week01_slides_part1.html#assessments",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Assessments 📔",
    "text": "Assessments 📔\nThe breakdown of assessment for this class will be as follows:"
  },
  {
    "objectID": "slides/week01_slides_part1.html#assessments-1",
    "href": "slides/week01_slides_part1.html#assessments-1",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Assessments 📔",
    "text": "Assessments 📔\n\nProblem sets (60%)\n\n\nSummative problem sets released on Weeks 5, 8 & 11.\nThese will have a similar style to the formative problem sets, a mix of R tasks and your written interpretation of the analyses.\nYou will have 4-6 days to submit your solutions.\nEach of the three summative problem sets is worth 20% of the final mark, and will be graded on a 100 point scale."
  },
  {
    "objectID": "slides/week01_slides_part1.html#assessments-2",
    "href": "slides/week01_slides_part1.html#assessments-2",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Assessments 📔",
    "text": "Assessments 📔\n\nTake-home exam (40%)\n\n\nAn open-book take-home exam, taken during the January exams period.\nExam questions will be comparable in style to the problem sets.\nThe exam questions will be released on Moodle on 5 January 2023. (tentative)\nThe exam is due on 11 January at 4pm (tentative)\n⚠️ Update 11/10/2022: Last year, DS202 exam was performed entirely online due to COVID-19 mitigation procedures. We want to run it online via our own Moodle page again this academic term, we just need to understand LSE regulations about exams for this year. We will update you on this very soon (hopefully by the end of W04)."
  },
  {
    "objectID": "slides/week01_slides_part1.html#office-hours",
    "href": "slides/week01_slides_part1.html#office-hours",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Office hours",
    "text": "Office hours\n\n\nIt is probably a good idea to book office hours if:\n\nyou struggled with a technical or theoretical aspect of a problem set in the previous week,\nyou have queries about careers in data science,\nyou want guidance in how to apply data science to other things you are studying outside this course.\n\nCome prepared. You only have 15 minutes.\nAsk for help sooner rather than later.\nBook slots via StudentHub up to 12 hours in advance."
  },
  {
    "objectID": "slides/week01_slides_part1.html#communication",
    "href": "slides/week01_slides_part1.html#communication",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Communication",
    "text": "Communication\n\n\nJoin our Slack group (more info here).\nUse the public Slack channels to talk to share links, content (or memes) with your colleagues.\nOur teaching team will dedicate some time during the week to answer questions or other interactions on Slack.\nReserve 📧 e-mail for formal requests: extensions, deferrals, etc.\n\nNo need to e-mail to inform you will skip a class, for example."
  },
  {
    "objectID": "slides/week01_slides_part1.html#any-questions-1",
    "href": "slides/week01_slides_part1.html#any-questions-1",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Any questions?",
    "text": "Any questions?\n\n\n\n\n\n\nImage created with the DALL·E algorithm using the prompt: ‘35mm macro photography of a robot holding a question mark card, white background’"
  },
  {
    "objectID": "slides/week01_slides_part1.html#we-changed-how-we-consume-music",
    "href": "slides/week01_slides_part1.html#we-changed-how-we-consume-music",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "We changed how we consume music 🎧",
    "text": "We changed how we consume music 🎧\n\n\n\n\n\n\nTo interact with this plot, check reference (Fischer-Baum 2017) at the end of this presentation."
  },
  {
    "objectID": "slides/week01_slides_part1.html#we-changed-how-we-consume-video",
    "href": "slides/week01_slides_part1.html#we-changed-how-we-consume-video",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "We changed how we consume video 🎞️",
    "text": "We changed how we consume video 🎞️\n\n\n\n\n\n\nTo interact with this plot, check reference (Fischer-Baum 2017) at the end of this presentation."
  },
  {
    "objectID": "slides/week01_slides_part1.html#smartphones-are-a-very-recent-thing",
    "href": "slides/week01_slides_part1.html#smartphones-are-a-very-recent-thing",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "Smartphones 📱 are a very recent thing",
    "text": "Smartphones 📱 are a very recent thing\n\n\n\n\n\n\nTo interact with this plot, check reference (Fischer-Baum 2017) at the end of this presentation."
  },
  {
    "objectID": "slides/week01_slides_part1.html#we-spend-a-lot-more-time-connected",
    "href": "slides/week01_slides_part1.html#we-spend-a-lot-more-time-connected",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "We spend a lot more time connected",
    "text": "We spend a lot more time connected"
  },
  {
    "objectID": "slides/week01_slides_part1.html#references",
    "href": "slides/week01_slides_part1.html#references",
    "title": "🗓️ Week 01 Structure of this course",
    "section": "References",
    "text": "References\n\n\nFischer-Baum, Reuben. 2017. “What ‘Tech World’ Did You Grow up In?” Washington Post. https://www.washingtonpost.com/graphics/2017/entertainment/tech-generations/.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. 1st ed. Cambridge University Press. https://doi.org/10.1017/9781139161879.\n\n\nIsmay, Chester, and Albert Young-Sun Kim. 2020. Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. Chapman & Hall/CRC the R Series. Boca Raton: CRC Press / Taylor & Francis Group. https://moderndive.com/.\n\n\nKolawole, Emi. 2013. “About Those 2005 and 2013 Photos of the Crowds in St. Peter’s Square.” Washington Post. http://wapo.st/WKKTMh.\n\n\nWarne, Russell T. 2018. Statistics for the Social Sciences: A General Linear Model Approach. https://www.cambridge.org/highereducation/books/statistics-for-the-social-sciences/716FF25785A6154CC6822D067A959445.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. First edition. Sebastopol, CA: O’Reilly. https://r4ds.had.co.nz/.\n\n\n\n\n\nDS202 - Data Science for Social Scientists 🤖 🤹"
  },
  {
    "objectID": "slides/week01_slides_part2.html#data-science-is",
    "href": "slides/week01_slides_part2.html#data-science-is",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Data science is…",
    "text": "Data science is…\n\n\n“[…] a field of study and practice that involves the collection, storage, and processing of data in order to derive important 💡 insights into a problem or a phenomenon.\n\n\n\n\nSuch data may be generated by humans (surveys, logs, etc.) or machines (weather data, road vision, etc.),\n\n\n\n\nand could be in different formats (text, audio, video, augmented or virtual reality, etc.).”\n\n\n\n\n(Shah 2020, chap. 1) - Emphasis and emojis are of my own making."
  },
  {
    "objectID": "slides/week01_slides_part2.html#the-mythical-unicorn",
    "href": "slides/week01_slides_part2.html#the-mythical-unicorn",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "The mythical unicorn 🦄",
    "text": "The mythical unicorn 🦄\n\n\nknows everything about statistics\n\n\nable to communicate insights perfectly\n\n\nfully understands businesses like no one\n\n\nis a fluent computer programmer\n\n\n\nOf course, such a person does not exist!\n\n\nSee (Davenport 2020) for a more in-depth discussion about this"
  },
  {
    "objectID": "slides/week01_slides_part2.html#in-reality",
    "href": "slides/week01_slides_part2.html#in-reality",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "In reality…",
    "text": "In reality…\n\n\nWe are all jugglers 🤹\n\n\nEveryone brings a different skill set.\nWe need multi-disciplinary teams.\nGood data scientists know a bit of everything.\n\nNot fluent in all things\nUnderstands their strenghts and weaknessess\nThey know when and where to interface with others\n\n\n\n\n\n\n\n\n\nSee (Schutt and O’Neil 2013, chap. 1) for more on this."
  },
  {
    "objectID": "slides/week01_slides_part2.html#the-data-science-workflow-1",
    "href": "slides/week01_slides_part2.html#the-data-science-workflow-1",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "The Data Science Workflow",
    "text": "The Data Science Workflow\n\n\n\n\n\n   \n\nstart\n\n Start   \n\ngather\n\nGather data     \n\nstart->gather\n\n    \n\nstore\n\nStore it          somewhere   \n\ngather->store\n\n           \n\nclean\n\nClean &         pre-process   \n\nstore->clean\n\n           \n\nbuild\n\nBuild a  dataset   \n\nclean->build\n\n           \n\neda\n\nExploratory     data analysis   \n\nbuild->eda\n\n    \n\nml\n\nMachine learning   \n\neda->ml\n\n           \n\ninsight\n\nObtain    insights   \n\nml->insight\n\n           \n\ncommunicate\n\nCommunicate results            \n\ninsight->communicate\n\n           \n\nend\n\n End   \n\ncommunicate->end\n\n   \n\n\n\n\n\n\n\n⚠️ Note that this is a simplified version of what happens in a data science project.  In practice, the process is not linear and there are many feedback loops."
  },
  {
    "objectID": "slides/week01_slides_part2.html#the-data-science-workflow-2",
    "href": "slides/week01_slides_part2.html#the-data-science-workflow-2",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "The Data Science Workflow",
    "text": "The Data Science Workflow\n\n\n\n\n\n   \n\nstart\n\n Start   \n\ngather\n\n Gather data     \n\nstart->gather\n\n    \n\nend\n\n End   \n\nstore\n\n Store it          somewhere   \n\ngather->store\n\n           \n\nclean\n\n Clean &         pre-process   \n\nstore->clean\n\n           \n\nbuild\n\n Build a  dataset   \n\nclean->build\n\n           \n\neda\n\n Exploratory     data analysis   \n\nbuild->eda\n\n    \n\nml\n\n Machine learning   \n\neda->ml\n\n           \n\ninsight\n\n Obtain    insights   \n\nml->insight\n\n           \n\ncommunicate\n\n Communicate results            \n\ninsight->communicate\n\n           \n\ncommunicate->end\n\n   \n\n\n\n\n\nIt is often said that 80% of the time and effort spent on a data science project goes to the tasks highlighted above."
  },
  {
    "objectID": "slides/week01_slides_part2.html#the-data-science-workflow-3",
    "href": "slides/week01_slides_part2.html#the-data-science-workflow-3",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "The Data Science Workflow",
    "text": "The Data Science Workflow\n\n\n\n\n\n   \n\nstart\n\n Start   \n\ngather\n\n Gather data     \n\nstart->gather\n\n    \n\nend\n\n End   \n\nstore\n\n Store it          somewhere   \n\ngather->store\n\n           \n\nclean\n\n Clean &         pre-process   \n\nstore->clean\n\n           \n\nbuild\n\n Build a  dataset   \n\nclean->build\n\n           \n\neda\n\n Exploratory     data analysis   \n\nbuild->eda\n\n    \n\nml\n\n Machine learning   \n\neda->ml\n\n           \n\ninsight\n\n Obtain    insights   \n\nml->insight\n\n           \n\ncommunicate\n\n Communicate results            \n\ninsight->communicate\n\n           \n\ncommunicate->end\n\n   \n\n\n\n\n\nThis course is about Machine Learning. So, in most examples and tutorials, we will assume that we already have good quality data."
  },
  {
    "objectID": "slides/week01_slides_part2.html#data-science-and-social-science",
    "href": "slides/week01_slides_part2.html#data-science-and-social-science",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Data Science and Social Science",
    "text": "Data Science and Social Science\n\nIn reality, data scientists work as a multidisciplinary group, collaborating towards a common goal.\nContent borrowed from ME314 Day 1\n\n\n\nSocial science: The goal is typically explanation\nData science: The goal is frequently prediction, or data exploration\nMany of the same methods are used for both objectives\n\n\n\n\nCheck (Shmueli 2010) for a discussion about this topic."
  },
  {
    "objectID": "slides/week01_slides_part2.html#what-does-it-mean-to-learn-something",
    "href": "slides/week01_slides_part2.html#what-does-it-mean-to-learn-something",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "What does it mean to learn something?",
    "text": "What does it mean to learn something?\n\n\n\n\n\n\nImage created with the DALL·E algorithm using the prompt: ‘35mm macro photography of a robot holding a question mark card, white background’"
  },
  {
    "objectID": "slides/week01_slides_part2.html#predicting-a-sequence-intuitively",
    "href": "slides/week01_slides_part2.html#predicting-a-sequence-intuitively",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Predicting a sequence intuitively",
    "text": "Predicting a sequence intuitively\n\n\nSay our data is the following simple sequence:  \\(6, 9, 12, 15, 18, 21, 24, ...\\) \nWhat number do you expect to come next? Why?\nIt is very likely that you guessed that \\(\\operatorname{next number}=27\\)\nWe spot that the sequence follows a pattern\nFrom this, we notice — we learn — that the sequence is governed by: \\(\\operatorname{next number} = \\operatorname{previous number} + 3\\)"
  },
  {
    "objectID": "slides/week01_slides_part2.html#predicting-a-sequence-formula",
    "href": "slides/week01_slides_part2.html#predicting-a-sequence-formula",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Predicting a sequence (formula)",
    "text": "Predicting a sequence (formula)\nThe next number is a function of the previous one:\n \\[\n\\operatorname{next number} = f(\\operatorname{previous number})\n\\]"
  },
  {
    "objectID": "slides/week01_slides_part2.html#predicting-a-sequence-generic-formula",
    "href": "slides/week01_slides_part2.html#predicting-a-sequence-generic-formula",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Predicting a sequence (generic formula)",
    "text": "Predicting a sequence (generic formula)\nIn general terms, we can represented it as:\n \\[\n\\operatorname{Y} = f(\\operatorname{X})\n\\] \n\nwhere:\n\n\\(Y\\): a quantitative response.  It goes by many names: dependent variable, response, target, outcome\n\\(X\\): a set of predictors,  also called inputs, regressors, covariates, features, independent variables.\n\\(f\\): the systematic information that \\(X\\) provides about \\(Y\\)"
  },
  {
    "objectID": "slides/week01_slides_part2.html#predicting-a-sequence-generic-formula-1",
    "href": "slides/week01_slides_part2.html#predicting-a-sequence-generic-formula-1",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Predicting a sequence (generic formula)",
    "text": "Predicting a sequence (generic formula)\nIn general terms, we can represented it as:\n\n\\[\n\\operatorname{Y} = f(\\operatorname{X}) + \\epsilon\n\\]\n\nwhere:\n\n\\(Y\\): the output\n\\(X\\): a set of inputs\n\\(f\\): the systematic information that \\(X\\) provides about \\(Y\\)\n\\(\\epsilon~~\\): a random error term\n\n\nIn reality, there is some error \\(\\epsilon\\) that cannot be reduced."
  },
  {
    "objectID": "slides/week01_slides_part2.html#approximating-f",
    "href": "slides/week01_slides_part2.html#approximating-f",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Approximating \\(f\\)",
    "text": "Approximating \\(f\\)\n\n\n\\(f\\) is almost always unknown\nWe aim to find an approximation (a model). Let’s call it \\(\\hat{f}\\)\nthat can then use it to predict values of \\(Y\\) for whatever \\(X\\).\nThat is: \\(\\hat{Y} = \\hat{f}(X)\\)"
  },
  {
    "objectID": "slides/week01_slides_part2.html#what-is-machine-learning",
    "href": "slides/week01_slides_part2.html#what-is-machine-learning",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\n\n\nStatistical learning, or Machine learning, refers to a set of approaches for estimating \\(f\\).\nEach algorithm you will learn on this course has its own way to determine \\(\\hat{f}\\) given data"
  },
  {
    "objectID": "slides/week01_slides_part2.html#types-of-learning",
    "href": "slides/week01_slides_part2.html#types-of-learning",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Types of learning",
    "text": "Types of learning\n\nIn general terms, there are two main ways to learn from data:\n\n\nSupervised Learning\n\nEach observation (\\(x_i\\)) has an outcome associated with it (\\(y_i\\)).\nYour goal is to find a \\(\\hat{f}\\) that produces \\(\\hat{Y}\\) value close to the true \\(Y\\) values.\nOur focus on 🗓️ Weeks 2, 3, 4 & 5.\n\n\nUnsupervised Learning\n\nYou have observations (\\(x_i\\)) but there is no response variable.\nYour goal is to find a \\(\\hat{f}\\), focused only on \\(X\\) that best represents the patterns in the data.\nOur focus on 🗓️ Weeks 7 & 8."
  },
  {
    "objectID": "slides/week01_slides_part2.html#data-structure",
    "href": "slides/week01_slides_part2.html#data-structure",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Data Structure",
    "text": "Data Structure\nLet’s go back to our example:\n\n\n\nOur simple sequence:\n \\(6, 9, 12, 15, 18, 21, 24\\) \n\n\n\nBecomes:\n\n\n\n\\(X\\)\n\\(Y\\)\n\n\n\n\n6\n9\n\n\n9\n12\n\n\n12\n15\n\n\n15\n18\n\n\n18\n21\n\n\n21\n24\n\n\n\n\n\n\nAnd for prediction:\n\n\n\n\n\n\n\n\\(X\\)\n\\(\\hat{Y}\\)\n\n\n\n\n24\n?\n\n\n\nwe present the \\(X\\) values and ask the fitted model to give us \\(\\hat{Y}\\).\n\n\n\n\nSame data but now in tabular format\na few other terms: - training data/test data - fitted model"
  },
  {
    "objectID": "slides/week01_slides_part2.html#the-ground-truth",
    "href": "slides/week01_slides_part2.html#the-ground-truth",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "The ground truth",
    "text": "The ground truth\nLet’s create a dataframe to illustrate the process of training an algorithm:\n\nlibrary(tidyverse)\n\ndf = tibble(X=as.integer(seq(6, 21, 3)),\n            Y=as.integer(seq(6+3, 21+3, 3)))\nprint(df)\n\n\n\n# A tibble: 6 × 2\n      X     Y\n  <int> <int>\n1     6     9\n2     9    12\n3    12    15\n4    15    18\n5    18    21\n6    21    24"
  },
  {
    "objectID": "slides/week01_slides_part2.html#adding-noise",
    "href": "slides/week01_slides_part2.html#adding-noise",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Adding noise",
    "text": "Adding noise\nLet’s simulate the introduction of some random error:\n\n# Let's simulate some noise\ngaussian_noise = rnorm(n=nrow(df), mean=0, sd=1.5)\n\n# Call it \"observed Y\"\ndf$obsY = df$Y + gaussian_noise\nprint(df)\n\n\n\n# A tibble: 6 × 3\n      X     Y  obsY\n  <int> <int> <dbl>\n1     6     9  7.05\n2     9    12 12.2 \n3    12    15 15.9 \n4    15    18 18.6 \n5    18    21 21.1 \n6    21    24 25.6"
  },
  {
    "objectID": "slides/week01_slides_part2.html#visualizing-the-data",
    "href": "slides/week01_slides_part2.html#visualizing-the-data",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Visualizing the data",
    "text": "Visualizing the data"
  },
  {
    "objectID": "slides/week01_slides_part2.html#visualizing-the-data-w-noise",
    "href": "slides/week01_slides_part2.html#visualizing-the-data-w-noise",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Visualizing the data (w/ noise)",
    "text": "Visualizing the data (w/ noise)"
  },
  {
    "objectID": "slides/week01_slides_part2.html#visualizing-the-data-w-noise-1",
    "href": "slides/week01_slides_part2.html#visualizing-the-data-w-noise-1",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Visualizing the data (w/ noise)",
    "text": "Visualizing the data (w/ noise)\n\n\n\n\n\n\n\nWhich line is closer to the “truth”?"
  },
  {
    "objectID": "slides/week01_slides_part2.html#visualizing-the-data-w-noise-2",
    "href": "slides/week01_slides_part2.html#visualizing-the-data-w-noise-2",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Visualizing the data (w/ noise)",
    "text": "Visualizing the data (w/ noise)\n\n\n\n\n\n\n\nHow much error can we accept?"
  },
  {
    "objectID": "slides/week01_slides_part2.html#assessing-error",
    "href": "slides/week01_slides_part2.html#assessing-error",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Assessing error",
    "text": "Assessing error\nHow much error was introduced by \\(\\epsilon\\) per sample?\n\ndf$error    <- df$Y - df$obsY  # Calculate the error\ndf$absError <- abs(df$error)   # Ignore the sign of error\ndf\n\n\n\n# A tibble: 6 × 5\n      X     Y  obsY  error absError\n  <int> <int> <dbl>  <dbl>    <dbl>\n1     6     9  7.05  1.95     1.95 \n2     9    12 12.2  -0.203    0.203\n3    12    15 15.9  -0.904    0.904\n4    15    18 18.6  -0.552    0.552\n5    18    21 21.1  -0.130    0.130\n6    21    24 25.6  -1.57     1.57 \n\n\n\nOn average, what is the error?\n\nmean(df$absError)\n\n[1] 0.8850281\n\n\n\n\nThis measure is called the Mean Absolute Error."
  },
  {
    "objectID": "slides/week01_slides_part2.html#measures-of-error",
    "href": "slides/week01_slides_part2.html#measures-of-error",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "Measures of error",
    "text": "Measures of error\nThis is what we computed:\n\\[\n\\operatorname{MAE} = \\frac{\\sum_{i=1}^n{|(y_i + \\epsilon) - y_i|}}{n}\n\\]\n\n\nWe were able to compute this error because we knew what the ground truth \\(Y\\), we knew what its real value was.\nIt was only possible because it was a simulation, not real data.\nIn practice, we will almost never be able to assess the impact of \\(\\epsilon\\).\nWe will use this same way of thinking to assess how good and accurate our models are. 🔜"
  },
  {
    "objectID": "slides/week01_slides_part2.html#references",
    "href": "slides/week01_slides_part2.html#references",
    "title": "🗓️ Week 01Overview of core concepts",
    "section": "References",
    "text": "References\n\n\nDavenport, Thomas. 2020. “Beyond Unicorns: Educating, Classifying, and Certifying Business Data Scientists.” Harvard Data Science Review 2 (2). https://doi.org/10.1162/99608f92.55546b4a.\n\n\nSchutt, Rachel, and Cathy O’Neil. 2013. Doing Data Science. First edition. Beijing ; Sebastopol: O’Reilly Media. https://ebookcentral.proquest.com/lib/londonschoolecons/detail.action?docID=1465965.\n\n\nShah, Chirag. 2020. A Hands-on Introduction to Data Science. Cambridge, United Kingdom ; New York, NY, USA: Cambridge University Press. https://librarysearch.lse.ac.uk/permalink/f/1n2k4al/TN_cdi_askewsholts_vlebooks_9781108673907.\n\n\nShmueli, Galit. 2010. “To Explain or to Predict?” Statistical Science 25 (3). https://doi.org/10.1214/10-STS330.\n\n\n\n\n\nDS202 - Data Science for Social Scientists 🤖 🤹"
  },
  {
    "objectID": "slides/week02_slides_part1.html#the-basic-models",
    "href": "slides/week02_slides_part1.html#the-basic-models",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "The basic models",
    "text": "The basic models\n\nLinear regression is a simple approach to supervised learning.\n\n\n\n\nThe generic supervised model:\n\\[\nY = \\operatorname{f}(X) + \\epsilon\n\\]\nis defined more explicitly as follows ➡️\n\n\n\n\nSimple linear regression\n\n\\[\n\\begin{align}\nY = \\beta_0 +& \\beta_1 X + \\epsilon, \\\\\n\\\\\n\\\\\n\\end{align}\n\\] \nwhen we use a single predictor, \\(X\\).\n\n\n\n\nMultiple linear regression\n\n\\[\n\\begin{align}\nY = \\beta_0 &+ \\beta_1 X_1 + \\beta_2 X_2 \\\\\n   &+ \\dots \\\\\n   &+ \\beta_p X_p + \\epsilon\n\\end{align}\n\\]\n\nwhen there are multiple predictors, \\(X_p\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nTrue regression functions are never linear!\nAlthough it may seem overly simplistic, linear regression is extremely useful both conceptually and practically.\n\n\n\n\n\n\n\n\n\nSee 📺 Regression: Crash Course Statistics on YouTube for inspiration on how to present linear regression to students.  \nWe will talk about both types of models, how we can estimate the values of all \\(\\beta\\) and assess how good our models are."
  },
  {
    "objectID": "slides/week02_slides_part1.html#linear-regression-with-a-single-predictor",
    "href": "slides/week02_slides_part1.html#linear-regression-with-a-single-predictor",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "Linear Regression with a single predictor",
    "text": "Linear Regression with a single predictor\n\n\nWe assume a model:\n\n\\[\nY = \\beta_0 + \\beta_1 X + \\epsilon ,\n\\]\n\n\n\n\n\n\n\n\n\n\nwhere:\n\n\\(\\beta_0\\): an unknown constant that represents the intercept of the line.\n\\(\\beta_1\\): an unknown constant that represents the slope of the line\n\\(\\epsilon\\): the random error term (irreducible)\n\n\n\n\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are also known as coefficients or parameters of the model."
  },
  {
    "objectID": "slides/week02_slides_part1.html#linear-regression-with-a-single-predictor-1",
    "href": "slides/week02_slides_part1.html#linear-regression-with-a-single-predictor-1",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "Linear Regression with a single predictor",
    "text": "Linear Regression with a single predictor\n\n\nWe want to estimate:\n\\[\n\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} x\n\\]\n\n\n\n\n\n\n\n\n\nwhere:\n\n\\(\\hat{y}\\): is a prediction of \\(Y\\) on the basis of \\(X = x\\).\n\\(\\hat{\\beta_0}\\): is an estimate of the “true” \\(\\beta_0\\).\n\\(\\hat{\\beta_1}\\): is an estimate of the “true” \\(\\beta_1\\).\n\n\n\n\n\nThe hat symbol denotes an estimated value."
  },
  {
    "objectID": "slides/week02_slides_part1.html#different-estimators-different-equations",
    "href": "slides/week02_slides_part1.html#different-estimators-different-equations",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "Different estimators, different equations",
    "text": "Different estimators, different equations\n\n\n\nThere are multiple ways to estimate the coefficients.\n\nIf you use different techniques, you might get different equations\nThe most common algorithm is called  Ordinary Least Squares (OLS)\nJust to name a few other estimators (Karafiath 2009):\n\nLeast Absolute Deviation (LAD)\nWeighted Least Squares (WLS)\nGeneralized Least Squares (GLS)\nHeteroskedastic-Consistent (HC) variants\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will only cover OLS in this course."
  },
  {
    "objectID": "slides/week02_slides_part1.html#the-concept-of-residuals",
    "href": "slides/week02_slides_part1.html#the-concept-of-residuals",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "The concept of residuals",
    "text": "The concept of residuals\nSuppose you came across some data:\n\n\n\n\n\n\n\nFirst, let’s think of the concept of residuals…\n\n\nAnd you suspect there is a linear relationship between X and Y."
  },
  {
    "objectID": "slides/week02_slides_part1.html#the-concept-of-residuals-1",
    "href": "slides/week02_slides_part1.html#the-concept-of-residuals-1",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "The concept of residuals",
    "text": "The concept of residuals\nSo, you decide to fit a line to it.\n\n\n\n\n\n\n\nA line that goes right through the middle of the cloud of data."
  },
  {
    "objectID": "slides/week02_slides_part1.html#the-concept-of-residuals-2",
    "href": "slides/week02_slides_part1.html#the-concept-of-residuals-2",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "The concept of residuals",
    "text": "The concept of residuals\n\nResiduals are the distances from each data point to this line. \n\n\n\n\n\n\n\n\\(e_i\\)\\(=y_i-\\hat{y}_i\\) represents the \\(i\\)th residual"
  },
  {
    "objectID": "slides/week02_slides_part1.html#residual-sum-of-squares-rss",
    "href": "slides/week02_slides_part1.html#residual-sum-of-squares-rss",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "Residual Sum of Squares (RSS)",
    "text": "Residual Sum of Squares (RSS)\nFrom this, we can define the  Residual Sum of Squares  (RSS) as\n\\[\n\\mathrm{RSS}= e_1^2 + e_2^2 + \\dots + e_n^2,\n\\]\n\nor equivalently as\n\\[\n\\mathrm{RSS}= (y_1 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_1)^2 + (y_2 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_2)^2 + \\dots + (y_n - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_n)^2.\n\\]\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe (ordinary) least squares approach chooses \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) to minimize the RSS.\n\n\n\n\n\nThat is how it does its job."
  },
  {
    "objectID": "slides/week02_slides_part1.html#a-question-for-you",
    "href": "slides/week02_slides_part1.html#a-question-for-you",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "A question for you",
    "text": "A question for you\n\n\n\n\nWhy the squares and not, say, just the sum of residuals?\n\nImage created with the DALL·E algorithm using the prompt: ‘35mm macro photography of a robot holding a question mark card, white background’\n\n\n\n\nExplain that the sum penalizes individual large errors a lot more Consider adding a visualisation to illustrate this point."
  },
  {
    "objectID": "slides/week02_slides_part1.html#the-objective-function",
    "href": "slides/week02_slides_part1.html#the-objective-function",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "The objective function",
    "text": "The objective function\nWe treat this as an optimisation problem. We want to minimize RSS: \\[\n\\begin{align}\n\\min \\mathrm{RSS} =& \\sum_i^n{e_i^2} \\\\\n             =& \\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2} \\\\\n             =& \\sum_i^n{\\left(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i\\right)^2}\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/week02_slides_part1.html#estimating-hatbeta_0",
    "href": "slides/week02_slides_part1.html#estimating-hatbeta_0",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "Estimating \\(\\hat{\\beta}_0\\)",
    "text": "Estimating \\(\\hat{\\beta}_0\\)\nTo find \\(\\hat{\\beta}_0\\), we have to solve the following partial derivative:\n\\[\n\\frac{\\partial ~\\mathrm{RSS}}{\\partial \\hat{\\beta}_0}{\\sum_i^n{(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2}} = 0\n\\]\n\n… which will lead you to:\n\n\n\\[\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x},\n\\]\n\n\nwhere we made use of the sample means:\n\n\\(\\bar{y} \\equiv \\frac{1}{n} \\sum_{i=1}^n y_i\\)\n\\(\\bar{x} \\equiv \\frac{1}{n} \\sum_{i=1}^n x_i\\)\n\n\n\n\nFull derivation if needed:\n\\[\n\\begin{align}\n0 &= \\frac{\\partial ~\\mathrm{RSS}}{\\partial \\hat{\\beta}_0}{\\sum_i^n{(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2}} & (\\text{chain rule})\\\\\n0 &= \\sum_i^n{-2 (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)} & (\\text{take $-2$ out})\\\\\n0 &= -2 \\sum_i^n{ (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)}  & (\\div -2) \\\\\n0 &=\\sum_i^n{ (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)}  & (\\text{sep. sums}) \\\\\n0 &=\\sum_i^n{y_i} - \\sum_i^n{\\hat{\\beta}_0} - \\sum_i^n{\\hat{\\beta}_1 x_i}  & (\\text{simplify}) \\\\\n0 &=\\sum_i^n{y_i} - n\\hat{\\beta}_0 - \\hat{\\beta}_1\\sum_i^n{ x_i}  & (+ n\\hat{\\beta}_0) \\\\\nn\\hat{\\beta}_0 &= \\sum_i^n{y_i} - \\hat{\\beta}_1\\sum_i^n{ x_i} & (\\text{isolate }\\hat{\\beta}_0 ) \\\\\n\\hat{\\beta}_0 &= \\frac{\\sum_i^n{y_i} - \\hat{\\beta}_1\\sum_i^n{ x_i}}{n} & (\\text{rearranging}) \\\\\n\\hat{\\beta}_0 &= \\frac{\\sum_i^n{y_i}}{n} - \\hat{\\beta}_1\\frac{\\sum_i^n{x_i}}{n} & (\\text{or simply}) \\\\\n\\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x} & \\blacksquare\n\\end{align}\n\\]\n\n\n\n📝 Give it a go! Pretend \\(\\hat{\\beta}_1\\) is constant and use the power rule to solve the equation and reach the same result."
  },
  {
    "objectID": "slides/week02_slides_part1.html#estimating-hatbeta_1",
    "href": "slides/week02_slides_part1.html#estimating-hatbeta_1",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "Estimating \\(\\hat{\\beta}_1\\)",
    "text": "Estimating \\(\\hat{\\beta}_1\\)\nSimilarly, to find \\(\\hat{\\beta}_1\\) we solve:\n\\[\n\\frac{\\partial ~\\mathrm{RSS}}{\\partial \\hat{\\beta}_1}{[\\sum_i^n{y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i}]} = 0\n\\]\n\n… which will lead you to:\n\n\n\\[\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})^2}\n\\]\n\n\n\nFull derivation if needed:\n\\[\n\\begin{align}\n0 &= \\frac{\\partial ~\\mathrm{RSS}}{\\partial \\hat{\\beta}_1}{\\sum_i^n{(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2}} & (\\text{chain rule})\\\\\n0 &= \\sum_i^n{\\left(-2x_i~ (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\\right)} & (\\text{take $-2$ out})\\\\\n0 &= -2\\sum_i^n{\\left( x_i~ (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\\right)} & (\\div -2) \\\\\n0 &= \\sum_i^n{\\left(x_i~ (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\\right)} & (\\text{distribute } x_i) \\\\\n0 &= \\sum_i^n{\\left(y_ix_i - \\hat{\\beta}_0x_i - \\hat{\\beta}_1 x_i^2\\right)} & (\\text{replace } \\hat{\\beta}_0) \\\\\n0 &= \\sum_i^n{\\left(y_ix_i - (\\bar{y} - \\hat{\\beta}_1 \\bar{x})x_i - \\hat{\\beta}_1 x_i^2\\right)} & (\\text{rearrange}) \\\\\n0 &= \\sum_i^n{\\left(y_ix_i - \\bar{y}x_i + \\hat{\\beta}_1 \\bar{x}x_i - \\hat{\\beta}_1 x_i^2\\right)} & (\\text{separate sums}) \\\\\n0 &= \\sum_i^n{\\left(y_ix_i - \\bar{y}x_i\\right)} + \\sum_i^n{\\left(\\hat{\\beta}_1 \\bar{x}x_i - \\hat{\\beta}_1 x_i^2\\right)} & (\\text{take $\\hat{\\beta}_1$ out}) \\\\\n0 &= \\sum_i^n{\\left(y_ix_i - \\bar{y}x_i\\right)} + \\hat{\\beta}_1\\sum_i^n{\\left(\\bar{x}x_i - x_i^2\\right)} & (\\text{isolate}) \\\\\n\\hat{\\beta}_1 &= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})^2}\n\\end{align}\n\\]\n\n\n\n📝 Give it a go! Use the same method as before to solve the equation and isolate \\(\\hat{\\beta}_1\\). Tip: Use the previous formula to substitute \\(\\hat{\\beta}_0\\)."
  },
  {
    "objectID": "slides/week02_slides_part1.html#parameter-estimation-ols",
    "href": "slides/week02_slides_part1.html#parameter-estimation-ols",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "Parameter Estimation (OLS)",
    "text": "Parameter Estimation (OLS)\nAnd that is how OLS works!\n\\[\n\\begin{align}\n\\hat{\\beta}_1 &= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})^2} \\\\\n\\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/week02_slides_part1.html#estimates-for-multiple-regression",
    "href": "slides/week02_slides_part1.html#estimates-for-multiple-regression",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "Estimates for Multiple Regression",
    "text": "Estimates for Multiple Regression\n\nThe process of estimation is similar when we have more than one predictor. To estimate:\n\\[\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\dots + \\hat{\\beta}_p x_p.\n\\]\n\n\nWe aim to minimize Residual Sum of Squares as before:\n\\[\n\\min \\mathrm{RSS} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_{i1} - \\hat{\\beta}_2 x_{i2} - \\dots - \\hat{\\beta}_p x_{ip})^2.\n\\]\nThis is done using standard statistical software — you need a good linear algebra solver.\n\n\nThe values \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_p\\) that minimize RSS are the multiple least squares regression coefficient estimates."
  },
  {
    "objectID": "slides/week02_slides_part1.html#example-advertising-data",
    "href": "slides/week02_slides_part1.html#example-advertising-data",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "Example: Advertising data",
    "text": "Example: Advertising data\n\n\nA sample of the data:\n\nlibrary(tidyverse)\n\nfile = \"https://www.statlearning.com/s/Advertising.csv\"\nadvertising <- read_csv(file) %>% select(-1)\nhead(advertising, 11)\n\n# A tibble: 11 × 4\n      TV radio newspaper sales\n   <dbl> <dbl>     <dbl> <dbl>\n 1 230.   37.8      69.2  22.1\n 2  44.5  39.3      45.1  10.4\n 3  17.2  45.9      69.3   9.3\n 4 152.   41.3      58.5  18.5\n 5 181.   10.8      58.4  12.9\n 6   8.7  48.9      75     7.2\n 7  57.5  32.8      23.5  11.8\n 8 120.   19.6      11.6  13.2\n 9   8.6   2.1       1     4.8\n10 200.    2.6      21.2  10.6\n11  66.1   5.8      24.2   8.6\n\n\n\nHow the data is spread:\n\nsummary(advertising$TV)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.70   74.38  149.75  147.04  218.82  296.40 \n\n\n\nsummary(advertising$radio)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   9.975  22.900  23.264  36.525  49.600 \n\n\n\nsummary(advertising$newspaper)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.30   12.75   25.75   30.55   45.10  114.00 \n\n\n\nsummary(advertising$sales)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.60   10.38   12.90   14.02   17.40   27.00"
  },
  {
    "objectID": "slides/week02_slides_part1.html#simple-linear-regression-models",
    "href": "slides/week02_slides_part1.html#simple-linear-regression-models",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "Simple linear regression models",
    "text": "Simple linear regression models\n\n\n\nTV 📺\n\n\ntv_model <- lm(sales ~ TV, data=advertising)\ncat(sprintf(\"Sales (1k units) = %.4f %+.4f TV ($ 1k)\\n\", \n            tv_model$coefficients[\"(Intercept)\"], \n            tv_model$coefficients[\"TV\"]))\n\nSales (1k units) = 7.0326 +0.0475 TV ($ 1k)\n\n\n\nRadio 📻\n\n\nradio_model <- lm(sales ~ radio, data=advertising)\ncat(sprintf(\"Sales (1k units) = %.4f %+.4f Radio ($ 1k)\\n\", \n            radio_model$coefficients[\"(Intercept)\"], \n            radio_model$coefficients[\"radio\"]))\n\nSales (1k units) = 9.3116 +0.2025 Radio ($ 1k)\n\n\n\n\nNewspaper 📰\n\n\nnewspaper_model <- lm(sales ~ newspaper, data=advertising)\ncat(sprintf(\"Sales (1k units) = %.4f %+.4f Newspaper ($ 1k)\\n\", \n            newspaper_model$coefficients[\"(Intercept)\"], \n            newspaper_model$coefficients[\"newspaper\"]))\n\nSales (1k units) = 12.3514 +0.0547 Newspaper ($ 1k)\n\n\n\n🗨️ How should we interpret these models?\n\n\n\n\n\nGather answers from students.\nFor every 1k dollars spent in advertising on a particular media channel, we expect more \\(\\hat{\\beta}_1\\) thousand units of the product to be sold.\nWhy don’t the models agree about the intercept?"
  },
  {
    "objectID": "slides/week02_slides_part1.html#confidence-interval-of-coefficients",
    "href": "slides/week02_slides_part1.html#confidence-interval-of-coefficients",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "Confidence Interval of coefficients",
    "text": "Confidence Interval of coefficients\n\n\n\nThe confidence interval of an estimate has the form: \\[\n\\hat{\\beta}_1 \\pm 2 \\times \\mathrm{SE}(\\hat{\\beta}_1).\n\\] where \\(SE\\) is the standard error and reflects how the estimate varies under repeated sampling.\n\n\n\n\nThat is, there is approximately a 95% chance that the interval \\[\n\\biggl[ \\hat{\\beta}_1 - 2 \\times \\mathrm{SE}(\\hat{\\beta}_1), \\hat{\\beta}_1 + 2 \\times \\mathrm{SE}(\\hat{\\beta}_1) \\biggr]\n\\] will contain the true value of \\(\\beta_1\\).\n\n\n\nHow SE differs from STD? See (Altman and Bland 2005)"
  },
  {
    "objectID": "slides/week02_slides_part1.html#standard-errors",
    "href": "slides/week02_slides_part1.html#standard-errors",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "Standard Errors",
    "text": "Standard Errors\n\n\nThe standard error of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) is shown below:\n\\[\n\\begin{align}\n  \\mathrm{SE}(\\hat{\\beta}_1)^2 &= \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}, \\\\\n  \\mathrm{SE}(\\hat{\\beta}_0)^2 &= \\sigma^2 \\biggl[ \\frac{1}{n} +  \\frac{\\bar{x}^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\biggr],\n\\end{align}\n\\]\nwhere \\(\\sigma^2 = \\operatorname{Var}(\\epsilon)\\).\n\n\n\nBut, wait, we don’t know \\(\\epsilon\\)! How would we compute \\(\\sigma^2\\)?\nIn practice, we aproximate \\(\\sigma^2 \\approx \\mathrm{RSE} = \\sqrt{\\mathrm{RSS}/(n-2)}\\).\n\n\n\n\n\n\n\n\n\nImportant\n\n\n💡 Standard errors are a type of standard deviation but are not the same! See (Altman and Bland 2005) for more on this.\n\n\n\n\n\n\nThese formulas are only valid if we assume the errors \\(\\epsilon_i\\) have common variance \\(\\sigma^2\\) and are uncorrelated.\nRSE makes a comeback in Section 3.1.3"
  },
  {
    "objectID": "slides/week02_slides_part1.html#back-to-our-advertising-linear-models",
    "href": "slides/week02_slides_part1.html#back-to-our-advertising-linear-models",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "Back to our Advertising linear models",
    "text": "Back to our Advertising linear models\nWhat are the confidence intervals of our independent linear models?\n\n\n\nTV 📺\n\n\nconfint(tv_model)\n\n                 2.5 %     97.5 %\n(Intercept) 6.12971927 7.93546783\nTV          0.04223072 0.05284256\n\n\n\nRadio 📻\n\n\nconfint(radio_model)\n\n                2.5 %     97.5 %\n(Intercept) 8.2015885 10.4216877\nradio       0.1622443  0.2427472\n\n\n\n\nNewspaper 📰\n\n\nconfint(newspaper_model)\n\n                  2.5 %      97.5 %\n(Intercept) 11.12595560 13.57685854\nnewspaper    0.02200549  0.08738071\n\n\n\n🗨️ What does it mean?\n\n\n\n\n\n\nFor every additional $1000 invested in Radio, we can expect an increase in sales of between 162 and 242 units.\n\n\n\nUse the function confint to compute confidence intervals in R."
  },
  {
    "objectID": "slides/week02_slides_part1.html#p-values",
    "href": "slides/week02_slides_part1.html#p-values",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "p-values",
    "text": "p-values\n\nTo test the null hypothesis, we compute a t-statistic, given by \\[\nt = \\frac{\\hat{\\beta}_1 - 0}{\\mathrm{SE}(\\hat{\\beta}_1)},\n\\]\nThis will have a t-distribution1 with \\(n - 2\\) degrees of freedom, assuming \\(\\beta_1 = 0\\).\nUsing statistical software, it is easy to compute the probability of observing any value equal to \\(\\mid t \\mid\\) or larger.\nWe call this probability the p-value.\n\n🤔 How are the t-distribution and the Normal distribution related? Check this link to find out."
  },
  {
    "objectID": "slides/week02_slides_part1.html#back-to-our-advertising-linear-models-1",
    "href": "slides/week02_slides_part1.html#back-to-our-advertising-linear-models-1",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "Back to our Advertising linear models",
    "text": "Back to our Advertising linear models\nHow significant are the linear models?\n\n\n\nTV 📺\n\n\nout <- capture.output(summary(tv_model))\ncat(paste(out[9:15]), sep=\"\\n\")\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 7.032594   0.457843   15.36   <2e-16 ***\nTV          0.047537   0.002691   17.67   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nRadio 📻\n\n\nout <- capture.output(summary(radio_model))\ncat(paste(out[9:15]), sep=\"\\n\")\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  9.31164    0.56290  16.542   <2e-16 ***\nradio        0.20250    0.02041   9.921   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nNewspaper 📰\n\n\nout <- capture.output(summary(newspaper_model))\ncat(paste(out[9:15]), sep=\"\\n\")\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 12.35141    0.62142   19.88  < 2e-16 ***\nnewspaper    0.05469    0.01658    3.30  0.00115 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n🗨️ What does it mean?"
  },
  {
    "objectID": "slides/week02_slides_part1.html#references",
    "href": "slides/week02_slides_part1.html#references",
    "title": "🗓️ Week 02:Linear Regression",
    "section": "References",
    "text": "References\n\n\nAltman, Douglas G, and J Martin Bland. 2005. “Standard Deviations and Standard Errors.” BMJ 331 (7521): 903. https://doi.org/10.1136/bmj.331.7521.903.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York NY: Springer. https://www.statlearning.com/.\n\n\nKarafiath, Imre. 2009. “Is There a Viable Alternative to Ordinary Least Squares Regression When Security Abnormal Returns Are the Dependent Variable?” Review of Quantitative Finance and Accounting 32 (1): 17–31. https://doi.org/10.1007/s11156-007-0079-y.\n\n\n\n\n\nDS202 - Data Science for Social Scientists 🤖 🤹"
  },
  {
    "objectID": "slides/week02_slides_part2.html#residual-standard-errors-rse",
    "href": "slides/week02_slides_part2.html#residual-standard-errors-rse",
    "title": "🗓️ Week 02:Multiple Linear Regression",
    "section": "Residual Standard Errors (RSE)",
    "text": "Residual Standard Errors (RSE)\n\n\nRecall the “true model”: \\(Y = f(X) + \\epsilon\\)\nEven if we knew the true values of \\(\\beta_0\\) and \\(\\beta_1\\) — not just the estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) — our predictions of sales might still be off.\nBy how much?"
  },
  {
    "objectID": "slides/week02_slides_part2.html#residual-standard-errors-rse-1",
    "href": "slides/week02_slides_part2.html#residual-standard-errors-rse-1",
    "title": "🗓️ Week 02:Multiple Linear Regression",
    "section": "Residual Standard Errors (RSE)",
    "text": "Residual Standard Errors (RSE)\n\n\nThis can be estimated by the variance of errors: \\(\\sigma^2 = \\operatorname{Var}(\\epsilon)\\).\nAs said earlier, this quantity can be approximated, for the simple linear regression case, by the Residual Standard Errors (\\(\\mathrm{RSE}\\)) formula below:\n\n\n\n\\[\n\\sigma^2 \\approx \\mathrm{RSE} = \\sqrt{\\frac{\\mathrm{RSS}}{(n-\\mathrm{df})}}\n\\]\nwhere \\(\\mathrm{RSS} = \\sum_i^n{(y_i - \\hat{y}_i)^2}\\) represents the residual sum of squares and \\(\\mathrm{df}\\) represents the degrees of freedom in our model.\n\n\n\n\n➡️ It turns out that \\(\\mathrm{RSE}\\) is a good way to assess the goodness-of-fit of a model."
  },
  {
    "objectID": "slides/week02_slides_part2.html#back-to-our-advertising-linear-models",
    "href": "slides/week02_slides_part2.html#back-to-our-advertising-linear-models",
    "title": "🗓️ Week 02:Multiple Linear Regression",
    "section": "Back to our Advertising linear models",
    "text": "Back to our Advertising linear models\nLet’s compare the linear models we fitted earlier:\n\n\n\n\n\n\nTV 📺\n\n\nout <- capture.output(summary(tv_model))\ncat(paste(out[16:16]), sep=\"\\n\")\n\nResidual standard error: 3.259 on 198 degrees of freedom\n\n\n\nRadio 📻\n\n\nout <- capture.output(summary(radio_model))\ncat(paste(out[16:16]), sep=\"\\n\")\n\nResidual standard error: 4.275 on 198 degrees of freedom\n\n\n\n\nNewspaper 📰\n\n\nout <- capture.output(summary(newspaper_model))\ncat(paste(out[16:16]), sep=\"\\n\")\n\nResidual standard error: 5.092 on 198 degrees of freedom\n\n\n\n🗨️ What does it mean?"
  },
  {
    "objectID": "slides/week02_slides_part2.html#the-r2-statistic",
    "href": "slides/week02_slides_part2.html#the-r2-statistic",
    "title": "🗓️ Week 02:Multiple Linear Regression",
    "section": "The \\(R^2\\) statistic",
    "text": "The \\(R^2\\) statistic\n\nR-squared or fraction of variance explained is defined as:\n\n\\[\nR^2 = \\frac{\\mathrm{TSS - RSS}}{\\mathrm{TSS}} = 1 - \\frac{\\mathrm{RSS}}{\\mathrm{TSS}}\n\\]\nwhere TSS = \\(\\sum_{i=1}^n (y_i - \\bar{y})^2\\) is the total sum of squares. \n\n\n\n\n\n\n\nTip\n\n\nIntuitively, \\(R^2\\) measures the proportion of variability in \\(Y\\) that can be explained using \\(X\\).\n\n\\(R^2\\) close to 1 means that a large proportion of the variance in \\(Y\\) is explained by the regression.\n\\(R^2\\) close to 0 means that the regression does not explain much of the variability in \\(Y\\)."
  },
  {
    "objectID": "slides/week02_slides_part2.html#sample-correlation-coefficient",
    "href": "slides/week02_slides_part2.html#sample-correlation-coefficient",
    "title": "🗓️ Week 02:Multiple Linear Regression",
    "section": "Sample correlation coefficient",
    "text": "Sample correlation coefficient\nBy the way, in the simple linear regression setting, it can be shown that \\(R^2 = (\\operatorname{Cor}(X, Y))^2\\), where \\(\\operatorname{Cor}(X, Y)\\) is the correlation between \\(X\\) and \\(Y\\):\n\\[\n\\operatorname{Cor}(X, Y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}}.\n\\]\n\n\n📝 Give it a go! Play around with the definition of \\(R^2\\) shown in the previous slide and verify that \\(R^2 = (\\operatorname{Cor}(X, Y))^2\\)."
  },
  {
    "objectID": "slides/week02_slides_part2.html#f-statistic",
    "href": "slides/week02_slides_part2.html#f-statistic",
    "title": "🗓️ Week 02:Multiple Linear Regression",
    "section": "F-statistic",
    "text": "F-statistic\nWe used t-statistic to compute p-values for the coefficients (\\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\)).Now how do I test whether the model, as a whole, makes sense?\n\n\n\n\nFor this, we perform the hypothesis test: \\[\n\\begin{align}\n&~~~~H_0:&\\beta_1 = \\beta_2 = \\ldots = \\beta_j = 0 \\\\\n&\\text{vs} \\\\\n&~~~~H_A:& \\text{at least one } \\beta_j \\neq 0.\n\\end{align}\n\\]\n\n\n\n\nwhich is performed by computing the F-statistic: \\[\nF = \\frac{(TSS - RSS) / p}{RSS/(n - p - 1)} \\sim F_{p, n-p-1}\n\\]\n\n\n\n\n\nIf F is close to 1, there is no relationship between the response and the predictor(s).\nIf \\(H_A\\) is true, then we expect \\(F\\) to be greater than 1.\nCheck (James et al. 2021, 75–77) for an in-depth explanation of this test.\n\n\n\n\n\n\n\n\nNote that the F-statistic applies to both simple and multiple linear regression models.\nCheck this link if you want to understand the difference between the t-test and the the F-test."
  },
  {
    "objectID": "slides/week02_slides_part2.html#back-to-our-advertising-linear-models-1",
    "href": "slides/week02_slides_part2.html#back-to-our-advertising-linear-models-1",
    "title": "🗓️ Week 02:Multiple Linear Regression",
    "section": "Back to our Advertising linear models",
    "text": "Back to our Advertising linear models\nHow well do our models explain the variability of the response?\n\n\n\nTV 📺\n\n\nout <- capture.output(summary(tv_model))\ncat(paste(out[17:18]), sep=\"\\n\")\n\nMultiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: < 2.2e-16\n\n\n\nRadio 📻\n\n\nout <- capture.output(summary(radio_model))\ncat(paste(out[17:18]), sep=\"\\n\")\n\nMultiple R-squared:  0.332, Adjusted R-squared:  0.3287 \nF-statistic: 98.42 on 1 and 198 DF,  p-value: < 2.2e-16\n\n\n\n\nNewspaper 📰\n\n\nout <- capture.output(summary(newspaper_model))\ncat(paste(out[17:18]), sep=\"\\n\")\n\nMultiple R-squared:  0.05212,   Adjusted R-squared:  0.04733 \nF-statistic: 10.89 on 1 and 198 DF,  p-value: 0.001148\n\n\n\n🗨️ What does it mean?"
  },
  {
    "objectID": "slides/week02_slides_part2.html#a-multiple-linear-regression-to-advertising",
    "href": "slides/week02_slides_part2.html#a-multiple-linear-regression-to-advertising",
    "title": "🗓️ Week 02:Multiple Linear Regression",
    "section": "A multiple linear regression to Advertising",
    "text": "A multiple linear regression to Advertising\n\n\nWhen you run a linear model in R, you can call the summary function to see and check all of these statistics we’ve covered so far.\nBy now, you should be able to understand its full output\n\n\nFitting all predictors:\n\n\nTV 📺 + Radio 📻 + Newspaper 📰\n\nfull_model <- lm(sales ~ ., data=advertising)\nsummary(full_model)\n\n\nCall:\nlm(formula = sales ~ ., data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.938889   0.311908   9.422   <2e-16 ***\nTV           0.045765   0.001395  32.809   <2e-16 ***\nradio        0.188530   0.008611  21.893   <2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: < 2.2e-16\n\n\n\nConfidence Intervals\n\nconfint(full_model)\n\n                  2.5 %     97.5 %\n(Intercept)  2.32376228 3.55401646\nTV           0.04301371 0.04851558\nradio        0.17154745 0.20551259\nnewspaper   -0.01261595 0.01054097"
  },
  {
    "objectID": "slides/week02_slides_part2.html#interpreting-the-coefficients",
    "href": "slides/week02_slides_part2.html#interpreting-the-coefficients",
    "title": "🗓️ Week 02:Multiple Linear Regression",
    "section": "Interpreting the coefficients",
    "text": "Interpreting the coefficients\n\nRecall the multiple regression model:\n\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p + \\epsilon ,\n\\]\n\n\nWe interpret \\(\\beta_j\\) as the average effect on \\(Y\\) of a one unit increase in \\(X_j\\), holding all other predictors fixed. In the advertising example, the model becomes\n\n\\[\n\\mathrm{sales} = \\beta_0 + \\beta_1 \\times \\mathrm{TV} + \\beta_2 \\times \\mathrm{radio} + \\beta_3 \\times \\mathrm{newspaper} + \\epsilon .\n\\]"
  },
  {
    "objectID": "slides/week02_slides_part2.html#interpreting-the-coefficients-1",
    "href": "slides/week02_slides_part2.html#interpreting-the-coefficients-1",
    "title": "🗓️ Week 02:Multiple Linear Regression",
    "section": "Interpreting the coefficients",
    "text": "Interpreting the coefficients\n\n\nThe ideal scenario is when the predictors are uncorrelated – a balanced design:\n\nEach coefficient can be estimated and tested separately.\nInterpretations such as “a unit change in \\(X_j\\) is associated with a \\(\\beta_j\\) change in \\(Y\\), while all the other variables stay fixed”, are possible.\n\nCorrelations amongst predictors cause problems:\n\nThe variance of all coefficients tends to increase, sometimes dramatically\nInterpretations become hazardous – when \\(X_j\\) changes, everything else changes.\n\nClaims of causality should be avoided for observational data."
  },
  {
    "objectID": "slides/week02_slides_part2.html#references",
    "href": "slides/week02_slides_part2.html#references",
    "title": "🗓️ Week 02:Multiple Linear Regression",
    "section": "References",
    "text": "References\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York NY: Springer. https://www.statlearning.com/.\n\n\n\n\n\nDS202 - Data Science for Social Scientists 🤖 🤹"
  },
  {
    "objectID": "slides/week03_slides_part1.html#classification",
    "href": "slides/week03_slides_part1.html#classification",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Classification",
    "text": "Classification\n\n\nWe have so far only modelled quantitative responses.\nToday, we focus on predicting categorical, or qualitative, responses.\n\n\n\n\n\nThe generic supervised model:\n\\[\nY = \\operatorname{f}(X) + \\epsilon\n\\]\nstill applies, only this time \\(Y\\) is categorical. ➡️\n\n\n\nOur categorical variables of interest take values in an unordered set \\(\\mathcal{C}\\), such as:\n\n\\(\\text{eye color} \\in \\mathcal{C} = \\{\\color{brown}{brown},\\color{blue}{blue},\\color{green}{green}\\}\\)\n\\(\\text{email} \\in \\mathcal{C} = \\{spam, ham\\}\\)\n\\(\\text{football results} \\in \\mathcal{C} \\{away\\ win,draw,home\\ win\\}\\)\n\n\n\n\n\nOpening slides - Unordered here is an important distinction. - We can also call it a class"
  },
  {
    "objectID": "slides/week03_slides_part1.html#why-cant-i-use-linear-regression",
    "href": "slides/week03_slides_part1.html#why-cant-i-use-linear-regression",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Why can’t I use linear regression?",
    "text": "Why can’t I use linear regression?\n\n\nWhat if I just coded each category as a number?\n\\[\nY =\n    \\begin{cases}\n        1 &\\text{if}~\\color{brown}{brown},\\\\\n        2 &\\text{if}~\\color{blue}{blue},\\\\\n        3 &\\text{if}~\\color{green}{green}.\n    \\end{cases}\n\\]\n\nWhat could go wrong?\n\n\n\n\n\n\nHow would you interpret a particular prediction if your model returned:\n\n\\(\\hat{y} = ~~1.5\\) or\n\\(\\hat{y} = ~~0.1\\) or\n\\(\\hat{y} = 20.0\\)?\n\n\n\n\n\n\n\nKey takeaway: Regression is not suitable for all problems. - regression cannot accommodate a qualitative response with more than two classes - regression will not provide meaningful estaimtes of Pr(Y|X)"
  },
  {
    "objectID": "slides/week03_slides_part1.html#more-on-classification",
    "href": "slides/week03_slides_part1.html#more-on-classification",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "More on Classification",
    "text": "More on Classification\n\n\nOften we are more interested in estimating the probabilities that \\(X\\) belongs to each category in \\(\\mathcal{C}\\).\n\n\n\n\nFor example, it is sometimes more valuable to have an estimate of the probability that an insurance claim is fraudulent, than a classification fraudulent or not.\n\n\n\n\n\n\n\nA successful gambling strategy, for instance, requires placing bets on outcomes to which you believe the bookmakers have assigned incorrect probabilities. Knowing the most likely outcome is not enough!\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nStatistical models for ordinal response, when sets are discrete but have an order, are outside the scope of this course. Should you need to create models for ordinal variables, consult “ordinal logistic regression”. A good reference about this is (Agresti 2019, chap. 6).\n\n\n\n\n\nKey takeaway: normally, we estimate"
  },
  {
    "objectID": "slides/week03_slides_part1.html#speaking-of-probabilities",
    "href": "slides/week03_slides_part1.html#speaking-of-probabilities",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Speaking of Probabilities…",
    "text": "Speaking of Probabilities…\nLet’s talk about three possible interpretations of probability:\n\n\n\n\nClassical\n\n\n\n\nFrequentist\n\n\n\n\nBayesian\n\n\n\n\n\nEvents of the same kind can be reduced to a certain number of equally possible cases.\nExample: coin tosses lead to either heads or tails \\(1/2\\) of the time ( \\(50\\%/50\\%\\))\n\n\n\n\nWhat would be the outcome if I repeat the process many times?\nExample: if I toss a coin \\(1,000,000\\) times, I expect \\(\\approx 50\\%\\) heads and \\(\\approx 50\\%\\) tails outcome.\n\n\n\n\nWhat is your judgement of the likelihood of the outcome? Based on previous information.\nExample: if I know that this coin has symmetric weight, I expect a \\(50\\%/50\\%\\) outcome.\n\n\n\n\n\n\nSource: (DeGroot and Schervish 2003)"
  },
  {
    "objectID": "slides/week03_slides_part1.html#speaking-of-probabilities-1",
    "href": "slides/week03_slides_part1.html#speaking-of-probabilities-1",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Speaking of Probabilities…",
    "text": "Speaking of Probabilities…\nFor our purposes:\n\nProbabilities are numbers between 0 and 1\nThe sum of all possible outcomes of an event must sum to 1.\nIt is useful to think of things as probabilities\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n💡 Although there is no such thing as “a probability of \\(120\\%\\)” or “a probability of \\(-23\\%\\)”, you could still use this language to refer to increase or decrease in an outcome."
  },
  {
    "objectID": "slides/week03_slides_part1.html#the-logistic-regression-model",
    "href": "slides/week03_slides_part1.html#the-logistic-regression-model",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "The Logistic Regression model",
    "text": "The Logistic Regression model\n\nConsider a binary response:\n\\[\nY = \\begin{cases}\n0 \\\\\n1\n\\end{cases}\n\\]\n\n\nWe model the probability that \\(Y = 1\\) as:\n\\[\nPr(Y = 1|X) = p(X) = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1 X}}\n\\]\n\n\n\n\n\nSource of illustration: TIBCO\n\n\n\nThis is how this function looks like"
  },
  {
    "objectID": "slides/week03_slides_part1.html#the-logistic-function",
    "href": "slides/week03_slides_part1.html#the-logistic-function",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "The Logistic function",
    "text": "The Logistic function\n\nChanging \\(\\beta_0\\) while keeping \\(\\beta_1 = 1\\):"
  },
  {
    "objectID": "slides/week03_slides_part1.html#the-logistic-function-cont.",
    "href": "slides/week03_slides_part1.html#the-logistic-function-cont.",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "The Logistic function (cont.)",
    "text": "The Logistic function (cont.)\n\nKeep \\(\\beta_0 = 0\\) but vary \\(\\beta_1\\):"
  },
  {
    "objectID": "slides/week03_slides_part1.html#maximum-likelihood-estimate",
    "href": "slides/week03_slides_part1.html#maximum-likelihood-estimate",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Maximum likelihood estimate",
    "text": "Maximum likelihood estimate\n\nAs with linear regression, the coefficients are unknown and need to be estimated from training data:\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1X}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}}\n\\]\n\n\nWe estimate these by maximising the likelihood function:\n\\[\n\\max \\ell(\\beta_0, \\beta_1) = \\prod_{i:y_i=1}{p(x_i)} \\prod_{i':y_{i'}=0} (1 - p(x_{i'})),\n\\]\nand we call this method the Maximum Likelihood Estimate (MLE).\n\n\n➡️ As usual, there are multiple ways to solve this equation!\n\n\nKey takeaway of this slide: MLE (logistic regression) is analogous to OLS (linear regression).\nIntuition: What are the values for \\(\\alpha\\) and \\(\\beta\\) that generate predicted probabilities, \\(\\hat{Y}_i\\) for each training observation that are as close as possible to the realised outcomes, \\(Y_i\\)?"
  },
  {
    "objectID": "slides/week03_slides_part1.html#solutions-to-mle",
    "href": "slides/week03_slides_part1.html#solutions-to-mle",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Solutions to MLE",
    "text": "Solutions to MLE\n\n\nMLE is much more difficult to solve than the least squares formulations.\nMost solutions rely on a variant of the Hill Climbing algorithm\n\n\n\n\n\n\n\n\n\nHow do you find the latitude and longitude of a mountain peak if you can’t see very far?\n\n\n\nStart somewhere.\nLook around for the best way to go up.\nGo a small distance in that direction.\nLook around for the best way to go up.\nGo a small distance in that direction.\n\\(\\cdots\\)\n\n\n\n\n\n\n\nAdvanced: If for whatever random reason, you find yourself enamored with the Maximum Likelihood Estimate, check (Agresti 2019) for a recent take on the statistical properties of this method."
  },
  {
    "objectID": "slides/week03_slides_part1.html#example-default-data",
    "href": "slides/week03_slides_part1.html#example-default-data",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Example: Default data",
    "text": "Example: Default data\n\n\nA sample of the data:\n\nlibrary(ISLR2)\n\nhead(ISLR2::Default, n=15)\n\n   default student   balance    income\n1       No      No  729.5265 44361.625\n2       No     Yes  817.1804 12106.135\n3       No      No 1073.5492 31767.139\n4       No      No  529.2506 35704.494\n5       No      No  785.6559 38463.496\n6       No     Yes  919.5885  7491.559\n7       No      No  825.5133 24905.227\n8       No     Yes  808.6675 17600.451\n9       No      No 1161.0579 37468.529\n10      No      No    0.0000 29275.268\n11      No     Yes    0.0000 21871.073\n12      No     Yes 1220.5838 13268.562\n13      No      No  237.0451 28251.695\n14      No      No  606.7423 44994.556\n15      No      No 1112.9684 23810.174\n\n\n\nHow the data is spread:\n\nsummary(ISLR2::Default$default)\n\n  No  Yes \n9667  333 \n\n\n\nsummary(ISLR2::Default$balance)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    0.0   481.7   823.6   835.4  1166.3  2654.3 \n\n\n\nsummary(ISLR2::Default$income)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    772   21340   34553   33517   43808   73554 \n\n\n\nsummary(ISLR2::Default$student)\n\n  No  Yes \n7056 2944"
  },
  {
    "objectID": "slides/week03_slides_part1.html#simple-logistic-regression-models",
    "href": "slides/week03_slides_part1.html#simple-logistic-regression-models",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Simple logistic regression models",
    "text": "Simple logistic regression models\n\n\n\nIncome 💰\n\n\nincome_model <- \n  glm(default ~ income, data=ISLR2::Default, family=binomial)\ncat(sprintf(\"beta_0 = %.5f | beta_1 = %e\",\n            income_model$coefficients[\"(Intercept)\"],\n            income_model$coefficients[\"income\"]))\n\nbeta_0 = -3.09415 | beta_1 = -8.352575e-06\n\n\n\nBalance 💸\n\n\nbalance_model <- \n  glm(default ~ balance, data=ISLR2::Default, family=binomial)\ncat(sprintf(\"beta_0 = %.5f | beta_1 = %.4f\",\n            balance_model$coefficients[\"(Intercept)\"],\n            balance_model$coefficients[\"balance\"]))\n\nbeta_0 = -10.65133 | beta_1 = 0.0055\n\n\n\n\nStudent 🧑‍🎓\n\n\nstudent_model <- \n  glm(default ~ student, data=ISLR2::Default, family=binomial)\ncat(sprintf(\"beta_0 = %.5f | beta_1 = %.4f\",\n            student_model$coefficients[\"(Intercept)\"],\n            student_model$coefficients[\"studentYes\"]))\n\nbeta_0 = -3.50413 | beta_1 = 0.4049\n\n\n\n\n\n\n\n\n\nNote\n\n\nLogistic regression coefficients are a bit trickier to interpret when compared to those of linear regression. Let’s look at how it works ➡️\n\n\n\n\n\n\n\n\nGather answers from students."
  },
  {
    "objectID": "slides/week03_slides_part1.html#the-concept-of-odds",
    "href": "slides/week03_slides_part1.html#the-concept-of-odds",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "The concept of odds",
    "text": "The concept of odds\nThe quantity below is called the odds:\n\\[\n\\frac{p(X)}{1 - p(X)} = e^{\\beta_0 + \\beta_1 X}\n\\]\n\n\n\n\n\n\nExample\n\n\nIf the odds are 9, then \\(\\frac{p(X)}{(1-p(X))} = 9 \\Rightarrow p(X) = 0.9\\).\nThis means that 9 out of 10 people will default.\n\n\n\n\n\n\n\n\n\nTip\n\n\nHow to interpret \\(\\beta_1\\)\nIf X increases one unit then the odds increase by a factor of \\(e^{\\beta_1}\\)\n\n\n\n\n\n📝 Give it a go! Using algebra, can you re-arrange the equation for \\(p(X)\\) presented in the Logistic regression model slides to arrive at the odds quantity shown above?"
  },
  {
    "objectID": "slides/week03_slides_part1.html#log-odds-or-logit",
    "href": "slides/week03_slides_part1.html#log-odds-or-logit",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Log odds or logit",
    "text": "Log odds or logit\n\nIt is also useful to think of the odds in log terms.\n\n\\[\nlog\\left(\\frac{p(X)}{1 - p(X)}\\right) = \\beta_0 + \\beta_1 X\n\\]\n\nWe call the quantity above the log odds or logit\n\n\n\n\n\n\n\nTip\n\n\nHow to interpret \\(\\beta_1\\)\nIf X increases one unit then the log odds increase by \\(\\beta_1\\)"
  },
  {
    "objectID": "slides/week03_slides_part1.html#example-default-vs-balance",
    "href": "slides/week03_slides_part1.html#example-default-vs-balance",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Example: Default vs Balance",
    "text": "Example: Default vs Balance\n\n\n\nModel: Default vs Balance 💸\n\n\\[\n\\hat{y} = \\frac{e^{-10.65133 + 0.005498917X}}{1 + e^{-10.65133 + 0.005498917X}}\n\\]\n\n  That is:\n\\[\n\\begin{align}\n\\hat{\\beta}_0 &= -10.65133\\\\\n\\hat{\\beta}_1 &= 0.005498917\n\\end{align}\n\\]\n\n\n\n\n\nInterpreting \\(\\hat{\\beta}_0\\):\n\nIn the absence of balance information:\n\nLog odds: \\(-10.65133\\)\nOdds : \\(e^{-10.65133} = 2.366933 \\times 10^{-5}\\) \\[\n\\begin{align}\np(\\text{default}=\\text{Yes}) &= \\frac{\\text{odds}}{(1 + \\text{odds})} \\\\\n                           &= 2.366877 \\times 10^{-5}\\end{align}\n\\]\n\n\n\n\n\nInterpreting \\(\\hat{\\beta}_1\\):\n\nWith balance information:\n\nLog odds: \\(0.005498917\\)\nOdds : \\(e^{0.005498917} = 1.005514\\)\nThat is, for every \\(\\$1\\) increase in balance, the probability of default increases \n\n\n\n\n\n\n\nNote that the increase is cumulative, not linear. It depends on where X is."
  },
  {
    "objectID": "slides/week03_slides_part1.html#example-default-vs-income",
    "href": "slides/week03_slides_part1.html#example-default-vs-income",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Example: Default vs Income",
    "text": "Example: Default vs Income\n\n\n\nModel: Default vs Income 💰\n\n\\[\n\\hat{y} = \\frac{e^{-3.094149 - 8.352575 \\times 10^{-6} X}}{1 + e^{-3.094149 - 8.352575 \\times 10^{-6} X}}\n\\]\n\n  That is:\n\\[\n\\begin{align}\n\\hat{\\beta}_0 &= - 3.094149\\\\\n\\hat{\\beta}_1 &= - 8.352575 \\times 10^{-6}\n\\end{align}\n\\]\n\n\n\n\n\nInterpreting \\(\\hat{\\beta}_0\\):\n\nIn the absence of balance information:\n\nLog odds: \\(- 3.094149\\)\nOdds : \\(e^{- 3.094149} = 0.04531355\\) \\[\n\\begin{align}\np(\\text{default}=\\text{Yes}) &= \\frac{\\text{odds}}{(1 + \\text{odds})} \\\\\n                           &= 0.04334924 = 4.33\\%\\end{align}\n\\]\n\n\n\n\n\nInterpreting \\(\\hat{\\beta}_1\\):\n\nWith balance information:\n\nLog odds: \\(- 8.352575\\)\nOdds : \\(e^{- 8.352575} = 0.9999916\\)\nThat is, for every \\(\\$1\\) increase in income, the probability of default decreases \n\n\n\n\n\n\n\nNote that the increase is cumulative, not linear. It depends on where X is."
  },
  {
    "objectID": "slides/week03_slides_part1.html#example-default-vs-is-student",
    "href": "slides/week03_slides_part1.html#example-default-vs-is-student",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Example: Default vs Is Student?",
    "text": "Example: Default vs Is Student?\n\n\n\nModel: Default vs Student 🧑‍🎓\n\n\\[\n\\hat{y} = \\frac{e^{-3.504128 + 0.4048871 X}}{1 + e^{-3.504128 + 0.4048871 X}}\n\\]\n\n  That is:\n\\[\n\\begin{align}\n\\hat{\\beta}_0 &= -3.504128\\\\\n\\hat{\\beta}_1 &= +0.4048871\n\\end{align}\n\\]\n\n\n\n\n\nInterpreting \\(\\hat{\\beta}_0\\):\n\nIn the absence of balance information:\n\nLog odds: \\(-3.504128\\)\nOdds : \\(e^{- 3.504128} = 0.03007299\\) \\[\n\\begin{align}\np(\\text{default}=\\text{Yes}) &= \\frac{\\text{odds}}{(1 + \\text{odds})} \\\\\n                           &= 0.02919501 \\approx 2.92\\%\\end{align}\n\\]\n\n\n\n\n\nInterpreting \\(\\hat{\\beta}_1\\):\n\nWith balance information:\n\nLog odds: \\(0.4048871\\)\nOdds : \\(e^{0.4048871} = 1.499133\\)\n\nIf person is a student, then the probability of default increases"
  },
  {
    "objectID": "slides/week03_slides_part1.html#model-info",
    "href": "slides/week03_slides_part1.html#model-info",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Model info",
    "text": "Model info\nThe output of summary is similar to that of linear regression:\n\n\n\nModel: Default vs Balance 💸\n\n\nsummary(balance_model)\n\n\nCall:\nglm(formula = default ~ balance, family = binomial, data = ISLR2::Default)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.2697  -0.1465  -0.0589  -0.0221   3.7589  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.065e+01  3.612e-01  -29.49   <2e-16 ***\nbalance      5.499e-03  2.204e-04   24.95   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1596.5  on 9998  degrees of freedom\nAIC: 1600.5\n\nNumber of Fisher Scoring iterations: 8\n\n\n\nConfidence Intervals\n\nconfint(balance_model)\n\n                    2.5 %       97.5 %\n(Intercept) -11.383288936 -9.966565064\nbalance       0.005078926  0.005943365"
  },
  {
    "objectID": "slides/week03_slides_part1.html#wraping-up-on-coefficients",
    "href": "slides/week03_slides_part1.html#wraping-up-on-coefficients",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Wraping up on coefficients:",
    "text": "Wraping up on coefficients:\n\n\n\nPay attention to the sign of the coefficient. The sign of the coefficients indicate the direction of the association.\nIf the value of a predictor increases, we look at the sign of its coefficient:\n\nIf it is a ➕ positive coefficient, we predict an increase in the probability of the class\nIf it is a ➖ negative coefficient, we predict a decrease in the probability of the class"
  },
  {
    "objectID": "slides/week03_slides_part1.html#multiple-logistic-regression-1",
    "href": "slides/week03_slides_part1.html#multiple-logistic-regression-1",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Multiple Logistic Regression",
    "text": "Multiple Logistic Regression\n\nIt is straightforward to extend the logistic model to include multiple predictors:\n\n\\[\nlog \\left( \\frac{p(X)}{1-p(X)} \\right)=\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p\n\\]\n\\[\np(X) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p}}\n\\]\n\nMost things are still available (hypothesis test, confidence intervals, etc.)\nLet’s explore the output and summary of the full model ⏭️"
  },
  {
    "objectID": "slides/week03_slides_part1.html#fitting-all-predictors-of-default",
    "href": "slides/week03_slides_part1.html#fitting-all-predictors-of-default",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "Fitting all predictors of Default",
    "text": "Fitting all predictors of Default\n\n\nFull Model\n\nfull_model <- glm(default ~ ., data=ISLR2::Default, family=binomial)\nsummary(full_model)\n\n\nCall:\nglm(formula = default ~ ., family = binomial, data = ISLR2::Default)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.4691  -0.1418  -0.0557  -0.0203   3.7383  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.087e+01  4.923e-01 -22.080  < 2e-16 ***\nstudentYes  -6.468e-01  2.363e-01  -2.738  0.00619 ** \nbalance      5.737e-03  2.319e-04  24.738  < 2e-16 ***\nincome       3.033e-06  8.203e-06   0.370  0.71152    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1571.5  on 9996  degrees of freedom\nAIC: 1579.5\n\nNumber of Fisher Scoring iterations: 8\n\n\n\nConfidence Intervals\n\nconfint(full_model)\n\n                    2.5 %        97.5 %\n(Intercept) -1.185902e+01 -9.928174e+00\nstudentYes  -1.109018e+00 -1.822147e-01\nbalance      5.294898e-03  6.204587e-03\nincome      -1.304712e-05  1.912447e-05"
  },
  {
    "objectID": "slides/week03_slides_part1.html#references",
    "href": "slides/week03_slides_part1.html#references",
    "title": "🗓️ Week 03 Classifiers - Part I",
    "section": "References",
    "text": "References\n\n\nAgresti, Alan. 2019. An Introduction to Categorical Data Analysis. Third edition. Wiley Series in Probability and Statistics. Hoboken, NJ: John Wiley & Sons.\n\n\nDeGroot, Morris H., and Mark J. Schervish. 2003. Probability and Statistics. 3. ed., international edition. Boston Munich: Addison-Wesley.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York NY: Springer. https://www.statlearning.com/.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDS202 - Data Science for Social Scientists 🤖 🤹"
  },
  {
    "objectID": "slides/week03_slides_part2.html#bayes-theorem",
    "href": "slides/week03_slides_part2.html#bayes-theorem",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\n\nBefore we go on to explain what Naive Bayes is about, we need to understand the formula below.\n\n\\[\nP(\\mathbf{Y} = k | \\mathbf{X} = x) = \\frac{P(k)P(\\mathbf{X}|\\mathbf{Y}=k)}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}\n\\]\n\n\nLet’s look at it step-by-step ⏭️"
  },
  {
    "objectID": "slides/week03_slides_part2.html#bayes-theorem-1",
    "href": "slides/week03_slides_part2.html#bayes-theorem-1",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\n\\[\nP(\\mathbf{Y} = k | \\mathbf{X} = x) = \\frac{P(k)P(\\mathbf{X}|\\mathbf{Y}=k)}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}\n\\]\nNew variables\n\n\n\\(K \\Rightarrow\\) is the set of classes. In the binary case, \\(K = \\{0, 1\\}\\).\n\\(P(k) \\Rightarrow\\) is the probability that a random sample belongs to class \\(k\\).\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe textbook uses a slightly different notation."
  },
  {
    "objectID": "slides/week03_slides_part2.html#bayes-theorem-2",
    "href": "slides/week03_slides_part2.html#bayes-theorem-2",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\n\\[\n\\color{blue}{P(\\mathbf{Y} = k | \\mathbf{X} = x)} \\color{Gainsboro}{= \\frac{P(k)P(\\mathbf{X}|\\mathbf{Y}=k)}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}}\n\\]\n\n\nThe quantity above (in blue) is called the posterior distribution\nIt is what we are interested in when making inferences/predictions\n\n\n\nRead it as:\n\nWhat is the probability that the class is \\(k\\) given that the sample is \\(x\\)?"
  },
  {
    "objectID": "slides/week03_slides_part2.html#bayes-theorem-3",
    "href": "slides/week03_slides_part2.html#bayes-theorem-3",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\n\\[\n\\color{Gainsboro}{P(\\mathbf{Y} = k | \\mathbf{X} = x) =} \\frac{\\color{blue}{P(k)}\\color{Gainsboro}{P(\\mathbf{X}|\\mathbf{Y}=k)}}{\\color{Gainsboro}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}}\n\\]\n\n\nThe quantity above (in blue) is called the prior distribution\nIt represents the proportion of samples of class \\(k\\) we believe (estimate) we would find if sampling at random.\n\n\n\nRead it as:\n\nWhat is the probability that the class is \\(k\\) given a random sample?"
  },
  {
    "objectID": "slides/week03_slides_part2.html#bayes-theorem-4",
    "href": "slides/week03_slides_part2.html#bayes-theorem-4",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\n\\[\n\\color{Gainsboro}{P(\\mathbf{Y} = k | \\mathbf{X} = x) =} \\frac{\\color{Gainsboro}{P(k)}\\color{blue}{P(\\mathbf{X}|\\mathbf{Y}=k)}}{\\color{Gainsboro}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}}\n\\]\n\n\nThe quantity above (in blue) is often called the likelihood\nIt represents the density function of \\(\\mathbf{X}\\) for samples of class \\(k\\).\n\n\n\nThink of it as:\n\nWhat values would I expect \\(X\\) to take when the class is \\(\\mathbf{Y} = k\\)?"
  },
  {
    "objectID": "slides/week03_slides_part2.html#bayes-theorem-5",
    "href": "slides/week03_slides_part2.html#bayes-theorem-5",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\n\\[\n\\color{Gainsboro}{P(\\mathbf{Y} = k | \\mathbf{X} = x) =} \\frac{\\color{Gainsboro}{P(k)}\\color{Gainsboro}{P(\\mathbf{X}|\\mathbf{Y}=k)}}{\\color{blue}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}}\n\\]\n\n\nThe quantity above (in blue) represents the density function of \\(\\mathbf{X}\\) regardless of the class\nIt is often called the marginal probability of \\(\\mathbf{X}\\).\n\nNote that \\(\\color{blue}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}} = P(\\mathbf{X})\\)\n\n\n\n\nThink of it as:\n\nWhat values would I expect \\(X\\) if ignored the class?"
  },
  {
    "objectID": "slides/week03_slides_part2.html#bayes-theorem-6",
    "href": "slides/week03_slides_part2.html#bayes-theorem-6",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\n\\[\nP(\\mathbf{Y} = k | \\mathbf{X} = x) = \\frac{P(k)P(\\mathbf{X}|\\mathbf{Y}=k)}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}\n\\]\n\nLet’s look at how different algorithms explore this rule ⏭️"
  },
  {
    "objectID": "slides/week03_slides_part2.html#linear-discriminant-analysis-lda",
    "href": "slides/week03_slides_part2.html#linear-discriminant-analysis-lda",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Linear Discriminant Analysis (LDA)",
    "text": "Linear Discriminant Analysis (LDA)\n\n\nAssumptions:\n\nLikelihood follows a Gaussian distribution\n\nEach class has its own mean, \\(\\mu_k\\)\nAll classes have the same standard deviation\n\nThat is, \\(\\sigma^2_1 = \\sigma^2_2 = \\ldots = \\sigma^2_K\\), or simply \\(\\sigma^2\\)\n\n\nWe denote this as: \\(P(\\mathbf{X}|\\mathbf{Y}=k) \\sim N(\\mu_k, \\sigma^2)\\)"
  },
  {
    "objectID": "slides/week03_slides_part2.html#lda---estimates",
    "href": "slides/week03_slides_part2.html#lda---estimates",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "LDA - Estimates",
    "text": "LDA - Estimates\n\nWe estimate the mean per class and the shared standard deviation as follows:\n\n\n\\[\n\\begin{align}\n\\hat{\\mu}_k &= \\frac{1}{n_k}\\sum_{i:y_i=k}{x_i}\\\\\n\\hat{\\sigma}^2 &= \\frac{1}{n - K}\\sum_{k=1}^K{\\sum_{i:y_i=k}{\\left(x_i - \\hat{\\mu}_k\\right)^2}} \\\\\n\\hat{P}(k) &= \\frac{n_k}{n}\n\\end{align}\n\\]\n\n\n\nwhere:\n\n\\(n\\) is the total number of training observations\n\\(n_k\\) is the number of training observations in the \\(k\\)th class\n\n\n\n\n\nRead (James et al. 2021, sec. 4.4) to understand why these estimates are the way they are.\n\n\n\n\nMention that priors could come from prior knowledge"
  },
  {
    "objectID": "slides/week03_slides_part2.html#naive-bayes-classifier-1",
    "href": "slides/week03_slides_part2.html#naive-bayes-classifier-1",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Naive Bayes Classifier",
    "text": "Naive Bayes Classifier\n\n\nMain Assumption:\n\n\nWithin the \\(k\\)th class, the \\(p\\) predictors are independent\n\n\n\n\nAssuming features are not associated (not correlated), the likelihood becomes: \\[\nP(\\mathbf{X}|\\mathbf{Y}=k) = \\underbrace{P(x_1 |\\mathbf{Y}=k)}_{1\\text{st} \\text{ predictor}} \\times \\underbrace{P(x_2 |\\mathbf{Y}=k)}_{2\\text{nd} \\text{ predictor}} \\times \\ldots \\times \\underbrace{P(x_p |\\mathbf{Y}=k)}_{p\\text{-th} \\text{ predictor}}\n\\]\n\n\n\n\nThis means the posterior is given by: \\[\nP(\\mathbf{Y} = k| \\mathbf{X} = x) = \\frac{\\quad\\quad P(k) \\times P(x_1 |\\mathbf{Y}=k) \\times P(x_2 |\\mathbf{Y}=k) \\times \\ldots \\times P(x_p |\\mathbf{Y}=k)}{\\sum_{l=1}^K{P(l) \\times P(x_1 |\\mathbf{Y}=l) \\times P(x_2 |\\mathbf{Y}=l) \\times \\ldots \\times P(x_p |\\mathbf{Y}=l)}}\n\\]"
  },
  {
    "objectID": "slides/week03_slides_part2.html#a-naive-approach-indeed",
    "href": "slides/week03_slides_part2.html#a-naive-approach-indeed",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "A naive approach indeed",
    "text": "A naive approach indeed\n\n\nThis may all look very complicated but it is actually quite simple\n\n\n\n\nIf data is discrete (categorical), you just count the proportion of each category.\n\nExample:\n\\[\nP(\\mathbf{Y} = k| \\mathbf{X}_j = x_j) =\n\\begin{cases}\n0.32 & \\text{if } x_j = 1\\\\\n0.55 & \\text{if } x_j = 2\\\\\n0.13 & \\text{if } x_j = 3\n\\end{cases}\n\\]\n\n\n\nIf data is continuous, use a histogram as an estimate for the true density of \\(x_p\\)\n\nAlternatively, use a kernel density estimator"
  },
  {
    "objectID": "slides/week03_slides_part2.html#default-yes-or-no",
    "href": "slides/week03_slides_part2.html#default-yes-or-no",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Default: Yes or No?",
    "text": "Default: Yes or No?\n\n\nWe have looked at how the probabilities (risk of default) change according to the value of predictors\nBut in practice we need to decide whether the risk is too high or tolerable\nIn our example, we might want to ask:\n\n\n\n\n“Will this person default on their credit card? YES or NO?”"
  },
  {
    "objectID": "slides/week03_slides_part2.html#default-yes-or-no-1",
    "href": "slides/week03_slides_part2.html#default-yes-or-no-1",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Default: Yes or No?",
    "text": "Default: Yes or No?\n\n\nHow would you classify the following customers?\n\n\nCode\nlibrary(tidyverse)\n\nfull_model <- \n  glm(default ~ ., data=ISLR2::Default, family=binomial)\n\nset.seed(40)\nsample_customers <- \n  ISLR2::Default %>% \n  slice(9986, 9908, 6848, 9762, 9979, 7438)\npred <- predict(full_model, sample_customers, type=\"response\")\n# Format it as percentage\nsample_customers$prediction <- \n  sapply(pred, function(x){sprintf(\"%.2f %%\", 100*x)})\nsample_customers\n\n\n  default student   balance   income prediction\n1      No      No  842.9494 39957.13     0.27 %\n2      No      No 1500.5721 39891.86    10.53 %\n3     Yes     Yes 1957.1203 18805.95    44.23 %\n4      No      No 1902.1499 35008.67    53.71 %\n5     Yes      No 2202.4624 47287.26    87.09 %\n6     Yes     Yes 2461.5070 11878.56    93.34 %\n\n\n\n\n\n\n\n\n\nImage created with the DALL·E algorithm using the prompt: ‘35mm macro photography of a robot holding a question mark card, white background’\n\n\nFull model expression: \\[\n\\hat{y} \\approxeq \\frac{e^{-10.87 - 0.65\\times\\text{student[Yes]} + 5.74 \\times 10^{-3}\\times\\text{balance} + 3\\times 10^{-6}\\times\\text{income}}}{1 + e^{-10.87 - 0.65\\times\\text{student[Yes]} + 5.74 \\times 10^{-3}\\times\\text{balance} + 3\\times 10^{-6}\\times\\text{income}}}\n\\]"
  },
  {
    "objectID": "slides/week03_slides_part2.html#default-yes-or-no-2",
    "href": "slides/week03_slides_part2.html#default-yes-or-no-2",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Default: Yes or No?",
    "text": "Default: Yes or No?\n\n\nHow would you classify the following customers?\n\n\nCode\nlibrary(tidyverse)\n\nfull_model <- \n  glm(default ~ ., data=ISLR2::Default, family=binomial)\n\nset.seed(40)\nsample_customers <- \n  ISLR2::Default %>% \n  slice(9986, 9908, 6848, 9762, 9979, 7438)\npred <- predict(full_model, sample_customers, type=\"response\")\n# Format it as percentage\nsample_customers$prediction <- \n  sapply(pred, function(x){sprintf(\"%.2f %%\", 100*x)})\nsample_customers\n\n\n  default student   balance   income prediction\n1      No      No  842.9494 39957.13     0.27 %\n2      No      No 1500.5721 39891.86    10.53 %\n3     Yes     Yes 1957.1203 18805.95    44.23 %\n4      No      No 1902.1499 35008.67    53.71 %\n5     Yes      No 2202.4624 47287.26    87.09 %\n6     Yes     Yes 2461.5070 11878.56    93.34 %\n\n\n\n\nIf we set our threshold \\(= 50\\%\\), we get the following confusion matrix:\n\n\n\n\n\nActual\n\n\n\n\n\nPredicted\nNo\nYes\n\n\nNo\n2\n1\n\n\nYes\n1\n2\n\n\n\n\n\n\nIf we set our threshold \\(= 40\\%\\), we get the following confusion matrix:\n\n\n\n\n\nActual\n\n\n\n\n\nPredicted\nNo\nYes\n\n\nNo\n2\n0\n\n\nYes\n1\n3\n\n\n\n\n\n\n\n\n\nWhich of the two is more accurate?"
  },
  {
    "objectID": "slides/week03_slides_part2.html#thresholds",
    "href": "slides/week03_slides_part2.html#thresholds",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Thresholds",
    "text": "Thresholds\n\nWhen making predictions about classes, we always have to make decisions.\nThresholds, applied to the predicted probability scores, are a way to decide whether to favour a particular class over another\n⏭️ Next, we will explore several metrics that can help us decide whether our classification model is good or bad."
  },
  {
    "objectID": "slides/week03_slides_part2.html#confusion-matrix",
    "href": "slides/week03_slides_part2.html#confusion-matrix",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\nLet’s take another look at the confusion matrix. We can think of the numbers in each cell as the following: \n\n\n\n\n\n\n\n\n\n\nActual\n\n\n\n\n\nPredicted\nNo\nYes\n\n\nNo\nTrue Negative (TN)\nFalse Negative (FN)\n\n\nYes\nFalse Positive (FP)\nTrue Positive (TP)\n\n\n\n\n\nIdeally, we would have no False Negatives and no False Positives but, of course, that is never the case."
  },
  {
    "objectID": "slides/week03_slides_part2.html#classification-metrics-1",
    "href": "slides/week03_slides_part2.html#classification-metrics-1",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Classification metrics",
    "text": "Classification metrics\n\n\nIt is convenient to aggregate those quantities into a few other metrics\nTwo of the most common ones are called sensitivity and specificity\n\n\n\n\\[\n\\begin{align}\n\\text{Sensitivity} &= \\text{True Positive Rate (TPR)} = \\frac{TP}{P} \\\\\n\\text{Specificity} &= \\text{True Negative Rate (TNR)} = \\frac{TN}{N}\n\\end{align}\n\\]\n\n\n\nAnother common one is accuracy:\n\n\\[\n\\text{Accuracy} = \\frac{TP + TN}{P + N}\n\\]\n\n\nA good model has high sensitivity and high specificity and high accuracy.\n\n\n\nThere are many other ways to assess the results of a classification model"
  },
  {
    "objectID": "slides/week03_slides_part2.html#which-threshold-is-better",
    "href": "slides/week03_slides_part2.html#which-threshold-is-better",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Which threshold is better?",
    "text": "Which threshold is better?\n\n\n\n📝 Now, looking at the logistic regression model we built for the entire dataset, work out the sensitivity, specificity and accuracy of the following confusion matrices:\n\n\n\n\n\n\n\nPractice\n\n\n\n⏲️ 5 min to work out the math\n🗳️ Vote on your preferred threshold (on  Slack)\n\n\n\n\n\n\\(\\text{Threshold} = 50\\%\\):\n\n\n\n\nActual\n\n\n\n\n\nPredicted\nNo\nYes\n\n\nNo\n9627\n228\n\n\nYes\n40\n105\n\n\n\n\n\\(\\text{Threshold} = 40\\%\\):\n\n\n\n\nActual\n\n\n\n\n\nPredicted\nNo\nYes\n\n\nNo\n9588\n199\n\n\nYes\n79\n134"
  },
  {
    "objectID": "slides/week03_slides_part2.html#meet-the-roc-curve",
    "href": "slides/week03_slides_part2.html#meet-the-roc-curve",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Meet the ROC curve",
    "text": "Meet the ROC curve\n\n\n\nThe Receiver Operating Characteristic (ROC) curve is another way to assess the model.\nIt shows how sensitivity and specificity change as we vary the threshold from 0 to 1 (threshold not shown).\n\n\n\n\n\n\n\n\nAsk: how would an ideal curve look like?"
  },
  {
    "objectID": "slides/week03_slides_part2.html#generalisation-problems",
    "href": "slides/week03_slides_part2.html#generalisation-problems",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Generalisation problems",
    "text": "Generalisation problems\n\n\nThe data used to train algorithms is called training data\nOften, we want to use the fitted models to make predictions on new previously unseen data\n\n\n\n\n\n\n\n\n\nImportant\n\n\n⚠️ A model that performs well on training data will not necessarily perform well on new data ⚠️\n\n\n\n\n\n\nTo make a robust assessment of our model, we have to split the data in two:\n\nthe training data and\nthe test data\n\nWe do NOT use the test data to fit the model\nWe will come back to this next week, this is the topic of 🗓️ Week 04."
  },
  {
    "objectID": "slides/week03_slides_part2.html#inappropriate-reliance-on-metrics",
    "href": "slides/week03_slides_part2.html#inappropriate-reliance-on-metrics",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "Inappropriate reliance on metrics",
    "text": "Inappropriate reliance on metrics\n\n\nAccuracy can be very misleading when classes are imbalanced\nConsider the following model: \\(\\hat{y} = \\text{Yes}\\) (always)\n\nOnly \\(3\\%\\) of customers default on their credit cards\nTherefore, this model would have a \\(97\\%\\) accuracy!\nIt is correct ninety-seven percent of times. But is it a good model?\n\n🙅‍♂️ NO!\n\n\nSimilarly, you have to ask yourself about the usefulness of any other metric\n\nIs True Positive Rate more or less important than True Negative Rate for the classification problem at hand?\nWhy? Why not?\n\nUltimately, it boils down to how you plan to use this model afterwards."
  },
  {
    "objectID": "slides/week03_slides_part2.html#references",
    "href": "slides/week03_slides_part2.html#references",
    "title": "🗓️ Week 03:Classifiers - Part II",
    "section": "References",
    "text": "References\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York NY: Springer. https://www.statlearning.com/.\n\n\n\n\n\nDS202 - Data Science for Social Scientists 🤖 🤹"
  }
]