[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LSE DS202 (2022/2023)",
    "section": "",
    "text": "Important\n\n\n\nRecently updated pages:\n(27/11/2022)\n\nMachine Learning Cheatsheet (Work-In-Progress )\n\n(23/11/2022)\n\nâœ”ï¸ Summative Problem Set 01 | Solutions\n\n\n\n\nğŸ“‘ Course Brief\nFocus: learn and understand the most fundamental machine learning algorithms\nHow: practical use of machine learning techniques and its metrics, applied to relevant data sets\n\n\nğŸ¯ Learning Objectives\n\nUnderstand the fundamentals of the data science approach, with an emphasis on social scientific analysis and the study of the social, political, and economic worlds;\nUnderstand how classical methods such as regression analysis or principal components analysis can be treated as machine learning approaches for prediction or for data mining.\nKnow how to fit and apply supervised machine learning models for classification and prediction.\nKnow how to evaluate and compare fitted models, and to improve model performance.\nUse applied computer programming, including the hands-on use of programming through course exercises.\nApply the methods learned to real data through hands-on exercises.\nIntegrate the insights from data analytics into knowledge generation and decision-making.\nUnderstand an introductory framework for working with natural language (text) data using techniques of machine learning.\nLearn how data science methods have been applied to a particular domain of study (applications).\n\n\n\nğŸ§‘ğŸ»â€ğŸ« Our Team\n\nTeacher ResponsibleTeaching StaffStudentsAdministrative Support\n\n\n\nDr.Â Jonathan Cardoso-Silva  Assistant Professorial Lecturer  LSE Data Science Institute ğŸ“§ J.Cardoso-Silva at lse dot ac dot uk\nOffice Hours:\n\n15-min slots, on Wednesdays 13:00 â€“ 15:00 during Term Time\nRoom: PEL 9.01c (check out the ğŸ—ºï¸ campus map)\nBook via Student Hub up to 12 hours in advance\n\n\n\n\nDr.Â Stuart Bramwell  ESRC Postdoctoral Fellow  Department of Methodology PhD in Politics (Oxford) ğŸ“§ s.bramwell at lse dot ac dot uk\n\nYijun Wang  Guest Teacher at the LSE Data Science Institute PhD candidate in Health Informatics (KCL)  MSc in Data Science (KCL)  ğŸ“§ y.wang508 at lse dot ac dot uk\n\nMustafa Can Ozkan  Guest Teacher at the LSE Data Science Institute PhD candidate in the Spacetime Lab (UCL)  MSc in Transport (Imperial/UCL)  ğŸ“§ M.C.Ozkan at lse dot ac dot uk\n\nXiaowei Gao  Guest Teacher at the LSE Data Science Institute PhD candidate in the Spacetime Lab (UCL)  MSc in Data Science (KCL)  ğŸ“§ X.Gao23 at lse dot ac dot uk\n\nAnton Boichenko  Guest Teacher at the LSE Data Science Institute Product Developer at Decoded  MSc in Applied Social Data Science (LSE)  ğŸ“§ A.Boichenko at lse dot ac dot uk\n\n\n\nZhang Ruishan (Yoyo)  1st Year BSc Economics Student  Course Representative for DS202\n\nRachitha Raghuram  2nd Year BSc Economics Student  Course Representative for DS202\n\n\n\nNathaniel Ocquaye  Teaching Support and Events Officer Office: PEL 9.01 Email: DSI.UG at lse dot ac dot uk\n\nJill Beattie  Institute Coordinator Office: PEL 9.01E Tel: +44 (0) 20 7955 7759 Email: DSI.Admin at lse dot ac dot uk\n\n\n\n\n\nClass Groups\n\nGroup 01\n\nğŸ“† Mondays\nâŒš 09:00 â€” 10:30\nğŸ“ PAN.1.03\nğŸ§‘â€ğŸ« Xiaowei\n\n\n\nGroup 02\n\nğŸ“† Mondays\nâŒš 10:30 â€” 12:00\nğŸ“ PAN.1.03\nğŸ§‘â€ğŸ« Xiaowei\n\n\n\nGroup 03\n\nğŸ“† Mondays\nâŒš 13:00 â€” 14:30\nğŸ“ MAR.1.09\nğŸ§‘â€ğŸ« Stuart\n\n\n\nGroup 04\n\nğŸ“† Fridays\nâŒš 16:00 â€” 17:30\nğŸ“ NAB.1.04\nğŸ§‘â€ğŸ« Stuart\n\n\n\nGroup 05\n\nğŸ“† Mondays\nâŒš 09:00 â€” 10:30\nğŸ“ 32L.LG.11\nğŸ§‘â€ğŸ« Mustafa\n\n\n\nGroup 06\n\nğŸ“† Mondays\nâŒš 10:30 â€” 12:00\nğŸ“ 32L.LG.11\nğŸ§‘â€ğŸ« Mustafa\n\n\n\nGroup 07\n\nğŸ“† Fridays\nâŒš 09:30 â€” 11:00\nğŸ“ CBG.2.06\nğŸ§‘â€ğŸ« Yijun"
  },
  {
    "objectID": "blog/main.html",
    "href": "blog/main.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\nMachine Learning Cheatsheet\n\n\n6 min\n\n\n\nweek09\n\n\ntidymodels\n\n\ncheatsheet\n\n\n\n\n\n\n\n27 November 2022\n\n\n\n\n\n\n\n\n\n\n\nIdentifying outliers in linear model\n\n\n2 min\n\n\n\nweek03\n\n\nlinear regression\n\n\noutliers\n\n\n\n\n\n\n\n23 October 2022\n\n\n\n\n\n\n\n\n\n\n\nDonâ€™t give p-values more credit than they deserve\n\n\n1 min\n\n\n\nweek04\n\n\np-values\n\n\n\n\n\n\n\n19 October 2022\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Bayesâ€™ Theorem\n\n\n0 min\n\n\n\nweek03\n\n\nbayesian statistics\n\n\nequations\n\n\n\n\n\n\n\n17 October 2022\n\n\n\n\n\n\n\n\n\n\n\nWhat happens to R-squared when we add more predictors?\n\n\n3 min\n\n\n\nweek02\n\n\nR-squared\n\n\nequations\n\n\n\n\n\n\n\n10 October 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/bayes-theorem.html",
    "href": "blog/posts/bayes-theorem.html",
    "title": "Understanding Bayesâ€™ Theorem",
    "section": "",
    "text": "This video will help you grasp the intuition behind Bayesâ€™ Theorem.\nOn Week 03 (Part II), I showed you the following equation and mentioned that it forms the basis for one of the algorithms we explore in this course, Naive Bayes.\n\\[\nP(\\mathbf{Y} = k | \\mathbf{X} = x) = \\frac{P(k)P(\\mathbf{X}|\\mathbf{Y}=k)}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}\n\\]\nThe following video might help you understand the intuition behind the equation. Enjoy!"
  },
  {
    "objectID": "blog/posts/models-cheatsheet.html",
    "href": "blog/posts/models-cheatsheet.html",
    "title": "Machine Learning Cheatsheet",
    "section": "",
    "text": "â€œHow do I do ... using tidyverse/tidymodels?â€ Check this blog post for a compilation of tips and code snippets.\nWe started this course using just base R but since ~ Week 05 we started relying a lot more on tidyverse and tidymodels, as these tools proved to be more intuitive. Still, there might be a few gaps in your understanding. So, refer to this page whenever you need to learn or revisit how to train a model or run a resampling technique using these packages.\nThis page will be updated continuously. Let me know if you would like me to add something here."
  },
  {
    "objectID": "blog/posts/models-cheatsheet.html#setup",
    "href": "blog/posts/models-cheatsheet.html#setup",
    "title": "Machine Learning Cheatsheet",
    "section": "Setup",
    "text": "Setup\nWhat packages do you need?\nIn most cases, all you need is:\nlibrary(tidyverse)\nlibrary(tidymodels) \n\n\n\n\n\n\nImportant\n\n\n\nKeep in mind that tidymodels is just a convenient wrapper for a multitude of other Machine Learning algorithms that exist in a myriad of other R packages. If the algorithm you want to use is not installed by default, you might get an error message saying that you need to install.packages(...) a package.\nYou can browse all supported algorithms on this page."
  },
  {
    "objectID": "blog/posts/models-cheatsheet.html#regression",
    "href": "blog/posts/models-cheatsheet.html#regression",
    "title": "Machine Learning Cheatsheet",
    "section": "Regression",
    "text": "Regression\nWe use regression when we want to predict a numeric variable (for example: number of accidents, house price, etc.). If the target variable is not numeric you will get an error, or worse you might get counterintuitive results.\n\nLinear RegressionDecision TreeSupport Vector Machine\n\n\n\nMain Function: linear_reg()\n\nHow to train:\nmodel <-\n    linear_reg() %>%\n    set_mode(\"regression\") %>%\n    fit(Today ~ ., data=ISLR2::Smarket %>% select(-Direction))\nGet a summary of the fitted model:\nsummary(model$fit)\n\n\nMake Predictions\n\n\nYou can use your model to predict the outcome of a given data set. We use the augment() function for that:\ndf_augmented <- augment(model, df)\nKeep in mind that the dataframe MUST contain the same columns used to train the model. It is also common to â€œaugmentâ€ the same data that was used for training the model. This is how we check if our model fits the data well.\n\n\n\nTry a different implementation\n\n\nBy default, linear_reg() runs the same lm() algorithm we learned about in ğŸ’» Week 03 - Lab but if you want you can use an alternative implementation from another R package. For example, to use a Bayesian implementation of linear regression (from stan), use:\nmodel <-\n    linear_reg() %>%\n    set_engine(\"stan\") %>% \n    set_mode(\"regression\") %>%\n    fit(Today ~ ., data=ISLR2::Smarket %>% select(-Direction))\nRead about the alternatives in the linear_reg() documentation.\n\n\n\nDiagnostic Plots\n\n\nHow well did the model fit the data?\npar(mfrow = c(2, 2))\nplot(model$fit)\n\n\n\n\nMain Function: decision_tree()\nRequired libraries: rpart and rpart.plot\n\nHow to train:\nmodel <-\n    decision_tree() %>%\n    set_mode(\"regression\") %>%\n    fit(Today ~ ., data=ISLR2::Smarket %>% select(-Direction))\nGet a summary of the fitted model:\nsummary(model$fit)\nPlot the model\nlibrary(rpart.plot)\n\nrpart.plot(model$fit, roundint=FALSE)\n\n\nMake Predictions\n\n\nYou can use your model to predict the outcome of a given data set. We use the augment() function for that:\ndf_augmented <- augment(model, df)\nKeep in mind that the dataframe MUST contain the same columns used to train the model. It is also common to â€œaugmentâ€ the same data that was used for training the model. This is how we check if our model fits the data well.\n\n\n\nTry a different implementation\n\n\nBy default, decision_tree() runs the algorithm contained in the rpart package. This is the same package we learned about in ğŸ’» Week 07 - Lab but if you want you can use an alternative implementation from another R package. For example, to use a C5.0:\nmodel <-\n    linear_reg() %>%\n    set_engine(\"C5.0\") %>% \n    set_mode(\"regression\") %>%\n    fit(Today ~ ., data=ISLR2::Smarket %>% select(-Direction))\nRead about the alternatives in the decision_tree() documentation.\n\n\n\nChange parameters\n\n\nThere are three main parameters you can tweak when using the rpart engine:\n\ncost_complexity\ntree_depth\nmin_n\n\ntidymodels might use default values or attempt to guess the best values for the parameters. If you want to choose parameter values explicitly, pass those to the decision_tree() function. For example:\nmodel <-\n    decision_tree(tree_depth= integer(3)) %>%\n    set_mode(\"regression\") %>%\n    fit(Today ~ ., data=ISLR2::Smarket %>% select(-Direction))\nNote that the availability of parameters changes from engine to engine. The decision tree in the C5.0 engine, for example, has one tuning parameter, min_n. Always check the documentation!\n\n\n\n\nSupported kernels:\n\nRadial Basis Function (svm_rbf())\nPolynomial (svm_poly())\nLinear (svm_linear())\n\nDonâ€™t know what kernels are? Check ğŸ—“ï¸ Week 05 - Part II\nRequired libraries: kernlab and LiblineaR (for the linear kernel).\n\nHow to train:\nmodel <-\n    svm_rbf() %>%\n    set_mode(\"regression\") %>%\n    fit(Today ~ ., data=ISLR2::Smarket %>% select(-Direction))\nGet a summary of the fitted model:\nmodel\n\n\nMake Predictions\n\n\nYou can use your model to predict the outcome of a given data set. We use the augment() function for that:\ndf_augmented <- augment(model, df)\nKeep in mind that the dataframe MUST contain the same columns used to train the model. It is also common to â€œaugmentâ€ the same data that was used for training the model. This is how we check if our model fits the data well.\n\n\n\nPlot the decision space\n\nTHIS ONLY WORKS WITH TWO PREDICTORS!\nStep 1: Check the min and max values of the two SELECTED predictors\nReplace <predictor1> and <predictor2> with the name of your selected predictors.\ndata %>% \n    select(c(<predictor1>, <predictor2>)) %>%\n    summary()\nIdentify the min and max values of each predictor.\nStep 2: Create a simulated dataset\nYou will need to find a suitable step_val1 and step_val2. Play with different values until you find you that you like.\nsim.data <- \n  crossing(<predictor1>   = seq(min_val_predictor1, max_val_predictor1, step_val1),\n           <predictor2>   = seq(min_val_predictor2, max_val_predictor2, step_val2))\nStep 3: Run the fitted model on this simulated data\nsim.data <- augment(model, sim.data)\nStep 4: Run the fitted model on the data used to train the model\nplot_df <- augment(model, data) \nStep 5: Build the plot\nRemember to replace <predictor1> and <predictor2> with the name of your selected predictors.\ng <- (\n  plot_df %>%   \n    ggplot()\n  \n    ## Tile the background of the plot with SVM predictions\n    + geom_tile(data = sim.data, aes(x=<predictor1>, y=<predictor2>, fill = .pred), alpha = 0.45)\n  \n    ## Actual data\n    + geom_point(aes(x=<predictor1>, y=<predictor2>), size=2.5, stroke=0.95)\n  \n    ## Define X and Os\n    + scale_shape_manual(values = c(4, 1))\n    + scale_fill_viridis_c()\n    + scale_color_manual(values=c(\"black\", \"red\"))\n    + scale_alpha_manual(values=c(0.1, 0.7))\n    \n    ## (OPTIONAL) Customizing the colours and theme of the plot\n    + theme_minimal()\n    + theme(panel.grid = element_blank(), \n            legend.position = 'bottom', \n            plot.title = element_text(hjust = 0.5))\n)\n\ng\n\n\n\nTry a different kernel\n\n\nSimply replace svm_rbf() with one the other kernels available in tidymodels (svm_poly() or svm_linear()).\nNote that the availability of parameters changes from kernel to kernel. Always check the documentation!\n\n\n\nChange parameters\n\n\nThere are two main parameters you can tweak when using the svm_rbf() function:\n\ncost\nrbf_sigma\n\ntidymodels might use default values or attempt to guess the best values for the parameters. If you want to choose parameter values explicitly, pass those to the svm_rbf() function. For example:\nmodel <-\n    svm_rbf(cost= integer(1), rbf_sigma = 0.2) %>%\n    set_mode(\"regression\") %>%\n    fit(Today ~ ., data=ISLR2::Smarket %>% select(-Direction))\nNote that the availability of parameters changes from kernel to kernel. Always check the documentation!"
  },
  {
    "objectID": "blog/posts/models-cheatsheet.html#classification",
    "href": "blog/posts/models-cheatsheet.html#classification",
    "title": "Machine Learning Cheatsheet",
    "section": "Classification",
    "text": "Classification"
  },
  {
    "objectID": "blog/posts/models-cheatsheet.html#clustering",
    "href": "blog/posts/models-cheatsheet.html#clustering",
    "title": "Machine Learning Cheatsheet",
    "section": "Clustering",
    "text": "Clustering"
  },
  {
    "objectID": "blog/posts/models-cheatsheet.html#dimensionality-reduction",
    "href": "blog/posts/models-cheatsheet.html#dimensionality-reduction",
    "title": "Machine Learning Cheatsheet",
    "section": "Dimensionality Reduction",
    "text": "Dimensionality Reduction"
  },
  {
    "objectID": "blog/posts/outliers.html",
    "href": "blog/posts/outliers.html",
    "title": "Identifying outliers in linear model",
    "section": "",
    "text": "TLDR\n\n\n\nHere you will find R code to single out points with high â€œstudentizedâ€ residuals\nDuring Week 03 lab, we looked at diagnostic plots we can generate in R to obtain insights about the fit of a linear model.\nImport required libraries\nFit a linear model then produce diagnostic plots\nA frequent question we got in the labs was about the first of these plots, â€œResiduals vs Fittedâ€."
  },
  {
    "objectID": "blog/posts/outliers.html#residual-vs-fitted-plot",
    "href": "blog/posts/outliers.html#residual-vs-fitted-plot",
    "title": "Identifying outliers in linear model",
    "section": "Residual vs Fitted plot",
    "text": "Residual vs Fitted plot\nIdeally, the residual plot, also called the null residual plot, should show a random scatter of points centered around 0 and forming an approximately constant width band.\n\nbase Rtidyverse\n\n\n\n\nCode\n# Add dots\nplot(predict(lm.fit), rstudent(lm.fit), \n     xlab=\"Fitted values\",\n     ylab=\"Residuals\",\n     main=\"Residuals vs Fitted\")\n\n# Add lines\nabline(h = 3, lwd = 5,col = 'red')\nabline(h = 0, lwd = 5,col = 'yellow')\n\n\n\n\n\n\n\n\n\nCode\nplot_df <- data.frame(fitted_vals=predict(lm.fit),\n                      residuals=rstudent(lm.fit))\n\ng <- ggplot(plot_df, aes(x=fitted_vals, y=residuals)) +\n\n    # Add dots\n     geom_point(alpha=0.4, size=3.5) +\n     xlab(\"Fitted values\") +\n     ylab(\"Residuals\") +\n     ggtitle(\"Residuals vs Fitted\") +\n\n    # Add lines\n    geom_hline(yintercept=0, size=1.5, color='yellow') +\n    geom_hline(yintercept=3, size=1.5, color='red') +\n\n    # Customising the plot +\n     theme_bw()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nâ„¹ Please use `linewidth` instead.\n\n\nCode\ng"
  },
  {
    "objectID": "blog/posts/outliers.html#selecting-outliers",
    "href": "blog/posts/outliers.html#selecting-outliers",
    "title": "Identifying outliers in linear model",
    "section": "Selecting outliers",
    "text": "Selecting outliers\nHow do you identify the data points that have a high value of residuals (potential outliers)?\nTo produce this plot, we used the function rstudent() to calculate the so-called â€œstudentizedâ€ residuals. This function returns a vector with the same length as the number of data points:\n\nstudentized_residuals <- rstudent(lm.fit)\nlength(studentized_residuals)\n\n[1] 506\n\n\n\nnrow(Boston)\n\n[1] 506\n\n\nWe can use this information to filter and select those data points that produced a studentized-residual above 3:\n\ndf_potential_outliers <- Boston[studentized_residuals > 3, ]\ndf_potential_outliers\n\n       crim zn indus chas   nox    rm   age    dis rad tax ptratio lstat medv\n187 0.05602  0  2.46    0 0.488 7.831  53.6 3.1992   3 193    17.8  4.45 50.0\n215 0.28955  0 10.59    0 0.489 5.412   9.8 3.5875   4 277    18.6 29.55 23.7\n226 0.52693  0  6.20    0 0.504 8.725  83.0 2.8944   8 307    17.4  4.63 50.0\n258 0.61154 20  3.97    0 0.647 8.704  86.9 1.8010   5 264    13.0  5.12 50.0\n263 0.52014 20  3.97    0 0.647 8.398  91.5 2.2885   5 264    13.0  5.91 48.8\n268 0.57834 20  3.97    0 0.575 8.297  67.0 2.4216   5 264    13.0  7.44 50.0\n372 9.23230  0 18.10    0 0.631 6.216 100.0 1.1691  24 666    20.2  9.53 50.0\n373 8.26725  0 18.10    1 0.668 5.875  89.6 1.1296  24 666    20.2  8.88 50.0\n\n\nData frame df_potential_outliers above contain all potential outliers according to this criteria. In a real-life setting, you would check the values of these data points in comparison with the rest of the dataset to understand what makes them different."
  },
  {
    "objectID": "blog/posts/outliers.html#what-to-do-next",
    "href": "blog/posts/outliers.html#what-to-do-next",
    "title": "Identifying outliers in linear model",
    "section": "What to do next?",
    "text": "What to do next?\nIf the process that generated the dataset is indeed linear, it is possible that these are â€œtrueâ€ outliers, rare cases that deviate from the norm. But often, this indicates that a linear model is not able to capture all the nuances present in the data. Maybe the data generating procedure is nonlinear? Or maybe it depends on other features that are not present in your dataset?\nAs for your next actions, it all depends on what you plan to do with this model or how much risk you can take by predicting data similar to these edge cases. If you do not want or cannot afford to ignore these errors, you can try to collect more data, fit more complex algorithms, or talk to domain experts to try to understand these cases a bit more."
  },
  {
    "objectID": "blog/posts/p-values.html",
    "href": "blog/posts/p-values.html",
    "title": "Donâ€™t give p-values more credit than they deserve",
    "section": "",
    "text": "TLDR\n\n\n\nThere are dangers in relying too much on p-values.\n\n\nWhen you run a linear or logistic regression and find out that a regression coefficient has a low associated p-value, it is tempting to scream THIS FEATURE IS SIGNIFICANT AND I CAN PROVE!\nIn reality, although p-values might suggest a non-zero relationship between variables, you shouldnâ€™t judge the performance or explainability of a model simply by the p-values of coefficients, nor the p-value associated with the full model (say, the F-statistic).\nWhen assessing a model, look beyond goodness-of-fit. Perform train/test splits, cross-validation, bootstrap, and use appropriate measures of success to the problem you have at hand. Come to ğŸ—“ï¸ Week 04 workshop (the lecture) this Friday 21 October to learn more about this.\nThe reason I am saying all this is because p-values are very easy to hack. In fact, there is even a term for misuse of p-values in the scientific literature: p-hacking.\nWhere do I inform myself about this?\nI have separated a list of articles and commentaries about this topic. Check them out:\n\nNahm, Francis Sahngun. 2017. â€œWhat the P Values Really Tell Us.â€ The Korean Journal of Pain 30 (4): 241.\nAmrhein, Valentin, Sander Greenland, and Blake McShane. 2019. â€œScientists Rise up Against Statistical Significance.â€ Nature 567 (7748): 305â€“7.\nAschwanden, Christie. 2015. â€œScience Isnâ€™t Broken.â€ FiveThirtyEight.\nSterne, Jonathan A C, and George Davey Smith. 2001. â€œSifting the Evidenceâ€”Whatâ€™s Wrong with Significance Tests?â€ BMJ : British Medical Journal 322 (7280): 226â€“31.\n\nğŸ’¡ If you are in a hurry and want to read just ONE thing, read the â€œScience Isnâ€™t Broken.â€ piece at FiveThirtyEight. They have a cool visualisation to illustrate the problem."
  },
  {
    "objectID": "blog/posts/r-squared.html",
    "href": "blog/posts/r-squared.html",
    "title": "What happens to R-squared when we add more predictors?",
    "section": "",
    "text": "TLDR\n\n\n\nWhen using OLS, \\(R^2\\) will always stay the same or increase you add more features to a linear regression, even if those features are â€œuselessâ€."
  },
  {
    "objectID": "blog/posts/r-squared.html#about-r2",
    "href": "blog/posts/r-squared.html#about-r2",
    "title": "What happens to R-squared when we add more predictors?",
    "section": "About \\(R^2\\)",
    "text": "About \\(R^2\\)\nAs we saw briefly on Week 02 and as defined in Chapter 3 of the textbook (James et al. 2021), \\(R^2\\) represents the proportion of variability in the response variable that can be explained using the dependent variables (features). More specifically:\n\\[\nR^2 = 1 - \\frac{\\operatorname{RSS}}{\\operatorname{TSS}}\n\\]\nwhere \\(\\operatorname{TSS}\\) represents the Total Sum of Squares:\n\\[\n\\operatorname{TSS} =  \\sum_i^n{\\left(y_i - \\bar{y}\\right)^2},\n\\]\nand \\(\\bar{y}\\) is the mean of \\(\\mathbf{y}\\).\nAs for the \\(\\operatorname{RSS}\\), it represents the Residual Sum of Squares.\nTo understand why \\(R^2\\) can never decrease if we add more features, I think it is useful to visualise linear regression in matrix format."
  },
  {
    "objectID": "blog/posts/r-squared.html#data-and-variables",
    "href": "blog/posts/r-squared.html#data-and-variables",
    "title": "What happens to R-squared when we add more predictors?",
    "section": "Data and Variables",
    "text": "Data and Variables\nSuppose you have the following data:\n\\[\n\\mathbf{X} = \\left[\n\\begin{array}{cc}\nx_{11}\\\\\nx_{21}\\\\\n\\vdots\\\\\nx_{n1}\n\\end{array}\\right],\n\\]\nthat is, there i only \\(p=1\\) feature for the \\(n\\) observations of data, and you want to fit a linear regression to attempt to predict the \\(\\mathbf{y}\\) column below:\n\\[\n\\mathbf{y} = \\left[\n\\begin{array}{c}\ny_{1}\\\\\ny_2\\\\\n\\vdots\\\\\ny_n\n\\end{array}\\right].\n\\]"
  },
  {
    "objectID": "blog/posts/r-squared.html#simple-linear-regression-in-matrix-format",
    "href": "blog/posts/r-squared.html#simple-linear-regression-in-matrix-format",
    "title": "What happens to R-squared when we add more predictors?",
    "section": "Simple Linear Regression in matrix format",
    "text": "Simple Linear Regression in matrix format\nWe can also represent the regression coefficients obtained by Ordinary Least Squares (OLS) in matrix format. \\(\\hat{\\boldsymbol{\\beta}}\\) is a \\(1 \\times (p+1)\\) matrix:\n\\[\n\\hat{\\boldsymbol{\\beta}} = \\left[\n\\begin{array}{c}\n\\hat{\\beta}_{0}\\\\\n\\hat{\\beta}_1\n\\end{array}\\right].\n\\]\nWhen thinking about regression this way, it is often useful to add an additional column of 1s to \\(\\mathbf{X}\\):\n\\[\n\\mathbf{X} = \\left[\n\\begin{array}{cc}\n1 & x_{11}\\\\\n1 & x_{21}\\\\\n1 & \\vdots\\\\\n1 & x_{n1}\n\\end{array}\\right],\n\\]\nso that:\n\\[\n\\hat{\\boldsymbol{\\beta}}\\mathbf{X} =\\left[\n\\begin{array}{c}\n\\hat{\\beta}_{0} \\times 1 + \\hat{\\beta}_1 \\times x_{11} \\\\\n\\hat{\\beta}_{0} \\times 1 + \\hat{\\beta}_1 \\times x_{21} \\\\\n\\vdots  \\\\\n\\hat{\\beta}_{0} \\times 1 + \\hat{\\beta}_1 \\times x_{n1}\\\\\n\\end{array}\\right] = \\hat{\\mathbf{y}}\n\\]\nrepresents the estimated values \\(\\hat{\\mathbf{y}}\\).\nResiduals can then be represented as \\(\\mathbf{e} = (\\mathbf{y} - \\hat{\\mathbf{y}})\\), or\n\\[\n\\mathbf{e} =\\left[\n\\begin{array}{c}\ny_{1} - (\\hat{\\beta}_{0} \\times 1 + \\hat{\\beta}_1 \\times x_{11}) \\\\\ny_{2} - (\\hat{\\beta}_{0} \\times 1 + \\hat{\\beta}_1 \\times x_{21} )\\\\\n\\vdots  \\\\\ny_{n} - (\\hat{\\beta}_{0} \\times 1 + \\hat{\\beta}_1 \\times x_{n1})\\\\\n\\end{array}\\right]\n\\]\nOLS provides an optimal way to find coefficients \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes \\(\\operatorname{RSS}\\).\nIn matrix format, the \\(\\operatorname{RSS}\\) can be represented as:\n\\[\n\\operatorname{RSS} = \\mathbf{e}^T\\mathbf{e}.\n\\]\nOLS will find an optimal solution to \\(\\text{minimise } \\operatorname{RSS}\\). Letâ€™s call it \\(\\operatorname{RSS}_{(p=1)}\\)."
  },
  {
    "objectID": "blog/posts/r-squared.html#what-if-i-add-another-feature",
    "href": "blog/posts/r-squared.html#what-if-i-add-another-feature",
    "title": "What happens to R-squared when we add more predictors?",
    "section": "What if I add another feature?",
    "text": "What if I add another feature?\nğŸ’¡ What would happen if I added a second feature and ran a regression with \\(p=2\\)?\nThink of the implications this has for the matrix of coefficients \\(\\hat{\\boldsymbol{\\beta}}\\) and matrix of residuals \\(\\mathbf{e}\\) if I decided to add a second column of data to \\(\\mathbf{X}\\).\nThe new extended feature matrix, letâ€™s call it \\(\\mathbf{X}'\\), would look like the following:\n\\[\n\\mathbf{X} = \\left[\n\\begin{array}{ccc}\n1 & x_{11}&x_{12}\\\\\n1 & x_{21}&x_{22}\\\\\n1 & \\vdots&\\vdots\\\\\n1 & x_{n1}&x_{n2}\n\\end{array}\\right],\n\\]\nOLS would find new coefficients, letâ€™s call them \\(\\hat{\\boldsymbol{\\beta}}'\\):\n\\[\n\\hat{\\boldsymbol{\\beta}} = \\left[\n\\begin{array}{c}\n\\hat{\\beta}'_{0}\\\\\n\\hat{\\beta}'_1\\\\\n\\hat{\\beta}'_2\n\\end{array}\\right].\n\\]\nSimilarly, the new \\(\\mathbf{e}'\\) could be represented as:\n\\[\n\\mathbf{e}' =\\left[\n\\begin{array}{c}\ny_{1} - (\\hat{\\beta}'_{0} \\times 1 + \\hat{\\beta}'_1 \\times x_{11} + \\hat{\\beta}'_2 \\times x_{12})\\\\\ny_{2} - (\\hat{\\beta}'_{0} \\times 1 + \\hat{\\beta}'_1 \\times x_{21} + \\hat{\\beta}'_2 \\times x_{22})\\\\\n\\vdots  \\\\\ny_{n} - (\\hat{\\beta}'_{0} \\times 1 + \\hat{\\beta}'_1 \\times x_{n1} + \\hat{\\beta}'_2 \\times x_{n2})\\\\\n\\end{array}\\right]\n\\]\nAgain, if we run OLS, the algorithm will find an optimal solution to the the residuals above, a minimum \\(\\operatorname{RSS}_{(p=2)}\\). What can we expect of \\(\\operatorname{RSS}_{(p=2)}\\) in relation to \\(\\operatorname{RSS}_{(p=1)}\\)? Letâ€™s think through some scenarios:\nIf it turned out that \\(\\hat{\\beta}'_0 = \\hat{\\beta}_0\\) and \\(\\hat{\\beta}'_1 = \\hat{\\beta}_1\\):\n\nIf \\(\\hat{\\beta}'_2 = 0\\), then we could conclude that \\(\\mathbf{e}' = \\mathbf{e}\\) and \\(\\operatorname{RSS}_{(p=2)} = \\operatorname{RSS}_{(p=1)}\\). That is, by minimising the sum of squares, OLS cannot find a better solution that involves \\(\\hat{\\beta}'_2\\).\nIf \\(\\hat{\\beta}'_2 \\neq 0\\) then notice that the new residuals are related to the previous ones: \\({e_i}' = (e_i - \\hat{\\beta}'_2x_{i2}) ~~\\forall i\\). That means \\(({e_i}')^2 \\geq e_i^2~~\\forall i\\) and therefore \\(\\operatorname{RSS}_{(p=2)} \\geq \\operatorname{RSS}_{(p=1)}\\).\n\nA similar logic applies to the case where \\(\\hat{\\beta}'_0 \\neq \\hat{\\beta}_0\\) and \\(\\hat{\\beta}'_1 \\neq \\hat{\\beta}_1\\) â€” check 1 for a more formal proof."
  },
  {
    "objectID": "blog/posts/r-squared.html#conclusion",
    "href": "blog/posts/r-squared.html#conclusion",
    "title": "What happens to R-squared when we add more predictors?",
    "section": "Conclusion",
    "text": "Conclusion\nBy adding a new feature, given the fact that OLS uses squares of errors, it is inevitable that \\(({e_i}')^2 \\geq e_i^2~~\\forall i\\). OLS will always find an equivalent or a lower \\(\\operatorname{RSS}\\) value. If you check the \\(R^2\\) definition once again, you will realise that \\(\\operatorname{TSS}\\) will never change â€” as it is not related to \\(\\mathbf{X}\\), only to \\(\\mathbf{y}\\) â€” only \\(\\operatorname{RSS}\\) can vary when you add new features. Since \\(\\operatorname{RSS}\\) can only decrease or stay the same, \\(R^2\\) will always increase or stay the same, never decrease, if you add more features.\nTo correct this misleading tendency of R-squared, an adjusted index has been proposed. The ajusted R-squared takes the number of features into account and it is what you should rely on when assessing the goodness-of-fit of a linear regression."
  },
  {
    "objectID": "main/assessments.html",
    "href": "main/assessments.html",
    "title": "âœï¸ Assessments",
    "section": "",
    "text": "Click on the assignments below to go to the Moodle page and submit your responses:\n\nâœï¸ Formative Problem Set (01) | W03-W05: Moodle link & associated RMarkdown file.\n\nYou can find model solutions here\n\nâœï¸ Summative Problem Set (01) | W05-W07: Moodle link & associated webpage.\n\nYou can find model solutions here\n\nâœï¸ Summative Problem Set (02) | W08-W10: Moodle link & associated webpage.\n\nUse Slack to report any bugs or to ask for clarification."
  },
  {
    "objectID": "main/assessments.html#problem-sets-60",
    "href": "main/assessments.html#problem-sets-60",
    "title": "âœï¸ Assessments",
    "section": "ğŸ“ Problem Sets (60%)",
    "text": "ğŸ“ Problem Sets (60%)\n\nSummative problem sets released on Weeks 5, 8 & 11.\nThese will have a similar style to the formative problem sets, a mix of R tasks and your written interpretation of the analyses.\nTypically, you will have 4-6 days to submit your solutions.\nEach of the three summative problem sets is worth 20% of the final mark, and will be graded on a 100 point scale."
  },
  {
    "objectID": "main/assessments.html#exam-40",
    "href": "main/assessments.html#exam-40",
    "title": "âœï¸ Assessments",
    "section": "âœï¸ Exam (40%)",
    "text": "âœï¸ Exam (40%)\n\nAn open-book online exam.\n3 hours for working on the exam + 1 hour for submission.\nExam questions will be comparable in style to the problem sets, but will require less writing of R code.\nThe exam questions will be released on Moodle sometime in the Exam Period. Our exact timeslot has not yet been confirmed by LSE Exams team."
  },
  {
    "objectID": "main/communication.html",
    "href": "main/communication.html",
    "title": "LSE DS202",
    "section": "",
    "text": "Find out how to reach out to your peers, teaching and administrative staff during this course.\n\n\n\nMost of our â€œinformalâ€ communication and interactions will happen through Slack.\nSlack is a platform used by many companies and institutions where teams can collaborate and communicate about a specific project.\nThere will be channels dedicated to discussing each weekâ€™s content, a channel for sharing useful links and events, plus a random channel to share random stuff about data science.\n\nYou will receive an invitation to join our Slack group via e-mail. Send an e-mail to Jon (J.Cardoso-Silva at lse dot ac dot uk) if you have not received an invite by the time of the first lecture.\nCheck out the following links to understand more about this tool:\n\nSlack tutorials\nCollaborate effectively in channels\n\n\n\n\n\nIt is probably a good idea to book office hours if:\n\nyou struggled with a technical or theoretical aspect of a problem set in the previous week,\nyou have queries about careers in data science,\nyou want guidance in how to apply data science to other things you are studying outside this course.\n\nCome prepared. You only have 15 minutes.\nAsk for help sooner rather than later.\nBook slots via StudentHub up to 12 hours in advance.\n\nâš ï¸ Reserve ğŸ“§ e-mail for formal requests: extensions, deferrals, etc. No need to e-mail to inform you will skip a class, for example."
  },
  {
    "objectID": "main/courserep.html",
    "href": "main/courserep.html",
    "title": "Course Representative",
    "section": "",
    "text": "The Data Science Institute (DSI) is excited to announce that the elections for Student Academic Representatives have now gone live! You can nominate yourself if you are taking DS105M or DS202 this Term, and all students get to to cast their votes anonymously on the candidates. The nomination and voting process will be conducted via Slack.\nAs a Student Academic Representative (Course Rep), youâ€™ll work with staff to ensure your peersâ€™ feedback is seen and acted on, and that student voices are represented in institutional decision-making. It is also a fantastic way to develop new skills, get to know the Institute better, and bolster your CV.\nYou can see the positions available in the table below:"
  },
  {
    "objectID": "main/courserep.html#engagement",
    "href": "main/courserep.html#engagement",
    "title": "Course Representative",
    "section": "Engagement",
    "text": "Engagement\n\nYou will have a direct channel of communication to voice suggestions, concerns and recommendations to the teaching and administrative staff at the DSI.\nYou will be allowed to use the DSI space on certain periods during the week.\nYou will have direct contact to PhD students and researchers who are visiting the DSI.\nYou will be the first to know about internal research projects available to undergraduate students."
  },
  {
    "objectID": "main/courserep.html#teaching-committees",
    "href": "main/courserep.html#teaching-committees",
    "title": "Course Representative",
    "section": "Teaching Committees",
    "text": "Teaching Committees\nAs a course rep, you will be invited to attend two Teaching Committees at the DSI during Michaelmas Term:\n\nthe first on Week 07\nthe second on Week 11\n\n\n\n\n\n\n\nWhat are Teaching Committees?\n\n\n\n\n\nTeaching Committees are official meetings where academic and professional service staff at the DSI discuss how the courses are going, devise adjustments for what is not going as planned, and plan new ways to boost recognition of innovative work of students enrolled in DSI courses. These meetings typically involve Lecturers, the Teaching Support Officer, the Communications Officer, the Institute Manager, and the Director of the DSI."
  },
  {
    "objectID": "main/courserep.html#nominations",
    "href": "main/courserep.html#nominations",
    "title": "Course Representative",
    "section": "1. Nominations",
    "text": "1. Nominations\n\nWrite down a short paragraph (max. ~50 words) explaining why you feel you would be great for the role!\nHead to your Slack group and post your short paragraph on the #student-rep channel.\nYou can nominate yourself anytime but no later than Monday 10 October 12 p.m.."
  },
  {
    "objectID": "main/courserep.html#voting",
    "href": "main/courserep.html#voting",
    "title": "Course Representative",
    "section": "2. Voting",
    "text": "2. Voting\n\nWe will set up an (anonymous) poll on the #student-rep channel using the app Polly\nNo one will be able to see who is winning/losing. Only when the poll closes will we know the results.\nAll students will be able to cast their votes anonymously on their favourite candidates from Tuesday 11 October â€“ Thursday 13 October (end of Week 03).\nResults will be announced on the #student-rep channel."
  },
  {
    "objectID": "main/syllabus.html",
    "href": "main/syllabus.html",
    "title": "LSE DS202",
    "section": "",
    "text": "ğŸ““ Syllabus\nA list of what happens every week.\nClick on each Weekâ€™s link for more information (slides, lab instructions, recommended resources, etc.).\n\n\n\n\n\n\nIntro\n\n\n\n\n\n\n\n\nğŸ—“ï¸ W01\n\n\nLecture\n\n\nIntroduction, Context & Key Concepts  (James et al. 2021, chap. 2)\n\n\n\n\nLab\n\n\nNo class this week. (Use this time to revisit basic R programming)\n\n\n\n\n\nSupervised Learning\n\n\n\n\n\n\n\n\nğŸ—“ï¸ W02\n\n\nLecture\n\n\nSimple and Multiple Linear Regression  (James et al. 2021, chap. 3)\n\n\n\n\nLab\n\n\nRevision of R: data structures, basic commands and some tidyverse\n\n\n\n\n\n\n\n\n\nğŸ—“ï¸ W03\n\n\nLecture\n\n\nClassifiers (Logistic Regression & Naive Bayes)  (James et al. 2021, chap. 4)\n\n\n\n\nLab\n\n\nLinear Regression (James et al. 2021, chap. 3)\n\n\n\n\nFormative\n\n\n\n\nWorth: knowledge!\n\n\nProblem set involving linear regression.\n\n\nRelease date: 15 October 2022\n\n(link to Moodle)\n\nDeadline: 25 October 2022 (Week 05)\n\n\n\n\n\n\n\n\n\n\n\nğŸ—“ï¸ W04\n\n\nLecture\n\n\nResampling methods  (James et al. 2021, chap. 5)\n\n\n\n\nLab\n\n\nClassification Methods (James et al. 2021, chap. 4)\n\n\n\n\n\n\n\n\n\nğŸ—“ï¸ W05\n\n\nLecture\n\n\nNon-linear algorithms (SVM & tree-based models)  (James et al. 2021, chaps. 8â€“9)\n\n\n\n\nLab\n\n\nCross-Validation and the Bootstrap (James et al. 2021, chap. 5)\n\n\n\n\nSummative\n\n\n\n\nWorth: 20% of final marks\n\n\nProblem set involving regression, classification and resampling\n\n\nRelease date: 28 October 2022\n\n\nDeadline: 9 November 2022 (Week 07)\n\n\n\n\n\n\n\nUnsupervised Learning\n\n\n\n\n\n\n\n\nğŸ—“ï¸ W07\n\n\nLecture\n\n\nUnsupervised Learning: Clustering  (James et al. 2021, chap. 12)\n\n\n\n\nLab\n\n\nTree-based models (James et al. 2021, chap. 8)\n\n\n\n\n\n\n\n\n\nğŸ—“ï¸ W08\n\n\nLecture\n\n\nUnsupervised Learning: Dimensionality Reduction  (James et al. 2021, chap. 12)\n\n\n\n\nLab\n\n\nSupport Vector Machine + tidymodels + recap of cross-validation (James et al. 2021, chaps. 5, 9)\n\n\n\n\nSummative\n\n\n\n\nWorth: 20% of final marks\n\n\nProblem set about unsupervised learning\n\n\nRelease date: 18 November 2022\n\n\nDeadline: 29 November 2022 (Week 10)\n\n\n\n\n\n\n\nApplications\n\n\n\n\n\n\n\n\nğŸ—“ï¸ W09\n\n\nLecture\n\n\nApplications: Text as Data & Topic Modelling\n\n\n\n\nGuest:\n\n\nProf.Â Ken Benoit\n\n\n\n\nLab\n\n\nUnsupervised Learning: Clustering (James et al. 2021, chap. 12)\n\n\n\n\n\n\n\n\n\nğŸ—“ï¸ W10\n\n\nLecture\n\n\nApplications: Predictive Modelling on Tabular Data\n\n\n\n\nLab\n\n\nUnsupervised Learning: Principal Component Analysis (James et al. 2021, chap. 12)\n\n\n\n\nSummative\n\n\n\n\nWorth: 20% of final marks\n\n\nProblem set about ML applications (text mining/social media)\n\n\nRelease date: 2 December 2022\n\n\nDeadline: 15 December 2022 (Week 11+1)\n\n\n\n\n\n\n\n\n\n\n\nğŸ—“ï¸ W11\n\n\nLecture\n\n\nApplications: Social Media Data\n\n\n\n\nGuest:\n\n\nSara Luxmoore, MSc in Applied Social Data Science (LSE)\n\n\n\n\nLab\n\n\nWe will explore unsupervised models using a couple of text datasets\n\n\n\n\n\n\n\n\nÂ Â Â Â Â Â Â Â Â Â \n\n\n\n\n\n\n\n\nâ€¦\n\n\n\n\n\n\n\n\n\n\nğŸ—“ï¸ Jan/23\n\n\nExam\n\n\n\n\nWorth: 40% of final marks\n\n\nProblem set about supervised + unsupervised learning + applications\n\n\n3 hours + 1 hour for submission\n\n\nOnline exam via Moodle\n\n\nDate: Sometime during the Exam Period. Exact timeslot has not yet been confirmed by LSE Exams team.\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York NY: Springer. https://www.statlearning.com/."
  },
  {
    "objectID": "assessments/formative1_solutions.html",
    "href": "assessments/formative1_solutions.html",
    "title": "âœ”ï¸ Formative Problem Set 01 | Solutions",
    "section": "",
    "text": "Import required libraries.No need to change if you are not planning to use other packages.\n\nlibrary(car)\nlibrary(ISLR2)\nlibrary(tidyverse)\n# If you use other packages, add them here\n\nlibrary(ggcorrplot)\nlibrary(tidymodels) ## to use the tidy() function to extract data from lm models"
  },
  {
    "objectID": "assessments/formative1_solutions.html#questions",
    "href": "assessments/formative1_solutions.html#questions",
    "title": "âœ”ï¸ Formative Problem Set 01 | Solutions",
    "section": "ğŸ¯ Questions",
    "text": "ğŸ¯ Questions\nUse the Carseats data set in the ISLR2 package to answer the following questions:\n\nQ1. Variables\nList the names of the variables in this dataset and whether they are quantitative or qualitative. (Worth 1/100 mock marks)\n\nQuantitative:\n\nSales\nCompPrice\nIncome\nAdvertising\nPopulation\nPrice\nAge\nEducation\n\nQualitative:\n\nShelveLoc\nUrban\nUS\n\n\n\n\nQ2. Dimension\nUse R to list the number of rows in the dataset. (Worth 1/100 mock marks)\n\nnrow(Carseats)\n\n[1] 400\n\n\n\n\nQ3. Visual\nSelecting only the quantitative variables, plot the correlation between variables (Worth 5/100 mock marks)\nTip: If you want to use other R packages, add them to the list of libraries at the top.\n\nCarseats_quant <- Carseats %>% select(-c(ShelveLoc, Urban, US))\ncorr_Carseats <- cor(Carseats_quant)\n\nggcorrplot(corr_Carseats, lab=TRUE, digits=2, lab_size=3)\n\n\n\n\n\n\nQ4. Initial Variable Identification\nBased on just an initial inspection of the data, which variables would you select to train an algorithm to predict Sales? Why? (Worth 7/100 mock marks)\n\nPrice is clearly the best first candidate. It is the variable most highly correlated with the dependent variable (Sales) in absolute terms. The correlation between Price and Sales is -0.44. As price increases, sales decreases in an approximately linear fashion.\n\n\ng <- (\n  ggplot(Carseats, aes(x=Price, y=Sales))\n  \n  + geom_point(size=3, alpha=0.5)\n  \n  + theme_bw()\n  \n  )\n\ng\n\n\n\n\n\nIf we want to add more variables, the next candidates would be Age and Advertising, as these variables also exhibit a correlation with Sales that is not too close from zero, in absolute terms. Another interesting point about these two variables is the fact that they are not correlated to each other; that is, correlation Age vs Advertising and Age vs Price and Price vs Advertising is close to zero.\n\n\ng <- (\n  ggplot(Carseats, aes(x=Age, y=Sales))\n  \n  + geom_point(size=3, alpha=0.5)\n  \n  + theme_bw()\n  \n  )\n\ng\n\n\n\n\n\ng <- (\n  ggplot(Carseats, aes(x=Advertising, y=Sales))\n  \n  + geom_point(size=3, alpha=0.5)\n  \n  + theme_bw()\n  \n  )\n\ng\n\n\n\n\n\n\nQ5. Simple Linear Regression\nChose ONE SINGLE variable â€“ any variable â€“ and fit a linear regression to predict Sales. Show the summary of this linear model. (Worth 3/100 mock marks)\n\nsimple_model <- lm(Sales ~ Price, data=Carseats)\n\nsummary(simple_model)\n\n\nCall:\nlm(formula = Sales ~ Price, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5224 -1.8442 -0.1459  1.6503  7.5108 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 13.641915   0.632812  21.558   <2e-16 ***\nPrice       -0.053073   0.005354  -9.912   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.532 on 398 degrees of freedom\nMultiple R-squared:  0.198, Adjusted R-squared:  0.196 \nF-statistic: 98.25 on 1 and 398 DF,  p-value: < 2.2e-16\n\n\n\n\nQ6. Simple Linear Regression - Interpretation\nProvide an interpretation of each coefficient in the model, stating their values, whether they are significant and what they represent. Be carefulâ€”some of the variables in the model are qualitative! (Worth 10/100 mock marks)\n\nWhen Price = 0, the model predicts Sales = 13.64.\n\nFor every $100 increase in Price, Sales decrease by approximately 5.3 units.\n\nBoth the intercept and the regression coefficient for Price are statistically significant, their p-values are close to zero.\n\n\n\nQ7. Simple Linear Regression - Formula\nWrite the model in equation form, carefully handling the qualitative variables properly. (Worth 5/100 mock marks)\n\\[Sales = - 0.053073 \\times Price + 13.641915 + \\epsilon\\]\n\n\nQ8. Multiple Linear Regression\nChose ONLY THREE variables and fit a linear regression to predict Sales. Show the summary of this linear model. (Worth 3/100 mock marks)\n\nmultiple_model <- lm(Sales ~ Price + Age + Advertising, data=Carseats)\n\nsummary(multiple_model)\n\n\nCall:\nlm(formula = Sales ~ Price + Age + Advertising, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.6247 -1.5288  0.0148  1.5220  6.2925 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 16.003472   0.718754  22.266  < 2e-16 ***\nPrice       -0.058028   0.004827 -12.022  < 2e-16 ***\nAge         -0.048846   0.007047  -6.931 1.70e-11 ***\nAdvertising  0.123106   0.017095   7.201 3.02e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.269 on 396 degrees of freedom\nMultiple R-squared:  0.3595,    Adjusted R-squared:  0.3547 \nF-statistic:  74.1 on 3 and 396 DF,  p-value: < 2.2e-16\n\n\n\n\nQ9. Multiple Linear Regression - Interpretation\nProvide an interpretation of each coefficient in the model, stating their values, whether they are significant and what they represent. Be carefulâ€”some of the variables in the model are qualitative! (Worth 10/100 mock marks)\n\nThis multiple regression model is statistically significant (p-value associated with the F-statistic is very small). In addition, all regression coefficients were also found to be statistically significant (p-value close to zero).\nIn the hypothetical scenario in which \\(Price = 0\\) and \\(Age = 0\\) and \\(Advertising = 0\\) , the multiple regression model predicts \\(Sales \\approx 16\\) units.\n\nFor every fixed combination of Age and Advertising, an increase in \\(100\\) dollars in Price is associated with a Sales decrease by approximately 5.8 units.\nIf Price and Advertising are fixed, the Age of a car impacts negatively the number of sales. The regression coefficient is \\(-0.048846\\) , which means we would expect Sales to decrease by \\(1\\) unit for every \\(\\approx 20\\) years of age of the Car (since \\(\\frac{1}{0.048846} \\approx 20\\) ) .\nAdvertising, on the other hand, is associated with an increase in the number of Sales. If Price and Age are fixed, the model predicts that 12 more items of Sales for each increase in \\(100\\) Advertising units (dollars?)\n\n\n\n\nQ10. Multiple Linear Regression - Formula\nWrite the model in equation form, carefully handling the qualitative variables properly. (Worth 5/100 mock marks)\n\\[Sales = 16.003472 - 0.058028 \\times Price - 0.048846 \\times Age + 0.123106 \\times Advertising + \\epsilon\\]\n\n\nQ11. Model comparison\nWhich of the two models you created, in questions Q5 and Q8 provide a better fit? (Worth 10/100 mock marks)\n\nThe multiple model in Q8 fitted the Sales data better.\n\nThe adjusted R-squared is higher in the Q8 versus the Q5 model, that is, adding Age and Advertising explains more of the variance in Sales than the model with just Price.\nWe also see a reduction in the Residual Standard Error.\nThis is also somewhat apparent from the diagnostic plot of residual vs fitted (shown below). The standardized residuals look more concentrated around zero, on the multiple model.\n\n\nplot_df <- data.frame(fitted_vals=predict(simple_model),\n                      residuals=rstudent(simple_model))\n\ng <- (\n  ggplot(plot_df, aes(x=fitted_vals, y=residuals))\n\n  # Add dots\n  + geom_point(alpha=0.4, size=3.5) \n  + xlab(\"Fitted values\") \n  + ylab(\"Residuals\") \n  + ylim(c(-4,4))\n  + ggtitle(\"Residuals vs Fitted (Simple Model)\")\n\n  # Add lines\n  + geom_hline(yintercept=0, size=1.5, color='yellow') \n  + geom_hline(yintercept=3, size=1.5, color='red')\n\n  # Customising the plot +\n  + theme_bw()\n)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nâ„¹ Please use `linewidth` instead.\n\ng\n\n\n\n\n\nplot_df <- data.frame(fitted_vals=predict(multiple_model),\n                      residuals=rstudent(multiple_model))\n\ng <- (\n  ggplot(plot_df, aes(x=fitted_vals, y=residuals))\n\n  # Add dots\n  + geom_point(alpha=0.4, size=3.5) \n  + xlab(\"Fitted values\") \n  + ylab(\"Residuals\") \n  + ylim(c(-4, 4))\n  + ggtitle(\"Residuals vs Fitted (Multiple Model)\")\n\n  # Add lines\n  + geom_hline(yintercept=0, size=1.5, color='yellow')\n  + geom_hline(yintercept=3, size=1.5, color='red')\n\n  # Customising the plot +\n  + theme_bw()\n)\n\ng\n\n\n\n\n\n\nQ12. Collinearity\nWhat is the Variance Inflation Factor (VIF) of each variable? (Worth 3/100 mock marks)\nConsidering only the model built on Q8:\n\nvif(multiple_model)\n\n      Price         Age Advertising \n   1.012538    1.010550    1.001987 \n\n\nAlternatively, if you interpreted the question to refer to all variables, the following would also be accepted:\n\nvif(lm(Sales ~ ., data=Carseats))\n\n                GVIF Df GVIF^(1/(2*Df))\nCompPrice   1.554618  1        1.246843\nIncome      1.024731  1        1.012290\nAdvertising 2.103136  1        1.450219\nPopulation  1.145534  1        1.070296\nPrice       1.537068  1        1.239785\nShelveLoc   1.033891  2        1.008367\nAge         1.021051  1        1.010471\nEducation   1.026342  1        1.013086\nUrban       1.022705  1        1.011289\nUS          1.980720  1        1.407380\n\n\n\n\nQ13. Collinearity (cont.)\nBased on your responses to Q3 and the output of Q12, would you consider ignoring any variables when building a linear model? Why/Why not? (Worth 7/100 mock marks)\n\nIn terms of vif â€“ which measures the linear dependency of each predictor to ALL the other predictors â€“ there are no â€œproblematic variablesâ€. If there were, we would find variables with vif above 5 or 10.\nIn terms of pairwise collinearity (plot in Q3), we also donâ€™t find any problematic colinearities.\n\n\n\nQ14. Modelling\nConsidering ALL possible combinations of TWO variables in this dataset, find the one linear model that has the smallest Residual Standard Error. Explain how you reached that conclusion, show us the summary of that model and write the model in equation form. (Worth 15/100 mock marks)\n\nğŸ’¡Tip: Here I show an â€œelegantâ€ way of solving this question by selecting variables by their names and by using dataframes, the pipe and other R functions. There are simpler ways to solve this question, the simplest perhaps would be to solve it by iterating over the column indices instead of column names. If you did it like that, your answer will be accepted, if done correctly.\n\n\n# Select all columns in the dataset, except Sales and get a list of their names\nall_predictors <- Carseats %>% select(-Sales) %>% names()\nall_predictors\n\n [1] \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\"  \"Price\"      \n [6] \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"       \"US\"         \n\n\nUse the function combn to produce a list of all possible combination of pairs of predictors. In addition to that, letâ€™s reshape the data and transform it to a nicely formatted dataframe with two columns. Since these columns do not have names, the function as.data.frame() automatically name them V1 and V2.\nğŸ’¡ Tip: type ?combn and hit ENTER in the R console to read more about the combn function.\n\nall_combn_predictors <- combn(all_predictors, 2) %>% t() %>% as.data.frame() \n\nall_combn_predictors\n\n            V1          V2\n1    CompPrice      Income\n2    CompPrice Advertising\n3    CompPrice  Population\n4    CompPrice       Price\n5    CompPrice   ShelveLoc\n6    CompPrice         Age\n7    CompPrice   Education\n8    CompPrice       Urban\n9    CompPrice          US\n10      Income Advertising\n11      Income  Population\n12      Income       Price\n13      Income   ShelveLoc\n14      Income         Age\n15      Income   Education\n16      Income       Urban\n17      Income          US\n18 Advertising  Population\n19 Advertising       Price\n20 Advertising   ShelveLoc\n21 Advertising         Age\n22 Advertising   Education\n23 Advertising       Urban\n24 Advertising          US\n25  Population       Price\n26  Population   ShelveLoc\n27  Population         Age\n28  Population   Education\n29  Population       Urban\n30  Population          US\n31       Price   ShelveLoc\n32       Price         Age\n33       Price   Education\n34       Price       Urban\n35       Price          US\n36   ShelveLoc         Age\n37   ShelveLoc   Education\n38   ShelveLoc       Urban\n39   ShelveLoc          US\n40         Age   Education\n41         Age       Urban\n42         Age          US\n43   Education       Urban\n44   Education          US\n45       Urban          US\n\n\nSo, there are a total of 45 combinations of pairs of features. Now, we need to fit linear regression models for all combinations and compute the Residual Standard Error of each.\nHow do we get the Residual Standard Error of a model? The R documentation says I can do it with the sigma() function. Read more about it here (or just type ?sigma in the R console and hit ENTER):\n\nsigma(simple_model)\n\n[1] 2.532326\n\n\nHow do we create these 45 models ? We can iterate over each line of the all_combn_predictors using a for loop, grab the names of the variables and create a linear model with them.\nLinear models in R are typically defined as a formula, but how do I build a formula without typing the name of the variables manually? We can do it by using the as.formula as illustrated in this Stackoverflow response.\nFirst, let me show you how the as.formula combined with paste works. Look at all the formulas produced in this loop:\n\nfor(i in 1:nrow(all_combn_predictors)){\n  \n   # get the i-th row of the dataframe\n  selected_vars <- all_combn_predictors[i,]\n  \n  # build the formula\n  model_formula <- as.formula(paste(\"Sales ~\", paste(selected_vars, collapse=\"+\")))\n  print(model_formula)\n}\n\nSales ~ CompPrice + Income\nSales ~ CompPrice + Advertising\nSales ~ CompPrice + Population\nSales ~ CompPrice + Price\nSales ~ CompPrice + ShelveLoc\nSales ~ CompPrice + Age\nSales ~ CompPrice + Education\nSales ~ CompPrice + Urban\nSales ~ CompPrice + US\nSales ~ Income + Advertising\nSales ~ Income + Population\nSales ~ Income + Price\nSales ~ Income + ShelveLoc\nSales ~ Income + Age\nSales ~ Income + Education\nSales ~ Income + Urban\nSales ~ Income + US\nSales ~ Advertising + Population\nSales ~ Advertising + Price\nSales ~ Advertising + ShelveLoc\nSales ~ Advertising + Age\nSales ~ Advertising + Education\nSales ~ Advertising + Urban\nSales ~ Advertising + US\nSales ~ Population + Price\nSales ~ Population + ShelveLoc\nSales ~ Population + Age\nSales ~ Population + Education\nSales ~ Population + Urban\nSales ~ Population + US\nSales ~ Price + ShelveLoc\nSales ~ Price + Age\nSales ~ Price + Education\nSales ~ Price + Urban\nSales ~ Price + US\nSales ~ ShelveLoc + Age\nSales ~ ShelveLoc + Education\nSales ~ ShelveLoc + Urban\nSales ~ ShelveLoc + US\nSales ~ Age + Education\nSales ~ Age + Urban\nSales ~ Age + US\nSales ~ Education + Urban\nSales ~ Education + US\nSales ~ Urban + US\n\n\nNow, letâ€™s actually create the linear models and identify the one with the smallest RSE:\n\nbest_RSE        <- Inf   # we don't know yet \nbest_predictors <- c()   # start empty, fill later\nbest_model      <- NULL  # we don't know yet\n\nfor(i in 1:nrow(all_combn_predictors)){\n  \n   # get the i-th row of the dataframe\n  selected_vars <- all_combn_predictors[i,]\n  \n  # build the formula\n  model_formula <- as.formula(paste(\"Sales ~\", paste(selected_vars, collapse=\"+\")))\n  \n  fitted_model      <- lm(model_formula, data=Carseats)\n  current_model_RSE <- sigma(fitted_model)\n  \n  if(current_model_RSE < best_RSE){\n    best_RSE <- current_model_RSE\n    best_predictors <- selected_vars\n    best_model <- fitted_model\n    cat(\"Found a better model!\\n\")\n    cat(paste0(\"  Vars: [\", paste(selected_vars, collapse=\",\"), \"]\\n\"))\n    cat(paste0(\"  RSE:\", best_RSE, \"\\n\\n\"))\n  }\n  \n}\n\nFound a better model!\n  Vars: [CompPrice,Income]\n  RSE:2.7899309103294\n\nFound a better model!\n  Vars: [CompPrice,Advertising]\n  RSE:2.71911905676241\n\nFound a better model!\n  Vars: [CompPrice,Price]\n  RSE:2.26880676843526\n\nFound a better model!\n  Vars: [Advertising,ShelveLoc]\n  RSE:2.24418017219941\n\nFound a better model!\n  Vars: [Price,ShelveLoc]\n  RSE:1.91725333683979\n\n\nAs we can see from above, the best model â€“ if we consider only the minimum RSE â€“ is:\n\nsummary(best_model)\n\n\nCall:\nlm(formula = model_formula, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.8229 -1.3930 -0.0179  1.3868  5.0780 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     12.001802   0.503447  23.839  < 2e-16 ***\nPrice           -0.056698   0.004059 -13.967  < 2e-16 ***\nShelveLocGood    4.895848   0.285921  17.123  < 2e-16 ***\nShelveLocMedium  1.862022   0.234748   7.932 2.23e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.917 on 396 degrees of freedom\nMultiple R-squared:  0.5426,    Adjusted R-squared:  0.5391 \nF-statistic: 156.6 on 3 and 396 DF,  p-value: < 2.2e-16\n\n\n\\[Sales = 12.001802 - 0.056698\\times Price + 4.895848 \\times ShelveLoc[Good] + 1.862022 \\times ShelveLoc[Medium] + \\epsilon\\]\nTeeechnically, the model above has more than just two variables, it uses three variables (two combinations of ShelveLoc. This is because R converts categorical variables to independent binary variables automatically. Since we didnâ€™t distinguish this case in the question, this is an acceptable response.\n\n\nQ15. Interaction Effects\nUse the * and : symbols to fit linear regression models with interaction effects. Could you find any model with interactions that fit better than the models you built in Questions Q5 & Q8 & Q14? Feel free to use as many interactions and variables as you prefer. Justify your answer. Explain how you reached that conclusion, show us the summary of that model and write the model in equation form. (Worth 15/100 mock marks)\n\nğŸ’¡ Here, you were not expected to test ALL combinations. You could solve this by trial and error, combining multiple combinations of variables until you reached a model with better RSE, R-squared or other metric.\nFor didactic purposes, I will re-use the solution from Q14 and test all possible linear models with only two variables, this time considering * and : operator instead of the + operator.\nI will look for the model that optimizes for RSE.\n\n\n# Using the tidyverse function crossing\nnew_combn_predictors <- crossing(all_combn_predictors, tibble(interaction_type=c(\"*\", \":\")))\nnew_combn_predictors\n\n# A tibble: 90 Ã— 3\n   V1          V2         interaction_type\n   <chr>       <chr>      <chr>           \n 1 Advertising Age        :               \n 2 Advertising Age        *               \n 3 Advertising Education  :               \n 4 Advertising Education  *               \n 5 Advertising Population :               \n 6 Advertising Population *               \n 7 Advertising Price      :               \n 8 Advertising Price      *               \n 9 Advertising ShelveLoc  :               \n10 Advertising ShelveLoc  *               \n# â€¦ with 80 more rows\n\n\nThis time there are 90 possible combinations:\n\nfor(i in 1:nrow(new_combn_predictors)){\n  \n   # get the i-th row of the dataframe\n  selected_vars <- new_combn_predictors[i,c(\"V1\", \"V2\")]\n  \n  # build the formula\n  # Notice that this time I will use whatever is in the column `interaction_type`\n  model_formula <- as.formula(paste(\"Sales ~\", paste(selected_vars, collapse=new_combn_predictors[i, ]$interaction_type)))\n  \n  print(model_formula)\n  \n}\n\nSales ~ Advertising:Age\nSales ~ Advertising * Age\nSales ~ Advertising:Education\nSales ~ Advertising * Education\nSales ~ Advertising:Population\nSales ~ Advertising * Population\nSales ~ Advertising:Price\nSales ~ Advertising * Price\nSales ~ Advertising:ShelveLoc\nSales ~ Advertising * ShelveLoc\nSales ~ Advertising:Urban\nSales ~ Advertising * Urban\nSales ~ Advertising:US\nSales ~ Advertising * US\nSales ~ Age:Education\nSales ~ Age * Education\nSales ~ Age:Urban\nSales ~ Age * Urban\nSales ~ Age:US\nSales ~ Age * US\nSales ~ CompPrice:Advertising\nSales ~ CompPrice * Advertising\nSales ~ CompPrice:Age\nSales ~ CompPrice * Age\nSales ~ CompPrice:Education\nSales ~ CompPrice * Education\nSales ~ CompPrice:Income\nSales ~ CompPrice * Income\nSales ~ CompPrice:Population\nSales ~ CompPrice * Population\nSales ~ CompPrice:Price\nSales ~ CompPrice * Price\nSales ~ CompPrice:ShelveLoc\nSales ~ CompPrice * ShelveLoc\nSales ~ CompPrice:Urban\nSales ~ CompPrice * Urban\nSales ~ CompPrice:US\nSales ~ CompPrice * US\nSales ~ Education:Urban\nSales ~ Education * Urban\nSales ~ Education:US\nSales ~ Education * US\nSales ~ Income:Advertising\nSales ~ Income * Advertising\nSales ~ Income:Age\nSales ~ Income * Age\nSales ~ Income:Education\nSales ~ Income * Education\nSales ~ Income:Population\nSales ~ Income * Population\nSales ~ Income:Price\nSales ~ Income * Price\nSales ~ Income:ShelveLoc\nSales ~ Income * ShelveLoc\nSales ~ Income:Urban\nSales ~ Income * Urban\nSales ~ Income:US\nSales ~ Income * US\nSales ~ Population:Age\nSales ~ Population * Age\nSales ~ Population:Education\nSales ~ Population * Education\nSales ~ Population:Price\nSales ~ Population * Price\nSales ~ Population:ShelveLoc\nSales ~ Population * ShelveLoc\nSales ~ Population:Urban\nSales ~ Population * Urban\nSales ~ Population:US\nSales ~ Population * US\nSales ~ Price:Age\nSales ~ Price * Age\nSales ~ Price:Education\nSales ~ Price * Education\nSales ~ Price:ShelveLoc\nSales ~ Price * ShelveLoc\nSales ~ Price:Urban\nSales ~ Price * Urban\nSales ~ Price:US\nSales ~ Price * US\nSales ~ ShelveLoc:Age\nSales ~ ShelveLoc * Age\nSales ~ ShelveLoc:Education\nSales ~ ShelveLoc * Education\nSales ~ ShelveLoc:Urban\nSales ~ ShelveLoc * Urban\nSales ~ ShelveLoc:US\nSales ~ ShelveLoc * US\nSales ~ Urban:US\nSales ~ Urban * US\n\n\n\nbest_RSE        <- Inf   # we don't know yet \nbest_predictors <- c()   # start empty, fill later\nbest_model      <- NULL  # we don't know yet\n\nfor(i in 1:nrow(new_combn_predictors)){\n  \n   # get the i-th row of the dataframe\n  selected_vars <- new_combn_predictors[i,c(\"V1\", \"V2\")]\n  \n  # build the formula\n  # Notice that this time I will use whatever is in the column `interaction_type`\n  model_formula <- as.formula(paste(\"Sales ~\", paste(selected_vars, collapse=new_combn_predictors[i, ]$interaction_type)))\n  \n  fitted_model      <- lm(model_formula, data=Carseats)\n  current_model_RSE <- sigma(fitted_model)\n  \n  if(current_model_RSE < best_RSE){\n    best_RSE <- current_model_RSE\n    best_predictors <- selected_vars\n    best_model <- fitted_model\n    cat(\"Found a better model!\\n\")\n    cat(paste0(model_formula, \"\\n\"))\n    cat(paste0(\"  RSE:\", best_RSE, \"\\n\\n\"))\n  }\n  \n}\n\nFound a better model!\n~\n Sales\n Advertising:Age\n  RSE:2.77861087594811\n\nFound a better model!\n~\n Sales\n Advertising * Age\n  RSE:2.65041673414083\n\nFound a better model!\n~\n Sales\n Advertising * Price\n  RSE:2.40120656790555\n\nFound a better model!\n~\n Sales\n Advertising * ShelveLoc\n  RSE:2.24885583915041\n\nFound a better model!\n~\n Sales\n Price:ShelveLoc\n  RSE:1.96438627687265\n\nFound a better model!\n~\n Sales\n Price * ShelveLoc\n  RSE:1.91821983511205\n\n\nWe didnâ€™t find a better model, but we found a model that leads to almost the same RSE as the one in Q14:\n\nsummary(best_model)\n\n\nCall:\nlm(formula = model_formula, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.9037 -1.3461 -0.0595  1.3679  4.9037 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           11.832984   0.965788  12.252  < 2e-16 ***\nPrice                 -0.055220   0.008276  -6.672 8.57e-11 ***\nShelveLocGood          6.135880   1.392844   4.405 1.36e-05 ***\nShelveLocMedium        1.630481   1.171616   1.392    0.165    \nPrice:ShelveLocGood   -0.010564   0.011742  -0.900    0.369    \nPrice:ShelveLocMedium  0.001984   0.010007   0.198    0.843    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.918 on 394 degrees of freedom\nMultiple R-squared:  0.5444,    Adjusted R-squared:  0.5386 \nF-statistic: 94.17 on 5 and 394 DF,  p-value: < 2.2e-16\n\n\n\\[\n\\begin{eqnarray}\nSales &=& 11.832984 - 0.055220 \\times Price \\\\\n&&+ 6.135880 \\times ShelveLoc[Good] \\\\ &&+1.630481 \\times ShelveLoc[Medium] \\\\\n&&-0.010564 \\times (Price \\times ShelveLoc[Good] ) \\\\\n&&+0.001984 \\times (Price \\times ShelveLoc[Medium] )\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "assessments/summative1.html",
    "href": "assessments/summative1.html",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "",
    "text": "Welcome to the first Summative Problem Set of DS202 (2022/23)!\nThings to know before you start:"
  },
  {
    "objectID": "assessments/summative1.html#how-to-get-help",
    "href": "assessments/summative1.html#how-to-get-help",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "How to get help",
    "text": "How to get help\n\nIt is okay to use whatever additional R packages you can find to help you explore the data and/or models better.\nIt is okay to team up with class colleagues to brainstorm ideas and these problems together.\n\nMost questions do not have a single objective response. It is unlikely that you will all write the exact same response so we will spot plagiarism and full copy-pastes very easily.\n\nIt is ok to use Slack or a shared Google Drive document to share links to useful content. For example, you can share things like:\n\nâ€œTip: I am using this package called tidymodels and it is much simpler than writing for loops!â€\nâ€˜I found this link useful: What is the difference between type=â€œclassâ€ and type=â€œresponseâ€ in the predict function?â€™\nThis website has many examples of charts in R â€” and it has the source code!\nâ€˜This R package called lubridate helped me work with dates a lot easier!â€™\n\nIt is ok ask clarification about questions of this problem set publicly on Slack. For example, you can ask questions like:\n\nâ€œI am a little confused about Question X. Where it says ... does it mean ....? Am I getting this right?â€\n\nIt is also ok to ask generic programming-related questions publicly on Slack. For example, you can ask questions like:\n\nâ€œHow do I get just the last 10 items of a list in R/tidyverse?â€ or\nâ€œHow do I sum the number of occurrences of a value in a column?â€\nâ€œAnyone else getting an unequal lengths error when creating a new vector? How do I solve this?â€\nâ€œI am having a hard time understanding the code below (from Week X lab):\nnew_list <- seq(length(dim(some_dataframe)[1]))\nWhy so many parentheses? Does anyone how to interpret this? #helpâ€ or even\nâ€œHow do I select specific columns of a dataframe by their names?â€ or maybe\nâ€œThe pairs plot is too messy, anyone knows of a better way to visualise pairs of variables?â€\n\nWhat we CANNOT accept:\n\nsharing your entire script or RMarkdown with others. But it is ok to share snippets of code with best practices, or to ask for help, like the type of code people share on Stackoverflow\nasking others to do your work for you (LSE regulations on plagiarism applies to computer code too)\nWe will run TurnitIn on your submissions to help flag cases of plagiarism"
  },
  {
    "objectID": "assessments/summative1.html#setup",
    "href": "assessments/summative1.html#setup",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "âš™ï¸ Setup",
    "text": "âš™ï¸ Setup\nImport required libraries.No need to change if you are not planning to use other packages.\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n# If you use other packages, add them here"
  },
  {
    "objectID": "assessments/summative1.html#data-dictionary",
    "href": "assessments/summative1.html#data-dictionary",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Data Dictionary",
    "text": "Data Dictionary\nWe will use a dataset of Algerian forest fires used by Faroudja & Izeboudjen (2019) 2 and sourced from the UCI ML repository. It has observations on 244 days in Algeria from June to September 2012 in two regions:\n\nBejaia, and\nSidi Bel-abbes\n\nThe dataset contains the following variables:\n\nDate columns\n\n\n\nColumn\nDescription\n\n\n\n\nday\nday of monitoring\n\n\nmonth\nmonth of the monitoring (â€˜juneâ€™ to â€˜septemberâ€™)\n\n\nyear\nFixed: 2012\n\n\n\n\n\nColumns related to weather data observations\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nTemperature\ntemperature at noon (max temperature) in Celsius degrees\n\n\nRH\nRelative Humidity in %\n\n\nWs\nWind speed in km/h\n\n\nRain\ntotal day in mm\n\n\n\n\n\nColumns related to FWI components 3\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nFFMC\nFine Fuel Moisture Code index from the FWI system\n\n\nDMC\nDuff Moisture Code index from the FWI system\n\n\nDC\nDrought Code index from the FWI system\n\n\nISI\nInitial Spread Index index from the FWI system\n\n\nBUI\nBuildup Index index from the FWI system\n\n\nFWI\nFire Weather Index Index\n\n\n\n\n\nThe column we want to predict\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nClasses\ntwo classes representing the ocurrence of fire"
  },
  {
    "objectID": "assessments/summative1.html#loading-the-data",
    "href": "assessments/summative1.html#loading-the-data",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Loading the data",
    "text": "Loading the data\nUse the code below to load the two datasets used in this first problem set.\nIMPORTANT: Ensure all the dataset files are in the exact same directory as this RMarkdown.\n# read_csv is a function of the tidyverse package\n\ndf_forest_fires_bejaia <- read_csv(\"./Algeria_Forest_Fires_Bejaia_Region_Dataset.csv\")\n\ndf_forest_fires_sidi   <- read_csv(\"./Algeria_Forest_Fires_Sidi_Bel_Abbes_Region_Dataset.csv\")\n\n\nTake a look at the data\n# Look at the first few lines of the dataframe\ndf_forest_fires_bejaia %>% head()\n# Look at the first few lines of the dataframe\ndf_forest_fires_sidi %>% head()\n# What are the dimensions of the dataframes?\n\ndf_forest_fires_bejaia %>% dim()\n# What are the dimensions of the dataframes?\n\ndf_forest_fires_sidi %>% dim()"
  },
  {
    "objectID": "assessments/summative1.html#what-we-want-from-you",
    "href": "assessments/summative1.html#what-we-want-from-you",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "What we want from you",
    "text": "What we want from you\nYou main goal will be to predict the occurrence of fires and understand what this ocurrence is associated with.\nNext, you will go through a sequence of tasks. For each of them you are given a code cell where you are supposed to write solutions to the tasks."
  },
  {
    "objectID": "assessments/summative1.html#q1.-fire-days",
    "href": "assessments/summative1.html#q1.-fire-days",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Q1. Fire days",
    "text": "Q1. Fire days\nUsing R, count the number of fire days observed in the two regions. (2 points)\n\nBejaia region\n\n# Replace this by your code\n\nSidi Bel-abbes region\n\n# Replace this by your code"
  },
  {
    "objectID": "assessments/summative1.html#q2.-fire-days-in-common",
    "href": "assessments/summative1.html#q2.-fire-days-in-common",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Q2. Fire days in common",
    "text": "Q2. Fire days in common\nUsing R, calculate how many days of fire the two regions had in common, and explain how you calculated it. (3 points)\n# Replace this by your code\nExplain what you did in the code above:\n\nReplace this with your text. Use multiple lines if needed."
  },
  {
    "objectID": "assessments/summative1.html#q3.-exploratory-data-analysis---part-i",
    "href": "assessments/summative1.html#q3.-exploratory-data-analysis---part-i",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Q3. Exploratory Data Analysis - Part I",
    "text": "Q3. Exploratory Data Analysis - Part I\nRun the code below to look at the plot it produces. In your own words, explain what you see: what dataset was used in the plot, what are the variables in the X and Y axis and what do the colours mean? (2 points)\ng <-\n    (ggplot(df_forest_fires_bejaia,\n            aes(x = Temperature, y = RH, colour = Classes))\n        + geom_point(size = 3, alpha = 0.6)\n\n        # OPTIONAL: Customising the plot.\n        # You can delete these lines below if you don't like the theme\n        # Or you can choose other themes from\n        # https://ggplot2.tidyverse.org/reference/ggtheme.html\n        + theme_bw()\n    )\ng\n\nYour text goes here"
  },
  {
    "objectID": "assessments/summative1.html#q4.-exploratory-data-analysis---part-ii",
    "href": "assessments/summative1.html#q4.-exploratory-data-analysis---part-ii",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Q4. Exploratory Data Analysis - Part II",
    "text": "Q4. Exploratory Data Analysis - Part II\nNow, create a scatterplot using any two predictors from the Sidi Bel-abbes region data. Colour the dots according to their Classes (3 points)\nYou can use either base R or ggplot. 4\n# Your code here"
  },
  {
    "objectID": "assessments/summative1.html#q5.-exploratory-data-analysis---part-iii",
    "href": "assessments/summative1.html#q5.-exploratory-data-analysis---part-iii",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Q5. Exploratory Data Analysis - Part III",
    "text": "Q5. Exploratory Data Analysis - Part III\nCan you spot differences in the distributions of predictors between the two regions (Sidi Bel-abbes vs Bejaia)? Describe the differences for at least one variable. Write your response and provide evidence using R code. You could use, for example, cross-tabulation, descriptive statistics or visualisations to support your point. (8 points)\n\nReplace this with your text. Use multiple lines if needed.\n\n# Your code here"
  },
  {
    "objectID": "assessments/summative1.html#q6.-logistic-regression-model",
    "href": "assessments/summative1.html#q6.-logistic-regression-model",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Q6. Logistic Regression Model",
    "text": "Q6. Logistic Regression Model\nBuild a logistic regression model for the Bejaia dataset using THREE predictors to predict the ocurrence of fire (the Classes variable). You can also add interaction effects amongst these three predictors if you wish. Save it as a variable named model and use R to print its summary. (7 points)\nğŸ’¡ Tip: you might need to convert Classes to a factor.\nğŸ’¡ If you have questions about R programming or conceptual questions about logistic regression, itâ€™s ok to ask questions to teachers and colleagues. What you are not allowed to ask: things like â€œis my solution correct?â€ or â€œwhich variables did you use?â€, etc..\nYou can choose to print the summary using base R or any of the functions from the broom package (part of tidymodels).\n# Your code here\n\n# If you won't answer this question, erase or comment out the line of code below. Otherwise, you will get an error when knitting this notebook.\nmodel <-"
  },
  {
    "objectID": "assessments/summative1.html#q7.-logistic-regression-model---justification",
    "href": "assessments/summative1.html#q7.-logistic-regression-model---justification",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Q7. Logistic Regression Model - Justification",
    "text": "Q7. Logistic Regression Model - Justification\nProvide a reasonable explanation for your choice of the three predictors in Q6. Why did you chose those variables? (10 points)\n(Optional: add additional R code/visualisations that you feel might help support your answer)\n\nReplace this with your text. Use multiple lines if needed."
  },
  {
    "objectID": "assessments/summative1.html#q8.-logistic-regression-model---diagnostics",
    "href": "assessments/summative1.html#q8.-logistic-regression-model---diagnostics",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Q8. Logistic Regression Model - Diagnostics",
    "text": "Q8. Logistic Regression Model - Diagnostics\nRun the code below to look at the plot it produces. In your own words, explain what you see and what this plot tells you about your model. (8 points)\nâš ï¸ If you didnâ€™t build a model in Q6, erase or comment out the block of code below. Otherwise, you will get an error when knitting this notebook.\n\ntrain_classes     <- df_forest_fires_bejaia$Classes\ntrain_predictions <- predict(model, df_forest_fires_bejaia, type = \"response\")\n\nplot_df <- data.frame(train_classes     = train_classes,\n                      train_predictions = train_predictions)\n\ng <-\n    (ggplot(plot_df, aes(x = train_predictions, fill = train_classes))\n        + geom_histogram(alpha = 0.8, binwidth = 0.05, position = \"stack\")\n\n        # OPTIONAL: Customising the plot.\n        # You can delete these lines below if you don't like the theme\n        # Or you can choose other themes from\n        # https://ggplot2.tidyverse.org/reference/ggtheme.html\n        + theme_bw()\n        + labs(x = \"Predictions on the training set\",\n               y = \"Count\")\n        + scale_fill_brewer(name = \"Target\", type = \"qual\", palette = 2)\n        + scale_x_continuous(labels = scales::percent, breaks = seq(0, 1, 0.1))\n        + ggtitle(\"Histogram of probability distributions fitted to the data\")\n    )\ng\n\nYour text goes here"
  },
  {
    "objectID": "assessments/summative1.html#q9.-logistic-regression-model---confusion-matrix",
    "href": "assessments/summative1.html#q9.-logistic-regression-model---confusion-matrix",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Q9. Logistic Regression Model - Confusion Matrix",
    "text": "Q9. Logistic Regression Model - Confusion Matrix\nRun the code below to look at the table it produces. What does this table show and what does it tell you about your model? (3 points)\ntrain_classes           <- df_forest_fires_bejaia$Classes\ntrain_class_predictions <- apply_threshold(model, df_forest_fires_bejaia, threshold=0.50)\n\nconfusion_matrix <- table(train_classes, train_class_predictions)\nprint(confusion_matrix)"
  },
  {
    "objectID": "assessments/summative1.html#q10.-logistic-regression-model---classification-metrics",
    "href": "assessments/summative1.html#q10.-logistic-regression-model---classification-metrics",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Q10. Logistic Regression Model - Classification metrics",
    "text": "Q10. Logistic Regression Model - Classification metrics\nNow, consider three other options of threshold: \\(t \\in \\{0.20, 0.40, 0.60\\}\\). Which of these three options lead to the best f1-score for your model? Write the R code for this and justify your answer. (7 points)\n# Your code here\n\nReplace this with your text. Use multiple lines if needed."
  },
  {
    "objectID": "assessments/summative1.html#q11.-logistic-regression-model---optimal-threshold-challenging",
    "href": "assessments/summative1.html#q11.-logistic-regression-model---optimal-threshold-challenging",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Q11. Logistic Regression Model - Optimal Threshold (Challenging)",
    "text": "Q11. Logistic Regression Model - Optimal Threshold (Challenging)\nNow, consider another set of possible thresholds, \\(t \\in \\{0.00, 0.01, 0.02, \\ldots, 0.98, 0.99, 1.00\\}\\). Find the optimal threshold \\(t^*\\), the one that leads to the best f1-score. Write the R code for this and justify your answer. (12 points)\n# Your code here\n\nReplace this with your text. Use multiple lines if needed."
  },
  {
    "objectID": "assessments/summative1.html#q12.-test-set-predictions",
    "href": "assessments/summative1.html#q12.-test-set-predictions",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Q12. Test set predictions",
    "text": "Q12. Test set predictions\nFollow the instructions below to apply the model you trained in Q6 to predict the probability of forest fires in the Sidi Bel-abbes dataset and produce a plot similar to that of Q8. (7 points)\n\nCreate a vector named test_classes that contains the true observed data (fire vs not fire) of Sidi Bel-abbes (You might need to convert it to factor)\nCreate a vector named test_predictions that contains the predict probability of forest fires in the Sidi Bel-abbes region\nIf the plot is produced and correct, you will get full marks. No need to justify the response.\n\nâš ï¸ If you donâ€™t want to answer this question, erase or comment out the block of code below. Otherwise, you will get an error when knitting this notebook.\n# Your code here\n\ntest_classes <- \ntest_predictions <- \n\nplot_df <- data.frame(test_classes     = test_classes,\n                      test_predictions = test_predictions)\n\ng <-\n    (ggplot(plot_df, aes(x = test_predictions, fill = test_classes))\n        + geom_histogram(alpha = 0.8, binwidth = 0.05, position = \"stack\")\n\n        # OPTIONAL: Customising the plot.\n        # You can delete these lines below if you don't like the theme\n        # Or you can choose other themes from\n        # https://ggplot2.tidyverse.org/reference/ggtheme.html\n        + theme_bw()\n        + labs(x = \"Predictions on the test set\",\n               y = \"Count\")\n        + scale_fill_brewer(name = \"Target\", type = \"qual\", palette = 2)\n        + scale_x_continuous(labels = scales::percent, breaks = seq(0, 1, 0.1))\n        + ggtitle(\"Histogram of probability distributions when applied to Sidi Bel-abbes data\")\n    )\ng"
  },
  {
    "objectID": "assessments/summative1.html#q13.-diagnostics",
    "href": "assessments/summative1.html#q13.-diagnostics",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Q13. Diagnostics",
    "text": "Q13. Diagnostics\nUsing the best threshold you found in either Q10 or Q11, write R code to produce a confusion matrix for the test set (Sidi Bel-abbes dataset). What is the True Positive Rate and True Negative Rate of your model in the test set? Did your model generalise well from the training to test set? (8 points)\n# Your code here\n\n\nReplace this with your text. Use multiple lines if needed."
  },
  {
    "objectID": "assessments/summative1.html#q14.-alternative-models-challenging",
    "href": "assessments/summative1.html#q14.-alternative-models-challenging",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Q14. Alternative Models (Challenging)",
    "text": "Q14. Alternative Models (Challenging)\nFollow the instructions below to build and explore an alternative classification model. Add as many chunks of code, text and equations as you prefer. (20 points)\n\nChose another algorithm (either Naive Bayes, Decision Tree or Support Vector Machine) to build a new classification model.\nUse the same training data you used to build your logistic regression in Q6 (same predictors)\nIf the algorithm requires a threshold, chose one that maximises the F1-score using the same logic as in Q10 or Q11.\nUse the same test data you used to validate your logistic regression as in Q12\nIf the algorithm does not require a threshold, try to tweak the parameters of the algorithm so as to avoid overfitting the model.\nUse whatever means you find appropriate (for example metrics, matrices, tables, plots) to compare your new model to the logistic model you built in the rest of this notebook.\nWrite about what you think makes your alternative model better/worse.\nProvide the full R code you used to build and test your alternative model\n\n\nğŸ’¡ TIPS\n\nUse what you have learned about up until Week 05 (lecture)\nRefer to your readings of the textbook if you want to understand more about how the alternative algorithms work. You can find links to the appropriate chapters here.\n\n\n# Your code here. Copy this chunk of code if you need.\n\n\nReplace this with your text. Use multiple lines if needed."
  },
  {
    "objectID": "assessments/summative1.html#decompress",
    "href": "assessments/summative1.html#decompress",
    "title": "Summative Problem Set 01 | W05-W07",
    "section": "Decompress",
    "text": "Decompress\nHow do you plan to reward yourself for completing this problem set?\n\n<replace this text with your reward. A cookie? take X days off? etc.>"
  },
  {
    "objectID": "assessments/summative1_solutions.html",
    "href": "assessments/summative1_solutions.html",
    "title": "âœ”ï¸ Summative Problem Set 01 | Solutions",
    "section": "",
    "text": "Here you will find model solutions to the first Summative Problem Set of DS202 (2022/23)."
  },
  {
    "objectID": "assessments/summative1_solutions.html#setup",
    "href": "assessments/summative1_solutions.html#setup",
    "title": "âœ”ï¸ Summative Problem Set 01 | Solutions",
    "section": "âš™ï¸ Setup",
    "text": "âš™ï¸ Setup\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n# If you use other packages, add them here\nlibrary(psych)\nlibrary(mosaic)\nlibrary(ggcorrplot)\nlibrary(cvms)\nlibrary(car)\nlibrary(rpart)\nlibrary(rpart.plot)"
  },
  {
    "objectID": "assessments/summative1_solutions.html#q1.-fire-days",
    "href": "assessments/summative1_solutions.html#q1.-fire-days",
    "title": "âœ”ï¸ Summative Problem Set 01 | Solutions",
    "section": "Q1. Fire days",
    "text": "Q1. Fire days\nUsing R, count the number of fire days observed in the two regions. (2 points)\n\nBejaia region\n\n\ncount1 <- df_forest_fires_bejaia %>% count(Classes)\ncount1[1,]\n\n# A tibble: 1 Ã— 2\n  Classes     n\n  <chr>   <int>\n1 fire       59\n\n\n\nSidi Bel-abbes region\n\n\ncount2 <- df_forest_fires_sidi %>% count(Classes)\ncount2[1,]\n\n# A tibble: 1 Ã— 2\n  Classes     n\n  <chr>   <int>\n1 fire       79"
  },
  {
    "objectID": "assessments/summative1_solutions.html#q2.-fire-days-in-common",
    "href": "assessments/summative1_solutions.html#q2.-fire-days-in-common",
    "title": "âœ”ï¸ Summative Problem Set 01 | Solutions",
    "section": "Q2. Fire days in common",
    "text": "Q2. Fire days in common\nUsing R, calculate how many days of fire the two regions had in common, and explain how you calculated it. (3 points)\n\ndf_forest_fires <- bind_rows(df_forest_fires_bejaia, df_forest_fires_sidi, .id = \"Region\")\ncount3 <-\n  df_forest_fires %>%\n  count(Classes, day, month) %>%\n  filter(Classes == 'fire' & n==2) %>% \n  count()\ncount3\n\n# A tibble: 1 Ã— 1\n      n\n  <int>\n1    47\n\n\nExplain what you did in the code above:\n\nFirst, I created a new data frame called df_forest_fires with the bind_rows function from dplyr combining the data points from both regions and creating a dummy variable â€œRegionâ€ coding 1 for Bejaia and 2 for Sidi Bel-abbes. I decided to do this independently of my count pipe below so this dataset could be used for further analysis.\nThen, using the tidyverse and the count function from dplyr, I first count the number n of independent occurences of Classes, day and month. Then I filter those occurences using the dplyr filter function to only show me the ones including â€˜fireâ€™ AND two occurences n.Â  The n == 2 occurences signifies that on that day, there was a fire in both regions. Lastly, I count the overall number of occurences left and I get 47. The two regions had 47 days of fire in common."
  },
  {
    "objectID": "assessments/summative1_solutions.html#q3.-exploratory-data-analysis---part-i",
    "href": "assessments/summative1_solutions.html#q3.-exploratory-data-analysis---part-i",
    "title": "âœ”ï¸ Summative Problem Set 01 | Solutions",
    "section": "Q3. Exploratory Data Analysis - Part I",
    "text": "Q3. Exploratory Data Analysis - Part I\nRun the code below to look at the plot it produces. In your own words, explain what you see: what dataset was used in the plot, what are the variables in the X and Y axis and what do the colours mean? (2 points)\n\ng <-\n    (ggplot(df_forest_fires_bejaia,\n            aes(x = Temperature, y = RH, colour = Classes)) +\n       geom_point(aes(shape=Classes), alpha = 0.7, size = 4.5) +\n       scale_shape_manual(values=c(17, 3)) +\n       scale_colour_manual(values=c('#B71C1C', '#4CAF50')) +\n              labs(\n         title = \"Occurences of forest fires mapped on Temperature vs Relative Humidity\",\n         subtitle = \"(in the Bejaia region)\",\n          x = \"Maximum temperature at noon in Celsius degrees (Temperature)\",\n         y = \"Relative Humidity in % (RH)\"\n  ) +\n       theme_bw()\n    )\n\ng\n\n\n\n\n\nThis is a scatterplot created using the df_forest_fires_bejaia dataset, meaning the forest fire dataset for the Bejaia region. The variable used on the x-axis is Temperature (temperature at noon (max temperature) in Celsius degrees), and on the y-axis is RH (Relative Humidity in %). Additionally, the colour and shape argument uses Classes (the occurence of a forest fire that day). Thus on the graph, red triangle data points represent days that have had a forest fire, and green crosses data points days where there was no forest fire. The colour and shape of these data points are respectively determined by scale_colour_manual and scale_shape_manual arguments.\nInitially we can see RH and Temperature seem to be negatively correlated, meaning data points on higher humidity days also have lower temperature, and vice-versa, on high temperature days, humidity % seems to be lower. Additionally, fire occurences seem to be concentrated on this latter type of day: low humidity percentage and high temperature, however, this relationship doesnâ€™t seem very strong as there are still many non-fire occurences on those days."
  },
  {
    "objectID": "assessments/summative1_solutions.html#q4.-exploratory-data-analysis---part-ii",
    "href": "assessments/summative1_solutions.html#q4.-exploratory-data-analysis---part-ii",
    "title": "âœ”ï¸ Summative Problem Set 01 | Solutions",
    "section": "Q4. Exploratory Data Analysis - Part II",
    "text": "Q4. Exploratory Data Analysis - Part II\nNow, create a scatterplot using any two predictors from the Sidi Bel-abbes region data. Colour the dots according to their Classes (3 points)\nYou can use either base R or ggplot. 1\n\ng2 <-\n    (ggplot(df_forest_fires_sidi,\n            aes(x = Ws, y = Rain, colour = Classes)) +\n       geom_point(aes(shape=Classes), alpha = 0.7, size = 4.5) +\n       scale_shape_manual(values=c(17, 3)) +\n       scale_colour_manual(values=c('#B71C1C', '#4CAF50')) +\n       \n       labs(\n         title = \"Occurences of forest fires mapped on Wind Speed vs. Precipitations\",\n         subtitle = \"(in the Sidi Bel-abbes region)\",\n          x = \"Wind speed in km/h (Ws)\",\n         y = \"Total precipitations in a day in mm (Rain)\"\n  ) +\n    \n    theme_bw()\n    )\ng2"
  },
  {
    "objectID": "assessments/summative1_solutions.html#q5.-exploratory-data-analysis---part-iii",
    "href": "assessments/summative1_solutions.html#q5.-exploratory-data-analysis---part-iii",
    "title": "âœ”ï¸ Summative Problem Set 01 | Solutions",
    "section": "Q5. Exploratory Data Analysis - Part III",
    "text": "Q5. Exploratory Data Analysis - Part III\nCan you spot differences in the distributions of predictors between the two regions (Sidi Bel-abbes vs Bejaia)? Describe the differences for at least one variable. Write your response and provide evidence using R code. You could use, for example, cross-tabulation, descriptive statistics or visualisations to support your point. (8 points)\nHere, the learner used several functions from the mosaic package. Learn more about this package here.\nWe also see lots of different comparisons, which is great! If you used only one single variable for comparison, your solution will still be valid, donâ€™t worry.\n\nFor every predictor, I created a boxplot visualising the potential differences in the distribution of a predictor between the two regions. The boxplots are overlayed with the data points. Lastly, I also requested a favstats table for each predictor that outputs the minimum, Q1, median, Q3, maximum, mean, standard deviation, sample sizes, and cases of missing data for each predictor in each Region.\nI used the dataset combining both regions created in Q3 and then used my dummy variable region as an argument to separate the data points. I renamed the 1, 0 dummy values with their string character equivalent to make visualisation easier to interpret.\nA first look at all plots and tables shows some predictors seem to have distributional differences while others are fairly similar across regions. For example, there are noticeable differences between regions in RH, FFMC, DC, ISI and FWI; whereas the distribution of data points remains fairly similar across regions for Temperature, Rain, Ws, DMC, and BUI. For example, in the RH boxplot we can see that the median in the Bejaia region seems to be above Q3 for the Sidi region. Whatâ€™s more the range of values in the Sidi region seems much greater than for the Bejaia region, as the data points are more dispersed.\nThe favstats table allows us to quantify these differences. Indeed, while the maximum is similar for both regions (89% humidity in Bejaia and 90% in Sidi), the minimum value as well as Q1, the median, the mean, Q3 and the standard deviation differ greatly. For example, the minimum observed relative humidity in Bejaia is 45% opposed to 21% in Sidi. This greater range of data points in the Sidi region is what drives the differences in all these statistics: the mean in Bejaia is 68% relative humidity compared to 56% in Sidi, following the same logic, standard deviations from the mean are much smaller in Bejaia with sd = 11.2 compared to sd = 15.7 in Sidi.\n\n\nTemperatureRHWsRainFFMCDMCDCISIBUIFWI\n\n\n\ndf_forest_fires <- transform(df_forest_fires, Classes = as.factor(Classes))\ndf_forest_fires$Region <- ifelse(df_forest_fires$Region == 1L, 'Bejaia', 'Sidi')\ndf_forest_fires <- transform(df_forest_fires, Region = as.factor(Region))\n\nboxplot(Temperature~Region,data=df_forest_fires,\n        xlab=\"Region\", ylab=\"Temperature\")\nstripchart(Temperature~Region,\n           data = df_forest_fires,\n           method = \"jitter\",\n           pch = 19,\n           col = 2:4,\n           vertical = TRUE,\n           add = TRUE)\n\n\n\n\n\nfavstats(Temperature~Region, data = df_forest_fires)\n\n  Region min Q1 median Q3 max     mean       sd   n missing\n1 Bejaia  22 29     31 34  37 31.18033 3.320401 122       0\n2   Sidi  24 30     34 36  42 33.16393 3.675608 122       0\n\n\n\n\n\nboxplot(RH~Region,data=df_forest_fires, main=\"RH\", xlab=\"Region\", ylab=\"RH\")\n\nstripchart(RH~Region,\n           data = df_forest_fires,\n           method = \"jitter\",\n           pch = 19,\n           col = 2:4,\n           vertical = TRUE,\n           add = TRUE)\n\n\n\n\n\nfavstats(RH~Region, data = df_forest_fires)\n\n  Region min    Q1 median    Q3 max     mean       sd   n missing\n1 Bejaia  45 60.00     68 77.75  89 67.97541 11.15441 122       0\n2   Sidi  21 43.25     56 66.75  90 55.90164 15.71619 122       0\n\n\n\n\n\nboxplot(Ws~Region,data=df_forest_fires, main=\"Ws\", xlab=\"Region\", ylab=\"Ws\")\n\nstripchart(Ws~Region,\n           data = df_forest_fires,\n           method = \"jitter\",\n           pch = 19,\n           col = 2:4,\n           vertical = TRUE,\n           add = TRUE)\n\n\n\n\n\nfavstats(Ws~Region, data = df_forest_fires)\n\n  Region min Q1 median    Q3 max    mean       sd   n missing\n1 Bejaia  11 14     16 18.00  26 16.0000 2.848807 122       0\n2   Sidi   6 14     15 16.75  29 15.0082 2.692186 122       0\n\n\n\n\n\nboxplot(Rain~Region,data=df_forest_fires, main=\"Rain\", xlab=\"Region\", ylab=\"Rain\")\nstripchart(Rain~Region,\n           data = df_forest_fires,\n           method = \"jitter\",\n           pch = 19,\n           col = 2:4,\n           vertical = TRUE,\n           add = TRUE)\n\n\n\n\n\nfavstats(Rain~Region, data = df_forest_fires)\n\n  Region min Q1 median    Q3  max      mean       sd   n missing\n1 Bejaia   0  0      0 0.500 16.8 0.8426230 2.409208 122       0\n2   Sidi   0  0      0 0.475  8.7 0.6786885 1.486759 122       0\n\n\n\n\n\nboxplot(FFMC~Region,data=df_forest_fires, main=\"FFMC\", xlab=\"Region\", ylab=\"FFMC\")\nstripchart(FFMC~Region,\n           data = df_forest_fires,\n           method = \"jitter\",\n           pch = 19,\n           col = 2:4,\n           vertical = TRUE,\n           add = TRUE)\n\n\n\n\n\nfavstats(FFMC~Region, data = df_forest_fires)\n\n  Region  min     Q1 median     Q3  max     mean       sd   n missing\n1 Bejaia 28.6 65.925  80.90 86.775 90.3 74.67295 15.55871 122       0\n2   Sidi 37.9 77.650  84.85 89.275 96.0 81.10246 12.24406 122       0\n\n\n\n\n\nboxplot(DMC~Region,data=df_forest_fires, main=\"DMC\", xlab=\"Region\", ylab=\"DMC\")\nstripchart(DMC~Region,\n           data = df_forest_fires,\n           method = \"jitter\",\n           pch = 19,\n           col = 2:4,\n           vertical = TRUE,\n           add = TRUE)\n\n\n\n\n\nfavstats(DMC~Region, data = df_forest_fires)\n\n  Region min    Q1 median   Q3  max     mean       sd   n missing\n1 Bejaia 0.7 3.725   9.45 16.3 54.2 12.31475 11.27436 122       0\n2   Sidi 0.9 7.325  13.15 22.9 65.9 17.03197 12.99507 122       0\n\n\n\n\n\nboxplot(DC~Region,data=df_forest_fires, main=\"DC\",\n   xlab=\"Region\", ylab=\"DC\")\nstripchart(DC~Region,\n           data = df_forest_fires,\n           method = \"jitter\",\n           pch = 19,\n           col = 2:4,\n           vertical = TRUE,\n           add = TRUE)\n\n\n\n\n\nfavstats(DC~Region, data = df_forest_fires)\n\n  Region min    Q1 median     Q3   max     mean       sd   n missing\n1 Bejaia 6.9 10.05  35.55 79.025 220.4 53.16066 51.77826 122       0\n2   Sidi 7.3 14.70  31.50 56.975 177.3 45.41557 42.92756 122       0\n\n\n\n\n\nboxplot(ISI~Region,data=df_forest_fires, main=\"ISI\",\n   xlab=\"Region\", ylab=\"ISI\")\nstripchart(ISI~Region,\n           data = df_forest_fires,\n           method = \"jitter\",\n           pch = 19,\n           col = 2:4,\n           vertical = TRUE,\n           add = TRUE)\n\n\n\n\n\nfavstats(ISI~Region, data = df_forest_fires)\n\n  Region min    Q1 median    Q3  max     mean       sd   n missing\n1 Bejaia 0.0 1.125   2.65 5.600 12.5 3.655738 3.021768 122       0\n2   Sidi 0.1 1.825   4.60 8.625 19.0 5.863934 4.803667 122       0\n\n\n\n\n\nboxplot(BUI~Region,data=df_forest_fires, main=\"BUI\",\n   xlab=\"Region\", ylab=\"BUI\")\nstripchart(BUI~Region,\n           data = df_forest_fires,\n           method = \"jitter\",\n           pch = 19,\n           col = 2:4,\n           vertical = TRUE,\n           add = TRUE)\n\n\n\n\n\nfavstats(BUI~Region, data = df_forest_fires)\n\n  Region min  Q1 median     Q3  max     mean       sd   n missing\n1 Bejaia 1.1 5.1   11.2 21.675 67.4 15.42623 14.47430 122       0\n2   Sidi 1.4 7.4   13.9 23.875 68.0 17.92049 13.87078 122       0\n\n\n\n\n\nboxplot(FWI~Region,data=df_forest_fires, main=\"FWI\",\n   xlab=\"Region\", ylab=\"FWI\")\nstripchart(FWI~Region,\n           data = df_forest_fires,\n           method = \"jitter\",\n           pch = 19,\n           col = 2:4,\n           vertical = TRUE,\n           add = TRUE)\n\n\n\n\n\nfavstats(FWI~Region, data = df_forest_fires)\n\n  Region min    Q1 median    Q3  max     mean       sd   n missing\n1 Bejaia   0 0.500   3.00  8.70 30.2 5.577869 6.343051 122       0\n2   Sidi   0 0.925   6.05 13.65 31.1 8.520492 8.137424 122       0"
  },
  {
    "objectID": "assessments/summative1_solutions.html#q6.-logistic-regression-model",
    "href": "assessments/summative1_solutions.html#q6.-logistic-regression-model",
    "title": "âœ”ï¸ Summative Problem Set 01 | Solutions",
    "section": "Q6. Logistic Regression Model",
    "text": "Q6. Logistic Regression Model\nBuild a logistic regression model for the Bejaia dataset using THREE predictors to predict the ocurrence of fire (the Classes variable). You can also add interaction effects amongst these three predictors if you wish. Save it as a variable named model and use R to print its summary. (7 points)\nYou can choose to print the summary using base R or any of the functions from the broom package (part of tidymodels).\n\ndf_forest_fires_bejaia$Classes_d <- ifelse(df_forest_fires_bejaia$Classes == 'fire', 1, 0)\nmodel <- glm(Classes_d ~ Temperature + Rain + FFMC , \n             data = df_forest_fires_bejaia, family = 'binomial')\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n\nsummary(model)\n\n\nCall:\nglm(formula = Classes_d ~ Temperature + Rain + FFMC, family = \"binomial\", \n    data = df_forest_fires_bejaia)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-1.81664  -0.00014   0.00000   0.02022   2.06273  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)   \n(Intercept) -113.7918    45.9466  -2.477  0.01326 * \nTemperature   -0.2774     0.2853  -0.973  0.33078   \nRain           2.2264     1.2165   1.830  0.06722 . \nFFMC           1.5101     0.5818   2.596  0.00944 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 168.997  on 121  degrees of freedom\nResidual deviance:  18.588  on 118  degrees of freedom\nAIC: 26.588\n\nNumber of Fisher Scoring iterations: 11"
  },
  {
    "objectID": "assessments/summative1_solutions.html#q7.-logistic-regression-model---justification",
    "href": "assessments/summative1_solutions.html#q7.-logistic-regression-model---justification",
    "title": "âœ”ï¸ Summative Problem Set 01 | Solutions",
    "section": "Q7. Logistic Regression Model - Justification",
    "text": "Q7. Logistic Regression Model - Justification\nProvide a reasonable explanation for your choice of the three predictors in Q6. Why did you chose those variables? (10 points)\n(Optional: add additional R code/visualisations that you feel might help support your answer)\n\ng1g2g3g4g5\n\n\n\ng <-\n    (ggplot(df_forest_fires_bejaia,\n            aes(x = Temperature, y = RH, colour = Classes)) +\n       geom_point(aes(shape=Classes), alpha = 0.7, size = 4.5) +\n       scale_shape_manual(values=c(17, 3)) +\n       scale_colour_manual(values=c('#B71C1C', '#4CAF50')) +\n       labs(\n         title = \"Occurences of forest fires mapped on Temperature vs Relative Humidity\",\n         subtitle = \"(in the Bejaia region)\",\n         x = \"Maximum temperature at noon in Celsius degrees (Temperature)\",\n         y = \"Relative Humidity in % (RH)\") +\n       theme_bw()\n    )\n\ng\n\n\n\n\n\n\n\ng2 <-\n    (ggplot(df_forest_fires_bejaia,\n            aes(x = Ws, y = Rain, colour = Classes)) +\n       geom_point(aes(shape=Classes), alpha = 0.7, size = 4.5) +\n       scale_shape_manual(values=c(17, 3)) +\n       scale_colour_manual(values=c('#B71C1C', '#4CAF50')) +\n       labs(\n         title = \"Occurences of forest fires mapped on Wind Speed vs. Precipitations\",\n         subtitle = \"(in the Bejaia region)\",\n          x = \"Wind speed in km/h (Ws)\",\n         y = \"Total precipitations in a day in mm (Rain)\") +\n       theme_bw()\n    )\ng2\n\n\n\n\n\n\n\ng3 <-\n    (ggplot(df_forest_fires_bejaia,\n            aes(x = FWI, y = BUI, colour = Classes)) +\n       geom_point(aes(shape=Classes), alpha = 0.7, size = 4.5) +\n       scale_shape_manual(values=c(17, 3)) +\n       scale_colour_manual(values=c('#B71C1C', '#4CAF50')) +\n       labs(\n         title = \"Occurences of forest fires mapped on FWI vs. BUI\",\n         subtitle = \"(in the Bejaia region)\",\n         x = \"Fire Weather Index Index (FWI)\",\n         y = \"Buildup Index index from the FWI system (BUI)\") +\n    theme_bw()\n    )\ng3\n\n\n\n\n\n\n\ng4 <-\n    (ggplot(df_forest_fires_bejaia,\n            aes(x = FFMC, y = DC, colour = Classes)) +\n       geom_point(aes(shape=Classes), alpha = 0.7, size = 4.5) +\n       scale_shape_manual(values=c(17, 3)) +\n       scale_colour_manual(values=c('#B71C1C', '#4CAF50')) +\n       labs(\n         title = \"Occurences of forest fires mapped on FFMC vs. DC\",\n         subtitle = \"(in the Bejaia region)\",\n         x = \"Fine Fuel Moisture Code index from the FWI system (FFMC)\",\n         y = \"Drought Code index from the FWI system (DC)\") +\n    theme_bw()\n    )\ng4\n\n\n\n\n\n\n\ng5 <-\n    (ggplot(df_forest_fires_bejaia,\n            aes(x = ISI, y = DMC, colour = Classes)) +\n       geom_point(aes(shape=Classes), alpha = 0.7, size = 4.5) +\n       scale_shape_manual(values=c(17, 3)) +\n       scale_colour_manual(values=c('#B71C1C', '#4CAF50')) +\n       labs(\n         title = \"Occurences of forest fires mapped on ISI vs. DMC\",\n         subtitle = \"(in the Bejaia region)\",\n         x = \"Initial Spread Index index from the FWI system (ISI)\",\n         y = \"Duff Moisture Code index from the FWI system (DMC)\") +\n    theme_bw()\n    )\ng5\n\n\n\n\n\n\n\n\nBased on exploratory analysis of the variables, as done in the plots of Q3 and Q4 (above), we want to get an idea of the effect of predictors on the occurence of forest fires. The scatterplots allow us to rapidly see the effects of 2 variables at one time while also getting an idea of how those two variables are correlated with another.\nFrom g1: Initially we can see RH and Temperature seem to be negatively correlated, meaning data points on higher humidity days also have lower temperature, and vice-versa, on high temperature days, humidity percentage seems to be lower. Additionally, fire occurences seem to be concentrated on this latter type of day: low humidity percentage and high temperature, however, this relationship doesnâ€™t seem very strong as there are still many non-fire occurences on those days. We can keep Temperature and RH in mind as potential predictors, but especially RH shouldnâ€™t be our priority (as it doesnâ€™t separate non-fire vs fire occurences well).\nFrom g2: Ws and Rain do not seem like they have a strong relationship. Besides, fire occurences are concentrated on days where there are no precipitation but across all levels of wind speed. However, there are still many non-fire occurences on days with no precipitations, which implies the predictor isnâ€™t as strong as it could be. We can exclude Ws as a potential predictor but keep Rain in mind as potential predictors.\nFrom g3: We can see FWI and BUI seem to be positively correlated, meaning data points that are higher on FWI are higher on BUI, which is to be expected as FWI is calculated based on ISI and BUI. Additionally, fire occurences seem to be spread across BUI and FWI values above 8 and 2 respectively. However, while this seems to have little predictive power at first sight, we can see that the non-fire occurences are clearly separated from the fire occurences. We can keep FWI and BUI in mind as good potential predictors, however, they canâ€™t be used together as it is likely they are highly auto-correlated because of the way FWI is built.\nFrom g4: We can see FFMC and DC seem to be have a positive almost quadratic relationship, which is to be expected as they are both built using the same weather components, Temperature and Rain, with FFMC using Ws and RH as well. Additionally, fire occurences seem to be spread across DC values above 25 and FFMC values above 80. While FFMC seems like a great predictor, DC seems to have little predictive power at first sight. But we can see that the non-fire occurences are clearly separated from the fire occurences, which makes DC a potentially good predictors. We can keep FFMC and DC in mind as good potential predictors, but weâ€™ll need to check their autocorrelation first to see if we can use them together.\nFrom g5: We can see ISI and DMC seem to be positively correlated, meaning data points that are higher on ISI are higher on DMC, which could be expected as ISI and DMC are based on common variables such as Rain, Rh and Temperature. Additionally, fire occurences seem to be spread across ISI and DMC values above 3 and 5 respectively. However, while this seems to have little predictive power at first sight, we can see that the non-fire occurences are clearly separated from the fire occurences, separation which is much clearer for ISI than for DMC. We can keep FFMC and DC in mind as good potential predictors, but weâ€™ll need to check their autocorrelation first to see if we can use them together.\n\n\ndf_forest_fires_bejaia_quant <- df_forest_fires_bejaia %>% select(-c(day, month, year, Classes))\ncorr_df_forest_fires_bejaia_quant <- cor(df_forest_fires_bejaia_quant)\n\nggcorrplot(corr_df_forest_fires_bejaia_quant, lab=TRUE, digits=2, lab_size=3)\n\n\n\n\n\nThis plot confirms our concern about auto-correlation. It was to be expected as FWI is a product of ISI and BUI. ISI is a porduct of FFMC and Wind measures. And BUI is a product of DMC and DC. Lastly, FFMC, DC and DMC are built out of at least temperature and Rain, with some using RH and Ws as well. For our choice of predictors, we want 3 good predictors that have the least autocorrelation possible. A model with high auto-correlation would perform well on training data but is likely to severely underperform on test data.\n\n\nmodel2 <- glm(Classes_d ~ Rain + Temperature + Ws + RH +FFMC + DC + DMC + BUI + ISI + FWI, \n              data = df_forest_fires_bejaia, family = 'binomial')\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n\nvif(model2)\n\n       Rain Temperature          Ws          RH        FFMC          DC \n  17.664741   11.988734    6.210314    5.347892   40.005401  815.709269 \n        DMC         BUI         ISI         FWI \n1556.550307 6483.331662  237.969435  308.357741 \n\n\n\nIf we calculate the Variance Inflation Factor (VIF) of each variable to measure collinearity, on a model using all of them, we can see all of our predictors are problematic with a conservative cut-off of 5. This is a very peculiar dataset, so I think the best solution is to stick to the original variables which all others are made-off, Temperature, Ws, RH and Rain, and choose 2 which seem to have the most predictive power based on the graphs: Temperature and Rain. From there on, we can look whether one of the composite scores, has a low-enough amount of collinearity to be chosen without compromising the model.\nThe correlation plot and the VIF test show FFMC has the lowest amount of collinearity with all of the other composite scores. This is very sensible when we know how FFMC is built: it uses the most variables out of all predictors: in addition to Temperature and Rain, it also uses Ws and RH, AND yesterdayâ€™s FFMC, integrating an intra-day component. Thus using FFMC in our model enables us to capture many variables. This is how my final predictors were: Temperature, Rain and FFMC.\nI chose not to use any interactions, as the variables already interact in FFMC. My answer would be very different if we were to choose 2 predictors or 1, because the inital weather predictors are clearly not the most predictive ones, but I think it is better than compromising the whole model with a high degree of auto-collinearity."
  },
  {
    "objectID": "assessments/summative1_solutions.html#q8.-logistic-regression-model---diagnostics",
    "href": "assessments/summative1_solutions.html#q8.-logistic-regression-model---diagnostics",
    "title": "âœ”ï¸ Summative Problem Set 01 | Solutions",
    "section": "Q8. Logistic Regression Model - Diagnostics",
    "text": "Q8. Logistic Regression Model - Diagnostics\nRun the code below to look at the plot it produces. In your own words, explain what you see and what this plot tells you about your model. (8 points)\nâš ï¸ If you didnâ€™t build a model in Q6, erase or comment out the block of code below. Otherwise, you will get an error when knitting this notebook.\n\ntrain_classes     <- df_forest_fires_bejaia$Classes\ntrain_predictions <- predict(model, df_forest_fires_bejaia, type = \"response\")\n\nplot_df <- data.frame(train_classes     = train_classes,\n                      train_predictions = train_predictions)\n\ng <-\n    (ggplot(plot_df, aes(x = train_predictions, fill = train_classes))\n        + geom_histogram(alpha = 0.8, binwidth = 0.05, position = \"stack\")\n        + theme_bw()\n        + labs(x = \"Predictions on the training set\",\n               y = \"Count\")\n        + scale_fill_brewer(name = \"Target\", type = \"qual\", palette = 2)\n        + scale_x_continuous(labels = scales::percent, breaks = seq(0, 1, 0.1))\n        + ggtitle(\"Histogram of probability distributions fitted to the data\")\n    )\ng\n\n\n\n\n\nThe code builds a histogram of the prediction response of the dataset. The option type=â€˜responseâ€™ outputs probabilities of the form P(Y = 1|X). This means the probability output is the probability of a fire occuring that day based on the predictor variables.\nThe value on the x-axis are these probabilities, 100% means there is a very high chance that there will be a fire that day, and 0% means there is a very low chance a fire will be occuring that day. Values such as 50% represent an average chance a fire will be occuring that day.\nThe count on the y-axis represents the count of the number of predictions of a certain value (percentage) on the x-axis. For example, there are approximately 54 predictions of 0% chances of a fire occuring, 1 prediction of 50% chance and 52 predictions of 100% chance (according to our model, out of a sample size of 122).\nWhatâ€™s more the colour fill we can see corresponds to the real Classes corresponding to our predictions, for example a prediction that is orange, whether it scores 20% or 100%, in real life, was a \"not fire\" occurence, whereas a green prediction was an occurence of \"fire\". Thus, if our model is correct, we expect \"not fire\" occurences (orange) to gather on the left hand side of the graph, and \"fire\" occurences (green) to gather on the right-hand side of the graph.\nFor now we canâ€™t make any definite judgements on accuracy of our model as we havenâ€™t chosen a decision threshold yet, a default could be 50% but this can be adjusted after various model diagnostics and taking into account real life objectives.\nWhat we can see here is that our model tends to predict extreme values such as 0% or 100% and very little in between, this makes judging the model fit easy at this point because we are unlikely to choose a threshold outside of 0.10 and 0.90. In that case, it seems our model fits the training data pretty well. Some predictions such as the 10% green prediction are likely to be identified as a training error (provided or treshold is not outside of 0.10 and 0.90), as it predicts with 10% probability that there will be a fire on a day where there was actually a fire. Overall, it seems our model fits the training data pretty well."
  },
  {
    "objectID": "assessments/summative1_solutions.html#q9.-logistic-regression-model---confusion-matrix",
    "href": "assessments/summative1_solutions.html#q9.-logistic-regression-model---confusion-matrix",
    "title": "âœ”ï¸ Summative Problem Set 01 | Solutions",
    "section": "Q9. Logistic Regression Model - Confusion Matrix",
    "text": "Q9. Logistic Regression Model - Confusion Matrix\nRun the code below to look at the table it produces. What does this table show and what does it tell you about your model? (3 points)\n\ntrain_classes           <- df_forest_fires_bejaia$Classes\ntrain_class_predictions <- apply_threshold(model, df_forest_fires_bejaia, threshold=0.50)\n\nconfusion_matrix <- table(train_classes, train_class_predictions)\nprint(confusion_matrix)\n\n             train_class_predictions\ntrain_classes not fire fire\n     fire            1   58\n     not fire       61    2\n\n\n\nplot_confusion_matrix(as_tibble(confusion_matrix), \n                      target_col = \"train_classes\", \n                      prediction_col = \"train_class_predictions\",\n                      \n                      add_normalized = TRUE,\n                      add_col_percentages = FALSE,\n                      add_row_percentages = FALSE,\n                      counts_col = \"n\",\n                      )\n\nWarning in plot_confusion_matrix(as_tibble(confusion_matrix), target_col =\n\"train_classes\", : 'ggimage' is missing. Will not plot arrows and zero-shading.\n\n\n\n\n\n\nThis table shows the confusion matrix of predictions made from our model about fire occurences on the training dataset versus actual fire occurences on the same day on the training dataset. These predictions are set on a threshold of 0.50, this means any prediction of a probability above or equal to 50% of a â€œfireâ€ occuring that day gets classified as a fire occurence, and any prediction of a fire occuring that day strictly below 50% get classified as â€œnot fireâ€ occuring that day.\nThis table was converted into a visual confusion matrix plot, facilitating interpretation. It says about our model that with a treshold of 0.50, our model correctly predicts the dark blue occurences so 61 + 58 data points, in all 97.5% of our test data (50 + 47.5). This is called model accuracy. Our model incorrectly predicts the light blue occurences, 3 occurences eg. approximately 2.4% of our training data. For further specifications about model fit, the below calculations based on the above confusion matrix enable us to see details. These are relevant, as different real life scenario call for a focusing on optimizing certain prediction such as for example true positives above another (eg. false negatives).\nFirst, the exact accuracy as manually calculated above is calculated (97.54%). Then we calculate the specificity, also called the True Negative Rate which is 96.83 %. Then we calculate the true positive rate, called sensitivity or recall: 98.31 %. Lastly, we calculate the precision, the rate of true positives that predicted our alternate variable â€œfireâ€, here 96.67 %.\n\n\ntrain_class_predictions_v <- as.factor(as.vector(train_class_predictions))\n# Accuracy\naccuracy_test <- sum(train_class_predictions_v == train_classes)/nrow(df_forest_fires_bejaia)\ncat(sprintf(\"%.2f %%\", accuracy_test*100))\n\n97.54 %\n\n\n\n# Specificity: True Negative Rate\ntotal_real_no      <- sum(train_classes == \"not fire\")\ntotal_correct_no   <- sum(train_class_predictions_v == \"not fire\" & train_classes == \"not fire\")\n\nTNR_test <- total_correct_no/total_real_no\ncat(sprintf(\"%.2f %%\", TNR_test*100))\n\n96.83 %\n\n\n\n# Sensitivity or Recall: True Positive Rate\n\ntotal_real_yes      <- sum(train_classes == \"fire\")\ntotal_correct_yes   <- sum(train_class_predictions_v == \"fire\" & train_classes == \"fire\")\n\nTPR_test <- total_correct_yes/total_real_yes\ncat(sprintf(\"%.2f %%\", TPR_test*100)) \n\n98.31 %\n\n\n\n## Precision: How many True Positives over all predicted=\"fire\"? \ntotal_predicted_yes <- sum(train_class_predictions_v == \"fire\")\n\nprecision_test <- total_correct_yes/total_predicted_yes\ncat(sprintf(\"%.2f %%\", precision_test*100)) \n\n96.67 %"
  },
  {
    "objectID": "assessments/summative1_solutions.html#q10.-logistic-regression-model---classification-metrics",
    "href": "assessments/summative1_solutions.html#q10.-logistic-regression-model---classification-metrics",
    "title": "âœ”ï¸ Summative Problem Set 01 | Solutions",
    "section": "Q10. Logistic Regression Model - Classification metrics",
    "text": "Q10. Logistic Regression Model - Classification metrics\nNow, consider three other options of threshold: \\(t \\in \\{0.20, 0.40, 0.60\\}\\). Which of these three options lead to the best f1-score for your model? Write the R code for this and justify your answer. (7 points)\n\ntreshold <- c(0.20, 0.40, 0.60)\n\nfor (i in treshold) {\n  train_classes <- df_forest_fires_bejaia$Classes\n  train_class_predictions <- apply_threshold(model, df_forest_fires_bejaia, threshold=i)\n  train_class_predictions_v <- as.factor(as.vector(train_class_predictions))\n  \n  total_real_yes <- sum(train_classes == \"fire\")\n  total_correct_yes <- sum(train_class_predictions_v == \"fire\" & train_classes == \"fire\")\n  TPR_test <- total_correct_yes/total_real_yes\n  \n  total_predicted_yes <- sum(train_class_predictions_v == \"fire\")\n  precision_test <- total_correct_yes/total_predicted_yes\n  \n  f1_test <- (2*precision_test*TPR_test)/(precision_test + TPR_test)\n  cat(\"The f1-score for our model with a threshold of\", i, \"is\", f1_test, \".\", fill = TRUE)\n\n  }\n\nThe f1-score for our model with a threshold of 0.2 is 0.9508197 .\nThe f1-score for our model with a threshold of 0.4 is 0.9666667 .\nThe f1-score for our model with a threshold of 0.6 is 0.9565217 .\n\n\n\nOut of the three options given for tresholds (0.2; 0.4; 0.6), 0.4 gives the best f1 score (as the best f1 score is the closest to 1 possible) of approximately 0.97. As can be seen from the 3 outputs, the f1 score doesnâ€™t vary greatly for each threshold, but is lower for both 0.2 and 0.6. This output was created by creating a loop that calculates the f1 scores based on the predictions made by different thresholds. Lastly, the loop outputs a sentence associating the threshold and the f1-score preventing any confusion in interpretation."
  },
  {
    "objectID": "assessments/summative1_solutions.html#q11.-logistic-regression-model---optimal-threshold-challenging",
    "href": "assessments/summative1_solutions.html#q11.-logistic-regression-model---optimal-threshold-challenging",
    "title": "âœ”ï¸ Summative Problem Set 01 | Solutions",
    "section": "Q11. Logistic Regression Model - Optimal Threshold (Challenging)",
    "text": "Q11. Logistic Regression Model - Optimal Threshold (Challenging)\nNow, consider another set of possible thresholds, \\(t \\in \\{0.00, 0.01, 0.02, \\ldots, 0.98, 0.99, 1.00\\}\\). Find the optimal threshold \\(t^*\\), the one that leads to the best f1-score. Write the R code for this and justify your answer. (12 points)\n\ntreshold <- seq(from = 0, to = 0.99, by = 0.01)\nf1_vector  <- c()\nfor (i in treshold) {\n  train_classes <- df_forest_fires_bejaia$Classes\n  train_class_predictions <- apply_threshold(model, df_forest_fires_bejaia, threshold=i)\n  train_class_predictions_v <- as.factor(as.vector(train_class_predictions))\n  \n  total_real_yes <- sum(train_classes == \"fire\")\n  total_correct_yes <- sum(train_class_predictions_v == \"fire\" & train_classes == \"fire\")\n  TPR_test <- total_correct_yes/total_real_yes\n  \n  total_predicted_yes <- sum(train_class_predictions_v == \"fire\")\n  precision_test <- total_correct_yes/total_predicted_yes\n  \n  f1_test <- (2*precision_test*TPR_test)/(precision_test + TPR_test)\n  f1_vector <- c(f1_vector, f1_test)\n}\nmax(f1_vector)\n\n[1] 0.9747899\n\n\n\nwhich( f1_vector == max(f1_vector) )\n\n[1] 50 51 52 53 54 55\n\n\n\nI used a similar loop as the one for Q10, except that instead of only testing 3 values, here I test all values between 0 and 1, at intervals of 0.01. This is done easily by creating a vector of threshold values of values between 0 and 1 at an interval of 0.01, giving place to 100 values.\nHere I exclude 1.00 as we cannot calculate an F1 score with a threshold of 1. The threshold needs to be strictly below 1. Then I save the result of all of the F1 scores in a vector that I created before my loop in the global environment. We have to be careful as the F1 score for a threshold of 0 will be stored at index 1, so the value of the threshold will always be (index-1)/100 which we need to keep in mind when interpreting. Then, outside of my loop, I look for the maximum of the vector, the F1 score closest to 1, which is 0.97. Then I look for ALL of the indices that carry these values.\nI am careful not just to use which.max(f1_vector) because this would only give me the first value that reaches this maximum, and would ommit showing potential multiple maximums present in the data. Indeed here, there are 6 maximums, present at indices 50, 51, 52, 53, 54 and 55. Being mindful of how these indices are interpreted I find that there are 6 optimal threshold tâˆ— that lead to the best f1-score: these thresholds are 0.49, 0.50, 0.51, 0.52, 0.53 and 0.54."
  },
  {
    "objectID": "assessments/summative1_solutions.html#q12.-test-set-predictions",
    "href": "assessments/summative1_solutions.html#q12.-test-set-predictions",
    "title": "âœ”ï¸ Summative Problem Set 01 | Solutions",
    "section": "Q12. Test set predictions",
    "text": "Q12. Test set predictions\nFollow the instructions below to apply the model you trained in Q6 to predict the probability of forest fires in the Sidi Bel-abbes dataset and produce a plot similar to that of Q8. (7 points)\n\nCreate a vector named test_classes that contains the true observed data (fire vs not fire) of Sidi Bel-abbes (You might need to convert it to factor)\nCreate a vector named test_predictions that contains the predict probability of forest fires in the Sidi Bel-abbes region\nIf the plot is produced and correct, you will get full marks. No need to justify the response.\n\n\ndf_forest_fires_sidi$Classes_d <- ifelse(df_forest_fires_sidi$Classes == 'fire', 1, 0)\n\ntest_classes <- df_forest_fires_sidi$Classes\ntest_predictions <- predict(model, df_forest_fires_sidi, type = \"response\")\n\nplot_df <- data.frame(test_classes  = test_classes,\n                      test_predictions = test_predictions)\n\ng <-\n    (ggplot(plot_df, aes(x = test_predictions, fill = test_classes))\n        + geom_histogram(alpha = 0.8, binwidth = 0.05, position = \"stack\")\n        + theme_bw()\n        + labs(x = \"Predictions on the test set\",\n               y = \"Count\")\n        + scale_fill_brewer(name = \"Target\", type = \"qual\", palette = 2)\n        + scale_x_continuous(labels = scales::percent, breaks = seq(0, 1, 0.1))\n        + ggtitle(\"Histogram of probability distributions when applied to Sidi Bel-abbes data\")\n    )\ng"
  },
  {
    "objectID": "assessments/summative1_solutions.html#q13.-diagnostics",
    "href": "assessments/summative1_solutions.html#q13.-diagnostics",
    "title": "âœ”ï¸ Summative Problem Set 01 | Solutions",
    "section": "Q13. Diagnostics",
    "text": "Q13. Diagnostics\nUsing the best threshold you found in either Q10 or Q11, write R code to produce a confusion matrix for the test set (Sidi Bel-abbes dataset). What is the True Positive Rate and True Negative Rate of your model in the test set? Did your model generalise well from the training to test set? (8 points)\n\ntest_classes <- df_forest_fires_sidi$Classes\ntest_class_predictions <- apply_threshold(model, df_forest_fires_sidi, threshold=0.52)\n\nconfusion_matrix <- table(test_classes, test_class_predictions)\nprint(confusion_matrix)\n\n            test_class_predictions\ntest_classes not fire fire\n    fire            1   78\n    not fire       40    3\n\n\n\nplot_confusion_matrix(as_tibble(confusion_matrix), \n                      target_col = \"test_classes\", \n                      prediction_col = \"test_class_predictions\",\n                      \n                      add_normalized = TRUE,\n                      add_col_percentages = FALSE,\n                      add_row_percentages = FALSE,\n                      counts_col = \"n\",\n                      )\n\nWarning in plot_confusion_matrix(as_tibble(confusion_matrix), target_col =\n\"test_classes\", : 'ggimage' is missing. Will not plot arrows and zero-shading.\n\n\n\n\n\n\ntest_class_predictions_v <- as.factor(as.vector(test_class_predictions))\n\n# Accuracy\naccuracy_test <- sum(test_class_predictions_v == test_classes)/nrow(df_forest_fires_sidi)\ncat(sprintf(\"%.2f %%\", accuracy_test*100))\n\n96.72 %\n\n\n\n# Specificity: True Negative Rate\ntotal_real_no      <- sum(test_classes == \"not fire\")\ntotal_correct_no   <- sum(test_class_predictions_v == \"not fire\" & test_classes == \"not fire\")\n\nTNR_test <- total_correct_no/total_real_no\ncat(sprintf(\"%.2f %%\", TNR_test*100))\n\n93.02 %\n\n\n\n# Sensitivity or Recall: True Positive Rate\n\ntotal_real_yes      <- sum(test_classes == \"fire\")\ntotal_correct_yes   <- sum(test_class_predictions_v == \"fire\" & test_classes == \"fire\")\n\nTPR_test <- total_correct_yes/total_real_yes\ncat(sprintf(\"%.2f %%\", TPR_test*100)) \n\n98.73 %\n\n\n\n## Precision: How many True Positives over all predicted=\"fire\"? \ntotal_predicted_yes <- sum(test_class_predictions_v == \"fire\")\n\nprecision_test <- total_correct_yes/total_predicted_yes\ncat(sprintf(\"%.2f %%\", precision_test*100)) \n\n96.30 %\n\n\n\nf1_test <- (2*precision_test*TPR_test)/(precision_test + TPR_test)\nf1_test \n\n[1] 0.975\n\n\n\nAs I found 6 optimal threshold in Q11 [0.49, 0.50, 0.51, 0.52, 0.53, 0.54] which all yielded the best f1 score for the training dataset, here I will choose 0.52, the median of the 6 optimal thresholds. The confusion plot describes that with a treshold of 0.52, our model correctly predicts the darker blue occurences so 40 + 78 data points, in all 96.7% of our test data (32.8 + 63.9). This is called model accuracy. Our model incorrectly predicts the light blue occurences, 4 occurences eg. approximately 3.3% of our test data. For further specifications about model fit, the calculations based on the above confusion matrix enable us to see details.\nFirst, the exact accuracy as manually calculated above is calculated (96.72% vs.Â 97.54% on our training data). Then we calculate the specificity, also called the True Negative Rate which is 93.02 % (vs.Â 96.83% on our training data). Then we calculate the True Positive Rate, called Sensitivity or Recall: 98.73 % (vs.Â 98.31% on our training data). Subsequently, we calculate the precision, the rate of true positives that predicted our alternate variable â€œfireâ€, here 96.30 % (vs.Â 97.67% on our training data). Lastly, we calculate the F1 score, a composite measure of the sensitivity and precision and we get approximately 0.98 (vs.Â 0.97 on our training data for the best treshold)).\nThese measures suggest the model generalized very well from our training dataset to our test dataset, because even though they are slighty lower, the Accuracy, Specificity, and Precision of our the model on our test dataset are very close to those on our training dataset, all scoring in the 90% percentile. Whatâ€™s more the Sensitivity and the F1-score for the model on the test data are even higher than on the training data!"
  },
  {
    "objectID": "assessments/summative1_solutions.html#q14.-alternative-models-challenging",
    "href": "assessments/summative1_solutions.html#q14.-alternative-models-challenging",
    "title": "âœ”ï¸ Summative Problem Set 01 | Solutions",
    "section": "Q14. Alternative Models (Challenging)",
    "text": "Q14. Alternative Models (Challenging)\nFollow the instructions below to build and explore an alternative classification model. Add as many chunks of code, text and equations as you prefer. (20 points)\n\nChose another algorithm (either Naive Bayes, Decision Tree or Support Vector Machine) to build a new classification model.\nUse the same training data you used to build your logistic regression in Q6 (same predictors)\nIf the algorithm requires a threshold, chose one that maximises the F1-score using the same logic as in Q10 or Q11.\nUse the same test data you used to validate your logistic regression as in Q12\nIf the algorithm does not require a threshold, try to tweak the parameters of the algorithm so as to avoid overfitting the model.\nUse whatever means you find appropriate (for example metrics, matrices, tables, plots) to compare your new model to the logistic model you built in the rest of this notebook.\nWrite about what you think makes your alternative model better/worse.\nProvide the full R code you used to build and test your alternative model\n\n\n\nI am choosing a decision tree model.\n\n\n\n# Step 2\ntree.model <- rpart(Classes ~ Temperature + Rain + FFMC , data = df_forest_fires_bejaia, method = 'class')\nrpart.plot(tree.model)\n\n\n\n\n\n# No step 3 as the algorithm chosen does not require a threshold.\n\nTechnically, the algorithm does require a threshold (by default=0.5), it is just hidden from us. But since we had not explored thresholds for decision trees, we do not take points for this reason.\n\n\nOn the decision tree, we can observe that there are three nodes. The first node contains 100% of the data, and would predict \"not fire\" by default as the probability of no fire occuring in that node if of 0.52. This shouldnâ€™t be used for generalization, as we have to take into account is that the training dataset contains an uneven distribution of observations of each class. The decision tree only choose one predictor (FFMC) out of the three predictors given (Temperature, Rain and FFMC). This predictor is used to create the second level of the decision tree, where all the data points with an FFMC score equal or greater than 80 get transferred to the left node (52% of the original data), and all the data points with a FFMC score lower than 80 get transferred to the right node (48% of the original data). All observations in the right node are predicted as â€œnot fireâ€ as the probability of no fire occuring for these data points is of 1.00. This means that in the training dataset, all data points with an FFMC score strictly below 80 are no fire occurences. All observations in the left node are predicted as â€œfireâ€ as the probability of no fire occuring for these data points is of only 0.06. This means that in the training dataset, ALMOST all data points with an FFMC score strictly equal or greater than 80 are fire occurences. The probabilities in the children node enable calculation of the probability in the parent node:\n\n\n\n# the probability of \"not fire\" occurrences in the right node (1.00) \n#  times the proportion of the data contained in the right node (48%)\nno_fire_right <- 1.00*48/100\n\n# the probability of \"not fire\" occurrences in the left node (0.06) \n#  times the proportion of the data contained in the left node (52%)\nno_fire_left <- 0.06*52/100\n\n# the probability of \"not fire\" occurrences in the parent node (0.52) \n#  for the total data (100%)\nno_fire_total <- no_fire_right + no_fire_left \nno_fire_total\n\n[1] 0.5112\n\n\n\n\nThere is no step 3 as a decision tree does not require a threshold.\n\n\n\n# Step 4\nplot_df <- \n  df_forest_fires_sidi %>% \n    mutate(class_pred = predict(tree.model, newdata = ., type=\"class\"),\n           correct    = class_pred == Classes)\n\n\nconfusion_matrix <- \n    table(expected=plot_df$Classes, class_pred=plot_df$Classes)\nprint(confusion_matrix)\n\n          class_pred\nexpected   fire not fire\n  fire       79        0\n  not fire    0       43\n\n\n\nplot_confusion_matrix(as_tibble(confusion_matrix), \n                      target_col = \"expected\", \n                      prediction_col = \"class_pred\",\n                      \n                      add_normalized = TRUE,\n                      add_col_percentages = FALSE,\n                      add_row_percentages = FALSE,\n                      counts_col = \"n\",\n                      )\n\nWarning in plot_confusion_matrix(as_tibble(confusion_matrix), target_col =\n\"expected\", : 'ggimage' is missing. Will not plot arrows and zero-shading.\n\n\n\n\n\n\n\nIn this chunk of code, the decision tree model was used on the test data (the data frame for the Sidi region) to predict fire classes based on the data points in the Sidi forest fire dataframe. The predictted classes are saved in the plot_df dataframe. Then the dataframe is used to inform the confusion matrix comparing predicted classes from the decision tree to the expected classes in the Sidi (test) dataset. Lastly, the table was converted into a visual confusion matrix plot, facilitating interpretation.\n\nThe output seems very positive as it looks like the accuracy is of 100%. Basically, our model correctly predicts the darker blue occurences so 43 + 79 data points, in all 100% of our test data (35.2 + 64.8). Our model would incorrectly predicts the light blue occurences, here none.\n\n\n# Step 5\nsimple.tree.model <- rpart(Classes ~ Temperature + Rain + FFMC , \n                           data = df_forest_fires_bejaia, \n                           method = 'class', control = list(cp = 0.8))\nrpart.plot(simple.tree.model)\ncomplex.tree.model <- rpart(Classes ~ Temperature + Rain + FFMC , \n                            data = df_forest_fires_bejaia, \n                            method = 'class', control = list(cp = 0.001))\nrpart.plot(complex.tree.model)\n\n\n\n\n\nsimple.tree.model2 <- rpart(Classes ~ Temperature + Rain + FFMC , \n                            data = df_forest_fires_bejaia, \n                            method = 'class', control=list(minbucket=80))\nrpart.plot(simple.tree.model2)\n\n\n\n\n\ncomplex.tree.model2 <- rpart(Classes ~ Temperature + Rain + FFMC , \n                             data = df_forest_fires_bejaia, \n                             method = 'class', control=list(minbucket=1))\nrpart.plot(complex.tree.model2)\n\n\n\n\n\nsimple.tree.model3 <- rpart(Classes ~ Temperature + Rain + FFMC , \n                            data = df_forest_fires_bejaia, \n                            method = 'class', control = list(maxdepth = 0))\nrpart.plot(simple.tree.model3)\n\n\n\n\n\ncomplex.tree.model3 <- rpart(Classes ~ Temperature + Rain + FFMC , \n                             data = df_forest_fires_bejaia, \n                             method = 'class', control = list(maxdepth = 5))\nrpart.plot(complex.tree.model3)\n\n\n\n\n\n\nHere I tried to tweak different parameters to make sure the model isnâ€™t overfitting the data, however, they all lend to the same decision tree: no parameters will make this model more complex (increasing the risk of overfitting) but this model could be simpler as in simple.tree.model2 and simple.tree.model3 (decreasing the risk of overfitting). However here the simple models significantly underfit the data as they only render one node containing 100% of the data and predicting â€œnot fireâ€. We can say that it underfits the data because the prediction is solely based on the distribution of the training data and not on any predictive parameters. Thus we are keeping the original model for further steps. Here is a breakdown of the different parameters tweak I tried:\n\n\ncost complexity parameter (cp): I tried fitting a model with a very large cp (0.8) that would render a much simpler model, decreasing the risk of overfitting. Even though the exercises focused on overfitting, I tried fitting a model with a very small cp (0.001) to make sure the model wasnâ€™t already underfitting the data. But the original model is rendered despite of the value of the cost complexity parameter.\nthe minimum number of observations per node (minbucket): I tried fitting a model with a very large minbucket (80) that renders a simpler model, decreasing the risk of overfitting. Even though the exercises focused on overfitting, I tried fitting a model with a very small minbucket (1) to make sure the model wasnâ€™t already underfitting the data. But the original model is rendered with a small minbucket parameter.\nthe maximum depth of nodes in the final tree (maxdepth): I tried fitting a model with a null maxdepth (0) that renders a simpler model, as it by definition can only contain the root node, decreasing the risk of overfitting. Even though the exercises focused on overfitting, I tried fitting a model with a very large maxdepth (10) to make sure the model wasnâ€™t already underfitting the data. But the original model is rendered with a large maxdepth parameter.\n\n\n\n# Step 6\nclass_pred <- as.factor(as.vector(plot_df$class_pred))\n# Accuracy\naccuracy_test <- sum(class_pred == plot_df$Classes)/nrow(df_forest_fires_sidi)\ncat(sprintf(\"%.2f %%\", accuracy_test*100))\n\n100.00 %\n\n\n\n# Specificity: True Negative Rate\ntotal_real_no      <- sum(plot_df$Classes == \"not fire\")\ntotal_correct_no   <- sum(class_pred == \"not fire\" & plot_df$Classes == \"not fire\")\n\nTNR_test <- total_correct_no/total_real_no\ncat(sprintf(\"%.2f %%\", TNR_test*100))\n\n100.00 %\n\n\n\n# Sensitivity or Recall: True Positive Rate\n\ntotal_real_yes      <- sum(plot_df$Classes == \"fire\")\ntotal_correct_yes   <- sum(class_pred == \"fire\" & plot_df$Classes == \"fire\")\n\nTPR_test <- total_correct_yes/total_real_yes\ncat(sprintf(\"%.2f %%\", TPR_test*100)) \n\n100.00 %\n\n\n\n## Precision: How many True Positives over all predicted=\"fire\"? \ntotal_predicted_yes <- sum(class_pred == \"fire\")\n\nprecision_test <- total_correct_yes/total_predicted_yes\ncat(sprintf(\"%.2f %%\", precision_test*100)) \n\n100.00 %\n\n\n\nf1_test <- (2*precision_test*TPR_test)/(precision_test + TPR_test)\nf1_test\n\n[1] 1\n\n\n\n\nI calculate the same model diagnostics measures as for our precedent models. First, the exact accuracy as manually calculated above is calculated (100 % vs.Â 96.72% for our logistic regression model). Then we calculate the specificity, also called the True Negative Rate which is 100 % (vs.Â 93.02% for our logistic regression model). Then we calculate the True Positive Rate, called Sensitivity or Recall: 100 % (vs.Â 98.73% for our logistic regression model). Subsequently, we calculate the precision, the rate of true positives that predicted our alternate variable â€œfireâ€, here 100 % (vs.Â 96.30% for our logistic regression model). Lastly, we calculate the F1 score, a composite measure of the sensitivity and precision and we get 1 (vs.Â 0.97 on our training data for the best treshold)). These measures suggest the decision tree model generalized excellently from our training dataset to our test dataset, because the Accuracy, Sensitivity, Specificity, and Precision of our the model on our test dataset are very close are all of 100%, suggesting there was no overfitting of the training data. 100% is evidently the highest possible percentage, as well as the F1 score of 1, which is the maximum score possible.\nThis suggest our alternative model is better than our logistic regression model, as all the measures of model fit on the test data are superior. However, this might be due to the decision tree algorithm that only chose to use one predictor variable FFMC (out of the three given FFMC, Rain and Temperature) to predict classes. It could be that a logistic regression model solely fitted on FFMC as the same model fit on the test data, as the addition of Temperature and Rain could lead to overfitting the logistic regression model to the test data. Until we test this, we cannot say the the decision tree algorithm provides a better fit to the data than the logistic regression algorithm fitted on only FFMC. In all, we can say that fitted on the same three predictors, the alternative model is better than the original one but it could be due to the nature of the decision tree algorithm that chooses to only keep certain predictors."
  },
  {
    "objectID": "assessments/summative1_solutions.html#decompress",
    "href": "assessments/summative1_solutions.html#decompress",
    "title": "âœ”ï¸ Summative Problem Set 01 | Solutions",
    "section": "Decompress",
    "text": "Decompress\nHow do you plan to reward yourself for completing this problem set?\n\nBy working on my dissertation lol."
  },
  {
    "objectID": "assessments/summative2.html",
    "href": "assessments/summative2.html",
    "title": "Summative Problem Set 02 | W08-W10",
    "section": "",
    "text": "Welcome to the second Summative Problem Set of DS202 (2022/23)!\nThings to know before you start:"
  },
  {
    "objectID": "assessments/summative2.html#cycling-in-london-how-dangerous-is-it",
    "href": "assessments/summative2.html#cycling-in-london-how-dangerous-is-it",
    "title": "Summative Problem Set 02 | W08-W10",
    "section": "ğŸš´ Cycling in London: How dangerous is it?",
    "text": "ğŸš´ Cycling in London: How dangerous is it?\nIn this problem set, we will explore a real-world dataset recording the Greater Londonâ€™s cycling-involved incidents from 2017 to 2020. We obtained data from two different sources:\n\nRoad Safety Data from the Department for Transport, UK 3\nIndices of Multiple Deprivation (IMD) 2019, published by the Ministry of Housing, Communities and Local Government, UK 4\n\nTrivia: According to the Evening Standard, during the pandemic the risk of being killed or seriously injured (KSI) fell from 1.2 to 0.9 incidents per million kilometres cycled 5. Note that our data only contains data cycling-related accidents, we do not have data about all the cycle routes people take that do not lead to accidents.\n(Click on the links in the footnotes if you want to read more about the data)"
  },
  {
    "objectID": "assessments/summative2.html#your-unique-assignment",
    "href": "assessments/summative2.html#your-unique-assignment",
    "title": "Summative Problem Set 02 | W08-W10",
    "section": "Your Unique Assignment",
    "text": "Your Unique Assignment\nYour goal is to build a model to predict the severity of traffic accidents. But there is a twist: your summative is customised just for you!\nThis RMarkdown is common to everyone, the data is also the same, but each one of you are allocated to a unique combination of variables, metrics and parameters you MUST use when completing your assignment. This combination was randomly allocated to your candidate number (we will share the R Script that we used to generate these conditions, in case you are curious about it).\nBefore you continue to read the instructions, open the spreadsheet DS202_2022MT_Summative02_candidate_allocation (link available on Moodle) and identify the line that contains your candidate_number.\nIn there, you will find the following information about your assignment that is unique to you:\n\ntarget_variable\ntask_type\nmetric\npredictors identified by columns: predictor1, predictor2 & predictor3 as well as the formula_str\ncp: a parameter of the Decision Tree algorithm\n\nBelow, you find a description of the elements of this assignment that vary from person to person.\n\nElement 01: Target Variable\nEach one of you will be allocated a specific target variable and you can find it in the column target_variable of the spreadsheet. This is either:\n\naccident_severity, a number that ranges from 1 to 3; OR,\nis_grave_accident, a â€œYesâ€ vs â€œNoâ€ categorical variable.\n\nIf your target variable is accident_severity, you are asked to perform Regression; on the other hand, if your target variable is is_grave_accident, you are asked to perform a Classification task.\n\n\nElement 02: Goodness-of-Fit Metric\nYou are also asked to use a specific metric to assess the goodness-of-fit of your models. This metric will also be unique to you and you can find it in the column metric of the spreadsheet.\nIf you were allocated a Regression task, you might asked to use either the Mean Absolute Error (MAE) or the Adjusted R-square (adj-R2). On the other hand, if your task is Classification, you might asked to use either F1-score or Recall.\n\n\nElement 03: Selected Variables\nThe dataset contain many variables; but for this assignment, you are asked to use only THREE predictors. You must use the three predictors that are uniquely allocated to you. Your unique predictors are identified by the three columns below:\n\npredictor1\npredictor2\npredictor3\n\nor, simpler, check the column formula.\nFor example, if the row relative to your candidate number contains the following formula:\naccident_severity ~ light_conditions + road_surface_conditions + season\nThis means you are only allowed to use the predictors light_conditions, road_surface_conditions and season to build your models.\n\n\nElement 04: Decision Treeâ€™s cp parameter\nFinally, another element that is unique to you is the parameter cp used to control how the Decision Tree is built. You must use the cp that is associated to your candidate number in the spreadsheet."
  },
  {
    "objectID": "weeks/week01.html",
    "href": "weeks/week01.html",
    "title": "ğŸ—“ï¸ Week 01 - Introduction, Context & Key Concepts",
    "section": "",
    "text": "In this first week, we will cover what you can expect to learn from this course and the course logistics: all you need to know about the structure of the lectures, classes, assessments and how we will interact throughout this course.\nWe will also cover some of the basics: what do we mean by data science & machine learning, and what do you need to do to get the most of this course?\nJoin the lecture on 30 September 2022 at 2pm at NAB LG.01 (Wolfson Theatre)."
  },
  {
    "objectID": "weeks/week01.html#links",
    "href": "weeks/week01.html#links",
    "title": "ğŸ—“ï¸ Week 01 - Introduction, Context & Key Concepts",
    "section": "Links",
    "text": "Links\n\nğŸ‘¨â€ğŸ« Lecture slides\nğŸ“’ Preparing for next weekâ€™s lab\nğŸ”– Appendix"
  },
  {
    "objectID": "weeks/week02.html",
    "href": "weeks/week02.html",
    "title": "ğŸ—“ï¸ Week 02 - Simple and Multiple Linear Regression",
    "section": "",
    "text": "If you have already taken stats level at university, it is likely that you have met the linear regression method.\nIt is the most fundamental algorithm for regression and if you truly master it, you will be way ahead of the curve when it comes to the most advanced Machine Learning algorithms.\nJoin the lecture 7 October 2022 2pm at NAB LG.01 (Wolfson Theatre).."
  },
  {
    "objectID": "weeks/week02.html#links",
    "href": "weeks/week02.html#links",
    "title": "ğŸ—“ï¸ Week 02 - Simple and Multiple Linear Regression",
    "section": "Links",
    "text": "Links\n\nğŸ‘¨â€ğŸ« Lecture slides\nâœ… Take a look at this weekâ€™s checklist\nğŸ’» This weekâ€™s lab\nâœ”ï¸ Lab Solutions"
  },
  {
    "objectID": "weeks/week03.html",
    "href": "weeks/week03.html",
    "title": "ğŸ—“ï¸ Week 03 - Classifiers",
    "section": "",
    "text": "On ğŸ—“ï¸ Week 02, we learned how to make predictions about numerical variables. But what if you wanted to predict whether someone will perform an action (a Yes or No question)? Or, say, you were interested in assessing how the risk of fraud increases depending on the behaviour of a customer? These problems can be modelled using classifiers, a type of supervised learning.\nThis week, we will explore two classifier algorithms: the Logistic Regression and the Naive Bayes classifiers. We will learn how those methods relate (or not) to linear regression, and how to interpret its results. You will also meet a few new metrics and will learn of new ways to assess the â€˜accuracyâ€™ of models. These metrics will be incrediblly important on Week 04!\nJoin the lecture 14 October 2022 2pm at NAB LG.01 (Wolfson Theatre)."
  },
  {
    "objectID": "weeks/week03.html#links",
    "href": "weeks/week03.html#links",
    "title": "ğŸ—“ï¸ Week 03 - Classifiers",
    "section": "Links",
    "text": "Links\n\nğŸ‘¨â€ğŸ« Lecture slides\nâœ… Take a look at this weekâ€™s checklist\nğŸ’» This weekâ€™s lab\nâœ”ï¸ Lab Solutions"
  },
  {
    "objectID": "weeks/week04.html",
    "href": "weeks/week04.html",
    "title": "ğŸ—“ï¸ Week 04 - Resampling Methods",
    "section": "",
    "text": "We have made it to Week 04! In the labs, you will get a chance to explore different Classification algorithms (Logistic Regression and Naive Bayes) and how to think about their output. What can we say about predicted probabilities?\nThis weekâ€™s lecture (on Friday 21 October 2022, Wolfson Theatre NAB.LG.01 2pm-4pm) will be a bit different. We will not learn about new algorithms. Instead, we will learn how to compare models built from the three algorithms we learned about (linear regression, logistic regression and naive bayes). You will learn about the different metrics one could use to assess the predictive performance on a test set rather than just the goodness-of-fit of the training data.\nThe structure of the lecture will also be a bit different, it will resemble a workshop more than a taught lecture. See you there!"
  },
  {
    "objectID": "weeks/week04.html#links",
    "href": "weeks/week04.html#links",
    "title": "ğŸ—“ï¸ Week 04 - Resampling Methods",
    "section": "Links",
    "text": "Links\n\nğŸ‘¨â€ğŸ« Lecture\nâœ… Take a look at this weekâ€™s checklist\nğŸ’» This weekâ€™s lab\nâœ”ï¸ Lab Solutions"
  },
  {
    "objectID": "weeks/week05.html",
    "href": "weeks/week05.html",
    "title": "ğŸ—“ï¸ Week 05 - Non-linear algorithms",
    "section": "",
    "text": "Welcome to Week 05 of DS202!\nIn the labs, you will practice training/test splits, cross-validation and you will learn about the bootstrap. Check Chapter 5 of our textbook to reinforce these concepts during the week.\nAt the end of the week, we will explore two new algorithms: Support Vector Machine and Decision Trees.\nStarting this week, our lecture will take place in a new classroom. Join us this Friday, 28 October 2022 from 2 p.m. to 4 p.m. at NAB.LG.08."
  },
  {
    "objectID": "weeks/week05.html#links",
    "href": "weeks/week05.html#links",
    "title": "ğŸ—“ï¸ Week 05 - Non-linear algorithms",
    "section": "Links",
    "text": "Links\n\nğŸ‘¨â€ğŸ« Lecture\nâœ… Take a look at this weekâ€™s checklist\nğŸ’» This weekâ€™s lab\nâœ”ï¸ Lab Solutions\nâœï¸ First Summative W05-W07"
  },
  {
    "objectID": "weeks/week06.html",
    "href": "weeks/week06.html",
    "title": "ğŸ—“ï¸ Week 06 - Reading Week",
    "section": "",
    "text": "We know many of you will be using the Reading Week to work on your summative assessments (deadline: 9 Nov).\nTo support you with your problem sets, we have three drop-in sessions this week:\nYou donâ€™t need to book, just show up. But if you want to have the session on your calendar, you can book your visit via Student Hub."
  },
  {
    "objectID": "weeks/week06.html#links",
    "href": "weeks/week06.html#links",
    "title": "ğŸ—“ï¸ Week 06 - Reading Week",
    "section": "Links",
    "text": "Links\n\nâœ… Take a look at this weekâ€™s checklist\nğŸ’» RNotebook created during Drop-In Session (Jon)"
  },
  {
    "objectID": "weeks/week07.html",
    "href": "weeks/week07.html",
    "title": "ğŸ—“ï¸ Week 07 - Recap of tidyverse + Clustering Algorithms",
    "section": "",
    "text": "Welcome to Week 07 of DS202!\nThis weekâ€™s lab draws on ğŸ—“ï¸ Week 05 lecture content and on feedback given by the course representatives about the main struggles you are facing with R (tidyverse).\nIn the lecture, we will recap some tidyverse concepts â€“ things we wish we had taught you in the first weeks of this course! â€“ and then, on the second part, we will introduce the idea of unsupervised learning (clustering).\nJoin us on Friday, 11 November 2022 from 2 p.m. to 4 p.m. at NAB.LG.08."
  },
  {
    "objectID": "weeks/week07.html#links",
    "href": "weeks/week07.html#links",
    "title": "ğŸ—“ï¸ Week 07 - Recap of tidyverse + Clustering Algorithms",
    "section": "Links",
    "text": "Links\n\nâœï¸ First Summative W05-W07, worth 20%, deadline: 9 November 2022.\nğŸ’» This weekâ€™s lab\nğŸ‘¨â€ğŸ« Lecture\nâœ… Take a look at this weekâ€™s checklist\nâœ”ï¸ Lab Solutions"
  },
  {
    "objectID": "weeks/week08.html",
    "href": "weeks/week08.html",
    "title": "ğŸ—“ï¸ Week 08 - Dimensionality Reduction",
    "section": "",
    "text": "Welcome to Week 08 of DS202!"
  },
  {
    "objectID": "weeks/week08.html#links",
    "href": "weeks/week08.html#links",
    "title": "ğŸ—“ï¸ Week 08 - Dimensionality Reduction",
    "section": "Links",
    "text": "Links\n\nâœ… Take a look at this weekâ€™s checklist\nğŸ’» This weekâ€™s lab\nâœ”ï¸ Lab Solutions\nâœ¨ This weekâ€™s bonus lab (advanced)\nğŸ‘¨â€ğŸ« Lecture"
  },
  {
    "objectID": "weeks/week09.html",
    "href": "weeks/week09.html",
    "title": "ğŸ—“ï¸ Week 09 - Applications: Text as Data & Topic Modelling",
    "section": "",
    "text": "Welcome to Week 09 of DS202!\nThis week, Prof.Â Ken Benoit is confirmed to come and deliver a talk Applications: Text as Data & Topic Modelling, about applications of Machine Learning techniques to text data. You will have the chance to practice with text data in the Week 11 labs.\nThis talk was initially planned for Week 10 but it got moved to Week 09 due to a calendar clash. Syllabus page have been updated to reflect this change.\nJoin on Friday, 25 November 2022 from 2 p.m. to 4 p.m. at NAB.LG.08."
  },
  {
    "objectID": "weeks/week09.html#links",
    "href": "weeks/week09.html#links",
    "title": "ğŸ—“ï¸ Week 09 - Applications: Text as Data & Topic Modelling",
    "section": "Links",
    "text": "Links\n\nâœ… Take a look at this weekâ€™s checklist\nğŸ’» This weekâ€™s lab\nğŸ‘¨â€ğŸ« Lecture\nâœ”ï¸ Lab Solutions ğŸ”œ"
  },
  {
    "objectID": "weeks/week10.html",
    "href": "weeks/week10.html",
    "title": "ğŸ—“ï¸ Week 10 - Applications: Predictive Modelling on Tabular Data",
    "section": "",
    "text": "Welcome to Week 10 of DS202!\nThis week, you will get a chance to explore Principal Component Analysis (PCA) in the labs and I will also share a few short videos to help reinforce your theoretical understanding about PCA.\nIn the lecture, we will continue to explore real-life applications of Machine Learning. In the first part of the lecture, I will show you a few examples of applications and in the second half, we will hear from Dr.Â Stuart Bramwell and his research.\nJoin us on Friday, 2 December 2022 from 2 p.m. to 4 p.m. at NAB.LG.08."
  },
  {
    "objectID": "weeks/week10.html#links",
    "href": "weeks/week10.html#links",
    "title": "ğŸ—“ï¸ Week 10 - Applications: Predictive Modelling on Tabular Data",
    "section": "Links",
    "text": "Links\n\nâœï¸ Third Summative W10-W11+1 ğŸ”œ worth 20%, release: 2 December* Deadline: 15 December 2022.\nğŸ’» This weekâ€™s lab\nâœ¨ This weekâ€™s bonus lab (for everyone)\nğŸ‘¨â€ğŸ« Lecture\nâœ… Take a look at this weekâ€™s checklist\nâœ”ï¸ Last weekâ€™s lab solutions ğŸ”œ"
  },
  {
    "objectID": "weeks/week01/appendix.html",
    "href": "weeks/week01/appendix.html",
    "title": "ğŸ”– Week 01 - Appendix",
    "section": "",
    "text": "This weekâ€™s indicative reading: (James et al. 2021, chaps. 2, 2.1â€“2.2)\n\n\nNeed to recap probability and statistics concepts? Check the suggested readings below:\n\n(Warne 2018, chaps. 1-3,5,6,11-12)\n(Gelman, Hill, and Vehtari 2020, chaps. 1â€“4)\nIf you are a PBS student, you can revisit the content of PB130 (MT3, MT4, MT8-MT11)"
  },
  {
    "objectID": "weeks/week01/appendix.html#recommended-additional-reading",
    "href": "weeks/week01/appendix.html#recommended-additional-reading",
    "title": "ğŸ”– Week 01 - Appendix",
    "section": "Recommended (additional) reading",
    "text": "Recommended (additional) reading\nWhat are different ways one can approach a modelling problem?\nCheckout the upcoming book â€˜Modeling Mindsetsâ€™ (Molnar 2022, chaps. 2â€“3) (itâ€™s free to read online) to learn about the traditional frequentist statistics vs Bayesian statistics vs Machine Learning approaches.\nThe following twitter thread also summarises the main points of these different paradigms:\n\n\nIn a perfect world, you could effortlessly switch between modeling mindsets (statistics, machine learning, causal inference, â€¦).Realistically, you only have time to master a few mindsets.So what to do? A thread ğŸ§µ\n\nâ€” Christoph Molnar (@ChristophMolnar) August 30, 2022"
  },
  {
    "objectID": "weeks/week01/appendix.html#lse-digital-skills-lab",
    "href": "weeks/week01/appendix.html#lse-digital-skills-lab",
    "title": "ğŸ”– Week 01 - Appendix",
    "section": "LSE Digital Skills Lab",
    "text": "LSE Digital Skills Lab\nLSE Digital Skills Lab offers R and python workshops during Term time and they will also give DSI students access to self-paced programming courses via Dataquest.\nFollow the links below to take the pre-sessional self-paced courses:\n\nR for Data Science Pre-sessional Course 22/23\n\nAlso, keep an eye on the following pages for news of the in-person workshops:\n\nR workshops"
  },
  {
    "objectID": "weeks/week01/appendix.html#other-resources",
    "href": "weeks/week01/appendix.html#other-resources",
    "title": "ğŸ”– Week 01 - Appendix",
    "section": "Other resources",
    "text": "Other resources\n\nCheckout this summerâ€™s LSE Careers Skill Accelerator programme. Some of the self-paced courses will remain open to LSE students until the end of the year.\n\n\n\nReady to develop the key skills employers are looking for in 2022?Join this summer's LSE Careers Skills Accelerator programme to expand your skillset!â­Apply on CareerHub by 11.59pm, Wed 15 June for your chance to join the programmeâ­https://t.co/J5sI1NRaMA\n\nâ€” LSE Careers (@LSECareers) June 9, 2022\n\n\n\nThe book R for Data Science is free to read online and is a great resource to advance your R skills."
  },
  {
    "objectID": "weeks/week01/lecture.html",
    "href": "weeks/week01/lecture.html",
    "title": "ğŸ‘¨â€ğŸ« Week 01 - Slides",
    "section": "",
    "text": "Either click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides."
  },
  {
    "objectID": "weeks/week01/lecture.html#coffee-break-10-min",
    "href": "weeks/week01/lecture.html#coffee-break-10-min",
    "title": "ğŸ‘¨â€ğŸ« Week 01 - Slides",
    "section": "â˜• Coffee Break (10 min)",
    "text": "â˜• Coffee Break (10 min)\nUse this time to chat, stretch, drink some coffee or just relax for a bit by yourself."
  },
  {
    "objectID": "weeks/week01/lecture.html#part-ii---key-concepts-45-50-min",
    "href": "weeks/week01/lecture.html#part-ii---key-concepts-45-50-min",
    "title": "ğŸ‘¨â€ğŸ« Week 01 - Slides",
    "section": "Part II - Key Concepts (45-50 min)",
    "text": "Part II - Key Concepts (45-50 min)\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides."
  },
  {
    "objectID": "weeks/week01/prep.html",
    "href": "weeks/week01/prep.html",
    "title": "ğŸ“’ Week 01 - Preparing for next weekâ€™s lab",
    "section": "",
    "text": "We do not have classes this week. Instead, we recommend you use this time to learn or revisit the basics of the programming language R.\nSee sections below for advice on how to do that."
  },
  {
    "objectID": "weeks/week01/prep.html#preparing-for-the-next-week",
    "href": "weeks/week01/prep.html#preparing-for-the-next-week",
    "title": "ğŸ“’ Week 01 - Preparing for next weekâ€™s lab",
    "section": "ğŸ“’ Preparing for the next week",
    "text": "ğŸ“’ Preparing for the next week\n\nJoin the Slack group of this course (more info)\nUse this time to learn or revisit basic R programming skills.\nWrite down your R questions. Next weekâ€™s lab, we have a roadmap for a revision of basic commands but we can tailor it to address any questions you might have with R."
  },
  {
    "objectID": "weeks/week01/prep.html#recommended-reading-other-resources",
    "href": "weeks/week01/prep.html#recommended-reading-other-resources",
    "title": "ğŸ“’ Week 01 - Preparing for next weekâ€™s lab",
    "section": "ğŸ”– Recommended reading & other resources",
    "text": "ğŸ”– Recommended reading & other resources\nCheck out the Appendix page."
  },
  {
    "objectID": "weeks/week02/checklist.html",
    "href": "weeks/week02/checklist.html",
    "title": "âœ… Week 02 - Checklist",
    "section": "",
    "text": "Follow the suggested list of actions below to get the most out of this course:\n\nYour Checklist:\nYour Checklist:\n\nğŸ–¥ï¸ Before you come to the class, skim through the W02 lab roadmap page to have an idea of what we are going to do.\nğŸ‘¨â€ğŸ’» New to R? Or perhaps you are in one of the Monday sessions and struggled to follow the lab? There is still time to take the R pre-sessional course. (Read the Getting access and using Dataquest session carefully)\nğŸ“š Before you attend the lecture on Friday, try to catch up on the recommended reading of last week.\nğŸ’» Assess your own understanding: did you understand all the exercises of the lab?\nğŸ“ Take note of anything that is still not clear to you.\nğŸ“Ÿ Share your conceptual or programming-related questions on #week02 channel on Slack.\nğŸ“¤ Have anything else to share? If came across an interesting resource for R beginners, or curious articles about exploratory data analysis, feel free to share it on /week02 or /random channels in our Slack group.\n\nFollowing this will keep you well prepared for the Linear Regression lab of Week 03."
  },
  {
    "objectID": "weeks/week02/lab.html",
    "href": "weeks/week02/lab.html",
    "title": "ğŸ’» Week 02 - Lab Roadmap (90 min)",
    "section": "",
    "text": "This week, we will recap some basic R commands for social data science and then apply these commands to a practical case. We will learn about data structures and some simple data visualisation skills in R .\nIt is expected that R has been downloaded locally. We recommend that you run R within an integrated development environment (IDE) such as RStudio, which can be freely downloaded."
  },
  {
    "objectID": "weeks/week02/lab.html#step-1-basic-commands",
    "href": "weeks/week02/lab.html#step-1-basic-commands",
    "title": "ğŸ’» Week 02 - Lab Roadmap (90 min)",
    "section": "Step 1: Basic commands",
    "text": "Step 1: Basic commands\nWe will follow the instructions below step by step together while answering whatever questions you might encounter along the way.\n\nOpen R or RStudio. You can either run the folllowing commands in a R script or in the console window.\nCreate a vetor of numbers with the function c() and name it x. When we type x, it gives us back the vector:\n> x <- c(1, 3, 2, 5)\n> x\n[1] 1 3 2 5\nNote that the > is not part of the command; rather, it is printed by R to indicate that it is ready for another command to be entered. We can also save things using = rather than <-:\n> x = c(1, 3, 2, 5)\nCheck the length of vector x using the length() function:\n> length(x)\n[1] 4\nCreate a matrix of numbers with the function matrix() and name it y. When we type y, it gives us back the matrix:\n> y <- matrix(data = c(1:16), nrow = 4, ncol = 4)\n> y\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\nIf you want to learn about the meaning of some arguments like nrow or ncol:\n> ?matrix\nSelect one element in the matrix y:\n> y[2,3]\n[1] 10\nThe first number after the open-bracket symbol [ always refers to the row, and the second number always refers to the column\nSelect multiple rows and column at a time in the matrix y:\n> y[c(1, 3), c(2, 4)]\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n> y[1:3, 2:4]\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n> y[1:2, ]\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n> y[-c(1, 3), ]\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\nNo index for the columns or the rows indicates that R should include all columns or all rows, respectively. The use of a negative sign - in the index tells R to keep all rows or columns except those indicated in the index.\nCheck the number of rows and columns in a matrix:\n> dim(y)\n[1] 4 4\nGenerate a vector of random normal variables:\n> set.seed(1303)\n> x <- rnorm(50)\n> y <- x + rnorm(50, mean = 50, sd = .1)\n> cor(x, y)\n[1] 0.9942128\nBy default, rnorm() creates standard normal random variables with a mean of 0 and a standard deviation of 1. However, the mean and standard deviation can be altered as illustrated above.\nEach time we call the function rnorm(), we will get a different answer. However, sometimes we want our code to reproduce the exact same set of random numbers; we can use the set.seed() function to do this. We use set.seed() throughout the labs whenever we perform calculations involving random quantities.\nLetâ€™s check some descriptive statistics of these vectors:\n> mean(y)\n[1] 50.18446\n> var(y)\n[1] 0.8002002\n> sqrt ( var (y))\n[1] 0.8945391\n> sd(y)\n[1] 0.8945391\n> cor (x, y)\n[1] 0.9942128\nThe mean() and var() functions can be used to compute the mean and variance of a vector of numbers. Applying sqrt() to the output of var() will give the standard deviation. Or we can simply use the sd() function. The cor() function is to compute the correlation between vector x and y."
  },
  {
    "objectID": "weeks/week02/lab.html#step-2-graphics",
    "href": "weeks/week02/lab.html#step-2-graphics",
    "title": "ğŸ’» Week 02 - Lab Roadmap (90 min)",
    "section": "Step 2: Graphics",
    "text": "Step 2: Graphics\nWe will plot and save plots in R.\n\nProduce a scatterplot between two vectors of numbers using the function plot():\n> set.seed(1303)\n> x <- rnorm(100)\n> y <- rnorm(100)\n> plot(x,y)\n> plot(x, y, xlab = \" this is the x- axis \",\n       ylab = \" this is the y- axis \",\n       main = \" Plot of X vs Y\")\nBy default, the output plot will show in Plots window in the lower right cornor.\nSave the scatterplot in a pdf or a jpeg file:\n> pdf(\"Figure.pdf\")\n> plot(x, y, col = \"green\")\n> dev.off()\nnull device\n        1    \nTo create a jpeg, we use the function jpeg() instaed of pdf(). The function dev.off() indicates to R that we are done creating the plot.\nProduce a contour plot (like a topographical map) to represent 3-Dimentional data using the function contour():\n> x <- seq(1, 10)\n> y <- x\n> f <- outer(x, y, function (x, y) cos(y) / (1 + x^2))\n> contour(x, y, f)\n> contour(x, y, f, nlevels = 45, add = T)\n> fa <- (f - t(f)) / 2\n> contour(x, y, fa, nlevels = 15)\nThe image() function works the same way as contour(). Explore it if you are interested.\nUsing ggplot2 package for graphic:\nIn R, the data is stored in a structure called dataframe. Dataframe can be seen as a 2-dimensional table consisting of rows and columns and their values. These values might be in different types such as numeric, character or logical. However, each column should have the exactly same data type.\nWe can use the open-source data visualization package - ggplot2 to construct aesthetic mappings based on our data.\n\nSince tidyverse library includes ggplot2, if you install tidyverse you will have access to ggplot2; installation can be done;\n\n> install.packages(\"tidyverse\")\n\nAlternatively, ggplot2 package can be installed\n\n> install.packages(\"ggplot2\")\nAfter the installation is completed, it should be called in R environment:\n> library(ggplot2)\nThere are some ready datasets to play with in the package ggplot2. Letâ€™s explore and plot a dataset called diamonds showing the prices and some features of over 50000 diamonds. You can explore the meanings of the variables with ?diamonds command.\nPlease type:\n> View(diamonds)\nthe View() function can be used to view it in a spreadsheet-like window.\nwe can plot this dataset with desired variables.\n > ggplot(diamonds[0:50,], aes(x=carat, y=price)) +\ngeom_point() + \ngeom_text(label=diamonds[0:50,]$cut)\nx and y in aes shows the axis which are the carat and the price info each diamond. diamonds is the dataframe used in the plot and We used only the first 50 lines for clear visualisation. geom_point defines the shape of data to be plot and geom_text adds the labels. With $ sign, you can access a column in your dataset.\nWe can also plot a histogram showing price\n > ggplot(diamonds,aes(x=price)) + geom_histogram(binwidth=100)\nThis time all dataset is used for the visualisation.. For more detailed information and some examples you can use ?ggplot and ?aes\n\n\n\n\n\n\n\nFurther Study - Heatmap Example\n\n\n\n\n\nCreating a heatmap with ggplot2 package:\nThis time we will create a dummy dataframe with country names, a time period and random GDP for each country.\ncountries <- c(\"Canada\", \"France\",\"Greece\",\"Libya\",\"Malta\")\nyears <- c(2012:2021)\nLetâ€™s gather them together and see what our dataframe looks like:\ndata <- expand.grid(Country=countries, Year=years)\ndata\nexpand.grid creates a dataframe from all combinations of the supplied vectors.\nto create random GPD for each country and for each year, and to add these values into our dataframe as GDP column::\ngdps  <- runif(50, min=20000, max=500000)\ndata$GDP = gdps\nrunif generates a certain number of random values between min and maximum values with a uniform distribution. Since we have 5 countries and 10 year, we generated 50 random GPD value.\nTo check the data and type of the variable data:\nView(data)\nclass(data)\nWe can plot now a very basic heatmap\nggplot(data, aes(Year, Country, fill= GDP)) + geom_tile()\nTo create a heatmap, our dataframe should look like a tabular dataset with three columns. aes defines X,Y axis and the values filling these pairs in the heatmap. geom_tile creates a heatmap with rectangulars with different options. For detailed information ?geom_tile"
  },
  {
    "objectID": "weeks/week02/lab.html#step-3-loading-data",
    "href": "weeks/week02/lab.html#step-3-loading-data",
    "title": "ğŸ’» Week 02 - Lab Roadmap (90 min)",
    "section": "Step 3: Loading data",
    "text": "Step 3: Loading data\nNow, we will learn how to import a data set into R and explore the data set. For this lab session, we will use a ready-to-use dataset AUTO in the book â€œIntroduction to Statistical Learning, with Applications in Râ€. With the package ISLR2, we can use all the datasets in the book.\n\nFirst, we need to install ISLR2 into our R environment for future use.\n> install.packages(\"ISLR2\")\nTo use ISLR2 package and the datasets in our analyses, we need to call it in each R session with;\n> library(ISLR2)\nThatâ€™s it! We now can use all datasets by calling them by their names. The package includes numerous datasets and you can explore them with R.\nAUTO dataset is ready to be used in the analyse. You can explore the dataset by using:\n> View(Auto)\n> head(Auto)\nThe head() function can also be used to view the first few rows of the data\nYou may want to save this dataset on a local computer, which is useful for your future analyses while doing some changes on it. To save a dataset as a csv file:\n> write.csv(DataFrameName, file=\"Path to save the DataFrame//File Name.csv\", row.names = FALSE)\nThe option row.names = FALSE deletes the row names when you are saving the dataset. In this case, it will remove basic incremental indexes such as 1,2,â€¦ from the data. A detailed explanation of write.csv and its options could be found by typing ?write.csv\n\n\n\n\n\n\nExample\n\n\n\nYou need to include the path where you would like to save the dataset on your computer. For example, if you work in a folder called Test in your desktop in a Windows machine. The code:\n> write.csv(Auto, \"C:Users//LSE//Desktop//Test//autodataset.csv\", row.names = FALSE )\n\n\nTo use the dataset in the future, you need to load it into a dataframe by importing the csv file.\nWe will load this dataset in a dataframe called Auto. Dataframe name is changable, however we would like to use words understandable and readable.\n> Auto <- read.csv(\"C://Users//LSE//Desktop//Test//autodataset.csv\", na.strings = \"?\")\nUsing the option na.strings tells R that any time it sees a particular character or set of characters (such as a question mark), it should be treated as a missing element of the data matrix.\nYou can check the dataset:\n> View(Auto)\n> head(Auto)\nDeal with the missing data by removing rows with missing observations:\n> Auto <- na.omit(Auto)\n> dim(Auto)\n[1] 392 9\nThe function dim() is to check the size of the data frame.\nProduce a numerical summary of each variable in the particular data frame:\n> summary(Auto)"
  },
  {
    "objectID": "weeks/week02/lab.html#step-4-practical-exercises-in-pairs",
    "href": "weeks/week02/lab.html#step-4-practical-exercises-in-pairs",
    "title": "ğŸ’» Week 02 - Lab Roadmap (90 min)",
    "section": "Step 4: Practical exercises (in pairs)",
    "text": "Step 4: Practical exercises (in pairs)\nSo far, we have learnt some basic commands in R. In this practical case, we will continues with the data set Auto studied in Step 3. Make sure that the missing values have been removed from the data.\nSix questions are listed below. You are required to try to answer these questions in pair using R commands. We will go over the solutions once everyone has finished these questions.\nğŸ¯ Questions\n\nWhich of the predictors are quantitative, and which are qualitative?\nWhat is the range of each quantitative predictor? (hint: You can answer this using the range() function)\nWhat is the mean and standard deviation of each quantitative predictor?\nNow remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?\nUsing the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings.\nSuppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer."
  },
  {
    "objectID": "weeks/week02/lab_solutions.html",
    "href": "weeks/week02/lab_solutions.html",
    "title": "âœ”ï¸ Week 02 - Lab Solutions",
    "section": "",
    "text": "Use the function View()to identify the type of a variable (quantitative or qualitative):\nView(Auto)\nVariables mpg, cylinders, horsepower, weight, acceleration, year are quantitative variable. Variables origin, name are qualitative variable\nUse the function range() to check the range of each quantitative predictor:\nrange(Auto$mpg)\n[1]  9.0 46.6\nTo refer to a variable, we must type the data set and the variable name joined with a $ symbol.\nUsing summary() to have an overall look at all variables and statistical features (like mean and standard deviation) are included in the outputs:\nsummary(Auto)\nor\nmean(Auto$mpg)\nsd(Auto$mpg)\nRemove the 10th through 85th observations from the original data frame and store it as another new data frame:\nAuto_tmp = Auto[-c(10:85), ]\nsummary(Auto_tmp)\nmean(Auto_tmp$mpg)\nsd(Auto_tmp$mpg)\nCreate a scatterplot matrix using the function pairs():\npairs( ~ mpg + displacement + horsepower + weight + \n        acceleration + year + origin + cylinders, \n        data = Auto)\nNotice the linear or non-linear trends in the scatterplots.Then create a histogram of the variable mpg:\nhist (Auto$mpg , col = 2, breaks = 15)\nUse the hist() function to produce some histograms with differing numbers of bins for a few of the quantitative variables. You may find the command par(mfrow = c(2, 2)) useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways.\nAfter observing the first row of the scatterplot matrix which indicates the relationship between gas mileage (mpg) and other variables, you will find evident linear or non-linear trends exist in the scatterplots with variables displacement, horsepower, weight, year and origin. Therefore, these varibles might be useful in predicting mpg.\n\nIf you want to achieve ststistical robust when exploring the relationship between variables, you need to culculate some statistics (like the correlation using the function cor()) and conduct statistical tests. This will be further illustrated in Week 03."
  },
  {
    "objectID": "weeks/week02/lecture.html",
    "href": "weeks/week02/lecture.html",
    "title": "ğŸ‘¨â€ğŸ« Week 02 - Slides",
    "section": "",
    "text": "Either click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides. For a PDF copy, check Moodle."
  },
  {
    "objectID": "weeks/week02/lecture.html#coffee-break-10-min",
    "href": "weeks/week02/lecture.html#coffee-break-10-min",
    "title": "ğŸ‘¨â€ğŸ« Week 02 - Slides",
    "section": "â˜• Coffee Break (10 min)",
    "text": "â˜• Coffee Break (10 min)\nUse this time to chat, stretch, drink some coffee or just relax for a bit by yourself."
  },
  {
    "objectID": "weeks/week02/lecture.html#part-ii---multiple-linear-regression-45-50-min",
    "href": "weeks/week02/lecture.html#part-ii---multiple-linear-regression-45-50-min",
    "title": "ğŸ‘¨â€ğŸ« Week 02 - Slides",
    "section": "Part II - Multiple Linear Regression (45-50 min)",
    "text": "Part II - Multiple Linear Regression (45-50 min)\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides. For a PDF copy, check Moodle."
  },
  {
    "objectID": "weeks/week03/checklist.html",
    "href": "weeks/week03/checklist.html",
    "title": "âœ… Week 03 - Checklist",
    "section": "",
    "text": "Follow the suggested list of actions below to get the most out of this course:\n\nYour Checklist:\n\nğŸ“™ Read (James et al. 2021, chap. 3) to reinforce your theoretical knowledge of Linear Regression. The textbook is available online for free.\nğŸ§‘â€ğŸ’» If you already know linear regression from previous courses you have taken, why not take this knowledge to next level?\n\nTry to find a dataset online that contains a numerical variable you could predict by fitting a linear regression to it. I will be curious to see what you find. Share your findings on the #week03 channel in our Slack.\n\nğŸ–¥ï¸ Before you come to the class, skim the W03 lab roadmap page to have an idea of what we are going to do.\n\nThis week, instead of just typing things in the terminal, we will use R Markdown. You can read about it here. This is also how you will be submitting solutions to formative and summative assignments in the future.\nI will post solutions to the practical exercises at the end of the week.\n\nğŸ’» Assess yourself: did you understand all the exercises in the lab?\n\nIf you are new to linear regression and you are enrolled in the Monday sessions, it is likely that you will struggle a bit in the lab. During the week, reserve some time to read about Linear Regression and then practice the exercises again.\n\nğŸ“Ÿ Struggling with something? Donâ€™t know what a particular R command do? Share your questions on the #week03 channel in our Slack.\n\nI will also be posting follow up questions on Slack during the week.\n\nğŸ“ Keep in mind that: after the lecture on Friday, 14 October 2022, we will post the first formative assignment on Moodle.\n\nYou will have until Thursday of the following week (20 October 2022) to submit your solutions.\nThis assignment is not marked, it doesnâ€™t count towards your final grade, but you will receive feedback if you submit.\nThe assignment will have a similar format as the questions we explore in the lab.\n\nğŸ‘¨â€ğŸ« Attend the lecture. It will help you remember concepts more easily when revising later.\n\n\n\n\n\n\nReferences\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York NY: Springer. https://www.statlearning.com/."
  },
  {
    "objectID": "weeks/week03/formative.html",
    "href": "weeks/week03/formative.html",
    "title": "ğŸ“ Week 03 - Formative homework",
    "section": "",
    "text": "Use the Carseats data set in the ISLR2 package to answer the following questions:\n\nFit a multiple linear regression model to predict Sales using Price, Urban, and US. Show the summary output.\nProvide an interpretation of each coefficient in the model. Be carefulâ€”some of the variables in the model are qualitative!\nWrite the model in equation form, carefully handling the qualitative variables properly.\nFor which of the predictors can you reject the null hypothesis \\(H_0: \\beta_j = 0\\)?\nBased on your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of an association with the outcome. Justify your choices.\nHow well do the models in Questions 1 & 5 fit the data?\nUsing the model from question 5, obtain 95% confidence intervals for the coefficient(s).\nUse the * and : symbols to fit linear regression models with interaction effects. Could you find any model with interactions that fit better than the models you built in Questions 1 & 5? Justify your answer."
  },
  {
    "objectID": "weeks/week03/formative_solutions.html",
    "href": "weeks/week03/formative_solutions.html",
    "title": "ğŸ“ Week 03 - Formative homework",
    "section": "",
    "text": "Use the Carseats data set in the ISLR2 package to answer the following questions:\n\nFit a multiple linear regression model to predict Sales using Price, Urban, and US. Show the summary output.\n> library(ISLR2)\n> lm.fit <- lm(Sales ~ Price + Urban + US, data = Carseats)\n> summary(lm.fit)\nProvide an interpretation of each coefficient in the model. Be carefulâ€”some of the variables in the model are qualitative!\nÎ²1: Holding Urban and US fixed, the Sales decrease 54.459 units on average when the Price company charges for car seats at each site increases 1000 units.\nÎ²2: Holding Price and US fixed, the Sales decrease 0.021916 units on average when the store is in urban area.\nÎ²3: Holding Price and Urban fixed, the Sales increase 1.200573 units on average when the store is in US.\nWrite the model in equation form, carefully handling the qualitative variables properly.\nSales = Î²0 + Î²1 x Price + Î²2 + Î²3, when the store is in urban and in US\nSales = Î²0 + Î²1 x Price + Î²3, when the store is not in urban and but in US\nSales = Î²0 + Î²1 x Price + Î²2, when the store is in urban and but not in US\nSales = Î²0 + Î²1 x Price, when the store is not in urban and not in US\nFor which of the predictors can you reject the null hypothesis H0: Î²j = 0?\nFrom the p-values of t-test, we could reject the null hypothesis H0: Î²1 = 0 for the predictor Price and the null hypothesis H0: Î²3 = 0 for the predictor US.\nBased on your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of an association with the outcome. Justify your choices.\n> lm.fit1 <- lm(Sales ~ Price + US, data = Carseats)\n> summary(lm.fit1)\nHow well do the models in Questions 1 & 5 fit the data?\nNeither model fits the data well according to the small value of R2 and Adjusted R2. It means that the predictors included in both models can only interpret a small part of the change pattern of the response.\nUsing the model from question 5, obtain 95 % confidence intervals for the coefficient(s).\n> confint(lm.fit1)\n    2.5 %      97.5 %\n(Intercept) 11.79032020 14.27126531\nPrice       -0.06475984 -0.04419543\nUSYes        0.69151957  1.70776632\n95% confidence interval for Î²0 = [11.79032020, 14.27126531]; 95% confidence interval for Î²1 = [-0.06475984 -0.04419543]; 95% confidence interval for Î²2 = [0.69151957 1.70776632].\nUse the * and : symbols to fit linear regression models with interaction effects. Could you find any model with interactions that fit better than the models you built in Questions 1 & 5? Justify your answer.\n> lm.fit2 <- lm(Sales ~ Price * US, data = Carseats)\n> summary(lm.fit2)\n> lm.fit3 <- lm(Sales ~ Price * Urban, data = Carseats)\n> summary(lm.fit3)\n> lm.fit4 <- lm(Sales ~ Price * US + Urban, data = Carseats)\n> summary(lm.fit4)\n> lm.fit5 <- lm(Sales ~ Price * Urban + US, data = Carseats)\n> summary(lm.fit5)\n> lm.fit6 <- lm(Sales ~ Price + US + Urban + Price:US + Price:Urban, data = Carseats)\n> summary(lm.fit6)\n> lm.fit7 <- lm(Sales ~ Price + US + Urban + Price:US + Price:Urban + Urban:US, data = Carseats)\n> summary(lm.fit7)\nDifferent multiple linear regression models with interactions have been built. However, after comparision there is almost no difference between the values of the \\(R^2\\) and adjusted \\(R^2\\), which means the goodness of fit is not significantly improved. The reason is that there is no interaction effects between Price and US, and between Price and Urban, and between US and Urban which is supported by the significance of these coeficientsâ€™ t-tests."
  },
  {
    "objectID": "weeks/week03/lab.html",
    "href": "weeks/week03/lab.html",
    "title": "ğŸ’» Week 03 - Lab Roadmap (90 min)",
    "section": "",
    "text": "This week, we will fit simple and multiple linear regression models in R and learn to interpret the R output. We will apply this method to practical cases and deal with problems that commonly arise during this process."
  },
  {
    "objectID": "weeks/week03/lab.html#step-1-simple-linear-regression",
    "href": "weeks/week03/lab.html#step-1-simple-linear-regression",
    "title": "ğŸ’» Week 03 - Lab Roadmap (90 min)",
    "section": "Step 1: Simple linear regression",
    "text": "Step 1: Simple linear regression\nWe will follow the instructions below step by step together while answering whatever questions you might encounter along the way.\n\nInstall and load the ISLR2 package, which contains a large collection of data sets and functions.\ninstall.packages(\"ISLR2\").\nlibrary (ISLR2)\nThe function install.packages() is used to download packages that donâ€™t come with R. This installation only needs to be done the first time you use a package. However, the library() function must be called within each R session to load packages.\nUse the Boston data set in the ISLR2 library. It records medv (median house value) for 506 census tracts in Boston. Have a look at the first few rows of the Boston data set:\nhead (Boston)\n    crim zn indus chas   nox    rm  age    dis rad tax ptratio lstat medv\n1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3  4.98 24.0\n2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8  9.14 21.6\n3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8  4.03 34.7\n4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7  2.94 33.4\n5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7  5.33 36.2\n6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7  5.21 28.7\nWe want to predict medv using the available predictors, such as rm (average number of rooms per house), age (average age of houses), and lstat (percentage of households with low socioeconomic status). To find out more about the data set, we can type ?Boston.\nFit a simple linear regression lm() model, with medv as the response and lstat as the predictor:\n> lm.fit <- lm(medv ~ lstat , data = Boston)\nThe basic syntax is lm(y âˆ¼ x, data), where y is the response, x is the predictor, and data is the data set in which we keep these two variables.\nUse the tidy function to create a dataframe with columns for the estimate, standard error, f-statistic (estimate/standard error), p-values, and 95 percent confidence intervals:\ninstalling/loading broom:\ninstall.packages(\"broom\")\nlibrary(broom)\n> tidy(lm.fit, conf.int = TRUE)\n\n\n# A tibble: 2 Ã— 7\nterm        estimate std.error statistic   p.value conf.low conf.high\n<chr>          <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)   34.6      0.563       61.4 3.74e-236    33.4     35.7  \n2 lstat         -0.950    0.0387     -24.5 5.08e- 88    -1.03    -0.874\nBecause lm.fit is a simple linear regression model, there are only two coefficients: \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). The goodness-of-fit of the model can be measured by the \\(R^2\\) in the output, which can be obtained (along with other model statistics) using the glance function.\n> glance(lm.fit)$r.squared\n[1] 0.5441463\nPlot medv and lstat along with the least squares regression line using the geom_point() and geom_abline() functions.::\nlibrary(tidyverse)\n\n> ggplot(data = Boston, aes(x = lstat, y = medv)) +\ngeom_point() + \ngeom_abline(intercept = lm.fit$coefficients[1], slope = lm.fit$coefficients[2])"
  },
  {
    "objectID": "weeks/week03/lab.html#step-2-multiple-linear-regression",
    "href": "weeks/week03/lab.html#step-2-multiple-linear-regression",
    "title": "ğŸ’» Week 03 - Lab Roadmap (90 min)",
    "section": "Step 2: Multiple linear regression",
    "text": "Step 2: Multiple linear regression\nWe will still use the Boston data set to fit multiple linear regression. The fitting process is similar to simple linear regression.\n\nFit a multiple linear regression lm() model, with medv as the response, lstat and age as the predictors:\n> lm.fit <- lm(medv ~ lstat + age , data = Boston)\n> tidy(lm.fit, conf.int = TRUE)\n\n# A tibble: 3 Ã— 7\nterm        estimate std.error statistic   p.value conf.low conf.high\n<chr>          <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)  33.2       0.731      45.5  2.94e-180  31.8      34.7   \n2 lstat        -1.03      0.0482    -21.4  8.42e- 73  -1.13     -0.937 \n3 age           0.0345    0.0122      2.83 4.91e-  3   0.0105    0.0586\nThe syntax lm(y ~ x1 + x2 + x3) is used to fit a model with three predictors, x1, x2, and x3. The tidy() function now outputs the regression coefficients for all the predictors.\nFit a multiple linear regression lm() model, with medv as the response, all rest variables as the predictors:\n> lm.fit <- lm(medv ~ ., data = Boston)\n> tidy(lm.fit, conf.int = TRUE)\n\n# A tibble: 13 Ã— 7\nterm         estimate std.error statistic  p.value conf.low conf.high\n<chr>           <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)  41.6       4.94        8.43  3.79e-16  31.9     51.3    \n2 crim         -0.121     0.0330     -3.68  2.61e- 4  -0.186   -0.0565 \n3 zn            0.0470    0.0139      3.38  7.72e- 4   0.0197   0.0742 \n4 indus         0.0135    0.0621      0.217 8.29e- 1  -0.109    0.136  \n5 chas          2.84      0.870       3.26  1.17e- 3   1.13     4.55   \n6 nox         -18.8       3.85       -4.87  1.50e- 6 -26.3    -11.2    \n7 rm            3.66      0.420       8.70  4.81e-17   2.83     4.48   \n8 age           0.00361   0.0133      0.271 7.87e- 1  -0.0226   0.0298 \n9 dis          -1.49      0.202      -7.39  6.17e-13  -1.89    -1.09   \n10 rad           0.289     0.0669      4.33  1.84e- 5   0.158    0.421  \n11 tax          -0.0127    0.00380    -3.34  9.12e- 4  -0.0202  -0.00521\n12 ptratio      -0.938     0.132      -7.09  4.63e-12  -1.20    -0.678  \n13 lstat        -0.552     0.0507    -10.9   6.39e-25  -0.652   -0.452  \n\nWe can access the individual components of a summary object by name (type ?glance to see what is available). Hence glance(lm.fit)$r.squared gives us the \\(R^2\\).\n\nSelect variables:\nIn these two multiple linear regression models, the t-tests and F-test results suggest that many of the predictors are significant for the response variable. However, some do not achieve statistical significance. Can you see which variables these are?\nWe call the process of determining which predictors are associated with the response as variable selection.\nIf the number of predictors is very small, we could perform the variable selection by trying out a lot of different models, each containing a different subset of the predictors. We can then select the best model out of all of the models we have considered.\nUsing the template below, try figuring out the model which produces the highest adjusted \\(R^2\\). The adjusted \\(R^2\\) has a similar interpretation to \\(R^2\\), only it is an advantage here as it penalises models that include insignificant parameters.\nlm.fit <- lm(medv ~ ., data = Boston)\nglance(lm.fit)$adj.r.squared\n[1] 0.7278399\nWe found that if you remove indus and age, the adjusted \\(R^2\\) becomes slightly larger compared to including all predictors.\nlm.fit <- lm(medv ~ ., data = Boston[,-c(3,7)])\nglance(lm.fit)$adj.r.squared\n[1] 0.7288734"
  },
  {
    "objectID": "weeks/week03/lab.html#step-3-some-potential-problems",
    "href": "weeks/week03/lab.html#step-3-some-potential-problems",
    "title": "ğŸ’» Week 03 - Lab Roadmap (90 min)",
    "section": "Step 3: Some potential problems",
    "text": "Step 3: Some potential problems\nMany problems may occur when we fit a linear regression model to a particular data set. These problems will lead to inaccurate estimation. In this step, we will identify and overcome potential problems such as outliers, collinearity and interaction effects.\nWe present a few of the many methods available, but those interested can explore more after class.\n\nHandle interaction terms:\nIn regression, an interaction effect exists when the effect of an independent variable on the response variable changes, depending on the values of one or more independent variables. When you believe there is an interaction effect, it is easy to include interaction terms in a linear model using the lm() function.\n> tidy(lm(medv ~ lstat * age , data = Boston))\n\n# A tibble: 4 Ã— 5\nterm         estimate std.error statistic  p.value\n<chr>           <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept) 36.1        1.47      24.6    4.91e-88\n2 lstat       -1.39       0.167     -8.31   8.78e-16\n3 age         -0.000721   0.0199    -0.0363 9.71e- 1\n4 lstat:age    0.00416    0.00185    2.24   2.52e- 2\nThe syntax lstat:age tells R to include an interaction term between lstat and age. The syntax lstat*age simultaneously includes lstat, age, and the interaction term lstatÃ—age as predictors; it is a shorthand for lstat+age+lstat:age.\nIdentify outliers through residual plots:\nAn outlier is a point for which \\(\\hat{y}_i\\) is far from the value predicted by the model. Outliers can arise for a variety of reasons, such as incorrect recording of observation during data collection. Outliers could be identified through residual plots:\n> par(mfrow = c(2, 2))\n> plot(lm.fit)\nThe plot function automatically produces four diagnostic plots when you pass the output from lm(). Plots on the left column are residual plots, indicating the relationship between residuals and fitted values.\nIn practice, it can be difficult to decide how large a residual needs to be before we consider the point to be an outlier. Instead of plotting the residuals, we can address this problem by plotting the studentized residuals. These are computed by dividing each residual ei by its estimated standard studentized residual error. Observations with studentized residuals greater than 3 in absolute value are possible outliers. Using the plot() function to plot the studentized residuals:\n> plot(predict(lm.fit), rstudent(lm.fit)\nHandle outliers:\nIf we believe an outlier is due to an error in data collection or recording, then one solution is to simply remove the observation. However, care should be taken, as an outlier may instead signal a deficiency with our model, such as a missing predictor.\nDetect multicollinearity using the correlation matrix:\nMulticollinearity refers to the situation in which two or more predictor variables are highly correlated to one another. It can be detected through the correlation matrix:\ncor(Boston)\nIgnoring the last row and the last column in the matrix, which indicate the relationship with response variable medv, an element of this matrix that is large in absolute value indicates a pair of highly correlated variables, and therefore a collinearity problem in the data.\nWe can detect multicollinearity quantitatively using vif() function in the `carâ€™ package:\ninstall.packages(\"car\"))\nlibrary(car)\n> vif(lm.fit)\nInstead of inspecting the correlation matrix, a better way to assess multicollinearity is to compute the variance inflation factor (VIF). As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity.\n\n\n\n\n\n\n\nRead more about VIF\n\n\n\n\n\nCheck out our textbook (James et al. 2021, 99â€“103) for a description of the Variance Inflation Factor (VIF).\n\n\n\n\nHandle collinearity:\nWhen faced with the problem of multicollinearity, there are two simple solutions.\n-The first is to drop one of the problematic variables from the regression.\n-The second solution is to combine the collinear variables into a single predictor, where such combination makes theoretical sense."
  },
  {
    "objectID": "weeks/week03/lab.html#step-4-practical-exercises-in-pairs",
    "href": "weeks/week03/lab.html#step-4-practical-exercises-in-pairs",
    "title": "ğŸ’» Week 03 - Lab Roadmap (90 min)",
    "section": "Step 4: Practical exercises (in pairs)",
    "text": "Step 4: Practical exercises (in pairs)\nSo far, we have learnt to fit simple and multiple linear regression models in R. In this practical case, we will continue to use the data set Auto studied in the last lab. Make sure that the missing values have been removed from the data.\nEight questions are listed below. You are required to try to answer these questions in pairs using R commands. We will go over the solutions once everyone has finished these questions.\nğŸ¯ Questions\n\nUse the lm() function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the tidy() function to print the results. Comment on the output. For example:\n\nIs there a relationship between the predictor and the response?\nHow strong is the relationship between the predictor and the response?\nIs the relationship between the predictor and the response positive or negative?\nWhat is the predicted mpg associated with a horsepower of 98? What are the associated 95 % confidence intervals?\n\nPlot the response and the predictor. Use the geom_abline() function to display the least squares regression line.\nUse the plot() function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.\nProduce a scatterplot matrix that includes all the variables in the data set.\nCompute the matrix of correlations between the variables using the function cor(). You will need to exclude the name and origin variable, which are qualitative.\nUse the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the tidy() function to print the results. Comment on the output. For instance:\n\nIs there a relationship between the predictors and the response?\nWhich predictors appear to have a statistically significant relationship to the response?\nWhat does the coefficient for the year variable suggest?\n\nUse the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers?\nUse the * and : symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?\n\n\n\n\n\n\n\nTip\n\n\n\nIf you could not finish all eight questions during the lab, take that as a home exercise.\nUse the #week03 channel on Slack if you have any questions."
  },
  {
    "objectID": "weeks/week03/lab.html#solutions-to-exercises",
    "href": "weeks/week03/lab.html#solutions-to-exercises",
    "title": "ğŸ’» Week 03 - Lab Roadmap (90 min)",
    "section": "ğŸ”‘ Solutions to exercises",
    "text": "ğŸ”‘ Solutions to exercises\n\nlibrary(tidyverse)\nlibrary(ISLR2)\n\n\nQ1\nUse the lm() function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the tidy() function to print the results. Comment on the output.\nFor example:\n\nIs there a relationship between the predictor and the response?\nHow strong is the relationship between the predictor and the response?\nIs the relationship between the predictor and the response positive or negative?\nWhat is the predicted mpg associated with a horsepower of 98? What are the associated 95 % confidence intervals?\n\n\nAuto <- na.omit(Auto)\nlm.fit <- lm(mpg ~ horsepower, data = Auto)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = mpg ~ horsepower, data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5710  -3.2592  -0.3435   2.7630  16.9240 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 39.935861   0.717499   55.66   <2e-16 ***\nhorsepower  -0.157845   0.006446  -24.49   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.906 on 390 degrees of freedom\nMultiple R-squared:  0.6059,    Adjusted R-squared:  0.6049 \nF-statistic: 599.7 on 1 and 390 DF,  p-value: < 2.2e-16\n\n\nRegarding to the p-values of t-test and F-test, there is a strong relationship between the predictor horsepower and the response mpg. From the sign of coefficients, the relationship between the predictor and the response is negative. Using the function predict() to predict the value of response and the confidence interval, we get:\n\npredict(lm.fit, data.frame(horsepower = 98), interval = \"confidence\")\n\n       fit      lwr      upr\n1 24.46708 23.97308 24.96108\n\n\nTherefore, the predicted mpg associated with a horsepower of 98 is 24.47, and the associated 95 % confidence interval is [23.97308, 24.96108].\n\n\nQ2\nPot the response and the predictor. Use the geom_abline() function to display the least squares regression line.\nIn base R (without using any tidverse or any other package):\n\nplot(Auto$horsepower, Auto$mpg, xlim = c(0, 250))\nabline (lm.fit, lwd = 3, col = \"red\")\n\n\n\n\nUsing ggplot (from tidyverse):\n\nggplot(data = Auto, aes(x = horsepower, y = mpg)) +\n  geom_point(alpha=0.6, size=2.5) +\n  geom_abline(intercept = lm.fit$coefficients[1], \n              slope = lm.fit$coefficients[2],\n              color=\"red\", size=1.2) +\n  theme_bw()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nâ„¹ Please use `linewidth` instead.\n\n\n\n\n\n\n\nQ3\nUse the plot() function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.*\n\npar(mfrow = c(2, 2))\nplot(lm.fit)\n\n\n\n\nBy observing four diagnostic plots, we could find non-linear pattern in residual plots. The quadratic trend of the residuals could be a problem. Then we plot studentized residuals to identify outliers:\n\nplot(predict(lm.fit), rstudent(lm.fit))\n\n\n\n\nThere are possible outliers as seen in the plot of studentized residuals because there are data with a value greater than 3.\n\n\nQ4\nProduce a scatterplot matrix that includes all the variables in the data set.\n\npairs(Auto)\n\n\n\n\n\n\nQ5\nCompute the matrix of correlations between the variables using the function cor(). You will need to exclude the name and origin variable, which is qualitative.\n\ncor(subset(Auto, select = -name))\n\n                    mpg  cylinders displacement horsepower     weight\nmpg           1.0000000 -0.7776175   -0.8051269 -0.7784268 -0.8322442\ncylinders    -0.7776175  1.0000000    0.9508233  0.8429834  0.8975273\ndisplacement -0.8051269  0.9508233    1.0000000  0.8972570  0.9329944\nhorsepower   -0.7784268  0.8429834    0.8972570  1.0000000  0.8645377\nweight       -0.8322442  0.8975273    0.9329944  0.8645377  1.0000000\nacceleration  0.4233285 -0.5046834   -0.5438005 -0.6891955 -0.4168392\nyear          0.5805410 -0.3456474   -0.3698552 -0.4163615 -0.3091199\norigin        0.5652088 -0.5689316   -0.6145351 -0.4551715 -0.5850054\n             acceleration       year     origin\nmpg             0.4233285  0.5805410  0.5652088\ncylinders      -0.5046834 -0.3456474 -0.5689316\ndisplacement   -0.5438005 -0.3698552 -0.6145351\nhorsepower     -0.6891955 -0.4163615 -0.4551715\nweight         -0.4168392 -0.3091199 -0.5850054\nacceleration    1.0000000  0.2903161  0.2127458\nyear            0.2903161  1.0000000  0.1815277\norigin          0.2127458  0.1815277  1.0000000\n\n\nA nicer way to plot correlations is through the package ggcorrplot:\n\nlibrary(ggcorrplot)\n\nggcorrplot(cor(Auto %>% select(-c(name))))\n\n\n\n\n\n\nQ6\nUse the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the tidy() function to print the results. Comment on the output.\nFor instance:\n\nIs there a relationship between the predictors and the response?\nWhich predictors appear to have a statistically significant relationship to the response?\nWhat does the coefficient for the year variable suggest?\n\n\nlm.fit1 <- lm(mpg ~ . -name, data = Auto)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = mpg ~ . - name, data = Auto)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5903 -2.1565 -0.1169  1.8690 13.0604 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -17.218435   4.644294  -3.707  0.00024 ***\ncylinders     -0.493376   0.323282  -1.526  0.12780    \ndisplacement   0.019896   0.007515   2.647  0.00844 ** \nhorsepower    -0.016951   0.013787  -1.230  0.21963    \nweight        -0.006474   0.000652  -9.929  < 2e-16 ***\nacceleration   0.080576   0.098845   0.815  0.41548    \nyear           0.750773   0.050973  14.729  < 2e-16 ***\norigin         1.426141   0.278136   5.127 4.67e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.328 on 384 degrees of freedom\nMultiple R-squared:  0.8215,    Adjusted R-squared:  0.8182 \nF-statistic: 252.4 on 7 and 384 DF,  p-value: < 2.2e-16\n\n\n\nYes, there is a relationship between the predictors and the response by testing the null hypothesis of whether all the regression coefficients are zero. The F-statistic is far from 1 (with a small p-value), indicating evidence against the null hypothesis.\nObserving the p-values associated with each predictorâ€™s t-statistic, we see that displacement, weight, year, and origin have a statistically significant relationship, while cylinders, horsepower and acceleration do not.\nThe regression coefficient for year is 0.75. This suggests that, considering all other predictors fixed, mpg increases by additional 0.75 unit. In other words, cars become more fuel efficient every year by almost 1 mpg/year.\n\n\n\nQ7\nUse the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers?\n\npar(mfrow = c(2, 2))\nplot(lm.fit1)\n\n\n\n\nFrom the leverage plot, we see that point 14 appears to have a high leverage, although not a high magnitude residual. Besides, the quadratic trend of the residuals could be a problem. Maybe linear regression is not the best fit for this prediction.\nWe plot studentized residuals to identify outliers:\n\nplot(predict(lm.fit1), rstudent(lm.fit1))\n\n\n\n\nThere are possible outliers as seen in the plot of studentized residuals because there are data with a value greater than 3.\n\n\nQ8\nUse the * and : symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?\n\nlm.fit2 <-  lm(mpg ~ cylinders * displacement + displacement * weight, data = Auto)\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = mpg ~ cylinders * displacement + displacement * \n    weight, data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.2934  -2.5184  -0.3476   1.8399  17.7723 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             5.262e+01  2.237e+00  23.519  < 2e-16 ***\ncylinders               7.606e-01  7.669e-01   0.992    0.322    \ndisplacement           -7.351e-02  1.669e-02  -4.403 1.38e-05 ***\nweight                 -9.888e-03  1.329e-03  -7.438 6.69e-13 ***\ncylinders:displacement -2.986e-03  3.426e-03  -0.872    0.384    \ndisplacement:weight     2.128e-05  5.002e-06   4.254 2.64e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.103 on 386 degrees of freedom\nMultiple R-squared:  0.7272,    Adjusted R-squared:  0.7237 \nF-statistic: 205.8 on 5 and 386 DF,  p-value: < 2.2e-16\n\n\nInteraction between displacement and weight is statistically signifcant, while the interaction between cylinders and displacement is not."
  },
  {
    "objectID": "weeks/week03/lab_solutions.html",
    "href": "weeks/week03/lab_solutions.html",
    "title": "âœ”ï¸ Week 03 - Lab Solutions",
    "section": "",
    "text": "Use the lm() function to perform a simple linear regressionï¼Œand use the summary() function to print the results:\n> library(ISLR2)\n> Auto <- na.omit(Auto)\n> lm.fit <- lm(mpg ~ horsepower, data = Auto)\n> summary(lm.fit)\n\nCall:\nlm(formula = mpg ~ horsepower, data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5710  -3.2592  -0.3435   2.7630  16.9240 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 39.935861   0.717499   55.66   <2e-16 ***\nhorsepower  -0.157845   0.006446  -24.49   <2e-16 ***\n---\n Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1\n\nResidual standard error: 4.906 on 390 degrees of freedom\nMultiple R-squared:  0.6059,    Adjusted R-squared:  0.6049 \nF-statistic: 599.7 on 1 and 390 DF,  p-value: < 2.2e-16\nRegarding to the p-values of t-test and F-test, there is a strong relationship between the predictor horsepower and the reponse mpg. From the sign of coefficients, the relationship between the predicator and the response is negative. Using the function predict() to predict the value of reponse and the confidence interval, we get:\n> predict(lm.fit, data.frame(horsepower = 98), interval = \"confidence\")\n       fit      lwr      upr\n1 24.46708 23.97308 24.96108\nTherefore, the predicted mpg associated with a horsepower of 98 is 24.47, and the associated 95 % confidence interval is [23.97308, 24.96108].\nUse the function plot() and abline():\n> attach(Auto)\n> plot(mpg, horsepower, ylim = c(0, 250))\n> abline (lm.fit, lwd = 3, col = \"red\")\n> dev.off()\nUsing plot() function to produce diagnostic plots:\n> par(mfrow = c(2, 2))\n> plot (lm.fit)\n> dev.off()\nBy observing four diagnostic plots, we could find non-linear patttern in residual plots. The quadratic trend of the residuals could be a problem. Then we plot studentized residuals to identify outliers:\n> plot(predict(lm.fit), rstudent(lm.fit))\n> dev.off()\nThere are possible outliers as seen in the plot of studentized residuals because there are data with a value greater than 3.\nUse the pairs() function to produce a scatterplot matrix:\n> pairs(Auto)\n> dev.off()\nUse the cor() function to compute the matrix of correlations between the variables while excluding the name variable:\n> cor(subset(Auto, select = -name))\nUse the lm() function to perform a multiple linear regressionï¼Œand use the summary() function to print the results:\n> lm.fit1 <- lm(mpg ~ . -name, data = Auto)\n> summary (lm.fit1)\n\nCall:\nlm(formula = mpg ~ . - name, data = Auto)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5903 -2.1565 -0.1169  1.8690 13.0604 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -17.218435   4.644294  -3.707  0.00024 ***\ncylinders     -0.493376   0.323282  -1.526  0.12780    \ndisplacement   0.019896   0.007515   2.647  0.00844 ** \nhorsepower    -0.016951   0.013787  -1.230  0.21963    \nweight        -0.006474   0.000652  -9.929  < 2e-16 ***\nacceleration   0.080576   0.098845   0.815  0.41548    \nyear           0.750773   0.050973  14.729  < 2e-16 ***\norigin         1.426141   0.278136   5.127 4.67e-07 ***\n---\nSignif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1\n\nResidual standard error: 3.328 on 384 degrees of freedom\nMultiple R-squared:  0.8215,    Adjusted R-squared:  0.8182 \nF-statistic: 252.4 on 7 and 384 DF,  p-value: < 2.2e-16\n\nYes, there is a relationship between the predictors and the response by testing the null hypothesis of whether all the regression coefficients are zero. The F-statistic is far from 1 (with a small p-value), indicating evidence against the null hypothesis.\nObverving the p-values associated with each predictorâ€™s t-statistic, we see that displacement, weight, year, and origin have a statistically significant relationship, while cylinders, horsepower and acceleration do not.\nThe regression coefficient for year is 0.75. This suggests that, considering all other predictors fixed, mpg increases by additional 0.75 unit. In other words, cars become more fuel efficient every year by almost 1 mpg/year.\n\nUse the plot() function to produce diagnostic plots:\n> par(mfrow = c(2, 2))\n> plot (lm.fit1)\n> dev.off()\nFrom the leverage plot, we see that point 14 appears to have a high leverage, although not a high magnitude residual. Besides, the quadratic trend of the residuals could be a problem. Maybe linear regression is not the best fit for this prediction.\nWe plot studentized residuals to identify outliers:\n> plot(predict(lm.fit1), rstudent(lm.fit1))\n> dev.off()\nThere are possible outliers as seen in the plot of studentized residuals because there are data with a value greater than 3.\nUse the * and : symbols to fit linear regression models with interaction effects:\n> lm.fit2 <-  lm(mpg ~ cylinders * displacement + displacement * weight, data = Auto)\n> summary(lm.fit2)\nInteraction between displacement and weight is statistically signifcant, while the interaction between cylinders and displacement is not."
  },
  {
    "objectID": "weeks/week03/lecture.html",
    "href": "weeks/week03/lecture.html",
    "title": "ğŸ‘¨â€ğŸ« Week 03 - Slides",
    "section": "",
    "text": "Either click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides. For a PDF copy, check Moodle."
  },
  {
    "objectID": "weeks/week03/lecture.html#coffee-break-10-min",
    "href": "weeks/week03/lecture.html#coffee-break-10-min",
    "title": "ğŸ‘¨â€ğŸ« Week 03 - Slides",
    "section": "â˜• Coffee Break (10 min)",
    "text": "â˜• Coffee Break (10 min)\nUse this time to chat, stretch, drink some coffee or just relax for a bit by yourself."
  },
  {
    "objectID": "weeks/week03/lecture.html#part-ii---classifiers-naive-bayes-45-50-min",
    "href": "weeks/week03/lecture.html#part-ii---classifiers-naive-bayes-45-50-min",
    "title": "ğŸ‘¨â€ğŸ« Week 03 - Slides",
    "section": "Part II - Classifiers (Naive Bayes) (45-50 min)",
    "text": "Part II - Classifiers (Naive Bayes) (45-50 min)\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides. For a PDF copy, check Moodle."
  },
  {
    "objectID": "weeks/week04/checklist.html",
    "href": "weeks/week04/checklist.html",
    "title": "âœ… Week 04 - Checklist",
    "section": "",
    "text": "Here is a suggestion of how to program your week in relation to this course:\nExtra:"
  },
  {
    "objectID": "weeks/week04/checklist.html#if-your-lab-is-on-monday",
    "href": "weeks/week04/checklist.html#if-your-lab-is-on-monday",
    "title": "âœ… Week 04 - Checklist",
    "section": "If your lab is on Monday:",
    "text": "If your lab is on Monday:\n\nğŸ“¥ Download: Before or once you arrive at the classroom, download the DS202_2022MT_w04_lab_rmark.Rmd file that contains the lab roadmap or browse the webpage version here.\nğŸ§‘â€ğŸ’»Participate: Actively engage with the material in the lab. Ask your class teacher for help if anything is unclear. Work with others whenever possible and take notes of theoretical concepts or practical coding skill you might want to revisit later in the week.\n\nIt is ok if you feel a bit lost in the lab. After all, you havenâ€™t had much time to read the material to reinforce the concepts in your mind. That is why participation and note-taking is so important.\n\nğŸ“™ Read: Find some time before Thursday to read (James et al. 2021, chap. 4) and reinforce your theoretical knowledge of Classifiers (Logistic Regression & other methods, including NaÃ¯ve Bayes); the textbook is available online for free.\n\nAs you go through the text, try to connect what you read to the things you heard about in the lecture or the examples you explored in the lab.\n\nâœï¸ Solve: Also before Thursday, take a look at the problem sets of the first formative assessment. Either try to complete it before Thursday or at least have a look to see what it contains. Take note of whatever questions you might have about R or linear regression.\n\nThe Formative Problem Set 01 is available on Moodle and you can submit your solutions until Tuesday 25 October 2022, 23:50 UK time.\n\nâœ‹Teacher Support: Stuart will host a drop-in session on Thursday 20 October 2022 2pm-4pm (provisionally at the FAW 9.04 room). Bring your notes and questions or just simply use this shared space to work on your problem set.\nğŸ§‘â€ğŸ« Attend the lecture: This week, the lecture will look more like a workshop. We wonâ€™t explore new algorithms but we will work on regression/classification metrics and explore the concepts of train/test splits and cross-validation. You will need to use those in your first summative problem set."
  },
  {
    "objectID": "weeks/week04/checklist.html#if-your-lab-is-on-friday",
    "href": "weeks/week04/checklist.html#if-your-lab-is-on-friday",
    "title": "âœ… Week 04 - Checklist",
    "section": "If your lab is on Friday:",
    "text": "If your lab is on Friday:\n\nğŸ“™ Read: Find some time before Thursday to read (James et al.Â 2021, chap.Â 4) and reinforce your theoretical knowledge of Classifiers (Logistic Regression & other methods, including NaÃ¯ve Bayes); the textbook is available online for free. As you go through the text, try to connect what you read to the things you heard about in the lecture.\n\nYou can ask questions on Slack (#week04) or take them with you to the drop-in session on Thursday.\n\nâœï¸ Solve: Also before Thursday, take a look at the problem sets of the first formative assessment. Either try to complete it before Thursday or at least have a look to see what it contains. Take note of whatever questions you might have about R or linear regression.\n\nThe Formative Problem Set 01 is available on Moodle and you can submit your solutions until Tuesday 25 October 2022, 23:50 UK time.\n\nâœ‹Teacher Support: Stuart will host a drop-in session on Thursday 20 October 2022 2pm-4pm (provisionally at the FAW 9.04 room). Bring your notes and questions or just simply use this shared space to work on your problem set.\nğŸ“¥Download: Before or once you arrive at the classroom, download the DS202_2022MT_w04_lab_rmark.Rmd file that contains the lab roadmap or browse the webpage version here.\nğŸ§‘â€ğŸ’»Participate: Actively engage with the material in the lab. Ask your class teacher for help if anything is unclear. Keep your notes and the textbook handy so you can consult them during the session.\nğŸ§‘â€ğŸ«Attend the lecture: This week, the lecture will look more like a workshop. We wonâ€™t explore new algorithms but we will work on regression/classification metrics and explore the concepts of train/test splits and cross-validation. You will need to use those in your first summative problem set."
  },
  {
    "objectID": "weeks/week04/lab.html",
    "href": "weeks/week04/lab.html",
    "title": "ğŸ’» Week 04 - Lab Roadmap (90 min)",
    "section": "",
    "text": "This week, we will build diverse classification models to deal with situation when the response variable is qualitative in R. We predict these qualitative variables through some widely-used classifiers including Logistic Regression (one of many Generalized Linear Models) and NaÃ¯ve Bayes in this lab session. We will also apply these classification models into practical practices and compare their performance on different data sets.\nWe will follow the instructions below step by step together while answering whatever questions you might encounter along the way.\nR packages you will need:"
  },
  {
    "objectID": "weeks/week04/lab.html#step-1-explore-the-dataset",
    "href": "weeks/week04/lab.html#step-1-explore-the-dataset",
    "title": "ğŸ’» Week 04 - Lab Roadmap (90 min)",
    "section": "Step 1: Explore the dataset",
    "text": "Step 1: Explore the dataset\n\nStep 1.1 Load the Data\nLoad the ISLR2 package, which contains a large collection of data sets and functions. We will begin by examining some numerical and graphical summaries of the Smarket data, which is part of the ISLR2 library.\n\nlibrary(\"ISLR2\")\nhead(Smarket)\n\n  Year   Lag1   Lag2   Lag3   Lag4   Lag5 Volume  Today Direction\n1 2001  0.381 -0.192 -2.624 -1.055  5.010 1.1913  0.959        Up\n2 2001  0.959  0.381 -0.192 -2.624 -1.055 1.2965  1.032        Up\n3 2001  1.032  0.959  0.381 -0.192 -2.624 1.4112 -0.623      Down\n4 2001 -0.623  1.032  0.959  0.381 -0.192 1.2760  0.614        Up\n5 2001  0.614 -0.623  1.032  0.959  0.381 1.2057  0.213        Up\n6 2001  0.213  0.614 -0.623  1.032  0.959 1.3491  1.392        Up\n\n\nThis data set consists of percentage returns for the S&P 500 stock index over 1250 days, from the beginning of 2001 until the end of 2005. We use the command names() to obtain the variable names of this data set:\n\nnames(Smarket)\n\n[1] \"Year\"      \"Lag1\"      \"Lag2\"      \"Lag3\"      \"Lag4\"      \"Lag5\"     \n[7] \"Volume\"    \"Today\"     \"Direction\"\n\n\nHow many rows and columns do we have in this dataset?\n\ndim(Smarket)\n\n[1] 1250    9\n\n\nLetâ€™s add another column day to index the number of days in this dataset:\n\nSmarket$day <- 1:nrow(Smarket)\nhead(Smarket)\n\n  Year   Lag1   Lag2   Lag3   Lag4   Lag5 Volume  Today Direction day\n1 2001  0.381 -0.192 -2.624 -1.055  5.010 1.1913  0.959        Up   1\n2 2001  0.959  0.381 -0.192 -2.624 -1.055 1.2965  1.032        Up   2\n3 2001  1.032  0.959  0.381 -0.192 -2.624 1.4112 -0.623      Down   3\n4 2001 -0.623  1.032  0.959  0.381 -0.192 1.2760  0.614        Up   4\n5 2001  0.614 -0.623  1.032  0.959  0.381 1.2057  0.213        Up   5\n6 2001  0.213  0.614 -0.623  1.032  0.959 1.3491  1.392        Up   6\n\n\nFor each date, we have recorded the percentage returns for each of the five previous trading days, Lag1 through Lag5. We have also recorded Volume (the number of shares traded on the previous day, in billions), Today (the percentage return on the date in question) and Direction (whether the market was Up or Down on this date). Our goal is to predict Direction (a qualitative response) using the other features.\nLetâ€™s look at a generic summary of this data and how each pair of variables are related:\n\nsummary(Smarket)\n\n      Year           Lag1                Lag2                Lag3          \n Min.   :2001   Min.   :-4.922000   Min.   :-4.922000   Min.   :-4.922000  \n 1st Qu.:2002   1st Qu.:-0.639500   1st Qu.:-0.639500   1st Qu.:-0.640000  \n Median :2003   Median : 0.039000   Median : 0.039000   Median : 0.038500  \n Mean   :2003   Mean   : 0.003834   Mean   : 0.003919   Mean   : 0.001716  \n 3rd Qu.:2004   3rd Qu.: 0.596750   3rd Qu.: 0.596750   3rd Qu.: 0.596750  \n Max.   :2005   Max.   : 5.733000   Max.   : 5.733000   Max.   : 5.733000  \n      Lag4                Lag5              Volume           Today          \n Min.   :-4.922000   Min.   :-4.92200   Min.   :0.3561   Min.   :-4.922000  \n 1st Qu.:-0.640000   1st Qu.:-0.64000   1st Qu.:1.2574   1st Qu.:-0.639500  \n Median : 0.038500   Median : 0.03850   Median :1.4229   Median : 0.038500  \n Mean   : 0.001636   Mean   : 0.00561   Mean   :1.4783   Mean   : 0.003138  \n 3rd Qu.: 0.596750   3rd Qu.: 0.59700   3rd Qu.:1.6417   3rd Qu.: 0.596750  \n Max.   : 5.733000   Max.   : 5.73300   Max.   :3.1525   Max.   : 5.733000  \n Direction       day        \n Down:602   Min.   :   1.0  \n Up  :648   1st Qu.: 313.2  \n            Median : 625.5  \n            Mean   : 625.5  \n            3rd Qu.: 937.8  \n            Max.   :1250.0  \n\n\n\npairs(Smarket)\n\n\n\n\n\n\nStep 1.2 Initial exploratory data analysis\nProduce a matrix that contains all of the pairwise correlations among the predictors in a data set:\n\ncor(Smarket[, -9])\n\n             Year         Lag1         Lag2         Lag3         Lag4\nYear   1.00000000  0.029699649  0.030596422  0.033194581  0.035688718\nLag1   0.02969965  1.000000000 -0.026294328 -0.010803402 -0.002985911\nLag2   0.03059642 -0.026294328  1.000000000 -0.025896670 -0.010853533\nLag3   0.03319458 -0.010803402 -0.025896670  1.000000000 -0.024051036\nLag4   0.03568872 -0.002985911 -0.010853533 -0.024051036  1.000000000\nLag5   0.02978799 -0.005674606 -0.003557949 -0.018808338 -0.027083641\nVolume 0.53900647  0.040909908 -0.043383215 -0.041823686 -0.048414246\nToday  0.03009523 -0.026155045 -0.010250033 -0.002447647 -0.006899527\nday    0.97977289  0.035414677  0.036022487  0.038988767  0.041437137\n               Lag5      Volume        Today        day\nYear    0.029787995  0.53900647  0.030095229 0.97977289\nLag1   -0.005674606  0.04090991 -0.026155045 0.03541468\nLag2   -0.003557949 -0.04338321 -0.010250033 0.03602249\nLag3   -0.018808338 -0.04182369 -0.002447647 0.03898877\nLag4   -0.027083641 -0.04841425 -0.006899527 0.04143714\nLag5    1.000000000 -0.02200231 -0.034860083 0.03502515\nVolume -0.022002315  1.00000000  0.014591823 0.54634793\nToday  -0.034860083  0.01459182  1.000000000 0.03527333\nday     0.035025152  0.54634793  0.035273325 1.00000000\n\n\nThe function cor() can only take quantitative variables. Because the Direction variable is qualitative, therefore we exclude it when calculating the correlation matrix.\nAs one would expect, the correlations between the lag variables and todayâ€™s returns are close to zero. In other words, there appears to be little correlation between todayâ€™s returns and previous daysâ€™ returns. The only substantial correlation is between Year and Volume. We could explore how Volume changed chronologically.\n\nlibrary(tidyverse)\n\nggplot(data = Smarket, aes(x = day, y = Volume)) +\n    geom_col(fill=\"#665797\") +\n\n    ggtitle(\"Volume of shares traded over the period of this dataset\") +\n\n    xlab(\"Day\") + ylab(\"Volume (in billion US dollars)\") +\n\n    theme_minimal() +\n    theme(aspect.ratio = 2/5)\n\n\n\n\nBy plotting the data, which is ordered chronologically, we see that Volume is increasing over time. In other words, the average number of shares traded daily increased from 2001 to 2005."
  },
  {
    "objectID": "weeks/week04/lab.html#step-2-logistic-regression",
    "href": "weeks/week04/lab.html#step-2-logistic-regression",
    "title": "ğŸ’» Week 04 - Lab Roadmap (90 min)",
    "section": "Step 2: Logistic Regression",
    "text": "Step 2: Logistic Regression\nWe will still use the Smarket data set to fit a Logistic Regression Model to predict Direction.\n\nStep 2.1 Separate some data just for training\nBuild a training and a testing dataset. In practice we will be interested in our modelâ€™s performance not on the data that we used to fit the model, but rather on days in the future for which the marketâ€™s movements are unknown. Therefore, we will first create a training data set corresponding to the observations from 2001 through 2004. We will then create a testing data set of observations from 2005.:\n\ntrain <- (Smarket$Year < 2005)\nSmarket.before.2005 <- Smarket [ train , ]\n\nSmarket.2005 <- Smarket[ Smarket$Year==2005, ]\nDirection.2005 <- Smarket$Direction [ Smarket$Year==2005 ]\n\nSmarket$Year < 2005 returns True for the values satisfying <2005 (smaller than 2005) condition in Year column in Smarket dataset. The same logic can be applied to >, <= , >=, ==(equal) or != (not equal).\nTo access corresponding rows, we can create a vector (train) and put it in open brackets to make it more readable and reusable, or we can explicitly specify in the open brackets.\nUsing Smarket[ Smarket$Year==2005,] gives all the rows satisfying this condition with all columns. The same logic can be applied to < , >, <= , >= or != (not equal).\nSmarket$Direction [ Smarket$Year==2005 ] tells the direction values corresponding to the rows equal to 2005 in the year column are requested.\nThus, we can construct a training data set named Smarket.before.2005, and a testing data set named Smarket.2005.\nHow many observations we have in the training set (< 2005)?\n\nnrow(Smarket.before.2005)\n\n[1] 998\n\n\nWhat about the test set (2005)?\n\nnrow(Smarket.2005)\n\n[1] 252\n\n\n\n\nStep 2.2 Fit a logistic regression model\nFit a Logistic Regression Model in order to predict Direction using Lag1 through Lag5 and Volume based on training data set:\n\nglm.fits <- \n    glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, \n        data = Smarket.before.2005, family = binomial)\n\nThe generalized linear model syntax of the glm() function is similar to that of lm(), except that we must pass in the argument family = binomial in order to tell R to run a logistic regression rather than some other type of generalized linear model.\nWe now fit a logistic regression model using only the subset of the observations that correspond to dates before 2005, using the subset argument.\n\n\nStep 2.3 Inspect the model\nHave a look at p-values of this Logistic Regression Model:\n\nsummary(glm.fits)\n\n\nCall:\nglm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + \n    Volume, family = binomial, data = Smarket.before.2005)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.302  -1.190   1.079   1.160   1.350  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)\n(Intercept)  0.191213   0.333690   0.573    0.567\nLag1        -0.054178   0.051785  -1.046    0.295\nLag2        -0.045805   0.051797  -0.884    0.377\nLag3         0.007200   0.051644   0.139    0.889\nLag4         0.006441   0.051706   0.125    0.901\nLag5        -0.004223   0.051138  -0.083    0.934\nVolume      -0.116257   0.239618  -0.485    0.628\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1383.3  on 997  degrees of freedom\nResidual deviance: 1381.1  on 991  degrees of freedom\nAIC: 1395.1\n\nNumber of Fisher Scoring iterations: 3\n\n\nOr, alternatively, you can gather the same information using the broom package:\n\nlibrary(broom)\n\ntidy(glm.fits)\n\n# A tibble: 7 Ã— 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)  0.191      0.334     0.573    0.567\n2 Lag1        -0.0542     0.0518   -1.05     0.295\n3 Lag2        -0.0458     0.0518   -0.884    0.377\n4 Lag3         0.00720    0.0516    0.139    0.889\n5 Lag4         0.00644    0.0517    0.125    0.901\n6 Lag5        -0.00422    0.0511   -0.0826   0.934\n7 Volume      -0.116      0.240    -0.485    0.628\n\n\nThe smallest p-value here is associated with Lag1. The negative coefficient for this predictor suggests that if the market had a positive return yesterday, then it is less likely to go up today. However, at a value of 0.295, the p-value is still relatively large, and so there is no clear evidence of a real association between Lag1 and Direction. So do other predictors.\n\n\nStep 2.4 Make predictions about the future (2005)\nObtain predicted probabilities of the stock market going up for each of the days in our testing data set, that is, for the days in 2005:\n\nglm.probs <- predict(glm.fits, Smarket.2005, type = \"response\") \nglm.probs[1:10]   \n\n      999      1000      1001      1002      1003      1004      1005      1006 \n0.5282195 0.5156688 0.5226521 0.5138543 0.4983345 0.5010912 0.5027703 0.5095680 \n     1007      1008 \n0.5040112 0.5106408 \n\n\nThe predict() function can be used to predict the probability that the market will go up, given values of the predictors. The type = \"response\" option tells R to output probabilities of the form P(Y = 1|X), as opposed to other information such as the logit. If no data set is supplied to the predict() function, then the probabilities are computed for the training data set that was used to fit the logistic regression model. Here we have printed only the first ten probabilities.\n\ncontrasts(Smarket$Direction)\n\n     Up\nDown  0\nUp    1\n\n\nWe know that these values correspond to the probability of the market going up, rather than down, because the contrasts() function indicates that R has created a dummy variable with a 1 for Up.\nIn order to make a prediction as to whether the market will go up or down on a particular day, we must convert these predicted probabilities into class labels, Up or Down. The following commands create a vector of class predictions based on whether the predicted probability of a market increase is greater than or less than 0.5.\n\nglm.pred <- rep(\"Down\", 252)\nglm.pred[glm.probs > .5] <- \"Up\"\n\nThe first command creates a vector of 252 Down elements. The second line transforms to Up all of the elements for which the predicted probability of a market increase exceeds 0.5.\n\n\nStep 2.5 Create a confusion matrix\nConstruct confusion matrix in order to determine how many observations in testing data set were correctly or incorrectly classified.\n\ntable(glm.pred, Direction.2005)    \n\n        Direction.2005\nglm.pred Down Up\n    Down   77 97\n    Up     34 44\n\n\nGiven the predictions, the table() function can be used to produce a confusion matrix in order to determine how many observations were correctly or incorrectly classified.\n\n\nStep 2.6 What is the error in the test set?\nCalculate the test set error rate:\n\nmean(glm.pred == Direction.2005)\n\n[1] 0.4801587\n\n\n\nmean(glm.pred != Direction.2005)\n\n[1] 0.5198413\n\n\nThe != notation means not equal to, and so the last command computes the test set error rate. The results are rather disappointing: the test error rate is 52%, which is worse than random guessing! Of course this result is not all that surprising, given that one would not generally expect to be able to use previous daysâ€™ returns to predict future market performance.\n\n\nStep 2.7 Can we find a better combination of features?\nRemove the variables that appear not to be helpful in predicting Direction and fit a new Logistic Regression model. We recall that the logistic regression model had very underwhelming p-values associated with all of the predictors, and that the smallest p-value, though not very small, corresponded to Lag1. Perhaps by removing the variables that appear not to be helpful in predicting Direction, we can obtain a more effective model. After all, using predictors that have no relationship with the response tends to cause a deterioration in the test error rate (since such predictors cause an increase in variance without a corresponding decrease in bias), and so removing such predictors may in turn yield an improvement.\n\nglm.fits <- glm(Direction ~ Lag1 + Lag2, \n                data = Smarket.before.2005, family = binomial)\nsummary(glm.fits)\n\n\nCall:\nglm(formula = Direction ~ Lag1 + Lag2, family = binomial, data = Smarket.before.2005)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.345  -1.188   1.074   1.164   1.326  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept)  0.03222    0.06338   0.508    0.611\nLag1        -0.05562    0.05171  -1.076    0.282\nLag2        -0.04449    0.05166  -0.861    0.389\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1383.3  on 997  degrees of freedom\nResidual deviance: 1381.4  on 995  degrees of freedom\nAIC: 1387.4\n\nNumber of Fisher Scoring iterations: 3\n\n\n\nglm.probs <- predict(glm.fits, Smarket.2005, type = \"response\")\nglm.pred <- rep(\"Down\", 252)\nglm.pred[glm.probs > .5] <- \"Up\"\ntable(glm.pred , Direction.2005)\n\n        Direction.2005\nglm.pred Down  Up\n    Down   35  35\n    Up     76 106\n\n\n\nmean(glm.pred == Direction.2005)\n\n[1] 0.5595238\n\n\nCheck proportion:\n\n106 / (106 + 76)\n\n[1] 0.5824176\n\n\nAbove we have refit the logistic regression using just Lag1 and Lag2, which seemed to have the highest predictive power in the original logistic regression model.\nNow the results appear to be a little better: 56% of the daily movements have been correctly predicted. It is worth noting that in this case, a much simpler strategy of predicting that the market will increase every day will also be correct 56% of the time! Hence, in terms of overall error rate, the logistic regression method is no better than the naive approach However, the confusion matrix shows that on days when logistic regression predicts an increase in the market, it has a 58% accuracy rate. This suggests a possible trading strategy of buying on days when the model predicts an increasing market, and avoiding trades on days when a decrease is predicted. Of course one would need to investigate more carefully whether this small improvement was real or just due to random chance.\nSuppose that we want to predict the returns associated with particular values of Lag1 and Lag2. In particular, we want to predict Direction on a day when Lag1 and Lag2 equal 1.2 and 1.1, respectively, and on a day when they equal 1.5 and âˆ’0.8. We do this using the predict() function.\n\npredict(glm.fits, \n        newdata = data.frame (Lag1 = c(1.2, 1.5), Lag2 = c(1.1, -0.8)), \n        type = \"response\")\n\n        1         2 \n0.4791462 0.4960939 \n\n\n\n\nStep 2.8 Logistic regression siblings\nIn this lab we used the glm() function with family = binomial to perform logistic regression. Other choices for the family argument can be used to fit other types of GLMs. For instance, family = Gamma fits a gamma regression model. You can alwarys use the following command to explore more about family argument and possible choices.\n?glm()"
  },
  {
    "objectID": "weeks/week04/lab.html#step-3-naive-bayes",
    "href": "weeks/week04/lab.html#step-3-naive-bayes",
    "title": "ğŸ’» Week 04 - Lab Roadmap (90 min)",
    "section": "Step 3: Naive Bayes",
    "text": "Step 3: Naive Bayes\nThe Smarket data set will still be utilised to fit a naive Bayes classifier to predict Direction.\n\nStep 3.1 Letâ€™s fit a Naive Bayes model\nFit a naive Bayes model to predict Direction using Lag1 and Lag2:\n\nlibrary(e1071)\nnb.fit <- naiveBayes (Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\nnb.fit\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n    Down       Up \n0.491984 0.508016 \n\nConditional probabilities:\n      Lag1\nY             [,1]     [,2]\n  Down  0.04279022 1.227446\n  Up   -0.03954635 1.231668\n\n      Lag2\nY             [,1]     [,2]\n  Down  0.03389409 1.239191\n  Up   -0.03132544 1.220765\n\n\nNaive Bayes is implemented in R using the naiveBayes() function, which is part of the e1071 library. By default, this implementation of the naive Bayes classifier models each quantitative feature using a Gaussian distribution. However, a kernel density method can also be used to estimate the distributions.\nThe output contains the estimated mean and standard deviation for each variable in each class. For example, the mean for Lag1 is 0.0428 for Direction=Down, and the standard deviation is 1.23. We can easily verify this:\n\nmean(Smarket$Lag1[train][Smarket$Direction[train] == \"Down\"])\n\n[1] 0.04279022\n\n\n\nsd(Smarket$Lag1[train][Smarket$Direction[train] == \"Down\"])\n\n[1] 1.227446\n\n\n\n\nStep 3.2 Predict the test set\nPredict Direction in testing data set:\n\nnb.class <- predict(nb.fit, Smarket.2005)\ntable(nb.class, Direction.2005)\n\n        Direction.2005\nnb.class Down  Up\n    Down   28  20\n    Up     83 121\n\n\n\nmean(nb.class == Direction.2005)\n\n[1] 0.5912698\n\n\nThe predict() function is straightforward. From the confusion matrix, Naive Bayes performs very well on this data, with accurate predictions over 59% of the time. This is better than Logistic Regression Model.\nThe predict() function can also generate estimates of the probability that each observation belongs to a particular class.\n\nnb.preds <- predict(nb.fit, Smarket.2005, type = \"raw\")\nnb.preds[1:5, ]\n\n          Down        Up\n[1,] 0.4873164 0.5126836\n[2,] 0.4762492 0.5237508\n[3,] 0.4653377 0.5346623\n[4,] 0.4748652 0.5251348\n[5,] 0.4901890 0.5098110"
  },
  {
    "objectID": "weeks/week04/lab.html#step-4-practical-exercises-in-pairs",
    "href": "weeks/week04/lab.html#step-4-practical-exercises-in-pairs",
    "title": "ğŸ’» Week 04 - Lab Roadmap (90 min)",
    "section": "Step 4: Practical exercises (in pairs)",
    "text": "Step 4: Practical exercises (in pairs)\nSo far, we have learnt to fit some kinds of classification models in R. In this practical case, we will continue to use the data set Auto. Make sure that the missing values have been removed from the data.\nSix questions are listed below. In this part, you will develop a model to predict whether a given car gets high or low gas mileage based on the Auto data set.\nğŸ¯ Questions\n\nCreate a binary variable, mpg01, that contains a 1 if mpg contains a value above its median, and a 0 if mpg contains a value below its median. You can compute the median using the median() function. Note you may find it helpful to use the data.frame() function to create a single data set containing both mpg01 and the other Auto variables.\nExplore the data graphically in order to investigate the association between mpg01 and the other features. Which of the other features seem most likely to be useful in predicting mpg01? Scatterplots and boxplots may be useful tools to answer this question. Describe your findings.\nSplit the data into a training set and a test set. Train set contains observations before 1979. Test set contains the rest of the observations.\nPerform logistic regression on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in question 2. What is the test error of the model obtained?\nPerform naive Bayes on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in question 2. What is the test error of the model obtained?\nWhich of these two methods appears to provide the best results on this data? Justify your choice."
  },
  {
    "objectID": "weeks/week04/lab_solutions.html",
    "href": "weeks/week04/lab_solutions.html",
    "title": "âœ”ï¸ Week 04 - Lab Solutions",
    "section": "",
    "text": "Create a binary variable, mpg01, that contains a 1 if mpg contains a value above its median, and a 0 if mpg contains a value below its median. You can compute the median using the median() function. Note you may find it helpful to use the data.frame() function to create a single data set containing both mpg01 and the other Auto variables.\nlibrary(ISLR2)\nAuto = na.omit(Auto)\nmpg01 = rep(0, dim(Auto)[1])\nmpg01[Auto$mpg > median(Auto$mpg)] = 1\nAuto = data.frame(Auto, mpg01)\nhead(Auto)\nor a easier way by using ifelse() function:\nlibrary(ISLR2)\nAuto = na.omit(Auto)\nAuto$mpg01 <- ifelse(Auto$mpg > median(Auto$mpg), 1, 0)\nhead(Auto)\nBefore we move to next step, we need to lable mpg01 and orgin as factor so that R could recognized them as quanlitative variables instead of quantitative variables.\nAuto$mpg01 = as.factor(Auto$mpg01)\nAuto$origin = as.factor(Auto$origin)\nExplore the data graphically in order to investigate the association between mpg01 and the other features. Which of the other features seem most likely to be useful in predicting mpg01? Scatterplots and boxplots may be useful tools to answer this question. Describe your findings.\npar(mfrow = c(2, 3))\nplot(Auto$mpg01, Auto$cylinders, xlab = \"mpg01\", ylab = \"Number of engine cylinders\")\nplot(Auto$mpg01, Auto$displacement, xlab = \"mpg01\", ylab = \"Engine displacement (cubic inches)\")\nplot(Auto$mpg01, Auto$horsepower, xlab = \"mpg01\", ylab = \"Horsepower\")\nplot(Auto$mpg01, Auto$weight, xlab = \"mpg01\", ylab = \"Weight (pounds)\")\nplot(Auto$mpg01, Auto$acceleration, xlab = \"mpg01\", ylab = \"Time to reach 60mpg (seconds)\")\nplot(Auto$mpg01, Auto$year, xlab = \"mpg01\", ylab = \"Manufacture year\")\nmtext(\"Boxplots for cars with above(1) and below(0) median mpg\", outer = TRUE, line = -3)\nBoxplots were plotted to compare the distributions for each of the quantitative variables between cars with above-median mpg and those with below median-mpg. If the distribution of a predictor significantly varies with the response variable, then it may contribute to the prediction of response variable. If the distribution of a predictor does not significantly differ between different values of the response variable, then it may not contribute to the prediction of response variable. The boxplots suggest that cylinders, displacement, horsepower, and weight might be the most useful in predicting mpg01. The function par() is used to change the layout of output plots.\nWe could try to use ggplot() function to creat some fancy plots which is a combination of boxplot and scatterplot:\nggplot(data = Auto, aes(acceleration, mpg01, colour = mpg01, fill = mpg01)) +\ngeom_boxplot(alpha = 0.125) +\ngeom_jitter(alpha = 0.5, size = 2)  \nTo visualise the association between mpg01 and year, scatterplot is used.\npar(mfrow = c(1, 1))\nplot(Auto$year, Auto$mpg)\nabline(h = median(Auto$mpg), lwd = 2, col = \"red\")\nThe above scatterplot of mpg vs year shows that the newer cars in the data set tend to be more fuel efficient. Therefore, while manufacture year might not be as useful as the other four quantitative variables, it still seems worth including.\nplot(Auto$origin, Auto$mpg, xlab = \"Origin\", ylab = \"mpg\")\nabline(h = median(Auto$mpg), lwd = 2, col = \"red\")\nLastly, when looking at a boxplot that compares the mpg values for each car, categorized by country of origin, we see that there is a clear difference between American cars, which tend to have below-median fuel efficiency, and European and Japanese cars, which tend to have above-median fuel efficiency. Thus, it seems that origin will also be useful in predicting mpg01.\nIn conclusion, all of the predictors except for acceleration and name will be used in fitting this classification model for trying to predict mpg01. Also, mpg will be excluded because that was directly used to create the classification label.\nSplit the data into a training set and a test set. Train set contains observations before 1979. Test set contains the rest of the observations.\nattach(Auto)\ntrain <- (year < 79)\nAuto_train <- Auto[train , ]\nAuto_test <- Auto[!train , ]\nPerform logistic regression on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in question 2. What is the test error of the model obtained?\nglm.fit = glm(mpg01 ~ cylinders + displacement + horsepower + weight + year + origin, data = Auto, subset = train, family = \"binomial\")\nsummary(glm.fit)\nglm.probs = predict(glm.fit, Auto_test, type = \"response\")\nglm.pred = rep(0, dim(Auto_test)[1])\nglm.pred[glm.probs > 0.5] = 1\ntable(glm.pred, Auto_test$mpg01, dnn = c(\"Predicted\", \"Actual\"))\nmean(glm.pred == Auto_test$mpg01)\n[1] 0.877193\nAs is shown, the test error of the model ontained is (1-0.877193) = 0.122807.\nPerform naive Bayes on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in question 2. What is the test error of the model obtained?\nlibrary (e1071)\nnb.fit = naiveBayes(mpg01 ~ cylinders + displacement + horsepower + weight + year + origin, data = Auto, subset = train)\nnb.fit\nnb.class <- predict(nb.fit, Auto_test)\ntable(nb.class, Auto_test$mpg01, dnn = c(\"Predicted\", \"Actual\"))\nmean(nb.class == Auto_test$mpg01)\nAs is shown, the test error of the model ontained is (1-0.877193) = 0.122807.\nWhich of these two methods appears to provide the best results on this data? Justify your choice.\nAfter comparing the test errors, these two classification models were equally good. To further compare the performance of these two model, we have to look at the confusion matrix and find these two classification models have identiical confusion matrix. It means that Precision, Recall, Accuracy and F-Score of these two models are all same. Therefore, we can conclude that these two classification models have equally good performace on this data set. One thing that should be cautious of is that the data set is imbalnced. Therefore, we cannot judge the performance of this data set based on test error. More detailed explaination about be found here: https://www.analyticsvidhya.com/blog/2021/06/5-techniques-to-handle-imbalanced-data-for-a-classification-problem/."
  },
  {
    "objectID": "weeks/week04/lecture.html",
    "href": "weeks/week04/lecture.html",
    "title": "ğŸ‘¨â€ğŸ« Week 04 - Lecture",
    "section": "",
    "text": "There are no slides this week! Instead, you will form groups (of five people) and we will run a couple of live experiments together.\n\nğŸ’» Bring your laptop or team up with someone who will bring one!\n\n\n\n\nHead over to ğŸ“¥W04 Lecture Files (on Moodle) to download the files you will need for this lecture.\nYou will form groups of five people to work on getting the â€œbestâ€ model for your specific dataset by following a set of tasks marked as ğŸ¯ ACTION POINT in the RMarkdown.\nAs you work on the solutions, you will send me the responses on Slack and I will update the tables below live during the lecture. The final outcome will be available on the website later.\n\n\n\n\n\n\n\nIf there are no slides, how do I revise this later?\n\n\n\n\n\n\nYou can read all about resampling methods in our textbook, more specifically (James et al. 2021, chap. 5).\nThe RMarkdown you will use during the lecture will help you revise later, too.\n\n\n\n\n\n\n\nHow did I create the datasets you are using in this lecture?\n\n\nFirst, I selected the dataset Wage from the ISLR2 package in R and randomly split it into two subsets:\n\n\\(10\\%\\) for external validation set: this is a portion of data that I have kept hidden from everyone.\n\n\\(90\\%\\) available for model training, that I further distributed to groups. I call this the internal validation set\n\n\n\n\nI then took the internal validation set and further split it into five random subsets:\n\nFold1\n\n\nFold2\n\n\nFold3\n\n\nFold4\n\n\nFold5\n\nWe call each of this subsets a fold. Therefore, we have 5 folds of data.\n\n\n\nThe point of doing this is to perform cross-validation. The goal is to answer the following question:\n\nHow well does our model perform on data it has not seen yet?\n\nNotice that this goes beyond asessing goodness-of-fit. Instead of focusing on how well our model fits the current data, we ask whether it would still be generalisable should be receive new data.\nHow do we do that? We could run the same model on different subsets of the data, holding one of the folds for testing.\nFor example:\n\nTrain\n\n\nTrain\n\n\nTrain\n\n\nTrain\n\n\nTest\n\n\n\n\nSince I have split the data into five folds, I could train and test algorithms using five different splits of my data. This 5-fold cross-validation looks like this:\n\nSplit 1:\n\n\nTest\n\n\nTrain\n\n\nTrain\n\n\nTrain\n\n\nTrain\n\n\n\nSplit 2:\n\n\nTrain\n\n\nTest\n\n\nTrain\n\n\nTrain\n\n\nTrain\n\n\n\nSplit 3:\n\n\nTrain\n\n\nTrain\n\n\nTest\n\n\nTrain\n\n\nTrain\n\n\n\nSplit 4:\n\n\nTrain\n\n\nTrain\n\n\nTrain\n\n\nTest\n\n\nTrain\n\n\n\nSplit 5:\n\n\nTrain\n\n\nTrain\n\n\nTrain\n\n\nTrain\n\n\nTest\n\nEach group will be working on a separate split of this data. You will train your models on 80% of the data, evaluate the goodness-of-fit in the training data, then assess the performance in the test data. Once you find a model that has a good performance, balanced in training vs test data, then we will see how well your model performs in the external data set.\nThe action points below will be updated during the lecture:\n\n\nğŸ¯ ACTION POINT 1: Check if your numbers match\n\n\n\n\nDistribution of above150k\n(Training)\n  No  Yes \n1910  250 \n(Test)\n No Yes \n473  67 \n\n\n\nDistribution of above150k\n(Training)\n  No  Yes \n1900  260 \n(Test)\n No Yes \n483  57 \n\n\n\nDistribution of above150k\n(Training)\n  No  Yes \n1904  256 \n(Test)\n No Yes \n479  61 \n\n\n\nDistribution of above150k\n(Training)\n  No  Yes \n1915  245 \n(Test)\n No Yes \n468  72  \n\n\n\nDistribution of above150k\n(Training)\n  No  Yes \n1903  257 \n(Test)\n No Yes \n480  60 \n\n\n\nDistribution of above150k\nJust for your knowledge:\n No Yes \n274  26 \n\n\n\n\n\nğŸ¯ ACTION POINT 2 & 3: Tell us your best threshold (training data)\n\nDATASET 1\n\nğŸ—£ï¸ Sofie\n\nBest threshold = $$\n\n(Training Stats)\n\nAccuracy = $$\nTNR = $$\nTPR = $$\nPrecision = $$\nRecall = $$\nF1-score = $$\n\n\n\nğŸ—£ï¸ Yujia\n\nBest threshold = \\(0.245\\)\n\n(Training Stats)\n\nAccuracy = \\(82.41 \\%\\)\nTNR = $ 84.97 %$\nTPR = $ 62.80 %$\nPrecision = $ 35.36 %$\nF1-score = \\(0.4524496\\)\n\n\nDATASET 2\n\nğŸ—£ï¸ Vansh\n\nBest threshold = \\(0.2\\)\n\n(Training Stats)\n\nAccuracy = \\(79.44 \\%\\)\nTNR = \\(80.58 \\%\\)\nTPR = \\(71.15 \\%\\)\nPrecision = \\(33.39 \\%\\)\nF1-score = \\(0.4545\\)\n\n\n\nğŸ—£ï¸ Ekki\n\nBest threshold = $$\n\n(Training Stats)\n\nAccuracy = $$\nTNR = $$\nTPR = $$\nPrecision = $$\nRecall = $$\nF1-score = $$\n\n\nDATASET 3\n\nğŸ—£ï¸ Yoyo\n\nBest threshold = \\(0.23\\)\n\n(Training Stats)\n\nAccuracy = \\(81.85 \\%\\)\nTNR = \\(84.40 \\%\\)\nTPR = \\(62.89 \\%\\)\nPrecision = \\(35.15 \\%\\)\nF1-score = \\(0.4509804\\)\n\n\n\nğŸ—£ï¸ Diljot\n\nBest threshold = $$\n\n(Training Stats)\n\nAccuracy = $$\nTNR = $$\nTPR = $$\nPrecision = $$\nRecall = $$\nF1-score = $$\n\n\nDATASET 4\n\nğŸ—£ï¸ Ashley\n\nBest threshold = \\(0.23\\)\n\n(Training Stats)\n\nAccuracy = $ 83.94 %$\nTNR = $ 86.95 %$\nTPR = \\(60.41 \\%\\)\nPrecision = $ 37.19 %$\nF1-score = \\(0.4603421\\)\n\n\n\nğŸ—£ï¸ Lisa\n\nBest threshold = $$\n\n(Training Stats)\n\nAccuracy = $$\nTNR = $$\nTPR = $$\nPrecision = $$\nRecall = $$\nF1-score = $$\n\n\nDATASET 5\n\nğŸ—£ï¸ Paul Keenan\n\nBest threshold = \\(0.245\\)\n\n(Training Stats)\n\nAccuracy = \\(82.59259 \\%\\)\nTNR = \\(85.54913 \\%\\)\nTPR (Recall) = \\(60.70039 \\%\\)\nPrecision = \\(36.1949 \\%\\)\nF1-score = \\(0.4534884\\)\n\n\n\nğŸ—£ï¸ Andres\n\nBest threshold = \\(0.25\\)\n\n(Training Stats)\n\nAccuracy = \\(83.01 \\%\\)\nTNR = \\(86.50 \\%\\)\nTPR (Recall) = \\(57.20 \\%\\)\nPrecision = \\(36.39 \\%\\)\nF1-score = $0.4447806 $\n\n\n\n\n\n\nğŸ¯ ACTION POINT 4 & 5: What is the best threshold both for training and test?\n\n\n\n\nDATASET 1\n\nDATASET 1\n\nğŸ—£ï¸ <Person>\n\nBest threshold = $$\n\n(Training vs Test)\n\nAccuracy = $ $ vs $ $\nTNR = $ $ vs $ $\nTPR = $ $ vs $ $\nPrecision = $ $ vs $ $\nRecall = $ $ vs $ $\nF1-score = $ $ vs $ $\n\n\n\nğŸ—£ï¸ <Person>\n\nBest threshold = $ $ vs $ $\n\n(Training Stats)\n\nAccuracy = $ $ vs $ $\nTNR = $ $ vs $ $\nTPR = $ $ vs $ $\nPrecision = $ $ vs $ $\nRecall = $ $ vs $ $\nF1-score = $ $ vs $ $\n\n\n\nğŸ—£ï¸ <Person>\n\nBest threshold = $ $ vs $ $\n\n(Training Stats)\n\nAccuracy = $ $ vs $ $\nTNR = $ $ vs $ $\nTPR = $ $ vs $ $\nPrecision = $ $ vs $ $\nRecall = $ $ vs $ $\nF1-score = $ $ vs $ $\n\n\n\nğŸ—£ï¸ <Person>\n\nBest threshold = $ $ vs $ $\n\n(Training Stats)\n\nAccuracy = $ $ vs $ $\nTNR = $ $ vs $ $\nTPR = $ $ vs $ $\nPrecision = $ $ vs $ $\nRecall = $ $ vs $ $\nF1-score = $ $ vs $ $\n\n\n\n\n\nDATASET 2\n\nDATASET 2\n\nğŸ—£ï¸ <Person>\n\nBest threshold = $ $ vs $ $\n\n(Training Stats)\n\nAccuracy = $ $ vs $ $\nTNR = $ $ vs $ $\nTPR = $ $ vs $ $\nPrecision = $ $ vs $ $\nRecall = $ $ vs $ $\nF1-score = $ $ vs $ $\n\n\n\nğŸ—£ï¸ <Person>\n\nBest threshold = $ $ vs $ $\n\n(Training Stats)\n\nAccuracy = $ $ vs $ $\nTNR = $ $ vs $ $\nTPR = $ $ vs $ $\nPrecision = $ $ vs $ $\nRecall = $ $ vs $ $\nF1-score = $ $ vs $ $\n\n\n\nğŸ—£ï¸ <Person>\n\nBest threshold = $ $ vs $ $\n\n(Training Stats)\n\nAccuracy = $ $ vs $ $\nTNR = $ $ vs $ $\nTPR = $ $ vs $ $\nPrecision = $ $ vs $ $\nRecall = $ $ vs $ $\nF1-score = $ $ vs $ $\n\n\n\nğŸ—£ï¸ <Person>\n\nBest threshold = $ $ vs $ $\n\n(Training Stats)\n\nAccuracy = $ $ vs $ $\nTNR = $ $ vs $ $\nTPR = $ $ vs $ $\nPrecision = $ $ vs $ $\nRecall = $ $ vs $ $\nF1-score = $ $ vs $ $\n\n\n\n\n\nDATASET 3\n\nDATASET 3\n\nğŸ—£ï¸ <Person>\n\nBest threshold = $ $ vs $ $\n\n(Training Stats)\n\nAccuracy = $ $ vs $ $\nTNR = $ $ vs $ $\nTPR = $ $ vs $ $\nPrecision = $ $ vs $ $\nRecall = $ $ vs $ $\nF1-score = $ $ vs $ $\n\n\n\nğŸ—£ï¸ <Person>\n\nBest threshold = $ $ vs $ $\n\n(Training Stats)\n\nAccuracy = $ $ vs $ $\nTNR = $ $ vs $ $\nTPR = $ $ vs $ $\nPrecision = $ $ vs $ $\nRecall = $ $ vs $ $\nF1-score = $ $ vs $ $\n\n\n\nğŸ—£ï¸ <Person>\n\nBest threshold = $ $ vs $ $\n\n(Training Stats)\n\nAccuracy = $ $ vs $ $\nTNR = $ $ vs $ $\nTPR = $ $ vs $ $\nPrecision = $ $ vs $ $\nRecall = $ $ vs $ $\nF1-score = $ $ vs $ $\n\n\n\nğŸ—£ï¸ <Person>\n\nBest threshold = $ $ vs $ $\n\n(Training Stats)\n\nAccuracy = $ $ vs $ $\nTNR = $ $ vs $ $\nTPR = $ $ vs $ $\nPrecision = $ $ vs $ $\nRecall = $ $ vs $ $\nF1-score = $ $ vs $ $\n\n\n\n\n\nDATASET 4\n\nDATASET 4\n\nğŸ—£ï¸ <Person>\n\nBest threshold = $ $ vs $ $\n\n(Training Stats)\n\nAccuracy = $ $ vs $ $\nTNR = $ $ vs $ $\nTPR = $ $ vs $ $\nPrecision = $ $ vs $ $\nRecall = $ $ vs $ $\nF1-score = $ $ vs $ $\n\n\n\nğŸ—£ï¸ <Person>\n\nBest threshold = $ $ vs $ $\n\n(Training Stats)\n\nAccuracy = $ $ vs $ $\nTNR = $ $ vs $ $\nTPR = $ $ vs $ $\nPrecision = $ $ vs $ $\nRecall = $ $ vs $ $\nF1-score = $ $ vs $ $\n\n\n\nğŸ—£ï¸ <Person>\n\nBest threshold = $ $ vs $ $\n\n(Training Stats)\n\nAccuracy = $ $ vs $ $\nTNR = $ $ vs $ $\nTPR = $ $ vs $ $\nPrecision = $ $ vs $ $\nRecall = $ $ vs $ $\nF1-score = $ $ vs $ $\n\n\n\nğŸ—£ï¸ <Person>\n\nBest threshold = $ $ vs $ $\n\n(Training Stats)\n\nAccuracy = $ $ vs $ $\nTNR = $ $ vs $ $\nTPR = $ $ vs $ $\nPrecision = $ $ vs $ $\nRecall = $ $ vs $ $\nF1-score = $ $ vs $ $\n\n\n\n\n\nDATASET 5\n\nDATASET 5\n\nğŸ—£ï¸ <Person>\n\nBest threshold = $ $ vs $ $\n\n(Training Stats)\n\nAccuracy = $ $ vs $ $\nTNR = $ $ vs $ $\nTPR = $ $ vs $ $\nPrecision = $ $ vs $ $\nRecall = $ $ vs $ $\nF1-score = $ $ vs $ $\n\n\n\nğŸ—£ï¸ <Person>\n\nBest threshold = $ $ vs $ $\n\n(Training Stats)\n\nAccuracy = $ $ vs $ $\nTNR = $ $ vs $ $\nTPR = $ $ vs $ $\nPrecision = $ $ vs $ $\nRecall = $ $ vs $ $\nF1-score = $ $ vs $ $\n\n\n\nğŸ—£ï¸ <Person>\n\nBest threshold = $ $ vs $ $\n\n(Training Stats)\n\nAccuracy = $ $ vs $ $\nTNR = $ $ vs $ $\nTPR = $ $ vs $ $\nPrecision = $ $ vs $ $\nRecall = $ $ vs $ $\nF1-score = $ $ vs $ $\n\n\n\nğŸ—£ï¸ <Person>\n\nBest threshold = $ $ vs $ $\n\n(Training Stats)\n\nAccuracy = $ $ vs $ $\nTNR = $ $ vs $ $\nTPR = $ $ vs $ $\nPrecision = $ $ vs $ $\nRecall = $ $ vs $ $\nF1-score = $ $ vs $ $\n\n\n\n\n\n\nğŸ¯ ACTION POINT 6 & 7: What about the external dataset?\n\n\n\nYou will be asked to upload your model to Slack. I will then run your model on the external data and report back the results!"
  },
  {
    "objectID": "weeks/week05/checklist.html",
    "href": "weeks/week05/checklist.html",
    "title": "âœ… Week 05 - Checklist",
    "section": "",
    "text": "Important\n\n\n\n\n\nKeep in mind that after the lecture at the end of this week, on Friday 28 October 2022, we will release the Summative Problem Set 01. This is the first summative assessment of this course and it is worth 20% of your final grade. You will have until 9 November 2022 â€” Wednesday of Week 07 â€” to submit your solutions via Moodle.\nHere is a suggestion of how to program your week in relation to this course:\nExtra:"
  },
  {
    "objectID": "weeks/week05/checklist.html#if-your-lab-is-on-monday",
    "href": "weeks/week05/checklist.html#if-your-lab-is-on-monday",
    "title": "âœ… Week 05 - Checklist",
    "section": "If your lab is on Monday:",
    "text": "If your lab is on Monday:\n\nğŸ“¥ Download: Before or once you arrive at the classroom, download the DS202_2022MT_w05_lab_rmark.Rmd file that contains the lab roadmap (under ğŸ—“ï¸ Week 05 - Non-linear algorithms/W04 Lab Files section on Moodle). Or browse the webpage version here.\nğŸ’» Participate: Actively engage with the material in the lab. Ask your class teacher for help if anything is unclear. Work with others whenever possible and take notes of theoretical concepts or practical coding skill you might want to revisit later in the week.\nğŸ“™ Read: Find some time to read (James et al. 2021, chap. 5) and reinforce your theoretical understanding of resampling methods; the textbook is available online for free.\n\nIn the lecture/workshop last week, we only explored training/test splits and the idea of cross-validation; in the lab, you will have a chance to explore another technique called the bootstrap.\nAs you go through the text, try to connect what you read to the things you heard about in the lecture/workshop or the examples you explored in the lab.\n\nâœï¸ Solve: There is still time to submit your solutions to the Formative Problem Set 01, the deadline is Tuesday 25 October 23:59 UK time.\nğŸ« Attend the lecture: This week, you will learn of two new algorithms: Support Vector Machine and Decision Tree. You will need to use these algorithms in the first summative that will be released on Friday."
  },
  {
    "objectID": "weeks/week05/checklist.html#if-your-lab-is-on-friday",
    "href": "weeks/week05/checklist.html#if-your-lab-is-on-friday",
    "title": "âœ… Week 05 - Checklist",
    "section": "If your lab is on Friday:",
    "text": "If your lab is on Friday:\n\nâœï¸ Solve: There is still time to submit your solutions to the Formative Problem Set 01, the deadline is Tuesday 25 October 23:59 UK time.\nğŸ“™ Read: Find some time to read (James et al. 2021, chap. 5) and reinforce your theoretical understanding of resampling methods; the textbook is available online for free.\n\nIn the lecture/workshop last week, we only explored training/test splits and the idea of cross-validation; in the lab, you will have a chance to explore another technique called the bootstrap.\nAs you go through the text, try to connect what you read to the things you heard about in the lecture/workshop or the examples you explored in the lab.\n\nğŸ“¥ Download: Before or once you arrive at the classroom, download the DS202_2022MT_w05_lab_rmark.Rmd file that contains the lab roadmap (under ğŸ—“ï¸ Week 05 - Non-linear algorithms/W04 Lab Files section on Moodle). Or browse the webpage version here.\nğŸ’» Participate: Actively engage with the material in the lab. Ask your class teacher for help if anything is unclear. Work with others whenever possible and take notes of theoretical concepts or practical coding skill you might want to revisit later in the week.\nğŸ« Attend the lecture: This week, you will learn of two new algorithms: Support Vector Machine and Decision Tree. You will need to use these algorithms in the first summative that will be released on Friday."
  },
  {
    "objectID": "weeks/week05/lab.html",
    "href": "weeks/week05/lab.html",
    "title": "ğŸ’» Week 05 - Lab Roadmap (90 min)",
    "section": "",
    "text": "Last week we learned how to do the Classification Methods in R. As data scientists, we need to know how to examine the results we generated and also to justify our results. So, we will focus on and practice the validation methods in the R language in this weekâ€™s lab.\nIf you feel confident at the current stage, free to explore more on your own. We have provided you with some supplementary online resources :\nR packages you will need:"
  },
  {
    "objectID": "weeks/week05/lab.html#step-1-the-validation-set-approach",
    "href": "weeks/week05/lab.html#step-1-the-validation-set-approach",
    "title": "ğŸ’» Week 05 - Lab Roadmap (90 min)",
    "section": "Step 1: The Validation Set Approach",
    "text": "Step 1: The Validation Set Approach\nIn this step, we will explore how to extract the subset of the whole dataset as a training dataset, and then estimate the test error rates of various linear models. The dataset is Auto from the ISLR2 package.\n\nStep 1.1: Training vs Testing splits\nSplit the Auto dataset into two halves, as randomly selecting 196 observations out of the original 392 observations. We performed splits using base R in the last lab. However, this can be done more easily using the rsample package. We first create a split object Auto.split using prop = 0.5 to specify a \\(50\\%/50\\%\\) train/test split. We also specify strata = mpg as we want our train/test split to have a similar mean/standard deviation for mpg. We then create dataframes using the training and testing functions.\n\nlibrary(ISLR2)\nlibrary(rsample)\n\nset.seed(1) # Just so you and I have the same \"random\" numbers.\n\nAuto.split <- initial_split(Auto, prop = 0.5, strata = mpg)\nAuto.train <- training(Auto.split)\nAuto.test <- testing(Auto.split)\n\n\n\n\n\n\n\nAbout set.seed()\n\n\n\n\n\nset.seed() is important here as it set a seed for the random number generator. Literally, the same results will be replicated in the following steps. Further information can be found in the official documentation.Seeding Random Variate Generators\n\n\n\n\n\nStep 1.2: How good is your model at predicting the test set?\nFit a linear regression model using the training dataset (Auto.train), making the mpg as the dependent variable(x) and horsepower as the independent variable(y). Then, using the fitted model to estimate the mpg from Auto.test. Finally, calculating the MSE of the 196 observations in the validation set.\n\n# use the lm() function to fit a linear regression model\nlm.fit <- lm(mpg ~ horsepower, data = Auto.train)\n\n# estimate the 'mpg' values by the lm.fit model\nlm.pred <- predict(lm.fit, Auto.test)\n\n# calculate MSE\nmean((Auto.test$mpg - lm.pred)^2)\n\n[1] 25.58657\n\n\nTherefore, we have estimated the test MSE for the linear regression model, \\(\\mathbf{25.59}\\). (Well Done! ğŸ’ª )\n\n\n\n\n\n\nHow to get help in R\n\n\n\n\n\nIf you are ever in doubt of what a particular R function do, you can consult the R documentation for it by using ?.\nFor example, you have been using lm and predict but you want to see what other arguments/parameters this function can take, just type ?lm or ?predict in your console.\n\n\n\n\n\n\n\n\n\nHow to access or create columns in R?\n\n\n\n\n\nIn base R, you generally use the $ sign to access columns or create new columns.\n\n#You can see all the values from the 'mpg' column.\nAuto$mpg\n\n  [1] 18.0 15.0 18.0 16.0 17.0 15.0 14.0 14.0 14.0 15.0 15.0 14.0 15.0 14.0 24.0\n [16] 22.0 18.0 21.0 27.0 26.0 25.0 24.0 25.0 26.0 21.0 10.0 10.0 11.0  9.0 27.0\n [31] 28.0 25.0 19.0 16.0 17.0 19.0 18.0 14.0 14.0 14.0 14.0 12.0 13.0 13.0 18.0\n [46] 22.0 19.0 18.0 23.0 28.0 30.0 30.0 31.0 35.0 27.0 26.0 24.0 25.0 23.0 20.0\n [61] 21.0 13.0 14.0 15.0 14.0 17.0 11.0 13.0 12.0 13.0 19.0 15.0 13.0 13.0 14.0\n [76] 18.0 22.0 21.0 26.0 22.0 28.0 23.0 28.0 27.0 13.0 14.0 13.0 14.0 15.0 12.0\n [91] 13.0 13.0 14.0 13.0 12.0 13.0 18.0 16.0 18.0 18.0 23.0 26.0 11.0 12.0 13.0\n[106] 12.0 18.0 20.0 21.0 22.0 18.0 19.0 21.0 26.0 15.0 16.0 29.0 24.0 20.0 19.0\n[121] 15.0 24.0 20.0 11.0 20.0 19.0 15.0 31.0 26.0 32.0 25.0 16.0 16.0 18.0 16.0\n[136] 13.0 14.0 14.0 14.0 29.0 26.0 26.0 31.0 32.0 28.0 24.0 26.0 24.0 26.0 31.0\n[151] 19.0 18.0 15.0 15.0 16.0 15.0 16.0 14.0 17.0 16.0 15.0 18.0 21.0 20.0 13.0\n[166] 29.0 23.0 20.0 23.0 24.0 25.0 24.0 18.0 29.0 19.0 23.0 23.0 22.0 25.0 33.0\n[181] 28.0 25.0 25.0 26.0 27.0 17.5 16.0 15.5 14.5 22.0 22.0 24.0 22.5 29.0 24.5\n[196] 29.0 33.0 20.0 18.0 18.5 17.5 29.5 32.0 28.0 26.5 20.0 13.0 19.0 19.0 16.5\n[211] 16.5 13.0 13.0 13.0 31.5 30.0 36.0 25.5 33.5 17.5 17.0 15.5 15.0 17.5 20.5\n[226] 19.0 18.5 16.0 15.5 15.5 16.0 29.0 24.5 26.0 25.5 30.5 33.5 30.0 30.5 22.0\n[241] 21.5 21.5 43.1 36.1 32.8 39.4 36.1 19.9 19.4 20.2 19.2 20.5 20.2 25.1 20.5\n[256] 19.4 20.6 20.8 18.6 18.1 19.2 17.7 18.1 17.5 30.0 27.5 27.2 30.9 21.1 23.2\n[271] 23.8 23.9 20.3 17.0 21.6 16.2 31.5 29.5 21.5 19.8 22.3 20.2 20.6 17.0 17.6\n[286] 16.5 18.2 16.9 15.5 19.2 18.5 31.9 34.1 35.7 27.4 25.4 23.0 27.2 23.9 34.2\n[301] 34.5 31.8 37.3 28.4 28.8 26.8 33.5 41.5 38.1 32.1 37.2 28.0 26.4 24.3 19.1\n[316] 34.3 29.8 31.3 37.0 32.2 46.6 27.9 40.8 44.3 43.4 36.4 30.0 44.6 33.8 29.8\n[331] 32.7 23.7 35.0 32.4 27.2 26.6 25.8 23.5 30.0 39.1 39.0 35.1 32.3 37.0 37.7\n[346] 34.1 34.7 34.4 29.9 33.0 33.7 32.4 32.9 31.6 28.1 30.7 25.4 24.2 22.4 26.6\n[361] 20.2 17.6 28.0 27.0 34.0 31.0 29.0 27.0 24.0 36.0 37.0 31.0 38.0 36.0 36.0\n[376] 36.0 34.0 38.0 32.0 38.0 25.0 38.0 26.0 22.0 32.0 36.0 27.0 27.0 44.0 32.0\n[391] 28.0 31.0\n\n\n\n\n\n\n\nStep 1.3: Does it get better if I modify the features using polynomial terms?\nRepeat the second part to estimate the test error for the quadratic and cubic regressions.\n\nlm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto.train)\n\n\nmean((Auto.test$mpg - predict(lm.fit2, Auto.test))^2)\n\n[1] 19.08374\n\n\n\nlm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto.train)\nmean((Auto.test$mpg - predict(lm.fit3, Auto.test))^2)\n\n[1] 19.00317\n\n\nğŸ’¡ We can see a model that predicts mpg using a quadratic function of horsepower performs better than a model that involves only a linear function of horsepower. Furthermore, we see that adding a cubic function of horsepower actually increases MSE when compared to the quadratic function. Thus, the quadratic function of horsepower appears to perform the best out of all the functions considered.\n\n\nStep 1.4: Visualisation\nWant to see how well does the quadratic fit maps onto the raw data?\nLetâ€™s create a scatter plot with horsepower on the x-axis and mpg on the y-axis. Now, we can predict mpg using lm.fit2 and specify interval = 'confidence' to get \\(95\\%\\) confidence intervals. Along with geom_line we can use geom_ribbon to plot the line of best fit and associated confidence intervals. Remember to specify alpha so that we can see the predicted value - otherwise the ribbon will not be translucent.\n\n    library(tidyverse)\n\n    sim.data <- data.frame(horsepower = 46:230)\n\n    sim.pred <- predict(lm.fit2, sim.data, interval = 'confidence') \n\n    sim.data <- cbind(sim.data, sim.pred)\n\n    ggplot(data = sim.data, aes(x = horsepower, y = fit)) +\n        geom_point(data = Auto, aes(x = horsepower, y = mpg)) +\n        geom_line() +\n        geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.125, fill = 'blue') +\n        theme_minimal() +\n        labs(x = 'Horsepower', y = 'MPG')"
  },
  {
    "objectID": "weeks/week05/lab.html#step-2-k-fold-cross-validation",
    "href": "weeks/week05/lab.html#step-2-k-fold-cross-validation",
    "title": "ğŸ’» Week 05 - Lab Roadmap (90 min)",
    "section": "Step 2: k-Fold Cross-Validation",
    "text": "Step 2: k-Fold Cross-Validation\nIt will be easy to follow the former procedure in Step 2, by the cv.glm to implement K-fold CV.\n\nStep 2.1: Using K=10 folds\nLetâ€™s estimate Cross-Validation errors corresponding to the polynomial fits of orders one to ten using ten-fold cross-validation (via K = 10).\nThe cv.glm() function is part of the boot library. Meanwhile, you can explore the cv.err by yourself to see what call,K,delta and seed mean. This online webpage will be useful when interpreting the results.\n\nlibrary(boot)\n\n\nset.seed(17)\n\ncv.error.10 <- c() \n\nfor(i in 1:10){\n    glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)\n    cv.error.10[i] <- cv.glm(data = Auto, glmfit = glm.fit, K = 10)$delta[1]\n}\n\ncv.error.10\n\n [1] 24.27207 19.26909 19.34805 19.29496 19.03198 18.89781 19.12061 19.14666\n [9] 18.87013 20.95520\n\n\n\n# we can plot the results by passing a data frame to ggplot\n\ncv.data <- data.frame(poly = 1:10, cv.errs = cv.error.10)\n\nggplot(data = cv.data, aes(x = poly, y = cv.errs)) +\n    geom_point() +\n    geom_line(linetype = 'dashed') +\n    scale_x_continuous(breaks = 1:10) +\n    theme_minimal() +\n    labs(x = 'Degree of polynomial', y = 'Cross-validated MSE')\n\n\n\n\nNote that in the line of cv.error.10[i] <- cv.glm(data = Auto, glmfit = glm.fit, K=10)$delta[1], it will be very strict to K rather than k."
  },
  {
    "objectID": "weeks/week05/lab.html#step-3-the-bootstrap",
    "href": "weeks/week05/lab.html#step-3-the-bootstrap",
    "title": "ğŸ’» Week 05 - Lab Roadmap (90 min)",
    "section": "Step 3: The Bootstrap",
    "text": "Step 3: The Bootstrap\nWe have learnt the theoretical method regarding Bootstrap. I understand that it may be a bit difficult for beginners in statistics, but we will mainly focus on the coding implementation and visualisation here. Also, we will introduce how to create a function below.\nFunctions are â€œself containedâ€ modules of code that accomplish a specific task. referenced from Functions and theri argumens\nWith the help of a function, you can reuse the same pattern codes with a simple function name. In fact, you work with functions all the time in R - perhaps without even realising it!\n\nStep 3.1 Estimating the Accuracy of a Linear Regression Model\nIn this step, we will use the boostrap approach to assess the variability of a coefficient estimate. For the sake of simplicity we will look at the relationship between weight and horsepower which appears to be linear.\nCreate a function as boot.fn() which\n\nboot.fn <- function(data, index) {\n\n    lm(horsepower ~ weight, data = data[index,])$coefficients\n\n}  \n\nboot.fn simply returns a vector of coefficient estimates. It takes two parameters: data and index. data is a placeholder for the dataframe used in the model. index is a placeholder for the sample used to subset the dataframe. Other than this, the body of the function should look familiar. We are estimating a linear model where we are looking to predict horsepower by weight, and then extracting the coefficients.\nNow, compare the results from bootstrap estimates and the standard estimates\n\n# bootstrap with 1000 times\n\nboot(Auto, boot.fn, 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Auto, statistic = boot.fn, R = 1000)\n\n\nBootstrap Statistics :\n        original        bias    std. error\nt1* -12.18348470  1.659517e-01 3.221766853\nt2*   0.03917702 -6.479214e-05 0.001251433\n\n\n\nsummary(lm(horsepower ~ weight, data = Auto))$coef\n\n                Estimate  Std. Error   t value      Pr(>|t|)\n(Intercept) -12.18348470 3.570431493 -3.412328  7.115312e-04\nweight        0.03917702 0.001153214 33.972031 1.364347e-118\n\n\nWe can find that in the bootstrap estimation process, \\(\\mathrm{SE}(\\hat{\\beta}_{0}) = 3.5704\\) and \\(\\mathrm{SE}(\\hat{\\beta}_{1}) = 0.0012\\) , while in the standard estimation process, \\(\\hat{\\beta}_{Intercept}=-12.1835\\) and \\(\\hat{\\beta}_{horsepower}=0.0392\\).\nTo get a better intuition of what the bootstrap algorithm does letâ€™s create a ggplot. We can get the intercepts and slopes estimated and overlay them on a scatterplot (weight on x-axis, horsepower on y-axis). We will create 50 bootstrap resamples for ease of visualisation and use geom_abline to overlay all the lines of best fit.\n\nboot.model <- boot(Auto, boot.fn, 50)\n\nboot.df <- as.data.frame(boot.model$t)\nnames(boot.df) <- c('b0','b1')\n\nggplot(data = Auto, aes(x = weight, y = horsepower)) +\n    geom_point() +\n    geom_abline(data = boot.df,\n                aes(intercept = b0, slope = b1), \n                alpha = 0.1, colour = 'blue') +\n    theme_minimal() +\n    labs(x = 'Weight (lbs.)', y = 'Engine horsepower')"
  },
  {
    "objectID": "weeks/week05/lab.html#step-4-practical-exercises",
    "href": "weeks/week05/lab.html#step-4-practical-exercises",
    "title": "ğŸ’» Week 05 - Lab Roadmap (90 min)",
    "section": "Step 4: ğŸ¯ Practical Exercises",
    "text": "Step 4: ğŸ¯ Practical Exercises\nSince then, we have known and implemented the coding with Cross-validation and Bootstrap. In this practical case, we will use the new dataset Default and also Weekly from the ISRL package. Do not forget to set a random seed before beginning your analysis.\nSome questions are listed below. You are required to try to answer these questions in pairs using R commands. We will go over the solutions once everyone has finished these questions.\n\nQ1: Train vs test sets\nFor the Default dataset, please split the sample set into a training set and a test set, then fit a logistic regression model that uses income and balance to predict default:\n\nQ1.1: Use three different splits of the observations into a training set and a test set.\nQ1.2: Fit three multiple logistic regression models using only the training observations.\nQ1.3: Based on the three models, obtain a prediction of default status for each individual in the test set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than 0.5.\nQ1.4: Based on the three models, compute the test set error, which is the fraction of the observations in the test set that are misclassified.\n\n\n\nQ2: Bootstrap\nFor the Default dataset, We continue to consider the use of a logistic regression model to predict the probability of default using income and balance on the Default data set.\nIn particular, we will now compute estimates for the standard errors of the income and balance logistic regression coefficients in two different ways: 2.1. Using the bootstrap, 2.2. Using the standard formula for computing the standard errors in the glm() function. As following,\n\nUsing the summary() and glm() functions, determine the estimated standard errors for the coefficients associated with income and balance in a multiple logistic regression model that uses both predictors.\nWrite a function,boot.fn(),that takes as input the Default data set as well as an index of the observations, and that outputs the coefficient estimates for income and balance in the multiple logistic regression model.\nUse the boot() function together with your boot.fn() function to estimate the standard errors of the logistic regression coefficients for income and balance. Then, Create a histogram of the bootstrap parameter estimates with ggplot2, and also set the bins=20, title as 1,000 Bootstrap Parameter Estimates - 'balance' & 'income."
  },
  {
    "objectID": "weeks/week05/lab_solutions.html",
    "href": "weeks/week05/lab_solutions.html",
    "title": "âœ”ï¸ Week 05 - Lab Solutions",
    "section": "",
    "text": "For the Default dataset, please split the sample set into a training set and a validation set, then fit a logistic regression model that uses income and balance to predict default.\nlibrary(ISLR2)\nlibrary(tidyverse)\n\n?Default\n\n\nUse three different splits of the observations into a training set and a test set.\nFirst generate the splits:\nlibrary(tidymodels)\n\nDefault.split1 <- initial_split(Default, prop = 0.5, strata = default)\nDefault.split2 <- initial_split(Default, prop = 0.5, strata = default)\nDefault.split3 <- initial_split(Default, prop = 0.5, strata = default)\nor maybe with different random proportions?\nDefault.split1 <- initial_split(Default, prop = 0.6, strata = default)\nDefault.split2 <- initial_split(Default, prop = 0.6, strata = default)\nDefault.split3 <- initial_split(Default, prop = 0.7, strata = default)\nThen, using those splits, create separate training and test sets from the original data:\nDefault.train1 <- training(Default.split1)\nDefault.test1 <- testing(Default.split1)\n\nDefault.train2 <- training(Default.split2)\nDefault.test2 <- testing(Default.split2)\n\nDefault.train3 <- training(Default.split3)\nDefault.test3 <- testing(Default.split3)\n\n\n\nFit three multiple logistic regression models using only the training observations.\nglm.fit.1 = glm(default ~ income + balance, data = Default.train1, family = \"binomial\")\n\nglm.fit.2 = glm(default ~ income + balance, data = Default.train2, family = \"binomial\")\n\nglm.fit.3 = glm(default ~ income + balance, data = Default.train3, family = \"binomial\")\n\nsummary(glm.fit.1)\n\n\n\nBased on the three models, obtain a prediction of default status for each individual in the test set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than 0.5.\n# Predict the test set\nglm.probs.1 <- predict(glm.fit.1, Default.test1, type = \"response\")\n\n# Make predictions using threshold=0.5\nglm.preds.1 = if_else(glm.probs.1 > 0.5, \"Yes\", \"No\")\n\n# do the same for the other datasets\n\n\n\nBased on the three models, compute the test set error, which is the fraction of the observations in the test set that are misclassified.\nmean(glm.preds.1 != Default.test1$default)\nmean(glm.preds.2 != Default.test2$default)\nmean(glm.preds.3 != Default.test3$default)\n\n\n\n\nFor the Default dataset, We continue to consider the use of a logistic regression model to predict the probability of default using income and balance on the Default data set.\nIn particular, we will now compute estimates for the standard errors of the income and balance logistic regression coefficients in two different ways: 2.1. Using the bootstrap, 2.2. Using the standard formula for computing the standard errors in the glm() function. As following,\n\n\nUsing the summary() and glm() functions, determine the estimated standard errors for the coefficients associated with income and balance in a multiple logistic regression model that uses both predictors.\n# multiple logistic regression model \nlog_def <- glm(default ~ income + balance, data = Default, family = \"binomial\")\n\nsummary(log_def)\nsummary(log_def)$coefficients[, 2]  ## standard errors\n\n\n\nWrite a function,boot.fn(),that takes as input the Default data set as well as an index of the observations, and that outputs the coefficient estimates for income and balance in the multiple logistic regression model.\n# This is a modified version of the original `boot.fn` function\nboot.fn <- function(data, index = 1:nrow(data)) {\n      coef(glm(default ~ income + balance, data = data, subset = index, family = \"binomial\"))[-1]\n    }\nTest it:\n## all data --without intercept\nboot.fn(Default)\n\n\n\nUse the boot() function together with your boot.fn() function to estimate the standard errors of the logistic regression coefficients for income and balance. Then, Create a histogram of the bootstrap parameter estimates with ggplot2, and also set the bins=20, title as 1,000 Bootstrap Parameter Estimates - 'balance' & 'income.\nboot_results <- boot(data = Default, statistic = boot.fn, R = 1000)\nboot_results"
  },
  {
    "objectID": "weeks/week05/lecture.html",
    "href": "weeks/week05/lecture.html",
    "title": "ğŸ‘¨â€ğŸ« Week 05 - Lecture",
    "section": "",
    "text": "Tip\n\n\n\nFeel free to browse the slides before the lecture, but it is probably safer to wait until the time of the lecture to download/save them, as we might make small changes to slides before then."
  },
  {
    "objectID": "weeks/week05/lecture.html#part-i---non-linear-algorithms-decision-tree-45-50-min",
    "href": "weeks/week05/lecture.html#part-i---non-linear-algorithms-decision-tree-45-50-min",
    "title": "ğŸ‘¨â€ğŸ« Week 05 - Lecture",
    "section": "Part I - Non-linear algorithms (Decision Tree) (45-50 min)",
    "text": "Part I - Non-linear algorithms (Decision Tree) (45-50 min)\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides. For a PDF copy, check Moodle."
  },
  {
    "objectID": "weeks/week05/lecture.html#coffee-break-10-min",
    "href": "weeks/week05/lecture.html#coffee-break-10-min",
    "title": "ğŸ‘¨â€ğŸ« Week 05 - Lecture",
    "section": "â˜• Coffee Break (10 min)",
    "text": "â˜• Coffee Break (10 min)\nUse this time to chat, stretch, drink some coffee or just relax for a bit by yourself."
  },
  {
    "objectID": "weeks/week05/lecture.html#part-ii---non-linear-algorithms-support-vector-machines-45-50-min",
    "href": "weeks/week05/lecture.html#part-ii---non-linear-algorithms-support-vector-machines-45-50-min",
    "title": "ğŸ‘¨â€ğŸ« Week 05 - Lecture",
    "section": "Part II - Non-linear algorithms (Support Vector Machines) (45-50 min)",
    "text": "Part II - Non-linear algorithms (Support Vector Machines) (45-50 min)\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides. For a PDF copy, check Moodle."
  },
  {
    "objectID": "weeks/week06/checklist.html",
    "href": "weeks/week06/checklist.html",
    "title": "âœ… Week 06 - Checklist",
    "section": "",
    "text": "Here is a suggestion of how to program your week in relation to this course:"
  },
  {
    "objectID": "weeks/week06/checklist.html#useful-links-about-tidyverse",
    "href": "weeks/week06/checklist.html#useful-links-about-tidyverse",
    "title": "âœ… Week 06 - Checklist",
    "section": "Useful Links about tidyverse",
    "text": "Useful Links about tidyverse\ntidyverse is a set of R packages that have several functions and facilities for working with data. I find tidyverse more intuitive than base R, and thereâ€™s an entire book available for free online (R for Data Science) that contains a lot of helpful tutorials about tidyverse. Let me point to a few specific chapters:\n\nYou might need or want to transform data when working on your problem sets. Check out Chapter 5 of R for Data Science online book.\nData visualization is another helpful skill. You can learn a bit more about ggplot, the tidyverse way of making plots, in Chapter 3 of R for Data Science online book.\nWhat should you be looking for when working with data? Check Chapter 7 to learn the basics of exploratory data analysis.\nDo you get confused about R Markdown, the idea of â€œknittingâ€ a file? Then read Chapter 27.\n(More advanced) Do you want to learn how to reshape data or deal with more complex data manipulation? Then have a look at Chapter 12 - Tidy data and Chapter 13 - Iteration.\nAre you already familiar with tidyverse, but you constantly need to Google how to do things? Save these cheatsheets to your computer."
  },
  {
    "objectID": "weeks/week06/drop_in_jon.html",
    "href": "weeks/week06/drop_in_jon.html",
    "title": "DS202 2022MT W06 Drop In session",
    "section": "",
    "text": "OBJECTIVE: Support with R programming skills"
  },
  {
    "objectID": "weeks/week06/drop_in_jon.html#comparing-vectors",
    "href": "weeks/week06/drop_in_jon.html#comparing-vectors",
    "title": "DS202 2022MT W06 Drop In session",
    "section": "Comparing vectors",
    "text": "Comparing vectors\n\nSay I have two string vectors:\n\nThey have the same length\nElements in the same index represents the same â€œdayâ€\n\n\nvector1 <- c(\"blue\", \"red\", \"green\", \"green\", \"blue\", \"red\")\nvector2 <- c(\"red\",  \"red\", \"blue\", \"blue\", \"blue\", \"green\")\n\n\n\nHow many blues do they have in common?\n\nvector1 == vector2\n\n[1] FALSE  TRUE FALSE FALSE  TRUE FALSE\n\n\n\nvector1 == \"blue\"\n\n[1]  TRUE FALSE FALSE FALSE  TRUE FALSE\n\n\nNow, put it all together:\n\noutput <- vector1 == \"blue\" & vector2 == \"blue\"\noutput\n\n[1] FALSE FALSE FALSE FALSE  TRUE FALSE\n\n\nKeyword: Logical Operators\n& stands for an AND operation\n| stands for an OR operation\n! stands for a NOT operation\nRead more about it here\n\nsum(output) # Count all occurences of TRUE in the vector\n\n[1] 1\n\n\nIf I wanted to do it all in a single line:\n\nsum(vector1 == \"blue\" & vector2 == \"blue\")\n\n[1] 1"
  },
  {
    "objectID": "weeks/week06/drop_in_jon.html#doing-the-same-with-dataframes",
    "href": "weeks/week06/drop_in_jon.html#doing-the-same-with-dataframes",
    "title": "DS202 2022MT W06 Drop In session",
    "section": "Doing the same with dataframes",
    "text": "Doing the same with dataframes\nIf I have the same info but now represented as a data frame, how would I count the number of blues in common?\n\n# A random dataframe\ndf <- data.frame(colourA=c(\"blue\", \"red\", \"green\", \"green\", \"blue\", \"red\"),\n                 colourB=c(\"red\",  \"red\", \"blue\", \"blue\", \"blue\", \"green\"))\ndf\n\n  colourA colourB\n1    blue     red\n2     red     red\n3   green    blue\n4   green    blue\n5    blue    blue\n6     red   green\n\n\nYou can access each column by using the $:\n\ndf$colourA\n\n[1] \"blue\"  \"red\"   \"green\" \"green\" \"blue\"  \"red\"  \n\n\n\nsum(df$colourA == \"blue\" & df$colourB == \"blue\")\n\n[1] 1"
  },
  {
    "objectID": "weeks/week06/drop_in_jon.html#now-lets-imagine-we-have-two-dataframes",
    "href": "weeks/week06/drop_in_jon.html#now-lets-imagine-we-have-two-dataframes",
    "title": "DS202 2022MT W06 Drop In session",
    "section": "Now letâ€™s imagine we have two dataframes",
    "text": "Now letâ€™s imagine we have two dataframes\nÂ \n\n# A random dataframe\ndf1 <- data.frame(observation=c(1, 2, 3, 4, 5, 6),\n                  colour=c(\"blue\", \"red\", \"green\", \"green\", \"blue\", \"red\"))\ndf1\n\n  observation colour\n1           1   blue\n2           2    red\n3           3  green\n4           4  green\n5           5   blue\n6           6    red\n\n\n\n# A random dataframe\ndf2 <- data.frame(observation=c(1, 2, 3, 4, 5, 5,  6),\n                  colour=c(\"red\",  \"red\", \"blue\", \"blue\", \"red\", \"blue\", \"green\"))\ndf2\n\n  observation colour\n1           1    red\n2           2    red\n3           3   blue\n4           4   blue\n5           5    red\n6           5   blue\n7           6  green\n\n\nFirst, letâ€™s calculate whether there was at least one â€œblueâ€ in each observation.\n\nlibrary(tidyverse)\n\nâ”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 1.3.2 â”€â”€\nâœ” ggplot2 3.4.0      âœ” purrr   0.3.5 \nâœ” tibble  3.1.8      âœ” dplyr   1.0.10\nâœ” tidyr   1.2.1      âœ” stringr 1.4.1 \nâœ” readr   2.1.3      âœ” forcats 0.5.2 \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\n\nselect(df2, observation)\n\n  observation\n1           1\n2           2\n3           3\n4           4\n5           5\n6           5\n7           6\n\n\nThe pipe\nI could do exactly the same thing using the pipe %>%\n\ndf2 %>% select(observation) # no need to pass `df2` to function select\n\n  observation\n1           1\n2           2\n3           3\n4           4\n5           5\n6           5\n7           6\n\n\n\ntail(select(df2,observation),n=1)\n\n  observation\n7           6\n\ntail(\n  select(\n      df2, \n      observation), \n  n=1)\n\n  observation\n7           6\n\ndf2 %>% select(observation) %>% tail(n=1)\n\n  observation\n7           6"
  },
  {
    "objectID": "weeks/week06/drop_in_jon.html#summarise-duplicated-groupings",
    "href": "weeks/week06/drop_in_jon.html#summarise-duplicated-groupings",
    "title": "DS202 2022MT W06 Drop In session",
    "section": "Summarise duplicated groupings",
    "text": "Summarise duplicated groupings\nCheck the idea of group_by (a tidyverse feature)\nsummarise and n() only works with groupings (group_by).\n\n# How many colours are there, per observation?\ndf2 %>% group_by(observation) %>% summarise(count=n())\n\n# A tibble: 6 Ã— 2\n  observation count\n        <dbl> <int>\n1           1     1\n2           2     1\n3           3     1\n4           4     1\n5           5     2\n6           6     1\n\n\n\nHow many observations of df2 have at least one colour â€œblueâ€?\n\n# How many colours are there, per observation?\ndf2 %>% group_by(observation) %>% summarise(has_blue=any(colour == \"blue\"))\n\n# A tibble: 6 Ã— 2\n  observation has_blue\n        <dbl> <lgl>   \n1           1 FALSE   \n2           2 FALSE   \n3           3 TRUE    \n4           4 TRUE    \n5           5 TRUE    \n6           6 FALSE   \n\n\n\n\nHow many observations of df1 have at least one colour â€œblueâ€?\n\n# How many colours are there, per observation?\ndf1 %>% group_by(observation) %>% summarise(has_blue=any(colour == \"blue\"))\n\n# A tibble: 6 Ã— 2\n  observation has_blue\n        <dbl> <lgl>   \n1           1 TRUE    \n2           2 FALSE   \n3           3 FALSE   \n4           4 FALSE   \n5           5 TRUE    \n6           6 FALSE   \n\n\n\n\nHow do I compare these two new dataframes?\n\n# Store those results into variables\ndf1_summary <- \n    df1 %>% group_by(observation) %>% summarise(has_blue=any(colour == \"blue\"))\n\ndf2_summary <- \n    df2 %>% group_by(observation) %>% summarise(has_blue=any(colour == \"blue\"))\n\nBoth dataframes now have the same number of rows, representing the same â€œobservationsâ€ and both have a column called has_blue. I can compare both like this:\n\nsum(df1_summary$has_blue & df2_summary$has_blue)\n\n[1] 1\n\n\nKeyword: Logical Operators\n& stands for an AND operation\n| stands for an OR operation\n! stands for a NOT operation\nRead more about it here\n\n\nAnother way to compare two dataframes\nUseful if the two dataframes are not aligned\n\noutput <- merge(df1, df2, by=c(\"observation\", \"colour\"))\noutput\n\n  observation colour\n1           2    red\n2           5   blue\n\n\n\nsum(output$colour == \"blue\")\n\n[1] 1"
  },
  {
    "objectID": "weeks/week06/drop_in_jon.html#remember-to-save-your-mutates",
    "href": "weeks/week06/drop_in_jon.html#remember-to-save-your-mutates",
    "title": "DS202 2022MT W06 Drop In session",
    "section": "Remember to save your mutates!",
    "text": "Remember to save your mutates!\n\n# mutate adds a new column to a dataframe\ndf2 %>% mutate(is_blue=colour == \"blue\")\n\n  observation colour is_blue\n1           1    red   FALSE\n2           2    red   FALSE\n3           3   blue    TRUE\n4           4   blue    TRUE\n5           5    red   FALSE\n6           5   blue    TRUE\n7           6  green   FALSE\n\n\nNote: mutate will add a new column but it will NOT update the dataframe. If you want to re-use the new column, you have to save the new dataframe:\n\n# df2 does not have a `is_blue` column\ndf2\n\n  observation colour\n1           1    red\n2           2    red\n3           3   blue\n4           4   blue\n5           5    red\n6           5   blue\n7           6  green\n\n\nIf I want to updated it to the SAME dataframe, I have to reassign it (using <-)\n\ndf2 <- df2 %>% mutate(is_blue=colour==\"blue\")\ndf2\n\n  observation colour is_blue\n1           1    red   FALSE\n2           2    red   FALSE\n3           3   blue    TRUE\n4           4   blue    TRUE\n5           5    red   FALSE\n6           5   blue    TRUE\n7           6  green   FALSE"
  },
  {
    "objectID": "weeks/week06/drop_in_jon.html#general-comparison-of-dataframes",
    "href": "weeks/week06/drop_in_jon.html#general-comparison-of-dataframes",
    "title": "DS202 2022MT W06 Drop In session",
    "section": "General comparison of dataframes",
    "text": "General comparison of dataframes\nBy manual inspection:\n\ntable(df1$colour) \n\n\n blue green   red \n    2     2     2 \n\n\n\ntable(df2$colour)\n\n\n blue green   red \n    3     1     3"
  },
  {
    "objectID": "weeks/week06/drop_in_jon.html#ggplot2",
    "href": "weeks/week06/drop_in_jon.html#ggplot2",
    "title": "DS202 2022MT W06 Drop In session",
    "section": "ggplot2",
    "text": "ggplot2\n\n- Colour histograms according to a category\nI will use iris as an example:\n\nlibrary(datasets)\ndata(iris)\niris\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1            5.1         3.5          1.4         0.2     setosa\n2            4.9         3.0          1.4         0.2     setosa\n3            4.7         3.2          1.3         0.2     setosa\n4            4.6         3.1          1.5         0.2     setosa\n5            5.0         3.6          1.4         0.2     setosa\n6            5.4         3.9          1.7         0.4     setosa\n7            4.6         3.4          1.4         0.3     setosa\n8            5.0         3.4          1.5         0.2     setosa\n9            4.4         2.9          1.4         0.2     setosa\n10           4.9         3.1          1.5         0.1     setosa\n11           5.4         3.7          1.5         0.2     setosa\n12           4.8         3.4          1.6         0.2     setosa\n13           4.8         3.0          1.4         0.1     setosa\n14           4.3         3.0          1.1         0.1     setosa\n15           5.8         4.0          1.2         0.2     setosa\n16           5.7         4.4          1.5         0.4     setosa\n17           5.4         3.9          1.3         0.4     setosa\n18           5.1         3.5          1.4         0.3     setosa\n19           5.7         3.8          1.7         0.3     setosa\n20           5.1         3.8          1.5         0.3     setosa\n21           5.4         3.4          1.7         0.2     setosa\n22           5.1         3.7          1.5         0.4     setosa\n23           4.6         3.6          1.0         0.2     setosa\n24           5.1         3.3          1.7         0.5     setosa\n25           4.8         3.4          1.9         0.2     setosa\n26           5.0         3.0          1.6         0.2     setosa\n27           5.0         3.4          1.6         0.4     setosa\n28           5.2         3.5          1.5         0.2     setosa\n29           5.2         3.4          1.4         0.2     setosa\n30           4.7         3.2          1.6         0.2     setosa\n31           4.8         3.1          1.6         0.2     setosa\n32           5.4         3.4          1.5         0.4     setosa\n33           5.2         4.1          1.5         0.1     setosa\n34           5.5         4.2          1.4         0.2     setosa\n35           4.9         3.1          1.5         0.2     setosa\n36           5.0         3.2          1.2         0.2     setosa\n37           5.5         3.5          1.3         0.2     setosa\n38           4.9         3.6          1.4         0.1     setosa\n39           4.4         3.0          1.3         0.2     setosa\n40           5.1         3.4          1.5         0.2     setosa\n41           5.0         3.5          1.3         0.3     setosa\n42           4.5         2.3          1.3         0.3     setosa\n43           4.4         3.2          1.3         0.2     setosa\n44           5.0         3.5          1.6         0.6     setosa\n45           5.1         3.8          1.9         0.4     setosa\n46           4.8         3.0          1.4         0.3     setosa\n47           5.1         3.8          1.6         0.2     setosa\n48           4.6         3.2          1.4         0.2     setosa\n49           5.3         3.7          1.5         0.2     setosa\n50           5.0         3.3          1.4         0.2     setosa\n51           7.0         3.2          4.7         1.4 versicolor\n52           6.4         3.2          4.5         1.5 versicolor\n53           6.9         3.1          4.9         1.5 versicolor\n54           5.5         2.3          4.0         1.3 versicolor\n55           6.5         2.8          4.6         1.5 versicolor\n56           5.7         2.8          4.5         1.3 versicolor\n57           6.3         3.3          4.7         1.6 versicolor\n58           4.9         2.4          3.3         1.0 versicolor\n59           6.6         2.9          4.6         1.3 versicolor\n60           5.2         2.7          3.9         1.4 versicolor\n61           5.0         2.0          3.5         1.0 versicolor\n62           5.9         3.0          4.2         1.5 versicolor\n63           6.0         2.2          4.0         1.0 versicolor\n64           6.1         2.9          4.7         1.4 versicolor\n65           5.6         2.9          3.6         1.3 versicolor\n66           6.7         3.1          4.4         1.4 versicolor\n67           5.6         3.0          4.5         1.5 versicolor\n68           5.8         2.7          4.1         1.0 versicolor\n69           6.2         2.2          4.5         1.5 versicolor\n70           5.6         2.5          3.9         1.1 versicolor\n71           5.9         3.2          4.8         1.8 versicolor\n72           6.1         2.8          4.0         1.3 versicolor\n73           6.3         2.5          4.9         1.5 versicolor\n74           6.1         2.8          4.7         1.2 versicolor\n75           6.4         2.9          4.3         1.3 versicolor\n76           6.6         3.0          4.4         1.4 versicolor\n77           6.8         2.8          4.8         1.4 versicolor\n78           6.7         3.0          5.0         1.7 versicolor\n79           6.0         2.9          4.5         1.5 versicolor\n80           5.7         2.6          3.5         1.0 versicolor\n81           5.5         2.4          3.8         1.1 versicolor\n82           5.5         2.4          3.7         1.0 versicolor\n83           5.8         2.7          3.9         1.2 versicolor\n84           6.0         2.7          5.1         1.6 versicolor\n85           5.4         3.0          4.5         1.5 versicolor\n86           6.0         3.4          4.5         1.6 versicolor\n87           6.7         3.1          4.7         1.5 versicolor\n88           6.3         2.3          4.4         1.3 versicolor\n89           5.6         3.0          4.1         1.3 versicolor\n90           5.5         2.5          4.0         1.3 versicolor\n91           5.5         2.6          4.4         1.2 versicolor\n92           6.1         3.0          4.6         1.4 versicolor\n93           5.8         2.6          4.0         1.2 versicolor\n94           5.0         2.3          3.3         1.0 versicolor\n95           5.6         2.7          4.2         1.3 versicolor\n96           5.7         3.0          4.2         1.2 versicolor\n97           5.7         2.9          4.2         1.3 versicolor\n98           6.2         2.9          4.3         1.3 versicolor\n99           5.1         2.5          3.0         1.1 versicolor\n100          5.7         2.8          4.1         1.3 versicolor\n101          6.3         3.3          6.0         2.5  virginica\n102          5.8         2.7          5.1         1.9  virginica\n103          7.1         3.0          5.9         2.1  virginica\n104          6.3         2.9          5.6         1.8  virginica\n105          6.5         3.0          5.8         2.2  virginica\n106          7.6         3.0          6.6         2.1  virginica\n107          4.9         2.5          4.5         1.7  virginica\n108          7.3         2.9          6.3         1.8  virginica\n109          6.7         2.5          5.8         1.8  virginica\n110          7.2         3.6          6.1         2.5  virginica\n111          6.5         3.2          5.1         2.0  virginica\n112          6.4         2.7          5.3         1.9  virginica\n113          6.8         3.0          5.5         2.1  virginica\n114          5.7         2.5          5.0         2.0  virginica\n115          5.8         2.8          5.1         2.4  virginica\n116          6.4         3.2          5.3         2.3  virginica\n117          6.5         3.0          5.5         1.8  virginica\n118          7.7         3.8          6.7         2.2  virginica\n119          7.7         2.6          6.9         2.3  virginica\n120          6.0         2.2          5.0         1.5  virginica\n121          6.9         3.2          5.7         2.3  virginica\n122          5.6         2.8          4.9         2.0  virginica\n123          7.7         2.8          6.7         2.0  virginica\n124          6.3         2.7          4.9         1.8  virginica\n125          6.7         3.3          5.7         2.1  virginica\n126          7.2         3.2          6.0         1.8  virginica\n127          6.2         2.8          4.8         1.8  virginica\n128          6.1         3.0          4.9         1.8  virginica\n129          6.4         2.8          5.6         2.1  virginica\n130          7.2         3.0          5.8         1.6  virginica\n131          7.4         2.8          6.1         1.9  virginica\n132          7.9         3.8          6.4         2.0  virginica\n133          6.4         2.8          5.6         2.2  virginica\n134          6.3         2.8          5.1         1.5  virginica\n135          6.1         2.6          5.6         1.4  virginica\n136          7.7         3.0          6.1         2.3  virginica\n137          6.3         3.4          5.6         2.4  virginica\n138          6.4         3.1          5.5         1.8  virginica\n139          6.0         3.0          4.8         1.8  virginica\n140          6.9         3.1          5.4         2.1  virginica\n141          6.7         3.1          5.6         2.4  virginica\n142          6.9         3.1          5.1         2.3  virginica\n143          5.8         2.7          5.1         1.9  virginica\n144          6.8         3.2          5.9         2.3  virginica\n145          6.7         3.3          5.7         2.5  virginica\n146          6.7         3.0          5.2         2.3  virginica\n147          6.3         2.5          5.0         1.9  virginica\n148          6.5         3.0          5.2         2.0  virginica\n149          6.2         3.4          5.4         2.3  virginica\n150          5.9         3.0          5.1         1.8  virginica\n\n\nGenerate a histogram of Petal.Length:\n\ng <- (\n  ggplot(iris, aes(x=Petal.Length))\n  \n  + geom_histogram()\n  \n  # Customize\n  + theme_minimal()\n)\n\ng\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nHow do I colour the histogram according to the Species?\n\ng <- (\n  ggplot(iris, aes(x=Petal.Length, fill=Species))\n  \n  + geom_histogram()\n  \n  # Customize\n  + theme_minimal()\n)\n\ng\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nEach geom_ â€œlistensâ€ to a different set of aesthetics. (Check Chapter 3 of R for Data Science for more info)\nFor example, geom_point does not understand the fill :\n\ng <- (\n  ggplot(iris, aes(x=Petal.Length, y=Petal.Width, fill=Species))\n  \n  + geom_point()\n  \n  # Customize\n  + theme_minimal()\n)\n\ng\n\n\n\n\n\ng <- (\n  ggplot(iris, aes(x=Petal.Length, y=Petal.Width, colour=Species))\n  \n  + geom_point()\n  \n  # Customize\n  + theme_minimal()\n)\n\ng\n\n\n\n\n\n\nCustomizing the colours\n\nManually\n\nmy_favourite_colours = c(\"#5bc0de\", \"#d9534f\",  \"#ffbf00\")\n\nPlaces to find colours: https://www.color-hex.com/color-palettes/popular.php\n\ng <- (\n  ggplot(iris, aes(x=Petal.Length, y=Petal.Width, colour=Species))\n  \n  + geom_point()\n\n  \n  # Customize\n  + theme_minimal()\n  \n  + scale_colour_manual(values=my_favourite_colours) # this is where you customize it\n)\n\ng\n\n\n\n\n\n\nBuilt-in palettes\nYou can use built-in palettes, you just need to know their names/numbers.\n\nCheck the documentation https://ggplot2.tidyverse.org/reference/scale_brewer.html -> for the different settings\nTo understand which colour palettes are available, check: https://colorbrewer2.org/\n\n\ng <- (\n  ggplot(iris, aes(x=Petal.Length, y=Petal.Width, colour=Species))\n  \n  + geom_point()\n\n  \n  # Customize\n  + theme_minimal()\n  \n  + scale_colour_brewer(type=\"qual\", palette=2) # Bult-in palette of colours for the aesthetic `colour`, as given by the Colour Brewer\n)\n\ng\n\n\n\n\n\ndf1$source <- \"df1\"\ndf2$source <- \"df2\"\n\n\nggplot(bind_rows(df1, df2), aes(x=colour)) +\n  geom_bar() + facet_wrap(~ source)"
  },
  {
    "objectID": "weeks/week06/drop_in_jon.html#faceting",
    "href": "weeks/week06/drop_in_jon.html#faceting",
    "title": "DS202 2022MT W06 Drop In session",
    "section": "Faceting",
    "text": "Faceting\nUseful when you want to plot two charts in the same image.\nObservation: you might need to combine (append) the two dataframes first. Use the tidyverse function bind_rows (same as rbind)\n\ng <- (\n  ggplot(iris, aes(x=Petal.Length, fill=Species))\n  \n  + geom_histogram()\n\n  \n  # Customize\n  + theme_bw()\n  \n  + scale_colour_manual(values=my_favourite_colours) \n  \n  + facet_grid(Species ~ .) #This is what does it\n)\n\ng\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "weeks/week06/drop_in_jon.html#about-tidyverse",
    "href": "weeks/week06/drop_in_jon.html#about-tidyverse",
    "title": "DS202 2022MT W06 Drop In session",
    "section": "About tidyverse",
    "text": "About tidyverse\ntidyverse is a set of R packages that have several functions and facilities for working with data. I find tidyverse more intuitive than base R, and thereâ€™s an entire book available for free online (R for Data Science) that contains a lot of helpful tutorials about tidyverse. Let me point to a few specific chapters:\n\nYou might need or want to transform data when working on your problem sets. Check out Chapter 5 of R for Data Science online book.\nData visualization is another helpful skill. You can learn a bit more about ggplot â€” the tidyverse way of making plots â€” in Chapter 3 of R for Data Science online book.\nWhat should you be looking for when working with data? Check Chapter 7 to learn the basics of exploratory data analysis.\nDo you get confused about R Markdown, the idea of â€œknittingâ€ a file? Then read Chapter 27.\n(More advanced) Do you want to learn how to reshape data or deal with more complex data manipulation? Then have a look at Chapter 12 - Tidy data and Chapter 13 - Iteration.\nAre you already familiar with tidyverse, but you constantly need to Google how to do things? Save these cheatsheets to your computer."
  },
  {
    "objectID": "weeks/week07/checklist.html",
    "href": "weeks/week07/checklist.html",
    "title": "âœ… Week 07 - Checklist",
    "section": "",
    "text": "Sadly, I didnâ€™t have the time this week to suggest a checklist for you!"
  },
  {
    "objectID": "weeks/week07/lab.html",
    "href": "weeks/week07/lab.html",
    "title": "ğŸ’» Week 07 - Lab Roadmap (90 min)",
    "section": "",
    "text": "ğŸ“¥ Download the RMarkdown version of this roadmap from Moodle.\nThis lab session draws on ğŸ—“ï¸ Week 05 lecture content and on feedback given by the course representatives about the main struggles you are facing with R (tidyverse).\nIf you are already very familiar with tidyverse, you can skip to Step 2.\nR packages you will need:\nYou might have already installed some of these packages. Some were used in ğŸ—“ï¸ Week 05 slides, others in previous labs. If you see an error like â€œpackage not foundâ€, then install the package using install.packages(\"<package name>\")."
  },
  {
    "objectID": "weeks/week07/lab.html#step-1.1-the-tibble-5-min",
    "href": "weeks/week07/lab.html#step-1.1-the-tibble-5-min",
    "title": "ğŸ’» Week 07 - Lab Roadmap (90 min)",
    "section": "Step 1.1: The tibble (5 min)",
    "text": "Step 1.1: The tibble (5 min)\nSo far, we have been working with data frames. We will introduce you to a different kind of data frame called a tibble. Tibbles are like data frames, only they have a more descriptive print function, and you can perform more advanced tasks like looping over lists (without needing to specify a for loop).\nLetâ€™s start by converting our now familiar Boston data to a tibble using as_tibble:\n\nboston <- as_tibble(Boston)\nboston\n\nInstead of printing a lot of rows, we only get to see the first ten rows.\nWe can also see the dimensions of the data 506 x 13 and the class of each variable.\nSo with one command, we can get a lot more useful information on our data without the need for multiple commands."
  },
  {
    "objectID": "weeks/week07/lab.html#step-1.2-basic-dplyr-verbs-10-minutes",
    "href": "weeks/week07/lab.html#step-1.2-basic-dplyr-verbs-10-minutes",
    "title": "ğŸ’» Week 07 - Lab Roadmap (90 min)",
    "section": "Step 1.2: Basic dplyr verbs (10 minutes)",
    "text": "Step 1.2: Basic dplyr verbs (10 minutes)\nIn previous labs, we have been using base R to subset data based on the rows and columns, to create new variables, and to create summary statistics. However, there is a more verbal way of performing these tasks using the tidyverse. We will introduce you to several key verbs, namely filter, select, mutate and summarise.\nAuto cylinders\nSuppose we wanted to select only the rows of the Auto data for cars with 4 cylinders. We can achieve this using the following base R command:\n\nAuto[Auto$cylinders == 4, ]\n\nNow here is the tidyverse solution using filter.\n\nfilter(Auto, cylinders == 4)\n\nBoston columns\nNext, letâ€™s only include lstat and medv from Boston:\n\nBoston[, c('medv','lstat')]\n\nNow here is the tidyverse solution using select:\n\nselect(Boston, medv, lstat)\n\nCarseats median sales\nNow that we can subset variables, letâ€™s create some new ones. Letâ€™s create a dummy variable SalesGTMedian for each car seat in Carseats:\n\nCarseats$SalesGTMedian <- if_else(Carseats$Sales > median(Carseats$Sales), TRUE, FALSE)\n\nNow here is the tidyverse solution using mutate:\n\nmutate(Carseats, SalesGTMedian = if_else(Sales > median(Sales), TRUE, FALSE))\n\nMissing data in the Hitters dataset\nFinally, suppose we wanted to find the average Salary in Hitters (a basketball data set). We specify na.rm = TRUE to get R to ignore all the missing values.\n\nmean(Hitters$Salary, na.rm = TRUE)\n\nNow here is the tidyverse solution using summarise.\n\nsummarise(Hitters, mean_salary = mean(Salary, na.rm = TRUE))\n\nCategorical variables in Default dataset\nSome of our variables will be categories, so letâ€™s find out the distribution of defaults in Default:\n\ntable(Default$default)\n\nNow here is the tidyverse solution using count:\n\ncount(Default, default)\n\nğŸ’¡ REFLECTION TIME\nLetâ€™s pause for a minute to see the advantages of these commands:\n\nthe commands themselves give a better indication of what it is we are trying to do. This is highly advantageous when it comes to communicating our code with others.\nwhen working with variables in data frames, we do not need to use $. Instead, we can just reference the variable on its own, provided we pass the command the data frame.\nevery time we use these verbs, a new data frame is created - meaning we can use the output to create ggplots!"
  },
  {
    "objectID": "weeks/week07/lab.html#step-1.3-the-pipe-10-minutes",
    "href": "weeks/week07/lab.html#step-1.3-the-pipe-10-minutes",
    "title": "ğŸ’» Week 07 - Lab Roadmap (90 min)",
    "section": "Step 1.3 The pipe (10 minutes)",
    "text": "Step 1.3 The pipe (10 minutes)\nYou may have seen %>% in some of our code. This is known as the pipe operator, and it enables us to chain together multiple verbs into one fluid sequence of steps. To get this quickly you can use ctrl+shift+m (for Windows users) or command+shift+m (for Mac users).\nSuppose we want to find out what proportion of American cars had mpg above the global average. We can find this out by using the following sequence of commands:\n\nAuto %>% \n  as_tibble() %>% \n  select(mpg, origin) %>% \n  mutate(mpg_gt_gavg = if_else(mpg > mean(mpg), TRUE, FALSE)) %>% \n  filter(origin == 1) %>% \n  summarise(prop_mpg_gt_gavg = mean(mpg_gt_gavg))\n\n# A tibble: 1 Ã— 1\n  prop_mpg_gt_gavg\n             <dbl>\n1            0.269\n\n\nLetâ€™s walk through what we just did:\n\nWe converted Auto to a tibble.\nWe selected only the variables of interest mpg and origin. (If you are working with larger data sets, removing superfluous columns can be an advantage.)\nWe create a new variable mpg_gt_gaverage which finds out whether an automobile has an MPG greater than the global average.\nWe filter all rows to only include American cars.\nWe summarised the data calculate, which helped us find that only 27 percent of American-made cars had MPGs greater than the global average.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nThink of the pipe as â€œtake this data and do something with itâ€\n\n\n\n\n\n\n\n\n\nHow would I do this in base R?\n\n\n\n\n\nLetâ€™s look at how to recreate this using base R.\n\nAuto_cleaned <- Auto[,c('mpg','origin')]\n\nAuto_cleaned$mpg_gt_gavg <- if_else(Auto_cleaned$mpg > mean(Auto_cleaned$mpg), TRUE, FALSE) \n\nAuto_cleaned <- Auto_cleaned[Auto_cleaned$origin == 1, ]\n\ndata.frame(prop_mpg_gt_gavg = mean(Auto_cleaned$mpg_gt_gavg))\n\n  prop_mpg_gt_gavg\n1        0.2693878\n\n\nWhile this code is technically correct, notice a few things. We need to keep updating the same object to save our results. Our code is disjointed and difficult to understand. The final product is also less satisfactory: we needed to convert it from a vector to a data frame, which displays no information on the class of prop_mpg_gt_gavg. That is, perhaps, why R can appear so confusing for first-time learners."
  },
  {
    "objectID": "weeks/week07/lab.html#step-1.4-practice-15-min",
    "href": "weeks/week07/lab.html#step-1.4-practice-15-min",
    "title": "ğŸ’» Week 07 - Lab Roadmap (90 min)",
    "section": "Step 1.4 PRACTICE! (15 min)",
    "text": "Step 1.4 PRACTICE! (15 min)\nNow letâ€™s use some of these skills to classify Chinstrap penguins. This is data from the package palmerpenguins 1.\n\n# Importing this library will make `penguins` data available\nlibrary(palmerpenguins) \n\nhead(penguins)\n\n# A tibble: 6 Ã— 8\n  species island    bill_length_mm bill_depth_mm flipper_lâ€¦Â¹ body_â€¦Â² sex    year\n  <fct>   <fct>              <dbl>         <dbl>       <int>   <int> <fct> <int>\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n2 Adelie  Torgersen           39.5          17.4         186    3800 femaâ€¦  2007\n3 Adelie  Torgersen           40.3          18           195    3250 femaâ€¦  2007\n4 Adelie  Torgersen           NA            NA            NA      NA <NA>   2007\n5 Adelie  Torgersen           36.7          19.3         193    3450 femaâ€¦  2007\n6 Adelie  Torgersen           39.3          20.6         190    3650 male   2007\n# â€¦ with abbreviated variable names Â¹â€‹flipper_length_mm, Â²â€‹body_mass_g\n\n\nğŸ¯ ACTION POINTS\nUse the verbs you learned above to modify the original data set and create a new one, call it penguins_cleaned, according to the following steps:\n\nRemove any observations with missing data. ğŸ’¡You can pipe na.omit() into you sequence of commands to achieve this.\nNext, create a binary variable, call it chinstrap that stores TRUE if the penguin is a Chinstrap, FALSE otherwise.\nNow, filter the dataset and keep only the following variables:\n\nis_chinstrap (our outcome of interest)\nbill_length_mm\nbill_depth_mm\n\n\nCan you do it without looking at the solution?\n\n\n\n\n\n\nClick here to view the solution\n\n\n\n\n\n\npenguins_cleaned <-\n  penguins %>% \n  na.omit() %>% \n  mutate(is_chinstrap = if_else(species == 'Chinstrap', TRUE, FALSE)) %>% \n  select(is_chinstrap, bill_length_mm, bill_depth_mm)\n\n\n\n\nIf you did it right, you should be able to run the code below and get same plot as shown here in the page:\n\npenguins_cleaned %>% \n  ggplot(aes(bill_length_mm, bill_depth_mm, colour=is_chinstrap)) +\n  geom_point(size=2.5, stroke=1.4, alpha=0.8, shape=21) +\n\n  # (Optional) customizing the plot\n  theme_bw()\n\n\n\n\nWe can see that Chinstrap penguins tend to have above average bill length and depth whereas the other two species of penguins tend to either have shallow yet long or deep yet short bills.\n\n\n\n\n\n\nâ€œWhat if I struggle to understand the pipe and theseâ€verbsâ€ above?â€œ\n\n\n\n\n\nThe R for Data Science book is a great resource to learn more about the tidyverse. For more guided tips related to our course, check section Useful Links about tidyverse of the âœ… Week 06 - Checklist."
  },
  {
    "objectID": "weeks/week07/lab.html#step-2.1-train-and-visualise-a-decision-tree-10-mins",
    "href": "weeks/week07/lab.html#step-2.1-train-and-visualise-a-decision-tree-10-mins",
    "title": "ğŸ’» Week 07 - Lab Roadmap (90 min)",
    "section": "Step 2.1: Train and visualise a Decision Tree (10 mins)",
    "text": "Step 2.1: Train and visualise a Decision Tree (10 mins)\nSince we know the data we are modelling has a nonlinear relationship, letâ€™s train a Decision Tree to classify Chinstrap penguins. R does not come with Decision Trees installed, so we need to import it from a library. Here we will use the function rpart from the rpart package and rpart.plot from the rpart.plot package.\nHere are a few things to know about the rpart function:\n\nThe rpart command is largely similar to other commands such as lm and glm in that the first parameter is a formula and the second is the data set.\nWe have to add method = class to tell the algorithm that we are performing a classification task.\n\n\ntree.model <- rpart(is_chinstrap ~ ., data = penguins_cleaned, method = 'class')\n\nrpart.plot(tree.model)\n\n\n\n\nğŸ¤ WORKING TOGETHER In pairs, discuss what you see in the plot.\n\nWhat do each node represents?\nWhat are the numbers inside the nodes?\nWhat do the percentages represent?\n\nThe video below explains how to read the numbers inside the nodes of the decision tree\n\n\n\n\nAlternative visualisation with parttree package\nAlternatively, you could visualise a decision tree as a partition tree plot. For this, you will need to have the parttree package installed (check instructions at the top of the page).\n\nggplot() +\n  geom_point(data = penguins_cleaned, \n             aes(bill_length_mm, bill_depth_mm, colour = is_chinstrap)) +\n  geom_parttree(data = tree.model, aes(fill = factor(is_chinstrap)), alpha = 0.25) +\n  theme_minimal() +\n  theme(panel.grid = element_blank(),\n        legend.position = 'bottom') +\n  scale_colour_lancet() +\n  scale_fill_lancet() +\n  labs(x = 'Bill length (mm)', y = 'Bill depth (mm)',\n       colour = 'Is actually chinstrap?', \n       fill = 'Is predicted chinstrap?')\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nâ„¹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\nDonâ€™t understand what the code above does? You might want to read Chapter 3 of R for Data Science to review ggplot."
  },
  {
    "objectID": "weeks/week07/lab.html#step-2.2-goodness-of-fit-of-the-decision-tree-10-mins",
    "href": "weeks/week07/lab.html#step-2.2-goodness-of-fit-of-the-decision-tree-10-mins",
    "title": "ğŸ’» Week 07 - Lab Roadmap (90 min)",
    "section": "Step 2.2: Goodness-of-Fit of the Decision Tree (10 mins)",
    "text": "Step 2.2: Goodness-of-Fit of the Decision Tree (10 mins)\nLetâ€™s investigate how well our model fits the data. Letâ€™s reuse the model we trained (tree.model) and predict the same samples we used to train it. To avoid modifying our original dataframe, letâ€™s save the output of the prediction in an auxiliary df (plot_df):\n\nplot_df <- \n    penguins_cleaned %>% \n    mutate(class_pred = predict(tree.model, newdata = ., type=\"class\"),\n           correct    = class_pred == is_chinstrap)\nplot_df\n\nğŸ¤ WORKING TOGETHER In pairs, discuss the following:\n\nExplain what you see in the output of the chunk of code above.\nWhat does the code above do?\n\nSimple confusion matrix:\n\nconfusion_matrix <- \n    table(expected=plot_df$is_chinstrap, class_pred=plot_df$class_pred)\nprint(confusion_matrix)\n\nNicer looking confusion matrix:\n\nlibrary(cvms)\n\nplot_confusion_matrix(as_tibble(confusion_matrix), \n                      target_col = \"expected\", \n                      prediction_col = \"class_pred\",\n                      \n                      # Customizing the plot\n                      add_normalized = TRUE,\n                      add_col_percentages = FALSE,\n                      add_row_percentages = FALSE,\n                      counts_col = \"n\",\n                      )"
  },
  {
    "objectID": "weeks/week07/lab.html#step-2.3-control-the-parameters-15-min",
    "href": "weeks/week07/lab.html#step-2.3-control-the-parameters-15-min",
    "title": "ğŸ’» Week 07 - Lab Roadmap (90 min)",
    "section": "Step 2.3 Control the parameters! (15 min)",
    "text": "Step 2.3 Control the parameters! (15 min)\nWe can tweak how the tree is built by controlling for certain parameters of the algorithm. To take a look at all possible parameters you can control, open the R console, type the command below and hit ENTER:\n?rpart.control\nFor example, letâ€™s reduce the minbucket parameter:\n\ntree.model <- rpart(is_chinstrap ~ ., data = penguins_cleaned, method = 'class', control=list(minbucket=1))\n\nrpart.plot(tree.model)\n\n\n\n\nğŸ¤ WORKING TOGETHER In pairs, discuss the following:\n\nExplain what is different in this model.\nVisualize the new tree using the parttree package (reuse from Step 2.1)"
  },
  {
    "objectID": "weeks/week07/lab.html#step-2.4-practice-15-min",
    "href": "weeks/week07/lab.html#step-2.4-practice-15-min",
    "title": "ğŸ’» Week 07 - Lab Roadmap (90 min)",
    "section": "Step 2.4 PRACTICE! (15 min)",
    "text": "Step 2.4 PRACTICE! (15 min)\nğŸ¯ ACTION POINTS\n\nBuild new tree models, this time trying out different control parameters.\nWhich parameters led to different trees?\nWhich parameters change the tree the most?\n\n\nğŸ”œ Next week, we will continue our journey of supervised learning by exploring the Support Vector Machine algorithm. We will also visit once again the topic of resampling (cross-validation) to select the optimal parameters for classifiers and regressors."
  },
  {
    "objectID": "weeks/week07/lab.html#q1-selecting-columns",
    "href": "weeks/week07/lab.html#q1-selecting-columns",
    "title": "ğŸ’» Week 07 - Lab Roadmap (90 min)",
    "section": "Q1: Selecting columns",
    "text": "Q1: Selecting columns\nUsing the Bikeshare data set present in ISLR2 subset the data to only include mnth, holiday, and bikers columns.\n\n# Your code goes here"
  },
  {
    "objectID": "weeks/week07/lab.html#q2-a-new-tidyverse-verb",
    "href": "weeks/week07/lab.html#q2-a-new-tidyverse-verb",
    "title": "ğŸ’» Week 07 - Lab Roadmap (90 min)",
    "section": "Q2: A new tidyverse verb",
    "text": "Q2: A new tidyverse verb\nUsing the Bikeshare data set present in ISLR2, replicate Step 1.3, only this time replace filter(origin == 1) with group_by(origin). How have the results changed?\n\n# Your code goes here\n\n\nYour text goes here"
  },
  {
    "objectID": "weeks/week07/lab.html#q3-exploratory-data-analysis-part-i",
    "href": "weeks/week07/lab.html#q3-exploratory-data-analysis-part-i",
    "title": "ğŸ’» Week 07 - Lab Roadmap (90 min)",
    "section": "Q3: Exploratory Data Analysis (Part I)",
    "text": "Q3: Exploratory Data Analysis (Part I)\nCalculate the average daily number of bikers in March, in the Bikeshare data set.\n\n# Your code goes here"
  },
  {
    "objectID": "weeks/week07/lab.html#q4-exploratory-data-analysis-part-ii0",
    "href": "weeks/week07/lab.html#q4-exploratory-data-analysis-part-ii0",
    "title": "ğŸ’» Week 07 - Lab Roadmap (90 min)",
    "section": "Q4: Exploratory Data Analysis (Part II0",
    "text": "Q4: Exploratory Data Analysis (Part II0\nDo people bike more during holiday seasons?\n\n# Your code goes here\n\n\nYour text goes here"
  },
  {
    "objectID": "weeks/week07/lab.html#q5-back-to-penguins",
    "href": "weeks/week07/lab.html#q5-back-to-penguins",
    "title": "ğŸ’» Week 07 - Lab Roadmap (90 min)",
    "section": "Q5: Back to penguinsâ€¦",
    "text": "Q5: Back to penguinsâ€¦\nLetâ€™s go back to the penguins data set. Create a new dataframe that omits missing values (NAs), remove the island and year columns but keep the rest of the dataset intact (donâ€™t create new columns).\n\n# Your code goes here"
  },
  {
    "objectID": "weeks/week07/lab.html#q6.-predict-penguin-species",
    "href": "weeks/week07/lab.html#q6.-predict-penguin-species",
    "title": "ğŸ’» Week 07 - Lab Roadmap (90 min)",
    "section": "Q6. Predict Penguin species",
    "text": "Q6. Predict Penguin species\nBuild a Decision Tree model to predict the species of penguins. Note that the outcome will not be a binary variable this time.\n\n# Your code goes here"
  },
  {
    "objectID": "weeks/week07/lab.html#q7.-control-parameters",
    "href": "weeks/week07/lab.html#q7.-control-parameters",
    "title": "ğŸ’» Week 07 - Lab Roadmap (90 min)",
    "section": "Q7. Control parameters",
    "text": "Q7. Control parameters\nThe decision tree algorithm we are using sets the cost complexity parameter (cp) by default as control = list(cp = 0.01). Build a new model with a smaller cp value, say control = list(cp = 0.001). Does this increase or reduce the complexity of the tree?\n\n# Your code goes here\n\n\nYour text goes here"
  },
  {
    "objectID": "weeks/week07/lab_solutions.html",
    "href": "weeks/week07/lab_solutions.html",
    "title": "âœ”ï¸ Week 07 - Lab Solutions",
    "section": "",
    "text": "This lab session draws on ğŸ—“ï¸ Week 05 lecture content and on feedback given by the course representatives about the main struggles you are facing with R (tidyverse).\nIf you are already very familiar with tidyverse, you can skip to Step 2.\nR packages you will need:\nYou might have already installed some of these packages. Some were used in ğŸ—“ï¸ Week 05 slides, others in previous labs. If you see an error like â€œpackage not foundâ€, then install the package using install.packages(\"<package name>\").\nInstallation instructions for parttree\nThe package parttree cannot be installed by install.packages. Instead, we have to follow the instructions set by the developers of the package:"
  },
  {
    "objectID": "weeks/week07/lab_solutions.html#q1-selecting-columns",
    "href": "weeks/week07/lab_solutions.html#q1-selecting-columns",
    "title": "âœ”ï¸ Week 07 - Lab Solutions",
    "section": "Q1: Selecting columns",
    "text": "Q1: Selecting columns\nUsing the Bikeshare data set present in ISLR2 subset the data to only include mnth, holiday, and bikers columns.\n\nBikeshare %>% select(mnth, holiday, bikers) %>% head()\n\n  mnth holiday bikers\n1  Jan       0     16\n2  Jan       0     40\n3  Jan       0     32\n4  Jan       0     13\n5  Jan       0      1\n6  Jan       0      1"
  },
  {
    "objectID": "weeks/week07/lab_solutions.html#q2-a-new-tidyverse-verb",
    "href": "weeks/week07/lab_solutions.html#q2-a-new-tidyverse-verb",
    "title": "âœ”ï¸ Week 07 - Lab Solutions",
    "section": "Q2: A new tidyverse verb",
    "text": "Q2: A new tidyverse verb\nUsing the Auto data set present in ISLR2, replicate Step 1.3, only this time replace filter(origin == 1) with group_by(origin). How have the results changed?\n\nAuto %>% \n  as_tibble() %>% \n  select(mpg, origin) %>% \n  mutate(mpg_gt_gavg = if_else(mpg > mean(mpg), TRUE, FALSE)) %>% \n  group_by(origin) %>% \n  summarise(prop_mpg_gt_gavg = mean(mpg_gt_gavg))\n\n# A tibble: 3 Ã— 2\n  origin prop_mpg_gt_gavg\n   <int>            <dbl>\n1      1            0.269\n2      2            0.75 \n3      3            0.873\n\n\n\nA: Instead of looking just at the origin=1, we have now summarized the data and computed prop_mpg_gt_gavg (Proportion of samples for which mpg is greater than average) for each unique value of origin."
  },
  {
    "objectID": "weeks/week07/lab_solutions.html#q3-exploratory-data-analysis-part-i",
    "href": "weeks/week07/lab_solutions.html#q3-exploratory-data-analysis-part-i",
    "title": "âœ”ï¸ Week 07 - Lab Solutions",
    "section": "Q3: Exploratory Data Analysis (Part I)",
    "text": "Q3: Exploratory Data Analysis (Part I)\nCalculate the average daily number of bikers in March, in the Bikeshare data set.\nThanks to @ZoÃ© Vanhersecke for spotting a bug in an earlier version of this solution\n\nBikeshare %>%\n  filter(mnth == \"March\") %>%\n  group_by(day) %>%\n  summarize(mean_bikers = mean(bikers)) %>%\n  summarize(mean_daily_bikers = mean(mean_bikers))\n\n# A tibble: 1 Ã— 1\n  mean_daily_bikers\n              <dbl>\n1              87.6"
  },
  {
    "objectID": "weeks/week07/lab_solutions.html#q4-exploratory-data-analysis-part-ii0",
    "href": "weeks/week07/lab_solutions.html#q4-exploratory-data-analysis-part-ii0",
    "title": "âœ”ï¸ Week 07 - Lab Solutions",
    "section": "Q4: Exploratory Data Analysis (Part II0",
    "text": "Q4: Exploratory Data Analysis (Part II0\nDo people bike more during holiday seasons?\n\nBikeshare %>% \n  group_by(holiday, day) %>% \n  summarise(daily_bikers=n()) %>%\n  summarise(mean_daily_bikers=mean(daily_bikers),\n            std_daily_bikers=sd(daily_bikers))\n\n# A tibble: 2 Ã— 3\n  holiday mean_daily_bikers std_daily_bikers\n    <dbl>             <dbl>            <dbl>\n1       0              23.7            1.32 \n2       1              23.9            0.316\n\n\n\nJust by summarising the data, itâ€™s not really possible to say. On average, the mean number of daily bikers seems to be about the same, just with a smaller standard deviation."
  },
  {
    "objectID": "weeks/week07/lab_solutions.html#q5-back-to-penguins",
    "href": "weeks/week07/lab_solutions.html#q5-back-to-penguins",
    "title": "âœ”ï¸ Week 07 - Lab Solutions",
    "section": "Q5: Back to penguinsâ€¦",
    "text": "Q5: Back to penguinsâ€¦\nLetâ€™s go back to the penguins data set. Create a new dataframe that omits missing values (NAs), remove the island and year columns but keep the rest of the dataset intact (donâ€™t create new columns).\n\nnew_penguins <- \n  penguins %>% \n  select(-c(island, year))\nnew_penguins\n\n# A tibble: 344 Ã— 6\n   species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   <fct>            <dbl>         <dbl>             <int>       <int> <fct> \n 1 Adelie            39.1          18.7               181        3750 male  \n 2 Adelie            39.5          17.4               186        3800 female\n 3 Adelie            40.3          18                 195        3250 female\n 4 Adelie            NA            NA                  NA          NA <NA>  \n 5 Adelie            36.7          19.3               193        3450 female\n 6 Adelie            39.3          20.6               190        3650 male  \n 7 Adelie            38.9          17.8               181        3625 female\n 8 Adelie            39.2          19.6               195        4675 male  \n 9 Adelie            34.1          18.1               193        3475 <NA>  \n10 Adelie            42            20.2               190        4250 <NA>  \n# â€¦ with 334 more rows"
  },
  {
    "objectID": "weeks/week07/lab_solutions.html#q6.-predict-penguin-species",
    "href": "weeks/week07/lab_solutions.html#q6.-predict-penguin-species",
    "title": "âœ”ï¸ Week 07 - Lab Solutions",
    "section": "Q6. Predict Penguin species",
    "text": "Q6. Predict Penguin species\nBuild a Decision Tree model to predict the species of penguins. Note that the outcome will not be a binary variable this time.\n\nset.seed(1)\n\ntree.model <- rpart(species ~ ., data = new_penguins, method = 'class')\n\nrpart.plot(tree.model)\n\n\n\n\n\nNote that we three different proportions in the middle of the nodes now (instead of just one). Each represent the proportion of penguins of a specific specie.\nFor example, the left-most leaf node shows: 0.97 0.03 0.00. That means: 97% of the samples that fall into that part of the decision tree are of the Adelie species, 3% are of the Chinstrap species and 0% are of the Gentoo species.\nHow do we know the order of the species represented by these numbers? Check the levels of the column species (which is a column of the type factor). For more on factors read: https://r4ds.had.co.nz/factors.html\n\n\nlevels(new_penguins$species)\n\n[1] \"Adelie\"    \"Chinstrap\" \"Gentoo\"   \n\n\nThe video below explains how to read the numbers inside the nodes of the decision tree"
  },
  {
    "objectID": "weeks/week07/lab_solutions.html#q7.-control-parameters",
    "href": "weeks/week07/lab_solutions.html#q7.-control-parameters",
    "title": "âœ”ï¸ Week 07 - Lab Solutions",
    "section": "Q7. Control parameters",
    "text": "Q7. Control parameters\nThe decision tree algorithm we are using sets the cost complexity parameter (cp) by default as control = list(cp = 0.01). Build a new model with a smaller cp value, say control = list(cp = 0.001). Does this increase or reduce the complexity of the tree?\n\nA smaller cp parameter would force the decision tree to try to find a more complex model (more branches) if that leads to a better overall model. (If you want to understand how the rpart method chooses to make the splits and branches of the tree, type ?rpart in the console and read the section about method).\n\nIt turns out that nothing will make this model more complex! Even if you set a very low cp, the tree stays the same:\n\n\nset.seed(1)\n\ntree.model <- rpart(species ~ ., data = new_penguins, method = 'class', control=list(cp=0.00000000000001))\n\nrpart.plot(tree.model)\n\n\n\n\n\nOn the other hand, it is possible to simplify the tree (fewer branches) if we increase cp parameter:\n\n\nset.seed(1)\n\ntree.model <- rpart(species ~ ., data = new_penguins, method = 'class', control=list(cp=0.4))\n\nrpart.plot(tree.model)\n\n\n\n\n\nIt all depends on what we care about and which metrics we will use to assess this model and we plan to use the model later on. But it looks like the above will underfit the data: one of the species is not even represented there."
  },
  {
    "objectID": "weeks/week07/lecture.html",
    "href": "weeks/week07/lecture.html",
    "title": "ğŸ‘¨â€ğŸ« Week 07 - Lecture",
    "section": "",
    "text": "Either click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides. For a PDF copy, check Moodle."
  },
  {
    "objectID": "weeks/week07/lecture.html#coffee-break-10-min",
    "href": "weeks/week07/lecture.html#coffee-break-10-min",
    "title": "ğŸ‘¨â€ğŸ« Week 07 - Lecture",
    "section": "â˜• Coffee Break (10 min)",
    "text": "â˜• Coffee Break (10 min)\nUse this time to chat, stretch, drink some coffee or just relax for a bit by yourself."
  },
  {
    "objectID": "weeks/week07/lecture.html#part-ii---unsupervised-learning-clustering-45-50-min",
    "href": "weeks/week07/lecture.html#part-ii---unsupervised-learning-clustering-45-50-min",
    "title": "ğŸ‘¨â€ğŸ« Week 07 - Lecture",
    "section": "Part II - Unsupervised Learning (Clustering) (45-50 min)",
    "text": "Part II - Unsupervised Learning (Clustering) (45-50 min)\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides. For a PDF copy, check Moodle."
  },
  {
    "objectID": "weeks/week08/checklist.html",
    "href": "weeks/week08/checklist.html",
    "title": "âœ… Week 08 - Checklist",
    "section": "",
    "text": "Important\n\n\n\n\n\nKeep in mind that after the lecture at the end of this week, on Friday 18 November 2022, we will release the Summative Problem Set 02. This is the second summative assessment of this course and it is worth 20% of your final grade. You will have until 29 November 2022 (11 days) to submit your solutions via Moodle.\nIn line with your feedback and the adjustments announced last week in the lecture, this problem set will require less writing and more reading of R code.\nComprehension Check\nGet ready for the Summative Problem Set 02. By the end of the week, you should be able to:\nTime Management Tips\nHere is a suggestion of how to program your week in relation to this course:"
  },
  {
    "objectID": "weeks/week08/checklist.html#if-your-lab-is-on-monday",
    "href": "weeks/week08/checklist.html#if-your-lab-is-on-monday",
    "title": "âœ… Week 08 - Checklist",
    "section": "If your lab is on Monday:",
    "text": "If your lab is on Monday:\nOn Monday:\n\nğŸ“¥ Download: Before or once you arrive at the classroom, download the DS202_2022MT_w08_lab_rmark.Rmd file that contains the lab roadmap (under ğŸ—“ï¸ Week 08 section on Moodle). Or browse the webpage version here.\nğŸ’» Participate: Actively engage with the material in the lab. Ask your class teacher for help if anything is unclear. Work with others whenever possible and take notes of theoretical concepts or practical coding skill you might want to revisit later in the week.\n\nTuesday to Thursday\n\nğŸ  Solve the take-home exercises: There are four take-home exercises in the W08 lab. Try to solve them before the lecture.\nğŸ“™ Read: Find some time to read (James et al. 2021, chap. 9) and reinforce your theoretical understanding of Support Vector Machines; the textbook is available online for free.\n\nAs you go through the text, try to connect what you read to the things you heard about in the W05 lecture or the examples you explored in the lab.\n\nğŸ… Can you solve the exercises in the Bonus lab roadmap? (Optional)\n\nFriday\n\nğŸ« Attend the lecture: This week, you will learn about Dimensionality Reduction, another Unsupervised Learning technique. This will be useful for Summative Problem Set 03 in a few weeks.\n\nAny time\n\nğŸ“Ÿ You know the drill. Share your questions on the #week08 channel in our Slack group.\nğŸ‘‚Want to talk to someone else about this course? Try reaching out to your course representatives, @Zhang Ruishan (Yoyo) or @Rachitha Raghuram."
  },
  {
    "objectID": "weeks/week08/checklist.html#if-your-lab-is-on-friday",
    "href": "weeks/week08/checklist.html#if-your-lab-is-on-friday",
    "title": "âœ… Week 08 - Checklist",
    "section": "If your lab is on Friday:",
    "text": "If your lab is on Friday:\nMonday - Wednesday:\n\nğŸ“™ Read: Find some time to read (James et al. 2021, chap. 9) and reinforce your theoretical understanding of Support Vector Machines; the textbook is available online for free.\n\nAs you go through the text, try to connect what you read to the things you heard about in the W05 lecture.\n\n\nThursday\n\nğŸ“¥ Download: Have a look at the DS202_2022MT_w08_lab_rmark.Rmd file that contains the lab roadmap (under ğŸ—“ï¸ Week 08 section on Moodle). Or browse the webpage version here.\n\nFriday\n\nğŸ« Attend the lecture: This week, you will learn about Dimensionality Reduction, another Unsupervised Learning technique. This will be useful for Summative Problem Set 03 in a few weeks.\nğŸ’» Participate: Actively engage with the material in the lab. Ask your class teacher for help if anything is unclear. Work with others whenever possible and take notes of theoretical concepts or practical coding skill you might want to revisit later in the week.\n\nEarly next week\n\nğŸ  Solve the take-home exercises: There are four take-home exercises in the W08 lab. They make help you solve you the summative problem set 02.\nğŸ… Can you solve the exercises in the Bonus lab roadmap? (Optional)\n\nAny time\n\nğŸ“Ÿ You know the drill. Share your questions on the #week08 channel in our Slack group.\nğŸ‘‚Want to talk to someone else about this course? Try reaching out to your course representatives, @Zhang Ruishan (Yoyo) or @Rachitha Raghuram."
  },
  {
    "objectID": "weeks/week08/lab.html",
    "href": "weeks/week08/lab.html",
    "title": "ğŸ’» Week 08 - Lab Roadmap (90 min)",
    "section": "",
    "text": "Context\nThis lab session draws on ğŸ—“ï¸ Week 04 lecture/workshop and ğŸ—“ï¸ Week 05 lecture content.\nIt also reuses elements of tidymodels introduced in previous labs where we used functions from library(broom) or library(rsample). These packages are part of tidymodels. For the record, tidymodelâ€™s â€˜Get Startedâ€™ tutorials are really good.\nTopics\nMore specifically, these are the things we will explore or revisit:"
  },
  {
    "objectID": "weeks/week08/lab.html#setup",
    "href": "weeks/week08/lab.html#setup",
    "title": "ğŸ’» Week 08 - Lab Roadmap (90 min)",
    "section": "Setup",
    "text": "Setup\nğŸ’¡ Some of you have mentioned that your R version cannot handle tidymodels. We recommend you update R to version 4.2.0 or above.\nPackages you will need\n\nlibrary('ISLR2')       # for the data\nlibrary('tidyverse')   # to use things like the pipe (%>%)\nlibrary('e1071')       # for SVM model\nlibrary('tidymodels')  # for model tuning, cross-validation etc.\n\n# Vanity packages:\nlibrary('GGally')      # for pretty correlation plot\nlibrary('ggsci')       # for pretty plot colours\nlibrary('cvms')        # for pretty confusion matrix plots"
  },
  {
    "objectID": "weeks/week08/lab.html#the-data-10-min",
    "href": "weeks/week08/lab.html#the-data-10-min",
    "title": "ğŸ’» Week 08 - Lab Roadmap (90 min)",
    "section": "The Data (10 min)",
    "text": "The Data (10 min)\n\nğŸŠOrange Juice\nThis week we will use a different ISLR2 dataset: OJ . We will perform a classification task with the goal to predict the Purchase column.\n\nThe data contains 1070 purchases where the customer either purchased Citrus Hill or Minute Maid Orange Juice. A number of characteristics of the customer and product are recorded.\n\n\nISLR2::OJ %>% head()\n\n  Purchase WeekofPurchase StoreID PriceCH PriceMM DiscCH DiscMM SpecialCH\n1       CH            237       1    1.75    1.99   0.00    0.0         0\n2       CH            239       1    1.75    1.99   0.00    0.3         0\n3       CH            245       1    1.86    2.09   0.17    0.0         0\n4       MM            227       1    1.69    1.69   0.00    0.0         0\n5       CH            228       7    1.69    1.69   0.00    0.0         0\n6       CH            230       7    1.69    1.99   0.00    0.0         0\n  SpecialMM  LoyalCH SalePriceMM SalePriceCH PriceDiff Store7 PctDiscMM\n1         0 0.500000        1.99        1.75      0.24     No  0.000000\n2         1 0.600000        1.69        1.75     -0.06     No  0.150754\n3         0 0.680000        2.09        1.69      0.40     No  0.000000\n4         0 0.400000        1.69        1.69      0.00     No  0.000000\n5         0 0.956535        1.69        1.69      0.00    Yes  0.000000\n6         1 0.965228        1.99        1.69      0.30    Yes  0.000000\n  PctDiscCH ListPriceDiff STORE\n1  0.000000          0.24     1\n2  0.000000          0.24     1\n3  0.091398          0.23     1\n4  0.000000          0.00     1\n5  0.000000          0.00     0\n6  0.000000          0.30     0\n\n\nTo understand what each variable represent, open the R Console, type the following and hit ENTER:\n?ISLR2::OJ\nWhich variables can help us distinguish the two different brands?\nTo simplify our plots later on, letâ€™s focus on just two predictors:\n\nplot_df <- ISLR2::OJ %>% select(Purchase, LoyalCH, PriceDiff)\n\ng = (\n  ggpairs(plot_df, aes(colour=Purchase))\n  \n  # Customizing the plot\n  + scale_colour_startrek()\n  + scale_fill_startrek()\n  + theme_bw()\n)\ng"
  },
  {
    "objectID": "weeks/week08/lab.html#stock-market",
    "href": "weeks/week08/lab.html#stock-market",
    "title": "ğŸ’» Week 08 - Lab Roadmap (90 min)",
    "section": "ğŸ“ˆ Stock Market",
    "text": "ğŸ“ˆ Stock Market\nWe will also use the Smarket dataset from the ISLR2 package. We will perform a regression task with the goal to predict the percentage of return of the S&P 500 stock index on any given day, as represented by the Today column.\n\nDaily percentage returns for the S&P 500 stock index between 2001 and 2005.\n\n\nISLR2::Smarket %>% head()\n\n  Year   Lag1   Lag2   Lag3   Lag4   Lag5 Volume  Today Direction\n1 2001  0.381 -0.192 -2.624 -1.055  5.010 1.1913  0.959        Up\n2 2001  0.959  0.381 -0.192 -2.624 -1.055 1.2965  1.032        Up\n3 2001  1.032  0.959  0.381 -0.192 -2.624 1.4112 -0.623      Down\n4 2001 -0.623  1.032  0.959  0.381 -0.192 1.2760  0.614        Up\n5 2001  0.614 -0.623  1.032  0.959  0.381 1.2057  0.213        Up\n6 2001  0.213  0.614 -0.623  1.032  0.959 1.3491  1.392        Up\n\n\n\nplot_df <- ISLR2::Smarket %>% select(Today, Volume, Lag1, Lag2)\n\ng = (\n  ggpairs(plot_df)\n  \n  # Customizing the plot\n  + scale_colour_startrek()\n  + scale_fill_startrek()\n  + theme_bw()\n)\ng \n\n\n\n\nTo understand what each variable represent, open the R Console, type the following and hit ENTER:\n?ISLR2::Smarket"
  },
  {
    "objectID": "weeks/week08/lab.html#step-1-svm-models-for-classification-30-min",
    "href": "weeks/week08/lab.html#step-1-svm-models-for-classification-30-min",
    "title": "ğŸ’» Week 08 - Lab Roadmap (90 min)",
    "section": "Step 1: SVM models for classification (30 min)",
    "text": "Step 1: SVM models for classification (30 min)\nSVM stands for Support Vector Machines. Revisit ğŸ—“ï¸ Week 05 lecture or Chapter 9 of our textbook to understand more about this algorithm.\nR does not come with SVM, so we need to import it from a library. Letâ€™s start with the function svm we used in the Week 05 lecture, imported from the e1071 package. Here are a few things to know about the svm function:\n\nThe svm command is largely similar to other commands such as lm and glm in that the first parameter is an R formula and the second is the data set.\nThere are a few other options, but we will focus on specifying a radial kernel using kernel = 'radial'.\nWe can specify the type of machine learning task we are performing. Since we are doing classification, we use the option type = 'C-classification'.\n\n\nStep 1.1: Train a SVM model\n\nfiltered_data <- ISLR2::OJ %>% select(Purchase, LoyalCH, PriceDiff)\n\norange_svm_model <- svm(Purchase ~ .,\n                        data=filtered_data,\n                        kernel='radial', \n                        type='C-classification')\norange_svm_model\n\n\nCall:\nsvm(formula = Purchase ~ ., data = filtered_data, kernel = \"radial\", \n    type = \"C-classification\")\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  1 \n\nNumber of Support Vectors:  427\n\n\nğŸ¯ ACTION POINT: What does the â€˜Number of Support Vectorsâ€™ represent?\n\nyour text here\n\nğŸ¯ ACTION POINT: What would happened if we changed kernel to kernel=\"linear\"?\n\nyour text here\n\n\n\nStep 1.2: Goodness-of-Fit of the SVM\nLetâ€™s investigate how well our model fits the data. Letâ€™s reuse the model we trained (orange_svm_model) and predict the same samples we used to train it. To avoid modifying our original dataframe, letâ€™s save the output of the prediction in an auxiliary df (plot_df):\n\nplot_df <- \n    filtered_data %>% \n    mutate(class_pred = predict(orange_svm_model, newdata = .))\nhead(plot_df)\n\nAdd a is_correct column to indicate whether the prediction was correct or not:\n\nplot_df <- plot_df %>% mutate(is_correct = class_pred == Purchase)\nhead(plot_df)\n\n  Purchase  LoyalCH PriceDiff class_pred is_correct\n1       CH 0.500000      0.24         CH       TRUE\n2       CH 0.600000     -0.06         CH       TRUE\n3       CH 0.680000      0.40         CH       TRUE\n4       MM 0.400000      0.00         MM       TRUE\n5       CH 0.956535      0.00         CH       TRUE\n6       CH 0.965228      0.30         CH       TRUE\n\n\nRemember what we did in the Decision Tree example last week!\n\nSimple confusion matrix:\n\nconfusion_matrix <- \n    table(expected=plot_df$Purchase, class_pred=plot_df$class_pred)\nprint(confusion_matrix)\n\n\n\nNicer looking confusion matrix:\n\nplot_confusion_matrix(as_tibble(confusion_matrix), \n                      target_col = \"expected\", \n                      prediction_col = \"class_pred\",\n                      \n                      # Customizing the plot\n                      add_normalized = TRUE,\n                      add_col_percentages = FALSE,\n                      add_row_percentages = FALSE,\n                      counts_col = \"n\",\n                      )\n\nğŸ¯ ACTION POINT: How well does the model fit the data? What is your opinion?\n\n\nMeasure: Precision\nRemember from Week 04 Lecture/Workshop (notebook can be found underğŸ“¥W04 Lecture Files on Moodle):\n\nPRECISION: Given all predictions for a specific class, how many were True Positives? In other words, Precision = True Positives/(True Positives + False Positives).\n\nLetâ€™s calculate the score for the CH class. That is, as if CH=â€œYesâ€ and MM=â€œNoâ€.\n\n# expected == CH & predicted == CH\ntotal_correct_CH   <- confusion_matrix[\"CH\", \"CH\"]  \n\n# sum of samples predicted == CH\ntotal_predicted_CH <- sum(confusion_matrix[, \"CH\"]) \n\nprecision          <- total_correct_CH/total_predicted_CH\ncat(sprintf(\"%.2f%%\", 100*precision))\n\n85.27%\n\n\n\n\nMeasure: Recall\n\nAlso called True Positive Rate = True Positive/(True Positives + False Negatives)\n\n\n# number of samples of brand CH\ntotal_real_CH      <- sum(confusion_matrix[\"CH\", ]) \n\nrecall <- total_correct_CH/total_real_CH\ncat(sprintf(\"%.2f%%\", 100*recall))\n\n87.75%\n\n\n\n\nMeasure: F1-SCORE\n\nF1-SCORE: A combination of Precision and Recall\n\\[\n\\operatorname{F1-score} = \\frac{2 \\times \\operatorname{Precision} \\times \\operatorname{Recall}}{(\\operatorname{Precision} + \\operatorname{Recall})}\n\\]\n\n\nf1_score <- (2*precision*recall)/(precision + recall)\n\ncat(f1_score)\n\n0.8649057\n\n\n\n\n\nStep 1.3: Visualize the SVM decision space\nHere we will demonstrate how you could simulate some data to cover the entire feature space of the data we are modelling. What do we mean by that? By inspecting LoyalCH and PriceDiff, we see the range values these variables can assume:\n\nfiltered_data %>% \n    select(c(LoyalCH, PriceDiff)) %>%\n    summary()\n\n    LoyalCH           PriceDiff      \n Min.   :0.000011   Min.   :-0.6700  \n 1st Qu.:0.325257   1st Qu.: 0.0000  \n Median :0.600000   Median : 0.2300  \n Mean   :0.565782   Mean   : 0.1465  \n 3rd Qu.:0.850873   3rd Qu.: 0.3200  \n Max.   :0.999947   Max.   : 0.6400  \n\n\nğŸ’¡ We can simulate data to account for all possible combinations of LoyalCH and PriceDiff. We achieve this using crossing, another tidyverse function:\n\nWe feed crossing a sequence of numbers that range from the minimal and maximal values of both variables, incremented by 0.1 values.\nWe then create a new variable class_pred which uses the SVM model object to predict the brand of orange juice purchased by the customer\nNote that we say newdata = . to indicate that we simply want to use the data set created with crossing as our new data.\n\n\nsim.data <- \n  crossing(LoyalCH   = seq(0,1,0.05),\n           PriceDiff = seq(-1,1,0.1)) %>% \n  mutate(class_pred = predict(orange_svm_model, newdata = .))\nhead(sim.data)\n\n# A tibble: 6 Ã— 3\n  LoyalCH PriceDiff class_pred\n    <dbl>     <dbl> <fct>     \n1       0      -1   MM        \n2       0      -0.9 MM        \n3       0      -0.8 MM        \n4       0      -0.7 MM        \n5       0      -0.6 MM        \n6       0      -0.5 MM        \n\n\nThe data above is all synthetic (â€œfakeâ€)! But it is very useful to colour the background of our plot.\nWe use geom_tile to show the area the SVM model identifies as Chinstrap penguins. We then use geom_point to overlay the actual data. Red dots in blue areas and vice versa indicate cases where the SVM model makes errors.\n\ng <- (\n  plot_df %>%   \n    ggplot()\n  \n    # Tile the background of the plot with SVM predictions\n    + geom_tile(data = sim.data, aes(x=LoyalCH, y=PriceDiff, fill = class_pred), alpha = 0.25)\n  \n    # Actual data\n    + geom_point(aes(x=LoyalCH, y=PriceDiff, colour = Purchase, shape = is_correct), size=2.5, stroke=0.95, alpha=0.7)\n  \n    # Define X and Os\n    + scale_shape_manual(values = c(4, 1))\n    \n    # (OPTIONAL) Customizing the colours and theme of the plot\n    + scale_x_continuous(labels=scales::percent)\n    + scale_colour_startrek()\n    + scale_fill_startrek()\n    + theme_minimal()\n    + theme(panel.grid = element_blank(), legend.position = 'bottom', plot.title = element_text(hjust = 0.5))\n    + labs(x = 'Customer brand loyalty for CH', y = 'Sale price of MM less sale price of CH', fill = 'Brand', colour = 'Brand', shape = 'Correct prediction?', title = sprintf('Overall Training Accuracy = %.2f %%', 100*(sum(plot_df$correct)/nrow(plot_df))))\n)\n\ng\n\n\n\n\nğŸ¤ WORKING TOGETHER In pairs, discuss what you see in the plot:\n\nWhat do the shape of dots represent? The X and Os?\n\n\nyour text here\n\n\nWhat do the colours of the dots represent?\n\n\nyour text here\n\n\nWhat do the background colour in the plot represent?\n\n\nyour text here\n\n\nCan you point in the plot roughly which dots you would expect to be the support vectors?"
  },
  {
    "objectID": "weeks/week08/lab.html#step-2-doing-the-same-with-tidymodels-20-min",
    "href": "weeks/week08/lab.html#step-2-doing-the-same-with-tidymodels-20-min",
    "title": "ğŸ’» Week 08 - Lab Roadmap (90 min)",
    "section": "Step 2: Doing the same with tidymodels (20 min)",
    "text": "Step 2: Doing the same with tidymodels (20 min)\nThe function svm from library(e1071) package is not the only way to run SVM in R. The package parsnip also have its own SVM functions. The functionality is roughly the same but there are differences in how you write the code.\nparsnip already comes installed in tidymodels, so we do not need to import or install anything else.\n\nStep 2.1 Training the SVM model\nWe specify a radial basis function SVM (see the part about kernels in the ğŸ—“ï¸ Week 05 lecture) with the function svm_rbf.\nIn the spirit of tidyverse, we pipe the SVM algorithm into the fit function, where we can define the R formula like we have been doing with other algorithms:\n\norange_tidymodel <-\n  svm_rbf() %>% \n  set_mode('classification') %>% \n  fit(Purchase ~ ., data = filtered_data)\n\norange_tidymodel\n\nparsnip model object\n\nSupport Vector Machine object of class \"ksvm\" \n\nSV type: C-svc  (classification) \n parameter : cost C = 1 \n\nGaussian Radial Basis kernel function. \n Hyperparameter : sigma =  1.75487711296194 \n\nNumber of Support Vectors : 435 \n\nObjective Function Value : -386.2313 \nTraining error : 0.169159 \nProbability model included. \n\n\nğŸ¯ ACTION POINT: Compare the output above to that of another colleague. Why donâ€™t you get the exact same output?\n\nyour personal notes go here\n\nğŸ’¡ If you want to try different kernels, you will need to replace svm_rbf() by svm_linear() or svm_poly().\n\n\nStep 2.2: Goodness-of-Fit of the SVM\nLetâ€™s investigate how well our model fits the data. Letâ€™s reuse the model we trained (orange_tidymodel) and predict the same samples we used to train it.\nFunction augment(<model>, <df>) of tidymodels applies a model to a dataframe and return the same data plus a few columns:\n\nplot_df <- augment(orange_tidymodel, filtered_data)\nhead(plot_df)\n\nğŸ¯ ACTION POINT: How is the plot_df data frame above different to the first plot_df we created in Step 1.2?\n\nyour notes go here\n\nAdd a is_correct column to indicate whether the prediction was correct or not:\n\nplot_df <- plot_df %>% mutate(is_correct = .pred_class == Purchase)\nhead(plot_df)\n\n# A tibble: 6 Ã— 7\n  Purchase LoyalCH PriceDiff .pred_class .pred_CH .pred_MM is_correct\n  <fct>      <dbl>     <dbl> <fct>          <dbl>    <dbl> <lgl>     \n1 CH         0.5        0.24 CH             0.863    0.137 TRUE      \n2 CH         0.6       -0.06 CH             0.719    0.281 TRUE      \n3 CH         0.68       0.4  CH             0.876    0.124 TRUE      \n4 MM         0.4        0    MM             0.168    0.832 TRUE      \n5 CH         0.957      0    CH             0.878    0.122 TRUE      \n6 CH         0.965      0.3  CH             0.876    0.124 TRUE      \n\n\n\nMeasure: Precision\nYou donâ€™t need to calculate precision by hand, just use the precision() function from tidymodels:\n\nplot_df %>% precision(Purchase, .pred_class) %>% head()\n\n# A tibble: 1 Ã— 3\n  .metric   .estimator .estimate\n  <chr>     <chr>          <dbl>\n1 precision binary         0.858\n\n\n\n\nMeasure: Recall\nYou donâ€™t need to calculate recall by hand, just use the recall() function from tidymodels:\n\nplot_df %>% recall(Purchase, .pred_class) %>% head()\n\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 recall  binary         0.867\n\n\n\n\nMeasure: F1-score\nYou donâ€™t need to calculate F1-score by hand, just use the f_meas() function from tidymodels:\n\nplot_df %>% f_meas(Purchase, .pred_class) %>% head()\n\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 f_meas  binary         0.862\n\n\n\n\n(Optional) ROC curve\nPlot the ROC curve for class Purchase==\"CH\" :\n\nplot_df %>% \n  roc_curve(Purchase, .pred_CH) %>% \n  autoplot\n\n\n\n\n\n\nğŸ  Take-home exercise Q1:\nEdit the cell below modifying event_level from \"second\" to \"first\". Why do you get different results? What do you think is going on?\nplot_df %>% f_meas(Purchase, .pred_class, event_level=...)\nğŸ’¡Tip: Read the documentation of f_meas to understand what event_level represents. (Type ?f_meas)\nğŸ’¡ Gold Tip: note the Levels of the factor variable called Purchase:\nplot_df$Purchase\n\n\nğŸ  Take-home exercise Q2:\nCreate a plot of the confusion matrix for the orange_tidymodel like we did in Step 1.\n\n## your code goes here\n\n\n\nğŸ  Take-home exercise Q3:\nCreate a plot of SVM decision space for the orange_tidymodel like we did in Step 1.\n\n## your code goes here\n\nğŸ¯ ACTION POINT: If you were to run the SVM algorithm by yourself in another dataset, which version would you prefer, the one in Step 1 or the one in Step 2?\n\nyour notes go here"
  },
  {
    "objectID": "weeks/week08/lab.html#step-3-what-about-regression-30-min",
    "href": "weeks/week08/lab.html#step-3-what-about-regression-30-min",
    "title": "ğŸ’» Week 08 - Lab Roadmap (90 min)",
    "section": "Step 3: What about regression? (30 min)",
    "text": "Step 3: What about regression? (30 min)\nHere we will be using the ğŸ“ˆ SMarket data.\n\nStep 3.1: Train the model\nLetâ€™s select just the predictors Volume and Lag1 and fit a regression model to predict Today:\n\n# Remove Direction, otherwise we would be \"cheating\" \nfiltered_data <- ISLR2::Smarket %>% select(Today, Volume, Lag1)\n\nsmarket_tidymodel <-\n  svm_rbf() %>% \n  set_mode('regression') %>% \n  fit(Today ~ ., data = filtered_data)\n\nsmarket_tidymodel\n\nparsnip model object\n\nSupport Vector Machine object of class \"ksvm\" \n\nSV type: eps-svr  (regression) \n parameter : epsilon = 0.1  cost C = 1 \n\nGaussian Radial Basis kernel function. \n Hyperparameter : sigma =  1.54888712639507 \n\nNumber of Support Vectors : 1119 \n\nObjective Function Value : -763.4993 \nTraining error : 0.933903 \n\n\n\n\nStep 3.2: Goodness-of-Fit of the SVM\nSince the target variable is continuous, not discrete, we cannot plot confusion matrix nor anything like that. We will have to go back to the idea of residuals (ğŸ—“ï¸ Week 02 Lecture & ğŸ’» Week 03 - Lab).\n\nplot_df <- \n  augment(smarket_tidymodel, filtered_data) %>% \n  mutate(row_number=row_number()) # adding this here just to make our plot easier\nplot_df %>% head()\n\n# A tibble: 6 Ã— 6\n   Today Volume   Lag1   .pred .resid row_number\n   <dbl>  <dbl>  <dbl>   <dbl>  <dbl>      <int>\n1  0.959   1.19  0.381 -0.0473  1.01           1\n2  1.03    1.30  0.959 -0.238   1.27           2\n3 -0.623   1.41  1.03  -0.247  -0.376          3\n4  0.614   1.28 -0.623  0.230   0.384          4\n5  0.213   1.21  0.614 -0.146   0.359          5\n6  1.39    1.35  0.213 -0.160   1.55           6\n\n\nğŸ¯ ACTION POINT: What do the different columns mean?\n\nyour text go here\n\nNow, letâ€™s look at the distribution of residuals and letâ€™s mark the absolute residuals above 2 to flag the worst predictions (2 was an arbitrary choice, it all depends on the context):\n\ng <- (\n  ggplot(plot_df, aes(x=row_number, y=.resid))\n  + geom_point(alpha=0.6)\n  \n  + theme_bw()\n  + geom_hline(yintercept = c(-2,2), color=\"red\", linetype=\"dashed\")\n  + labs(title=\"Distribution of residuals (the closer to zero the better)\")\n)\n\ng\n\n\n\n\n\nMeasure: Mean Absolute Error (MAE)\n\\[\nMAE = \\frac{\\sum_{i=1}^{n}{|y_i - \\hat{y}_i|}}{n}\n\\]\n\nplot_df %>% mae(Today, .pred)\n\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 mae     standard       0.792\n\n\n\n\nMeasure: Root Mean Squared Error (RMSE)\n\\[\nRMSE = \\frac{\\sum_{i=1}^{n}{(y_i - \\hat{y}_i)^2}}{n}\n\\]\n\nplot_df %>% rmse(Today, .pred)\n\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.10\n\n\nğŸ¯ ACTION POINT: Would a better model have a larger or smaller value of MAE/RMSE?\n\nyour text goes here\n\n\n\n\nStep 3.3 Visualize the SVM decision space (Regression)\nNow, letâ€™s replicate what we did in Step 1.3 only this time for the Smarket data and using predictions from the smarket_tidymodel.\nWe start by summarising the data. We want to find out the minimum and maximal values that the columns Volumn and Lag1 reach:\n\nfiltered_data %>% \n    select(c(Volume, Lag1)) %>%\n    summary()\n\n     Volume            Lag1          \n Min.   :0.3561   Min.   :-4.922000  \n 1st Qu.:1.2574   1st Qu.:-0.639500  \n Median :1.4229   Median : 0.039000  \n Mean   :1.4783   Mean   : 0.003834  \n 3rd Qu.:1.6417   3rd Qu.: 0.596750  \n Max.   :3.1525   Max.   : 5.733000  \n\n\nThen, we create a simulated dataset with combinations of the Volume and Lag1 columns:\n\nsim.data <- \n  crossing(Volume   = seq(0,3.5,0.1),\n           Lag1 = seq(-5,6,0.2))\nsim.data <- augment(smarket_tidymodel, sim.data)\nhead(sim.data)\n\n# A tibble: 6 Ã— 3\n  Volume  Lag1   .pred\n   <dbl> <dbl>   <dbl>\n1      0  -5   -0.0146\n2      0  -4.8 -0.0146\n3      0  -4.6 -0.0146\n4      0  -4.4 -0.0146\n5      0  -4.2 -0.0146\n6      0  -4   -0.0146\n\n\nğŸ’¡ Look at the entire dataset with View(sim.data)\nLooking back at the plot of residuals, letâ€™s flag the worst predictions, those with an absolute residual above 2.\n\nplot_df$residual_above_2 <- (plot_df$.resid) > 2\n\n\ng <- (\n  plot_df %>%   \n    ggplot()\n  \n    ## Tile the background of the plot with SVM predictions\n    + geom_tile(data = sim.data, aes(x=Volume, y=Lag1, fill = .pred), alpha = 0.45)\n  \n    ## Actual data\n    + geom_point(aes(x=Volume, y=Lag1, colour = residual_above_2, shape = residual_above_2, alpha=residual_above_2), size=2.5, stroke=0.95)\n  \n    ## Define X and Os\n    + scale_shape_manual(values = c(4, 1))\n    + scale_fill_viridis_c()\n    + scale_color_manual(values=c(\"black\", \"red\"))\n    + scale_alpha_manual(values=c(0.1, 0.7))\n    \n    ## (OPTIONAL) Customizing the colours and theme of the plot\n    + theme_minimal()\n    + theme(panel.grid = element_blank(), legend.position = 'bottom', plot.title = element_text(hjust = 0.5))\n    + labs(x = 'Volume', y = 'Lag 1', fill = \"Today's Prediction\", colour = 'Residual above 2?', shape = 'Residual above 2?', alpha='Residual above 2?', title='Worst predictions are marked as red circles')\n)\n\ng\n\n\n\n\nğŸ’¡ The plot above might not be as easy to understand as the one for classification.\n\n\nStep 3.3: Understand the parameters of svm_rbf\nThe svm_rbf function has three parameters you can tune:\n\ncost\nrbf_sigma\nmargin\n\nğŸ¯ ACTION POINT: Train your abilities to interact with code documentation. Type ?svm_rbf and hit ENTER. What do these parameters represent?\nğŸ’¡ Tip: At the bottom of the help page, you will find a link to kernlab engine details that has more useful info about SVM RBF.\n\nyour text goes here\n\n\n\nStep 3.4: Tweak the parameters\nğŸ¤ WORKING TOGETHER In pairs, change the values of cost, rbf_sigma and margin in the chunk below and run the other two chunks of code to look at the distribution of residuals and summary metrics.\nDiscuss your findings. Can you find any combination of values that makes the model better? Or any that makes it worse?\n\nalternative_smarket_model <-\n  svm_rbf(cost=1, rbf_sigma=10, margin=0.9) %>% \n  set_mode('regression') %>% \n  fit(Today ~ ., data = filtered_data)\n\nalternative_smarket_model\n\nparsnip model object\n\nSupport Vector Machine object of class \"ksvm\" \n\nSV type: eps-svr  (regression) \n parameter : epsilon = 0.9  cost C = 1 \n\nGaussian Radial Basis kernel function. \n Hyperparameter : sigma =  10 \n\nNumber of Support Vectors : 401 \n\nObjective Function Value : -195.7568 \nTraining error : 0.832623 \n\n\nResiduals plot\n\nplot_df <- \n  augment(alternative_smarket_model, filtered_data) %>% \n  mutate(row_number=row_number()) ## adding this here just to make our plot easier\n\ng <- (\n  ggplot(plot_df, aes(x=row_number, y=.resid))\n  + geom_point(alpha=0.6)\n  \n  + theme_bw()\n  + geom_hline(yintercept = c(-2,2), color=\"red\")\n)\n\ng\n\n\n\n\nMetrics\n\n## Use the vectorized version of MAE and RMSE functions\nplot_df %>% summarise(mae=mae_vec(Today, .pred),\n                      rmse=rmse_vec(Today, .pred))\n\n# A tibble: 1 Ã— 2\n    mae  rmse\n  <dbl> <dbl>\n1 0.787  1.04\n\n\n\nğŸ  Take-home exercise Q4:\n\nRetrain the model in Step 3 for Smarket, this time using svm_linear instead of svm_rbf.\nReuse the code in Step 3.4 to replicate the plots and metric calculations for this new model.\nWhich model fits the target variable (Today) better?\n\n\n# your code goes here\n\n\nyour text goes here"
  },
  {
    "objectID": "weeks/week08/lab.html#want-to-take-it-to-the-next-level-optional",
    "href": "weeks/week08/lab.html#want-to-take-it-to-the-next-level-optional",
    "title": "ğŸ’» Week 08 - Lab Roadmap (90 min)",
    "section": "Want to take it to the next level? (Optional)",
    "text": "Want to take it to the next level? (Optional)\nIn a separate, more advanced, bonus lab roadmap we show how to perform k-fold cross-validation using tidymodels to tune the parameters of SVM automatically.\nIt is optional, you donâ€™t need to read it, but it might be the best way to solidify your knowledge of SVM and its parameters."
  },
  {
    "objectID": "weeks/week08/lab_advanced.html",
    "href": "weeks/week08/lab_advanced.html",
    "title": "âœ¨ Bonus Lab (Optional) - Cross-validation",
    "section": "",
    "text": "Update 22/11/2022\n\n\n\nThis notebook has been modified. It now contains model solutions too.\nI show the code only, no plots, otherwise it would make building this website too slow!"
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#topics",
    "href": "weeks/week08/lab_advanced.html#topics",
    "title": "âœ¨ Bonus Lab (Optional) - Cross-validation",
    "section": "Topics",
    "text": "Topics\nHere we show an alternative way to perform k-fold cross-validation using tidymodels instead of the cv.glm we saw in W05 lab."
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#packages-you-will-need",
    "href": "weeks/week08/lab_advanced.html#packages-you-will-need",
    "title": "âœ¨ Bonus Lab (Optional) - Cross-validation",
    "section": "Packages you will need",
    "text": "Packages you will need\nlibrary('ISLR2')       # for the data\nlibrary('tidyverse')   # to use things like the pipe (%>%)\nlibrary('e1071')       # for SVM model\nlibrary('tidymodels')  # for model tuning, cross-validation etc.\n\n# Vanity packages:\nlibrary('GGally')      # for pretty correlation plot\nlibrary('ggsci')       # for pretty plot colours\nlibrary('cvms')        # for pretty confusion matrix plots"
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#orange-juice",
    "href": "weeks/week08/lab_advanced.html#orange-juice",
    "title": "âœ¨ Bonus Lab (Optional) - Cross-validation",
    "section": "ğŸŠOrange Juice",
    "text": "ğŸŠOrange Juice\nThis week we will use a different ISLR2 dataset: OJ . We will perform a classification task with the goal to predict the Purchase column.\n\nThe data contains 1070 purchases where the customer either purchased Citrus Hill or Minute Maid Orange Juice. A number of characteristics of the customer and product are recorded.\n\nISLR2::OJ %>% head()\nTo understand what each variable represent, open the R Console, type the following and hit ENTER:\n?ISLR2::OJ\n\nWhich variables can help us distinguish the two different brands?\nTo simplify our plots later on, letâ€™s focus on just two predictors:\nplot_df <- ISLR2::OJ %>% select(Purchase, LoyalCH, PriceDiff)\n\ng = (\n  ggpairs(plot_df, aes(colour=Purchase))\n  \n  # Customizing the plot\n  + scale_colour_startrek()\n  + scale_fill_startrek()\n  + theme_bw()\n)\ng"
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#stock-market",
    "href": "weeks/week08/lab_advanced.html#stock-market",
    "title": "âœ¨ Bonus Lab (Optional) - Cross-validation",
    "section": "ğŸ“ˆ Stock Market",
    "text": "ğŸ“ˆ Stock Market\nWe will also use the Smarket dataset from the ISLR2 package. We will perform a regression task with the goal to predict the percentage of return of the S&P 500 stock index on any given day, as represented by the Today column.\n\nDaily percentage returns for the S&P 500 stock index between 2001 and 2005.\n\nISLR2::Smarket %>% head()\nplot_df <- ISLR2::Smarket %>% select(Today, Volume, Lag1, Lag2)\n\ng = (\n  ggpairs(plot_df)\n  \n  # Customizing the plot\n  + scale_colour_startrek()\n  + scale_fill_startrek()\n  + theme_bw()\n)\ng \nTo understand what each variable represent, open the R Console, type the following and hit ENTER:\n?ISLR2::Smarket"
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#step-4.1-create-training-test-split",
    "href": "weeks/week08/lab_advanced.html#step-4.1-create-training-test-split",
    "title": "âœ¨ Bonus Lab (Optional) - Cross-validation",
    "section": "Step 4.1: Create training / test split",
    "text": "Step 4.1: Create training / test split\nWe start by creating a training / test split using the functions initial_split packages for ğŸ“ˆ SMarket data.\n\nset.seed(123)\n\n# Remove Direction, otherwise we would be \"cheating\" \nfiltered_data <- ISLR2::Smarket %>% select(Today, Volume, Lag1)\n\ndefault_split <- initial_split(filtered_data, prop = 0.75, strata = Today)\n\ninternal_validation_set <- training(default_split)\n\nexternal_validation_set <- testing(default_split)\nHow many samples are in the internal validation set?\nnrow(internal_validation_set)\nHow many samples were left in the external validation set?\nnrow(external_validation_set)\nThe external validation set will only be used at the end"
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#step-4.2-create-resampling-folds-for-cross-validation",
    "href": "weeks/week08/lab_advanced.html#step-4.2-create-resampling-folds-for-cross-validation",
    "title": "âœ¨ Bonus Lab (Optional) - Cross-validation",
    "section": "Step 4.2: Create resampling folds for cross validation",
    "text": "Step 4.2: Create resampling folds for cross validation\nNext, letâ€™s create 10-fold cross-validation data using internal_validation_set. We can achieve this by using the vfold_cv command, specifying v = 10.\nk_folds <- vfold_cv(internal_validation_set, v = 10)\nk_folds\nNotice anything odd? The output is a tibble but the first column splits is a series of lists. This is another thing that makes tibbles different to data frames - you can nest lists within tibbles but not data frames. We will use this more explicitly in the next lab when we build k-means clustering models."
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#step-4.3-specifying-a-recipe",
    "href": "weeks/week08/lab_advanced.html#step-4.3-specifying-a-recipe",
    "title": "âœ¨ Bonus Lab (Optional) - Cross-validation",
    "section": "Step 4.3: ğŸ³Specifying a recipe",
    "text": "Step 4.3: ğŸ³Specifying a recipe\nThe next step is to create a recipe. Luckily, recipe function takes the same values as the lm model! We first create a formula default ~ . and then use data = internal_validation_set. Printing smarket_recipe, we have one outcome and three predictors.\n\nsmarket_recipe <- recipe(Today ~ ., data = internal_validation_set)\nsmarket_recipe"
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#step-4.4-specify-a-model",
    "href": "weeks/week08/lab_advanced.html#step-4.4-specify-a-model",
    "title": "âœ¨ Bonus Lab (Optional) - Cross-validation",
    "section": "Step 4.4: Specify a model",
    "text": "Step 4.4: Specify a model\nNext, we will specify a support vector machine model. Hereâ€™s where things get a bit more involved.\nWe specify a radial basis function SVM. svm_rbf takes two hyperparameters: cost and rbf_sigma. Instead of specifying a single value for each, we will instead set them equal to tune(). This indicates that we want to try a range of different values.\n\nsvm_regressor <-\n  svm_rbf(cost = tune(), rbf_sigma = tune()) %>% \n  set_mode('regression')"
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#step-4.5-create-a-hyperparameter-grid",
    "href": "weeks/week08/lab_advanced.html#step-4.5-create-a-hyperparameter-grid",
    "title": "âœ¨ Bonus Lab (Optional) - Cross-validation",
    "section": "Step 4.5: Create a hyperparameter grid",
    "text": "Step 4.5: Create a hyperparameter grid\nWhich values for cost and rbf_sigma should we choose? It is often hard to tell, so instead we can experiment with different values.\nWe can use grid_regular to create a tibble of different hyperparameter combinations. levels = 5 indicates that we want to try out five different values for each hyperparameter.\n\nset.seed(234)\n\nsvm_grid <- grid_regular(cost(), rbf_sigma(), levels = 5)\nsvm_grid"
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#step-4.6-perform-cross-validation",
    "href": "weeks/week08/lab_advanced.html#step-4.6-perform-cross-validation",
    "title": "âœ¨ Bonus Lab (Optional) - Cross-validation",
    "section": "Step 4.6: Perform cross-validation",
    "text": "Step 4.6: Perform cross-validation\nWe now have all we need to run cross-validation, and the function tune_grid puts everything together. Letâ€™s think intuitively what this command is doing.\n\nWe are telling tune_grid that we want to run a classification model on a recipe using different resampling folds.\nInstead of specifying hyperparameter values we want to run a combination of different values.\nAfter this, we want to choose a metric to evaluate different combinations.\nWe opt for rmse but we can specify several metrics however using the metric_set command.\n\nâš ï¸ As you can probably notice, the code below takes quite some time to run. This is expected; after all, we are training A LOT of models to tune the parameters.\n\nsmarket_tuned <-\n  tune_grid(object = svm_regressor,\n            preprocessor = smarket_recipe,\n            resamples = k_folds,\n            grid = svm_grid,\n            metrics = metric_set(rmse),\n            control = control_grid(save_pred=TRUE))"
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#step-4.7-which-combination-of-hyperparameters-works-best",
    "href": "weeks/week08/lab_advanced.html#step-4.7-which-combination-of-hyperparameters-works-best",
    "title": "âœ¨ Bonus Lab (Optional) - Cross-validation",
    "section": "Step 4.7: Which combination of hyperparameters works best?",
    "text": "Step 4.7: Which combination of hyperparameters works best?\nNow we have tuned our models, letâ€™s find out which combination of hyperparameters work best. We can create a ggplot easily using the autoplot command.\n\nsmarket_tuned %>% \n  autoplot() +\n  theme_minimal() +\n  ggsci::scale_color_jco() +\n  labs(y = 'RMSE', colour = 'Sigma')\nğŸ¯ ACTION POINT: Can you explain what we see in the plot above?\n\n\nSee a model solution\n\n\nThe plot above shows the performance of each variation of the SVM algorithm, in terms of the RMSE metric (Y-axis). Two parameters vary: Cost (X-axis) and Sigma (Colour of the lines).\nFrom the plot, we can observe that lower values of Sigma (\\(\\le 1e-4\\)) lead to smaller RMSE regardless of Cost.\n\nOn a personal note, I do not really love this plot. autoplot tries to guess the best plot for the dataframe, but I donâ€™t think it communicates truthfully. Here is how I would re-do this plot from scratch using ggplot2:\nFirst, collect the metrics as a single dataframe:\nplot_df <- collect_metrics(smarket_tuned)\nhead(plot_df)\nThen take care of making the X and Y labels easier to read:\nplot_df$rbf_sigma <- factor(plot_df$rbf_sigma)\nplot_df$cost <- factor(plot_df$cost)\n\ng <-\n  (ggplot(plot_df, \n          aes(x=cost, y=rbf_sigma, size=mean, color=mean)) \n   + geom_point()\n   \n   + scale_size_continuous(limits=c(1.1, 1.25))\n   + scale_colour_viridis_c(limits=c(1.1,1.25))\n   + scale_x_discrete(labels = function(x) format(as.numeric(x), digits=2, scientific=TRUE))\n   + scale_y_discrete(labels = function(x) format(as.numeric(x), digits=2, scientific=TRUE))\n   \n   + theme_minimal()\n   + theme(axis.text.x = element_text(angle=90),\n           legend.position = \"bottom\",\n           legend.direction =\"horizontal\")\n   + labs(x=\"Cost\", y=\"RBF Sigma\", title=\"Results are similar\", subtitle=\"But we get better results for lower Cost and lower Sigma\")\n   )\ng\nWell, if we truly care about representing things accurately in the plot, the X and Y axes should be put in the right scale:\n# This time I won't convert cost and sigma to factors\nplot_df <- collect_metrics(smarket_tuned)\n\n# In the plot, scale_x_discrete has to be replaced by scale_x_continuous:\ng <-\n  (ggplot(plot_df, \n          aes(x=cost, y=rbf_sigma, size=mean, color=mean)) \n   + geom_point(alpha=0.3)\n   \n   + scale_size_continuous(limits=c(1.1, 1.25))\n   + scale_colour_viridis_c(limits=c(1.1,1.25))\n   + scale_x_continuous(labels = function(x) format(as.numeric(x), digits=2, scientific=TRUE))\n   + scale_y_continuous(labels = function(x) format(as.numeric(x), digits=2, scientific=TRUE))\n   \n   + theme_minimal()\n   + theme(axis.text.x = element_text(angle=90),\n           legend.position = \"bottom\",\n           legend.direction =\"horizontal\")\n   + labs(x=\"Cost\", y=\"RBF Sigma\", title=\"Results are similar\", subtitle=\"But we get better results for lower Cost and lower Sigma\")\n   )\ng\n\nWe can use the function select_best to identify the hyperparameter combination that leads to the highest precision.\nselect_best(smarket_tuned)"
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#q5-re-run-step3-regression",
    "href": "weeks/week08/lab_advanced.html#q5-re-run-step3-regression",
    "title": "âœ¨ Bonus Lab (Optional) - Cross-validation",
    "section": "Q5: Re-run Step3 (Regression)",
    "text": "Q5: Re-run Step3 (Regression)\nBuild a standalone SVM model (tidymodels version) on the same data we used in Step 3 of W08 lab, only this time set the parameters of the SVM to the optimal parameters identified in Step 4.\n\n\nSee a model solution\n\nfiltered_data <- ISLR2::Smarket %>% select(Today, Volume, Lag1)\n\nalternative_svm_model <-\n  svm_rbf(cost=0.0009765625, rbf_sigma=1) %>% \n  set_mode('regression') %>% \n  fit(Today ~ ., data = filtered_data)\n\nalternative_svm_model"
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#q6-predict-the-external-validation-set",
    "href": "weeks/week08/lab_advanced.html#q6-predict-the-external-validation-set",
    "title": "âœ¨ Bonus Lab (Optional) - Cross-validation",
    "section": "Q6: Predict the external validation set",
    "text": "Q6: Predict the external validation set\nUse the model you built in Q5 and make predictions on the external validation set. How does the RMSE of these predictions compare to the RMSE of the internal validation set?\n\n\nSee a model solution\n\naugment(alternative_svm_model, external_validation_set) %>% \n  rmse(Today, .pred)\n\nRMSE in the external validation set is comparable to that of the mean RMSE in the internal validation set (close to RMSE=1.1)!\nThat is what is good about cross-validation. The internal cross-validation error tells us about by how much we can expect to err, on average."
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#q7-svm-decision-space-regression",
    "href": "weeks/week08/lab_advanced.html#q7-svm-decision-space-regression",
    "title": "âœ¨ Bonus Lab (Optional) - Cross-validation",
    "section": "Q7: SVM Decision Space (Regression)",
    "text": "Q7: SVM Decision Space (Regression)\nReplicate the plot from Step 3.3 of W08 lab roadmap, only this time using the model from Q5.\n\n\nSee a model solution\n\n\nWhen using real data, I will focus on the external validation set. There are fewer samples, so the plot will not look as polluted.\n\n## Simulated data\nsim.data <- \n  crossing(Volume   = seq(0,3.5,0.1),\n           Lag1 = seq(-5,6,0.2))\nsim.data <- augment(smarket_tidymodel, sim.data)\n\n## Real Data\nplot_df <- \n  augment(alternative_svm_model, external_validation_set) %>% \n  mutate(row_number=row_number())\n\nplot_df$residual_above_2 <- (plot_df$.resid) > 2\n\n\ng <- (\n  plot_df %>%   \n    ggplot()\n  \n    # Tile the background of the plot with SVM predictions\n    + geom_tile(data = sim.data, aes(x=Volume, y=Lag1, fill = .pred), alpha = 0.45)\n  \n    # Actual data\n    + geom_point(aes(x=Volume, y=Lag1, colour = residual_above_2, shape = residual_above_2, alpha=residual_above_2), size=2.5, stroke=0.95)\n  \n    # Define X and Os\n    + scale_shape_manual(values = c(4, 1))\n    + scale_fill_viridis_c()\n    + scale_color_manual(values=c(\"black\", \"red\"))\n    + scale_alpha_manual(values=c(0.1, 0.7))\n    \n    # (OPTIONAL) Customizing the colours and theme of the plot\n    + theme_minimal()\n    + theme(panel.grid = element_blank(), legend.position = 'bottom')\n    + labs(x = 'Volume', y = 'Lag 1', fill = \"Today's Prediction\", colour = 'Residual above 2?', shape = 'Residual above 2?', alpha='Residual above 2?', title='Worst predictions are marked as red circles', subtitle=\"(showing external validation set only)\")\n)\n\ng"
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#q8-compare-svm-decision-space-plots",
    "href": "weeks/week08/lab_advanced.html#q8-compare-svm-decision-space-plots",
    "title": "âœ¨ Bonus Lab (Optional) - Cross-validation",
    "section": "Q8: Compare SVM Decision Space plots",
    "text": "Q8: Compare SVM Decision Space plots\nExplain if and how the decision space you obtained in Q8 differs from the one in Step 3.3.\n\n\nSee a model solution\n\n\nThe decision space in Q7 is a lot simpler than the one in Step 3.3 of W08 lab; predictions made by this SVM are very similar to each other, regardless of the value of Lag1 and Volume.\n\nIf you are not convinced, take a look at the colour legend at the bottom of the plot. Note that the range goes from 0.01 to 0.05 whereas in the first plot, colours represented a range from -1.0 to 1.5.\nThis is more apparent when we re-do the plot in Q7 overriding the colour range to be the same as before:\ng + scale_fill_viridis_c(limits=c(-1, 1.5))\nNot a lot of varianceâ€¦"
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#q9-grid-search-for-orange-juice-classification",
    "href": "weeks/week08/lab_advanced.html#q9-grid-search-for-orange-juice-classification",
    "title": "âœ¨ Bonus Lab (Optional) - Cross-validation",
    "section": "Q9: Grid search for Orange Juice (Classification)",
    "text": "Q9: Grid search for Orange Juice (Classification)\n\nFor this task, you will use the Orange Juice data set (ISLR2::OJ) with ALL the predictors included\nReplicate the entire procedure of Step 4, making the necessary adjustments to predict Purchase\nUse F1 score as the optimisation metric.\n\nğŸ’¡You will have two tweak at least two main things: metric_set and set_mode\n\n\nSee a model solution\n\n\n### Set aside some data for external validation\nset.seed(123)\n\ndefault_split <- initial_split(ISLR2::OJ , prop = 0.75, strata = Purchase)\n\ninternal_validation_set <- training(default_split)\n\nexternal_validation_set <- testing(default_split)\n\norange_tuned <-\n  tune_grid(object = svm_rbf(cost = tune(), rbf_sigma = tune()) %>% set_mode('classification'),\n            preprocessor = recipe(Purchase ~ ., data = internal_validation_set),\n            resamples = vfold_cv(internal_validation_set, v = 10),\n            grid = grid_regular(cost(), rbf_sigma(), levels = 5),\n            metrics = metric_set(f_meas),\n            control = control_grid(save_pred=TRUE))\nplot_df <- collect_metrics(orange_tuned)\nplot_df$rbf_sigma <- factor(plot_df$rbf_sigma)\nplot_df$cost <- factor(plot_df$cost)\n\nplot_df$mean %>% summary()\ng <-\n  (ggplot(plot_df, \n          aes(x=cost, y=rbf_sigma, size=mean, color=mean)) \n   + geom_point()\n   \n   + scale_size_continuous(limits=c(0.7, 0.9))\n   + scale_colour_viridis_c(limits=c(0.7, 0.9))\n   + scale_x_discrete(labels = function(x) format(as.numeric(x), digits=2, scientific=TRUE))\n   + scale_y_discrete(labels = function(x) format(as.numeric(x), digits=2, scientific=TRUE))\n   \n   + theme_minimal()\n   + theme(axis.text.x = element_text(angle=90),\n           legend.position = \"bottom\",\n           legend.direction =\"horizontal\")\n   + labs(x=\"Cost\", y=\"RBF Sigma\", title=\"Results are similar\", subtitle=\"But we get better (larger) F1-scores when sigma is closer to 3.2e-3\")\n   )\ng\nGet the best configuration of parameters:\nselect_best(orange_tuned)\nNow, I am curious about the external validation set:\noj_model2 <-\n  svm_rbf(cost=2.378414, rbf_sigma=0.003162278) %>% \n  set_mode('classification') %>% \n  fit(Purchase ~ ., data = external_validation_set)\n\naugment(oj_model2, external_validation_set) %>% \n  f_meas(Purchase, .pred_class)\nGood enough! F1-score ~ 0.86 is not bad and itâ€™s closer to what we expected from the cross-validation\nAlso, the ROC curve applied to the external validation set doesnâ€™t look too bad!\naugment(oj_model2, external_validation_set) %>% \n  roc_curve(Purchase, .pred_CH) %>% \n  autoplot()"
  },
  {
    "objectID": "weeks/week08/lab_advanced.html#q10-challenge",
    "href": "weeks/week08/lab_advanced.html#q10-challenge",
    "title": "âœ¨ Bonus Lab (Optional) - Cross-validation",
    "section": "Q10: Challenge",
    "text": "Q10: Challenge\nReplicate the same steps as in Q9 for different SVM kernels (linear, polynomial, etc.). Is it possible to fit a model that is better than the radial kernel,in terms of F1-score?\n\n\nSee a model solution\n\n\nLinear\nA few things are different:\n\nThe only parameter we can change is cost\nThe algorithm only works if we explicitly tell tidymodels that this SVM kernel can be found in the â€œkernlabâ€ package, hence the %>% set_engine(\"kernlab\") added to the model specification.\n\norange_linear_tuned <-\n  tune_grid(object = svm_linear(cost = tune()) %>% set_mode('classification') %>% set_engine(\"kernlab\") ,\n            preprocessor = recipe(Purchase ~ ., data = internal_validation_set),\n            resamples = vfold_cv(internal_validation_set, v = 10),\n            grid = grid_regular(cost(), levels = 5),\n            metrics = metric_set(f_meas),\n            control = control_grid(save_pred=TRUE))\norange_linear_tuned <- readRDS(\"RMarkdown/orange_linear_tuned.Rds\")\nResults do not vary too much from svm_rbf. As we long as cost is sufficiently high, F1-scores are pretty decent.\nplot_df <- collect_metrics(orange_linear_tuned)\nplot_df$cost <- factor(plot_df$cost)\n\nplot_df\n\n\nPolynomial\nA few things are different:\n\nWe have to tune cost, degree and scale_factor.\nSome combinations of parameters do not yield valid solutions. You might see a few errors in your console.\n\norange_polynomial_tuned <-\n  tune_grid(object = svm_poly(cost = tune(), degree=tune(), scale_factor=tune()) %>% set_mode('classification') ,\n            preprocessor = recipe(Purchase ~ ., data = internal_validation_set),\n            resamples = vfold_cv(internal_validation_set, v = 10),\n            grid = grid_regular(cost(), degree(), scale_factor(), levels = 5),\n            metrics = metric_set(f_meas),\n            control = control_grid(save_pred=TRUE))\nResults do not vary too much here either, but there is a â€œsweet spotâ€: if \nplot_df <- collect_metrics(orange_polynomial_tuned)\nplot_df$degree <- factor(plot_df$degree)\nplot_df$cost <- factor(plot_df$cost)\nplot_df$scale_factor <- factor(plot_df$scale_factor)\n\ng <-\n  (ggplot(plot_df, \n          aes(x=cost, y=scale_factor, size=mean, color=mean)) \n   + geom_point()\n   \n   + scale_radius(limits=c(0.7, 0.9), range=c(1, 4))\n   + scale_colour_viridis_c(limits=c(0.7, 0.9))\n   + scale_x_discrete(labels = function(x) format(as.numeric(x), digits=2, scientific=TRUE))\n   + scale_y_discrete(labels = function(x) format(as.numeric(x), digits=2, scientific=TRUE))\n   \n   + theme_bw()\n   + theme(axis.text.x = element_text(angle=90),\n           legend.position = \"bottom\",\n           legend.direction =\"horizontal\")\n   + labs(x=\"Cost\", y=\"Scale Factor\", title=\"Results are similar to svm_rbf\", subtitle=\"But we get better (larger) F1-scores when Scale Factor is large enough\")\n   \n   + facet_grid(~ degree, labeller = label_both)\n   )\ng\n\n\nExtra\nIt is possible to tune multiple models at the same time, instead of tune each algorithm at a time. Check Tuning and comparing models tutorial from tidymodels."
  },
  {
    "objectID": "weeks/week08/lab_solutions.html",
    "href": "weeks/week08/lab_solutions.html",
    "title": "âœ”ï¸ Week 08 - Lab Solutions",
    "section": "",
    "text": "This notebook contain solutions to Take-home exercises in Week 08 Lab."
  },
  {
    "objectID": "weeks/week08/lab_solutions.html#take-home-exercise-q1",
    "href": "weeks/week08/lab_solutions.html#take-home-exercise-q1",
    "title": "âœ”ï¸ Week 08 - Lab Solutions",
    "section": "ğŸ  Take-home exercise Q1:",
    "text": "ğŸ  Take-home exercise Q1:\nEdit the cell below modifying event_level from \"second\" to \"first\". Why do you get different results? What do you think is going on?\n\nğŸ’¡Tip: Read the documentation of f_meas to understand what event_level represents. (Type ?f_meas)\nğŸ’¡ Gold Tip: note the Levels of the factor variable called Purchase:\nplot_df$Purchase\n\nSolution:\n\nThe parameter event_level refers to the levels of the target variable:\n\n\nlevels(plot_df$Purchase)\n\n[1] \"CH\" \"MM\"\n\n\n\nFrom the above, we see that CH is the first level, and MM is the second level. Donâ€™t know what a level is? Take a look at Chapter 15 of R for Data Science to revisit the concept of factors (categorical variables).\nF1-score is always calculated in reference to one level. So, if we want to get the F1-score for the \"CH\" class, we use event_level=\"first\":\n\n\nplot_df %>% f_meas(Purchase, .pred_class, event_level=\"first\")\n\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 f_meas  binary         0.861\n\n\n\nSimilarly, if we want to calculate the F1-score for the \"MM\" class, we have to look at event_level=\"second\":\n\n\nplot_df %>% f_meas(Purchase, .pred_class, event_level=\"second\")\n\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 f_meas  binary         0.778"
  },
  {
    "objectID": "weeks/week08/lab_solutions.html#take-home-exercise-q2",
    "href": "weeks/week08/lab_solutions.html#take-home-exercise-q2",
    "title": "âœ”ï¸ Week 08 - Lab Solutions",
    "section": "ğŸ  Take-home exercise Q2:",
    "text": "ğŸ  Take-home exercise Q2:\nCreate a plot of the confusion matrix for the orange_tidymodel like we did in Step 1\n\nQ2. Solution\n\n\nCode\n# It's almost the same thing; only this time let's use the `.pred_class` column that was created when we augmented our data frame.\nconfusion_matrix <- table(expected=plot_df$Purchase, class_pred=plot_df$.pred_class)\n\n\nplot_confusion_matrix(as_tibble(confusion_matrix), \n                      target_col = \"expected\", \n                      prediction_col = \"class_pred\",\n                      \n                      # Customizing the plot\n                      add_normalized = TRUE,\n                      add_col_percentages = FALSE,\n                      add_row_percentages = FALSE,\n                      counts_col = \"n\",\n                      )\n\n\nWarning in plot_confusion_matrix(as_tibble(confusion_matrix), target_col =\n\"expected\", : 'ggimage' is missing. Will not plot arrows and zero-shading."
  },
  {
    "objectID": "weeks/week08/lab_solutions.html#take-home-exercise-q3",
    "href": "weeks/week08/lab_solutions.html#take-home-exercise-q3",
    "title": "âœ”ï¸ Week 08 - Lab Solutions",
    "section": "ğŸ  Take-home exercise Q3:",
    "text": "ğŸ  Take-home exercise Q3:\nCreate a plot of SVM decision space for the orange_tidymodel like we did in Step 1.\n\nQ3. Solution\n\nItâ€™s almost the same thing; only this time we use the augment() function and refer to .pred_class when creating the plot.\n\n\n\nCode\nsim.data <- crossing(LoyalCH   = seq(0,1,0.05), PriceDiff = seq(-1,1,0.1))\nsim.data <- augment(orange_tidymodel, sim.data)\n\n\ng <- (\n  plot_df %>%   \n    ggplot()\n  \n    # Tile the background of the plot with SVM predictions\n    + geom_tile(data = sim.data, aes(x=LoyalCH, y=PriceDiff, fill = .pred_class), alpha = 0.25)\n  \n    # Actual data\n    + geom_point(aes(x=LoyalCH, y=PriceDiff, colour = Purchase, shape = is_correct), size=2.5, stroke=0.95, alpha=0.7)\n  \n    # Define X and Os\n    + scale_shape_manual(values = c(4, 1))\n    \n    # (OPTIONAL) Customizing the colours and theme of the plot\n    + scale_x_continuous(labels=scales::percent)\n    + scale_colour_startrek()\n    + scale_fill_startrek()\n    + theme_minimal()\n    + theme(panel.grid = element_blank(), legend.position = 'bottom', plot.title = element_text(hjust = 0.5))\n    + labs(x = 'Customer brand loyalty for CH', y = 'Sale price of MM less sale price of CH', fill = 'Brand', colour = 'Brand', shape = 'Correct prediction?', title = sprintf('Overall Training Accuracy = %.2f %%', 100*(sum(plot_df$is_correct)/nrow(plot_df))))\n)\n\ng"
  },
  {
    "objectID": "weeks/week08/lab_solutions.html#take-home-exercise-q4",
    "href": "weeks/week08/lab_solutions.html#take-home-exercise-q4",
    "title": "âœ”ï¸ Week 08 - Lab Solutions",
    "section": "ğŸ  Take-home exercise Q4:",
    "text": "ğŸ  Take-home exercise Q4:\n\nRetrain the model in Step 3 for Smarket, this time using svm_linear instead of svm_rbf.\nReuse the code in Step 3.4 to replicate the plots and metric calculations for this new model.\nWhich model fits the target variable (Today) better?\n\n\nQ4. Solution\n\nTrain the model\n\n\n\nCode\n# Remove Direction, otherwise we would be \"cheating\" \nfiltered_data <- ISLR2::Smarket %>% select(Today, Volume, Lag1)\n\nalternative_svm <-\n  svm_linear() %>% \n  set_mode('regression') %>% \n  fit(Today ~ ., data = filtered_data)\n\nalternative_svm\n\n\nparsnip model object\n\n$TypeDetail\n[1] \"L2-regularized L2-loss support vector regression primal (L2R_L2LOSS_SVR)\"\n\n$Type\n[1] 11\n\n$W\n         Volume       Lag1        Bias\n[1,] 0.05000429 -0.0249507 -0.07358462\n\n$Bias\n[1] 1\n\n$NbClass\n[1] 2\n\nattr(,\"class\")\n[1] \"LiblineaR\"\n\n\n\nCreate a plot_df data frame to use when plotting residuals.\n\n\n\nCode\nplot_df <- \n  augment(alternative_svm, filtered_data) %>% \n  mutate(row_number=row_number()) # adding this here just to make our plot easier\nplot_df\n\n\n# A tibble: 1,250 Ã— 6\n    Today Volume   Lag1    .pred  .resid row_number\n    <dbl>  <dbl>  <dbl>    <dbl>   <dbl>      <int>\n 1  0.959   1.19  0.381 -0.0235   0.983           1\n 2  1.03    1.30  0.959 -0.0327   1.06            2\n 3 -0.623   1.41  1.03  -0.0288  -0.594           3\n 4  0.614   1.28 -0.623  0.00577  0.608           4\n 5  0.213   1.21  0.614 -0.0286   0.242           5\n 6  1.39    1.35  0.213 -0.0114   1.40            6\n 7 -0.403   1.44  1.39  -0.0361  -0.367           7\n 8  0.027   1.41 -0.403  0.00687  0.0201          8\n 9  1.30    1.16  0.027 -0.0161   1.32            9\n10  0.287   1.23  1.30  -0.0445   0.331          10\n# â€¦ with 1,240 more rows\n\n\n\nPlot the residuals:\n\n\n\nCode\ng <- (\n  ggplot(plot_df, aes(x=row_number, y=.resid))\n  + geom_point(alpha=0.6)\n  \n  + theme_bw()\n  + geom_hline(yintercept = c(-2,2), color=\"red\", linetype=\"dashed\")\n  + labs(title=\"Distribution of residuals (the closer to zero the better)\")\n)\n\ng\n\n\n\n\n\n\nIs this model better? Itâ€™s not easy to say from the residuals plot alone, so letâ€™s calculate the metrics:\n\n\nplot_df %>% mae(Today, .pred)\n\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 mae     standard       0.828\n\n\n\nplot_df %>% rmse(Today, .pred)\n\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.14\n\n\n\nThe linear SVM had a slightly worse fit than the SVM with Radial Basis Function (Gaussian) we ran earlier. Both MAE and RMSE were smaller in the svm_rbf results:\n\n\n\n\n\nMAE\nRMSE\n\n\n\n\nsvm_rbf\n0.789891\n1.09561\n\n\nsvm_linear\n0.82845\n1.135357"
  },
  {
    "objectID": "weeks/week08/lecture.html",
    "href": "weeks/week08/lecture.html",
    "title": "ğŸ‘¨â€ğŸ« Week 08 - Lecture",
    "section": "",
    "text": "Either click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides. For a PDF copy, check Moodle."
  },
  {
    "objectID": "weeks/week08/lecture.html#coffee-break-10-min",
    "href": "weeks/week08/lecture.html#coffee-break-10-min",
    "title": "ğŸ‘¨â€ğŸ« Week 08 - Lecture",
    "section": "â˜• Coffee Break (10 min)",
    "text": "â˜• Coffee Break (10 min)\nUse this time to chat, stretch, drink some coffee or just relax for a bit by yourself."
  },
  {
    "objectID": "weeks/week08/lecture.html#part-ii---your-summative-45-50-min",
    "href": "weeks/week08/lecture.html#part-ii---your-summative-45-50-min",
    "title": "ğŸ‘¨â€ğŸ« Week 08 - Lecture",
    "section": "Part II - Your Summative (45-50 min)",
    "text": "Part II - Your Summative (45-50 min)\n\nI will explain what you will be asked to do.\nSummative instructions will be sent after the last lab on Friday 18 November 2022"
  },
  {
    "objectID": "weeks/week09/checklist.html",
    "href": "weeks/week09/checklist.html",
    "title": "âœ… Week 09 - Checklist",
    "section": "",
    "text": "Important\n\n\n\n\n\nğŸ—£ï¸ Your feedback: you can access a summary of the survey we ran about the Summative 01 on this link.\nThanks for sharing your thoughts, worries and ideas. I took care to consider your concerns prior to the release of the Summative 02.\nComprehension Check\nBy the end of the week, you should be able to:\nTime Management Tips\nHere is a suggestion of how to program your week in relation to this course:"
  },
  {
    "objectID": "weeks/week09/checklist.html#if-your-lab-is-on-monday",
    "href": "weeks/week09/checklist.html#if-your-lab-is-on-monday",
    "title": "âœ… Week 09 - Checklist",
    "section": "If your lab is on Monday:",
    "text": "If your lab is on Monday:\nOn Monday:\n\nğŸ“¥ Download: Before or once you arrive at the classroom, download the DS202_2022MT_w09_lab_rmark.Rmd file that contains the lab roadmap (under ğŸ—“ï¸ Week 09 section on Moodle). Or browse the webpage version here.\nğŸ’» Participate: Actively engage with the material in the lab. Ask your class teacher for help if anything is unclear. Work with others whenever possible and take notes of theoretical concepts or practical coding skill you might want to revisit later in the week.\nğŸ“™ Read: Find some time to read (James et al. 2021, sec. 12.4.1) and reinforce your theoretical understanding of K-Means Clustering; it is a very short section.\n\nAs you go through the text, try to connect what you read to the things you heard about in the W05 lecture or the examples you explored in the lab.\n\n\nTuesday to Thursday\n\nâœï¸ Solve: the Summative Problem Set 02.\n\nThe deadline is Tuesday, 29 November 2022, 11:59 PM but better not to leave it to the last minute!\nIf you finish sooner, you will have more time to ask questions and you will have more â€œmental bandwidthâ€ to learn about text mining applications later this week.\n\nğŸ“Ÿ Study group: Talk to your colleagues on Slack or whatsapp and try to join or organize a study group to work on the summative problem set together.\n\nFriday\n\nğŸ« Attend the lecture: This week, Prof.Â Ken Benoit is confirmed to come and deliver a talk Applications: Text as Data & Topic Modelling.\nğŸ  Solve the take-home exercises: There are four take-home exercises in the W09 lab.\n\nAny time\n\nğŸ“Ÿ You know the drill. Share your questions on the #week09 channel in our Slack group.\nğŸ‘‚Want to talk to someone else about this course? Try reaching out to your course representatives, @Zhang Ruishan (Yoyo) or @Rachitha Raghuram."
  },
  {
    "objectID": "weeks/week09/checklist.html#if-your-lab-is-on-friday",
    "href": "weeks/week09/checklist.html#if-your-lab-is-on-friday",
    "title": "âœ… Week 09 - Checklist",
    "section": "If your lab is on Friday:",
    "text": "If your lab is on Friday:\nMonday - Thursday:\n\nâœï¸ Solve: the Summative Problem Set 02.\n\nThe deadline is Tuesday, 29 November 2022, 11:59 PM but better not to leave it to the last minute!\nIf you finish sooner, you will have more time to ask questions and you will have more â€œmental bandwidthâ€ to learn about text mining applications later this week.\n\nğŸ“Ÿ Study group: Talk to your colleagues on Slack or whatsapp and try to join or organize a study group to work on the summative problem set together.\n\nFriday\n\nğŸ“¥ Download: Before or once you arrive at the classroom, download the DS202_2022MT_w09_lab_rmark.Rmd file that contains the lab roadmap (under ğŸ—“ï¸ Week 09 section on Moodle). Or browse the webpage version here.\nğŸ’» Participate: Actively engage with the material in the lab. Ask your class teacher for help if anything is unclear. Work with others whenever possible and take notes of theoretical concepts or practical coding skill you might want to revisit later in the week.\nğŸ« Attend the lecture: This week, Prof.Â Ken Benoit is confirmed to come and deliver a talk Applications: Text as Data & Topic Modelling.\nğŸ“™ Read: Find some time to read (James et al. 2021, sec. 12.4.1) and reinforce your theoretical understanding of K-Means Clustering; it is a very short section.\n\nAs you go through the text, try to connect what you read to the things you heard about in the W05 lecture or the examples you explored in the lab.\n\nğŸ  Solve the take-home exercises: There are four take-home exercises in the W09 lab.\n\nAny time\n\nğŸ“Ÿ You know the drill. Share your questions on the #week08 channel in our Slack group.\nğŸ‘‚Want to talk to someone else about this course? Try reaching out to your course representatives, @Zhang Ruishan (Yoyo) or @Rachitha Raghuram."
  },
  {
    "objectID": "weeks/week09/lab.html",
    "href": "weeks/week09/lab.html",
    "title": "ğŸ’» Week 09 - Lab Roadmap (90 min)",
    "section": "",
    "text": "ğŸ’¡ Some of you have mentioned that your R version cannot handle tidymodels. We recommend you update R to version 4.2.0 or above.\n\n\nlibrary('tidyverse')   # to use things like the pipe (%>%)\nlibrary('tidymodels')  # for model tuning, cross-validation etc.\n\n# Vanity packages:\nlibrary('GGally')      # for pretty correlation plot"
  },
  {
    "objectID": "weeks/week09/lab.html#step-1.1.-explore-the-data",
    "href": "weeks/week09/lab.html#step-1.1.-explore-the-data",
    "title": "ğŸ’» Week 09 - Lab Roadmap (90 min)",
    "section": "Step 1.1. Explore the data",
    "text": "Step 1.1. Explore the data\n\nLoad the Data\nDownload the dataset from Moodle and place it in the same directory of this RMarkdown file. Run the cell below to read it as a data frame:\ndf <- read_csv(\"colonial_panel.csv\")\n\n\nHow many rows and columns are there in the dataset?\ndim(df)\n\n\nWhat are the columns?\ncolnames(df)\n\n\nSelect columns of interest\nWe will focus on a few columns and use distinct() to drop duplicated rows:\ndf_filtered <- \n  df %>% \n  select(cname, year_colonized, year_indep, first_elect_year) %>% \n  distinct(cname, year_colonized, year_indep, first_elect_year, .keep_all = TRUE)\nğŸ’¡: The column first_elect_year represents the first elected representative body in British territories colonized before 1800.\nğŸ¯: ACTION POINT: Type View(df_filtered) in the R Console to visualize our data of interest. Locate the missing values present in this data frame. What do they mean? How would deal with those?\n\nyour text here"
  },
  {
    "objectID": "weeks/week09/lab.html#step-1.2.-lets-create-ggpairs-plots",
    "href": "weeks/week09/lab.html#step-1.2.-lets-create-ggpairs-plots",
    "title": "ğŸ’» Week 09 - Lab Roadmap (90 min)",
    "section": "Step 1.2. Letâ€™s create ggpairs plots",
    "text": "Step 1.2. Letâ€™s create ggpairs plots\nğŸ¤ WORKING TOGETHER. We will split the class into three groups. Each group will have to explain one row of the pairs plot.\n\nGroup 01: Describe the first row\nGroup 02: Describe the second row\nGroup 03: Describe the third row\n\nggpairs(df_filtered %>% select(-cname), progress=FALSE) + theme_bw()\n\nyour text here\n\nğŸ—£ï¸: CLASSROOM DISCUSSIONS:\n\nWhat are our key takeaways of the plot above?\nWhen we run the code above, we see a few messages about warnings. What do they tell us?\n\n\nyour text here"
  },
  {
    "objectID": "weeks/week09/lab.html#step-1.3.-plot-year_indep-x-year_colonized",
    "href": "weeks/week09/lab.html#step-1.3.-plot-year_indep-x-year_colonized",
    "title": "ğŸ’» Week 09 - Lab Roadmap (90 min)",
    "section": "Step 1.3. Plot year_indep x year_colonized",
    "text": "Step 1.3. Plot year_indep x year_colonized\nplot_df <- df_filtered %>% select(cname, year_indep, year_colonized) %>% na.omit()\n\ng <- (\n  ggplot(plot_df, aes(x=year_indep, y=year_colonized)) \n  \n  + geom_point(alpha=0.4, size=3) \n  \n  + theme_bw()\n  + labs(x=\"Year of independence\", y=\"Year colonized\", \n         title=\"Do you spot any clusters?\")\n)\n\ng\nLetâ€™s highlight an area of the chart\nWe use the function annotate from ggplot and define the boundaries (xmin, xmax, ymin, ymax) on top of the plot we created above\ng + annotate(\"rect\", alpha=0.1,\n             color=\"red\", fill=\"red\", \n             xmin=1930, xmax=2000, \n             ymin=1600, ymax=1700)\nIn case you want to ZOOM IN ğŸ”â€¦\nYou can use limits=... of the scale_x_continuous and scale_y_continuous to control the limits of the X & Y axis of the plot, plus geom_text to plot the name of the countries instead of the circles:\n\ng2 <- (\n  ggplot(df_filtered, aes(x=year_indep, y=year_colonized, label=cname)) \n  \n  + geom_label(size=3) \n  \n  + theme_bw()\n  + labs(x=\"Year of independence\", y=\"Year colonized\")\n  + scale_y_continuous(breaks=seq(1600, 1680, 10), limits=c(1600, 1680))\n  + scale_x_continuous(breaks=seq(1930, 2000, 5), limits=c(1930, 2000))\n  + annotate(\"rect\", color=\"red\", alpha=0,\n             xmin=1930, xmax=2000, ymin=1600, ymax=1680)\n)\n\ng2\nğŸ¯: ACTION POINT: Reading the plot. When was Bahamas colonized and when did it get independent?\n\nyour text here"
  },
  {
    "objectID": "weeks/week09/lab.html#step-1.4.-find-your-own-clusters",
    "href": "weeks/week09/lab.html#step-1.4.-find-your-own-clusters",
    "title": "ğŸ’» Week 09 - Lab Roadmap (90 min)",
    "section": "Step 1.4. Find your own clusters!",
    "text": "Step 1.4. Find your own clusters!\nğŸ¤ WORKING TOGETHER. In groups or in pairs, you are now asked to manually cluster the data in the original graph g. Use what you just learned above about annotate and add several rectangles to the graph g. Give each rectangle a unique colour1.\n# This is just a template of three rectangles\n(g \n  + annotate(geom=\"rect\", alpha=0.1, fill=\"...\", colour=\"...\", \n             xmin=..., xmax=..., ymin=..., ymax=...)\n  + annotate(geom=\"rect\", alpha=0.1, fill=\"...\", colour=\"...\", \n             xmin=..., xmax=..., ymin=..., ymax=...)\n  + annotate(geom=\"rect\", alpha=0.1, fill=\"...\", colour=\"...\", \n             xmin=..., xmax=..., ymin=..., ymax=...)\n)\nğŸ—£ï¸: CLASSROOM DISCUSSIONS:\n\nCompare how many rectangles other people used in their own plots.\nHow are your rectangles/clusters different to others?"
  },
  {
    "objectID": "weeks/week09/lab.html#step-2.1.-initial-k-centroids",
    "href": "weeks/week09/lab.html#step-2.1.-initial-k-centroids",
    "title": "ğŸ’» Week 09 - Lab Roadmap (90 min)",
    "section": "Step 2.1. Initial k centroids",
    "text": "Step 2.1. Initial k centroids\nLetâ€™s run the k-means but only for ONE iteration. That is, after k-means adjusted the random centroids for the first time.\n# Say we believe there are three clusters\nk <- 10\nset.seed(10)\n\nmodel <- kmeans(plot_df %>% select(year_indep, year_colonized), \n                centers=k, \n                iter.max=1)\nmodel\nğŸ¯: ACTION POINT: Run the chunk of code below. What do you think each row represents?\n\nyour text here\n\nmodel$centers\nVisualize the centroids\ndf_centroids <- as_tibble(model$centers)\ndf_centroids$cluster_id <- factor(seq(1, nrow(df_centroids)))\n\ng3 <- (\n  ggplot(plot_df, aes(x=year_indep, y=year_colonized))\n  \n  + geom_text(aes(label=cname), alpha=0.2, size=3) \n  \n  + geom_point(data=df_centroids, aes(color=cluster_id), size=4, shape=\"X\") \n  \n  + theme_bw()\n  + labs(x=\"Year of independence\", y=\"Year colonized\", \n         title=\"Visualize the k initial centroids\")\n)\n\ng3\nğŸ—£ï¸: CLASSROOM DISCUSSIONS:\n\nLocate India, Cuba and Egypt in the plot above. Considering the centroids, which clusters do you expect will belong to?"
  },
  {
    "objectID": "weeks/week09/lab.html#step-2.2-colour-data-points-according-to-their-clusters",
    "href": "weeks/week09/lab.html#step-2.2-colour-data-points-according-to-their-clusters",
    "title": "ğŸ’» Week 09 - Lab Roadmap (90 min)",
    "section": "Step 2.2 Colour data points according to their clusters",
    "text": "Step 2.2 Colour data points according to their clusters\nUsing tidymodels, we can include the clusters in our data frame by augmenting it:\nplot_df <- augment(model, plot_df)\ntail(plot_df)\nAnd we can use the new column .cluster to colour the dots:\ng4 <- (\n  ggplot(plot_df, aes(x=year_indep, y=year_colonized, colour=.cluster)) \n  \n  + geom_point(alpha=0.1, size=3)\n  # Or use geom_text\n  #+ geom_text(aes(label=cname), alpha=0.25, size=3) \n  \n  + geom_point(data=df_centroids, aes(color=cluster_id), size=4, shape=\"X\")\n  \n  + theme_bw()\n  + labs(x=\"Year of independence\", y=\"Year colonized\")\n)\n\ng4"
  },
  {
    "objectID": "weeks/week09/lab.html#step-2.3.-run-k-means-fully",
    "href": "weeks/week09/lab.html#step-2.3.-run-k-means-fully",
    "title": "ğŸ’» Week 09 - Lab Roadmap (90 min)",
    "section": "Step 2.3. Run K-Means fully",
    "text": "Step 2.3. Run K-Means fully\nNow, letâ€™s allow k-means to run fully, not just for one single iteration.\nk <- 10\nset.seed(10)\n\nmodel_full <- kmeans(plot_df %>% select(year_indep, year_colonized), \n                centers=k, trace=TRUE)\nplot_df <- augment(model_full, plot_df)\n\ndf_centroids <- as_tibble(model_full$centers)\ndf_centroids$cluster_id <- factor(seq(1, nrow(df_centroids)))\n\ng5 <- (\n  ggplot(plot_df, aes(x=year_indep, y=year_colonized, colour=.cluster)) \n  \n  + geom_point(alpha=0.1, size=3)\n  # Or use geom_text\n  #+ geom_text(aes(label=cname), alpha=0.25, size=3) \n  \n  + geom_point(data=df_centroids, aes(color=cluster_id), size=4, shape=\"X\")\n  \n  + theme_bw()\n  + labs(x=\"Year of independence\", y=\"Year colonized\")\n)\n\ng5\nğŸ¯: ACTION POINT: We set the trace=TRUE parameter to see what the algorithm is doing. That is why, in addition to the plot, you also got a few messages printed out. What do you think this printed messages tell us?\n\nyour text here\n\nNow, look at the centroids of this last model:\nmodel_full$centers\nğŸ¯: ACTION POINT: Compare the centroids above to the ones you got in Step 2.1. How are they similar/different?\n\nyour text here"
  },
  {
    "objectID": "weeks/week09/lab.html#step-2.4-what-is-the-best-number-of-clusters",
    "href": "weeks/week09/lab.html#step-2.4-what-is-the-best-number-of-clusters",
    "title": "ğŸ’» Week 09 - Lab Roadmap (90 min)",
    "section": "Step 2.4 What is the best number of clusters?",
    "text": "Step 2.4 What is the best number of clusters?\nIs it k=3, k=10, k=20??? Unfortunately, there is no absolute final objective solution to this question. It usually depends on what you want to do with this data. Do you want to investigate and compare the characteristics of a large number of groups or just a few?\nBut here we will show you one possible strategy to automate the process of finding an â€œoptimalâ€ number of clusters based on the total within-cluster sum of squares, which we are going to refer to as $tot.withinss.\n(Your class teacher might need to use the blackboard here)\nHow?\nThe code below is a bit tricky so letâ€™s go through it step-by-step.\n\nWe subset the data using select and we get rid of the NAs\nWe then create a list column called data.\n\nTibbles are different from data frames as you can nest an entire object (whether a data frame, model object etc.) into a single cell.\nWhile it may seem like an odd thing to do, look at the next step.\n\nRemember crossing? We use crossing to come up with a new tibble of all possible combinations between n.clust which goes from 1 to 20 and data.\nBecause of the above, we now have the ability to run 20 k-means clustering models using one tibble, and we did not need to write a single for loop!\nBecause we are looping over a list column, we use different varieties of the map function (which is part of the tidyverse ecosystem).\n\nThe map which applies a function over the list column, and returns back an object of the same length.\nWe use map2 as we want to loop over data and n.clust.\n\nThe kmeans function takes two parameters to run, the first of which specifies the data, while the second specifies the number of clusters or centers to include in the model.\nFinally, we want to find the total within-cluster sum of squares tot.withinss. - We do this by using map_dbl on the k.means list column which returns a vector of doubles.\nTo access tot.withinss, we use glance on k.means and include $tot.withinss.\n\nkm.models <-\n  df_filtered %>% select(year_colonized, year_indep) %>% na.omit() %>% \n  nest(data = everything()) %>% \n  crossing(n.clust = 1:20) %>% \n  mutate(k.means = map2(.x = data,\n                        .y = n.clust, \n                        .f = ~ kmeans(.x, centers = .y)),\n         tot.withinss = map_dbl(.x = k.means, .f = ~ glance(.x)$tot.withinss)) \nkm.models\n\nCreate an elbow plot\nLetâ€™s create an elbow plot to evaluate what number of clusters is most appropriate.\nGenerally, we are looking to identify significant elbow points beyond which no significant decreases in TWCSS occur.\nFor our data, k=6 or k=10 are probably among the most appropriate. But do you see how that is a little bit subjective?\nkm.models %>% \n  ggplot(aes(n.clust, tot.withinss)) +\n  geom_line(linetype = 'dashed') +\n  geom_point() +\n  theme_minimal() +\n  theme(panel.grid.minor = element_blank()) +\n  scale_x_continuous(breaks = 1:20) +\n  scale_y_continuous(labels = comma) +\n  labs(x = 'Number of clusters', y = 'Total within cluster sum of squares')"
  },
  {
    "objectID": "weeks/week09/lab.html#q1-cluster-of-penguins",
    "href": "weeks/week09/lab.html#q1-cluster-of-penguins",
    "title": "ğŸ’» Week 09 - Lab Roadmap (90 min)",
    "section": "Q1: Cluster of penguins",
    "text": "Q1: Cluster of penguins\nLoad the palmerpenguins dataset and filter the data to omit missing values. Ignoring the columns species and year, run k-means on this data with \\(k=3\\).\nHow do the clusters identified by k-means compare to the species of penguins?"
  },
  {
    "objectID": "weeks/week09/lab.html#q2-elbow-plot",
    "href": "weeks/week09/lab.html#q2-elbow-plot",
    "title": "ğŸ’» Week 09 - Lab Roadmap (90 min)",
    "section": "Q2: Elbow plot",
    "text": "Q2: Elbow plot\nRun k-means on the same data, but this time vary k from \\(\\{1, 2, \\ldots, 19, 20\\}\\) and generate an elbow plot. According to the plot, what is a â€œgoodâ€ number of clusters, k."
  },
  {
    "objectID": "weeks/week09/lecture.html",
    "href": "weeks/week09/lecture.html",
    "title": "ğŸ‘¨â€ğŸ« Week 09 - Lecture",
    "section": "",
    "text": "This week, Prof.Â Ken Benoit will deliver a talk about Applications: Text as Data & Topic Modelling.\nWhen and where: 25 November 2022 from 2 p.m. to 4 p.m. at NAB.LG.08.\n\nSpeaker\n\n\n\nProf.Â Ken Benoit  Director of the Data Science Institute  Professor of Computational Social Science in the Department of Methodology LSE Data Science Institute"
  },
  {
    "objectID": "weeks/week10/bonus.html",
    "href": "weeks/week10/bonus.html",
    "title": "âœ¨ Intro to tidymodels recipes",
    "section": "",
    "text": "The following was didnâ€™t make it to this weekâ€™s lab roadmap; we would easily run out of time. So, I suggest you include in your studies of the week!"
  },
  {
    "objectID": "weeks/week10/bonus.html#what-are-recipes",
    "href": "weeks/week10/bonus.html#what-are-recipes",
    "title": "âœ¨ Intro to tidymodels recipes",
    "section": "What are recipes?",
    "text": "What are recipes?\nAmong the packages included with tidymodels, there is one called recipes. This package lets us indicate which role that the columns of our data frame play in our supervised or unsupervised models. Not only that, it lets us reuse the same formula, say for cross-validation or any other use. Check out the Introduction to recipes tutorial to learn more.\nLetâ€™s look at the supervised model case first (even though today is about an unsupervised technique).\n\nA recipe for supervised models\nRemember the Smarket dataset from W08? There, we used Volume and Lag1 to predict Today and we always represented this with an R Formula like:\nToday ~ Volume + Lag1\nor, simply:\nToday ~ .\nThat is, the target variable comes first, then we have the ~ symbol to represent what we should regress it on.\nTo represent this same idea using recipe, we simply do this:\n\nrecipe_obj <- recipe(Today ~ .,\n                     data = ISLR2::Smarket %>% select(Today, Volume, Lag1))\n\nprint(recipe_obj)\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          2\n\n\nğŸ¯ ACTION POINT: What are the variables listed under each role in the recipe object above?"
  },
  {
    "objectID": "weeks/week10/bonus.html#step-change-roles",
    "href": "weeks/week10/bonus.html#step-change-roles",
    "title": "âœ¨ Intro to tidymodels recipes",
    "section": "Step Change roles",
    "text": "Step Change roles\nYou might also remember that the Smarket dataset had more than just the variables above:\n\nISLR2::Smarket %>% colnames()\n\n[1] \"Year\"      \"Lag1\"      \"Lag2\"      \"Lag3\"      \"Lag4\"      \"Lag5\"     \n[7] \"Volume\"    \"Today\"     \"Direction\"\n\n\nNote and remember that variables Today and Direction are redundant. They represent almost the same thing. If I were to use all variables to predict the variable Today, I would have to discard Direction.\nğŸ¯ ACTION POINT: How would you write a recipe to predict Today using all variables of the SMarket, except Direction?\n\nFunction update_role()\nFrom recipeâ€™s documentation:\n\nroles define how variables will be used in the model. Examples are: predictor (independent variables), response, and case weight. This is meant to be open-ended and extensible.\n\nWe can use update_role() to change the role of a column, and we can name this role whatever we want basically. See for example an alternative for the problem of redundant variables in SMarket:\n\nrecipe(Today ~ .,data = ISLR2::Smarket) %>% update_role(Direction, new_role='redundant')\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          7\n redundant          1\n\n\nWe marked Direction as role=\"redudant\", and since this is not a standard role (â€œpredictorâ€, â€œoutcomeâ€, etc.), this variable will not be used in any algorithm."
  },
  {
    "objectID": "weeks/week10/bonus.html#unsupervised-case",
    "href": "weeks/week10/bonus.html#unsupervised-case",
    "title": "âœ¨ Intro to tidymodels recipes",
    "section": "Unsupervised case",
    "text": "Unsupervised case\nLetâ€™s go back to our dataset. We donâ€™t have a \\(\\mathbf{Y}\\) variable to predict, we care only about the features/predictors, \\(\\mathbf{X}\\). How would we represent a recipe for an unsupervised model?\nItâ€™s simple. We simply leave everything before ~ empty in our R formula representation:\n~ <var1> + <var2> + ...\nIn a dataset, called df:\nrecipe(~ ., data = df_preprocessed)\nDoes the above make sense to you? Do you see why we donâ€™t have an outcome role?\nğŸ¯ ACTION POINT: Write a recipe for df_preprocessed and then change the role of period and country_name to role=\"id\":"
  },
  {
    "objectID": "weeks/week10/checklist.html",
    "href": "weeks/week10/checklist.html",
    "title": "âœ… Week 10 - Checklist",
    "section": "",
    "text": "Comprehension Check\nBy the end of the week, you should be able to:\nTime Management Tips\nHere is a suggestion of how to program your week in relation to this course:"
  },
  {
    "objectID": "weeks/week10/checklist.html#if-your-lab-is-on-monday",
    "href": "weeks/week10/checklist.html#if-your-lab-is-on-monday",
    "title": "âœ… Week 10 - Checklist",
    "section": "If your lab is on Monday:",
    "text": "If your lab is on Monday:\nOn Monday:\n\nğŸ“¥ Download: Before or once you arrive at the classroom, download the DS202_2022MT_w10_lab_rmark.Rmd file that contains the lab roadmap (under ğŸ—“ï¸ Week 10 section on Moodle). Or browse the webpage version here.\nğŸ’» Participate: Actively engage with the material in the lab. Ask your class teacher for help if anything is unclear. Work with others whenever possible and take notes of theoretical concepts or practical coding skill you might want to revisit later in the week.\nğŸ¤– Drop-in Session: If you have any final questions about the Summative Problem Set 02, attend the Drop-in Session at CBG 2.05 on Monday 28 November, 2pm-4pm.\n\nTuesday to Thursday\n\nğŸ“™ Read: Find some time to read (James et al. 2021, sec. 12.2) and reinforce your theoretical understanding of Principal Component Analysis; it is a very short section.\nğŸ  Practice the tutorials: Dedicate some time to learn about recipes (W10 bonus content) and to practice the tutorials linked on W10 lab.\n\nHere is another content you might find useful: Dimensionality Reduction from the Tidy Modelling with R book.\nPractice group_by & summarise: R for Data Science | 5 Data transformation\nPractice reshaping data when needed: R for Data Science | 12 Tidy Data)\n\nğŸ“º Watch short videos: During the week, I will release short videos further explaining Principal Component Analysis. Try to reserve some time to watch those.\n\nFriday\n\nğŸ« Attend the lecture: This week, Dr.Â Stuart Bramwell will join us on the second half of the lecture to talk about his research.\nâœï¸ Get ready for the Summative 03: the final problem set, Summative Problem Set 03, will be released this Friday 2 December.\n\nThe deadline is Thursday, 15 December 2022, 11:59 PM but better not to leave it to the last minute!\nHow to practice: W09 & W10 labs + the tutorials linked in the ğŸ  Practice the tutorials section.\n\n\nAny time\n\nğŸ“Ÿ You know the drill. Share your questions on the #week10 channel in our Slack group.\nğŸ‘‚Want to talk to someone else about this course? Try reaching out to your course representatives, @Zhang Ruishan (Yoyo) or @Rachitha Raghuram."
  },
  {
    "objectID": "weeks/week10/checklist.html#if-your-lab-is-on-friday",
    "href": "weeks/week10/checklist.html#if-your-lab-is-on-friday",
    "title": "âœ… Week 10 - Checklist",
    "section": "If your lab is on Friday:",
    "text": "If your lab is on Friday:\nMonday\n\nğŸ¤– Drop-in Session: If you have any final questions about the Summative Problem Set 02, attend the Drop-in Session at CBG 2.05 on Monday 28 November, 2pm-4pm.\n\nMonday - Thursday:\n\nğŸ“™ Read: Find some time to read (James et al. 2021, sec. 12.2) and reinforce your theoretical understanding of Principal Component Analysis; it is a very short section.\nğŸ“º Watch short videos: During the week, I will release short videos further explaining Principal Component Analysis. Try to reserve some time to watch those.\n\nFriday\n\nğŸ“¥ Download: Before or once you arrive at the classroom, download the DS202_2022MT_w10_lab_rmark.Rmd file that contains the lab roadmap (under ğŸ—“ï¸ Week 10 section on Moodle). Or browse the webpage version here.\nğŸ’» Participate: Actively engage with the material in the lab. Ask your class teacher for help if anything is unclear. Work with others whenever possible and take notes of theoretical concepts or practical coding skill you might want to revisit later in the week.\nğŸ« Attend the lecture: This week, Dr.Â Stuart Bramwell will join us on the second half of the lecture to talk about his research.\nâœï¸ Get ready for the Summative 03: the final problem set, Summative Problem Set 03, will be released this Friday 2 December.\n\nThe deadline is Thursday, 15 December 2022, 11:59 PM but better not to leave it to the last minute!\nHow to practice: W09 & W10 labs + the tutorials linked in the ğŸ  Practice the tutorials section.\n\n\nSome time early next week\n\nğŸ  Practice the tutorials: Dedicate some time to learn about recipes (W10 bonus content) and to practice the tutorials linked on W10 lab.\n\nHere is another content you might find useful: Dimensionality Reduction from the Tidy Modelling with R book.\nPractice group_by & summarise: R for Data Science | 5 Data transformation\nPractice reshaping data when needed: R for Data Science | 12 Tidy Data)\n\n\nAny time\n\nğŸ“Ÿ You know the drill. Share your questions on the #week10 channel in our Slack group.\nğŸ‘‚Want to talk to someone else about this course? Try reaching out to your course representatives, @Zhang Ruishan (Yoyo) or @Rachitha Raghuram."
  },
  {
    "objectID": "weeks/week10/lab.html",
    "href": "weeks/week10/lab.html",
    "title": "ğŸ’» Week 10 - Lab Roadmap (90 min)",
    "section": "",
    "text": "This week, we will demonstrate how to pre-process a dataset before running any ML model. After preprocessing, we will use Principal Component Analysis (PCA) to explore similarities and dissimilarities in the data."
  },
  {
    "objectID": "weeks/week10/lab.html#packages-you-will-need",
    "href": "weeks/week10/lab.html#packages-you-will-need",
    "title": "ğŸ’» Week 10 - Lab Roadmap (90 min)",
    "section": "Packages you will need",
    "text": "Packages you will need\n\nlibrary(tidyverse)   # to use things like the pipe (%>%)\nlibrary(tidymodels)  # for model tuning, cross-validation etc.\n\n# Vanity packages:\nlibrary(ggcorrplot)\nlibrary(ggsci)       # we like pretty colours"
  },
  {
    "objectID": "weeks/week10/lab.html#step-2.1.-add-a-period-column",
    "href": "weeks/week10/lab.html#step-2.1.-add-a-period-column",
    "title": "ğŸ’» Week 10 - Lab Roadmap (90 min)",
    "section": "Step 2.1. Add a period column",
    "text": "Step 2.1. Add a period column\nLetâ€™s aggregate data about the countries over a common 20-year time window period.\nWe use the arithmetic operation of integer division to make this possible here.\n\ntmp_df <- \n  df %>%\n  filter(year >= 1900, year < 2020) %>%\n  mutate(period=paste0(\"[\", 20 * (year %/% 20),\n                       \"-\", 20 * (year %/% 20) + 19, \"]\"))\n\n# Selecting these columns just to make it easier to visualize\ntmp_df %>% \n  select(country_name, year, period) %>%\n  head(n=40)\n\n# A tibble: 40 Ã— 3\n   country_name  year period     \n   <chr>        <dbl> <chr>      \n 1 Mexico        1900 [1900-1919]\n 2 Mexico        1901 [1900-1919]\n 3 Mexico        1902 [1900-1919]\n 4 Mexico        1903 [1900-1919]\n 5 Mexico        1904 [1900-1919]\n 6 Mexico        1905 [1900-1919]\n 7 Mexico        1906 [1900-1919]\n 8 Mexico        1907 [1900-1919]\n 9 Mexico        1908 [1900-1919]\n10 Mexico        1909 [1900-1919]\n# â€¦ with 30 more rows"
  },
  {
    "objectID": "weeks/week10/lab.html#step-2.2.-count-the-number-of-elections-per-election-type",
    "href": "weeks/week10/lab.html#step-2.2.-count-the-number-of-elections-per-election-type",
    "title": "ğŸ’» Week 10 - Lab Roadmap (90 min)",
    "section": "Step 2.2. Count the number of elections per election type",
    "text": "Step 2.2. Count the number of elections per election type\nRun the code below and type View(tmp_df):\n\ntmp_df <-\n  # The same pre-processing as before\n  df %>%\n  filter(year >= 1900, year < 2020) %>%\n  mutate(period=paste0(\"[\", 20 * (year %/% 20),\n                       \"-\", 20 * (year %/% 20) + 19, \"]\")) %>%\n  \n  # This bit is new \n  group_by(period, country_name) %>%\n  summarize(across(c(v2eltype_0, v2eltype_1, v2eltype_6, v2eltype_7),\n                   ~ sum(., na.rm=TRUE),\n                   .names = \"sum_{.col}\"),\n            .groups=\"keep\")\n\nğŸ¤ WORKING TOGETHER: Now, in groups or in pairs, can you explain why the dimensions our (new) temporary data frame tmp_df is different to the original data frame df?\n\ndim(tmp_df)\n\n[1] 982   6\n\n\nğŸ¤ WORKING TOGETHER: What do the columns of tmp_df represent?\n\nyour notes here"
  },
  {
    "objectID": "weeks/week10/lab.html#step-2.3.-take-the-average-of-suffrage-levels-voting-age-and-turnout-rate",
    "href": "weeks/week10/lab.html#step-2.3.-take-the-average-of-suffrage-levels-voting-age-and-turnout-rate",
    "title": "ğŸ’» Week 10 - Lab Roadmap (90 min)",
    "section": "Step 2.3. Take the average of suffrage levels, voting age and turnout rate",
    "text": "Step 2.3. Take the average of suffrage levels, voting age and turnout rate\nEach period can contain multiple elections so variables such as v2asuffrage, v2elage and v2eltrnout cannot be used as they are, we need to aggregate them somehow.\nLetâ€™s use the same combo of summary+across as before and create columns to contain the mean of these values:\n\ntmp_df <-\n  # The same pre-processing as before\n  df %>%\n  filter(year >= 1900, year < 2020) %>%\n  mutate(period=paste0(\"[\", 20 * (year %/% 20),\n                       \"-\", 20 * (year %/% 20) + 19, \"]\")) %>%\n  \n  # It's a different summarise!\n  group_by(period, country_name) %>%\n  summarize(across(c(v2asuffrage, v2elage, v2eltrnout),\n                   ~ mean(., na.rm=T),\n                   .names = \"mean_{.col}\"),\n            .groups=\"keep\")\n\nğŸ¤ WORKING TOGETHER: In groups or in pairs, take a look at the new tmp_df, pay close attention to the NAs in this data frame. Then, answer the following questions:\n\nHow would you handle these NAs? Should we remove all rows that contain NA or should we replace the NAs by some value (a process called [data imputation](https://www.wikiwand.com/en/Imputation_(statistics)?\n\n\nyour notes here\n\nğŸ—£ï¸ CLASSROOM DISCUSSIONS: Share your thoughts!"
  },
  {
    "objectID": "weeks/week10/lab.html#step-2.4.-winners-and-losers",
    "href": "weeks/week10/lab.html#step-2.4.-winners-and-losers",
    "title": "ğŸ’» Week 10 - Lab Roadmap (90 min)",
    "section": "Step 2.4. Winners and Losers",
    "text": "Step 2.4. Winners and Losers\nIn the original data, v2elaccept_ord == 4 when the losers of all national elections in a particular year conceded defeat, and v2elasmoff_ord == 2 when all winners assumed office according to the result of elections without any restrictions.\nOut of all elections that happens in a period, what percentage of winners fully assumed office and what percentage of losers fully conceded defeat?\n\ntmp_df <-\n  # The same pre-processing as before\n  df %>%\n  filter(year >= 1900, year < 2020) %>%\n  mutate(period=paste0(\"[\", 20 * (year %/% 20),\n                       \"-\", 20 * (year %/% 20) + 19, \"]\")) %>%\n  group_by(period, country_name) %>%\n  \n  # This bit is new!\n  summarise(pctg_v2elaccept_full=sum(v2elaccept_ord == 4, na.rm=T)/sum(!is.na(v2elaccept_ord)),\n            pctg_v2elasmoff_full=sum(v2elasmoff_ord == 2, na.rm=T)/sum(!is.na(v2elasmoff_ord)),\n            .groups=\"keep\") \n\nğŸ¯ ACTION POINT Once again, type View(tmp_df) on the R console and hit ENTER to visualise the data."
  },
  {
    "objectID": "weeks/week10/lab.html#step-2.5-putting-it-all-together",
    "href": "weeks/week10/lab.html#step-2.5-putting-it-all-together",
    "title": "ğŸ’» Week 10 - Lab Roadmap (90 min)",
    "section": "Step 2.5 Putting it all together",
    "text": "Step 2.5 Putting it all together\nThe code below combines what we have done in Steps 2.1-2.4 above. Run it and View(df_preprocessed) to see the output. We will use the is_all_NAs column to filter out the true NAs afterwards.\n\ndf_preprocessed <-\n  df %>%\n  filter(year >= 1900, year < 2020) %>%\n  mutate(period=paste0(\"[\", 20 * (year %/% 20),\n                       \"-\", 20 * (year %/% 20) + 19, \"]\")) %>%\n  \n  # It's a different summarise!\n  group_by(period, country_name) %>%\n  summarize(across(c(v2eltype_0, v2eltype_1, v2eltype_6, v2eltype_7),\n                   ~ sum(., na.rm=T),\n                   .names = \"sum_{.col}\"),\n            across(c(v2asuffrage, v2elage, v2eltrnout),\n                   ~ mean(., na.rm=T),\n                   .names = \"mean_{.col}\"),\n            pctg_v2elaccept_full=sum(v2elaccept_ord == 4, na.rm=T)/sum(!is.na(v2elaccept_ord)),\n            pctg_v2elasmoff_full=sum(v2elasmoff_ord == 2, na.rm=T)/sum(!is.na(v2elasmoff_ord)),\n            .groups=\"keep\") %>% \n    ungroup()\n\nHow many time periods are we talking about?\n\ndf_preprocessed %>% distinct(period) %>% as.list()\n\n$period\n[1] \"[1900-1919]\" \"[1920-1939]\" \"[1940-1959]\" \"[1960-1979]\" \"[1980-1999]\"\n[6] \"[2000-2019]\"\n\n\n\ndim(df_preprocessed)\n\n[1] 982  11"
  },
  {
    "objectID": "weeks/week10/lab.html#step-2.6.-handling-missing-data",
    "href": "weeks/week10/lab.html#step-2.6.-handling-missing-data",
    "title": "ğŸ’» Week 10 - Lab Roadmap (90 min)",
    "section": "Step 2.6. Handling missing data",
    "text": "Step 2.6. Handling missing data\nWhere are the NAs?\n\ndf_preprocessed %>% summarise(across(everything(),~ sum(is.na(.)))) %>% t()\n\n                     [,1]\nperiod                  0\ncountry_name            0\nsum_v2eltype_0          0\nsum_v2eltype_1          0\nsum_v2eltype_6          0\nsum_v2eltype_7          0\nmean_v2asuffrage        4\nmean_v2elage          317\nmean_v2eltrnout       380\npctg_v2elaccept_full  246\npctg_v2elasmoff_full  246\n\n\nHere we choose the â€œlazyâ€ path, letâ€™s just ignore the rows that contain any NA:\n\ndf_preprocessed <- df_preprocessed %>% na.omit()\n\n\ndim(df_preprocessed)\n\n[1] 583  11\n\n\nğŸ’¡ You might disagree with this strategy of handling missing data, and that is great! If you have an alternative strategy in mind, do share it with us on Slack! I would be curious to hear about it."
  },
  {
    "objectID": "weeks/week10/lab.html#step-3.1-training-pca",
    "href": "weeks/week10/lab.html#step-3.1-training-pca",
    "title": "ğŸ’» Week 10 - Lab Roadmap (90 min)",
    "section": "Step 3.1 Training PCA",
    "text": "Step 3.1 Training PCA\nHere we take a pragmatic approach. Letâ€™s try to understand the technique by comparing our current dataset to the new data that will be produced by PCA.\ndf_preprocessed has 11 columns, 9 are predictors whereas the other 2 columns are identifiers (country_name and period). Letâ€™s look at the correlation between the features:\n\ndf_preprocessed %>% \n  select(-country_name, -period) %>% \n  na.omit() %>% \n  cor() %>% \n  ggcorrplot(outline.col = \"white\", lab=TRUE, \n             type=\"lower\", lab_size=2.2, tl.cex=10)\n\n\n\n\nğŸ  TAKE-HOME ACTIVITY: This week, instead of coding exercises, we suggest you practice some more tidymodels. We will point to a few suggested official tutorials. In this section, we need to use something called recipes; read the Preprocess your data with recipes tutorial on tidymodelâ€™s website to learn more.\nNow, letâ€™s see how you could train PCA using the tidymodels package:\n\npca_recipe <-\n  # First we specify a recipe of what data we are using\n  recipe(~ ., data = df_preprocessed) %>%\n  \n  # Columns period and country_name are not to be used as predictors.\n  # That is why we update their roles in the recipe\n  update_role(period, new_role = 'id') %>% \n  update_role(country_name, new_role = 'id') %>% \n  \n  # PCA requires that data have the same distribution\n  # This can be achieved by normalizing the data (mean=0 and std=1)\n  step_normalize(all_predictors()) %>% \n  \n  # This is where we tell the recipe to run PCA and return 9 Principal Components\n  step_pca(all_predictors(), num_comp=9)\n\n# pca_recipe created a recipe, but it didn't run any of those steps.\n# To train the PCA, we have to prepare the recipe -- with prep()\npca_prep <- prep(pca_recipe)\n\nThe object pca_recipe contains a recipe and we can choose to prepare a recipe (prep()) at any time."
  },
  {
    "objectID": "weeks/week10/lab.html#step-3.2-look-at-the-new-data",
    "href": "weeks/week10/lab.html#step-3.2-look-at-the-new-data",
    "title": "ğŸ’» Week 10 - Lab Roadmap (90 min)",
    "section": "Step 3.2 Look at the new data",
    "text": "Step 3.2 Look at the new data\nNow that we have prepared our recipe, letâ€™s bake it using our ingredients (the data):\n\nnew_df <- bake(pca_prep, df_preprocessed)\n\nğŸ¯ ACTION POINT Once again, type View(new_df) on the R console and hit ENTER to visualise the new data frame.\nğŸ—£ï¸ CLASSROOM DISCUSSIONS: What do you think the columns in this data frame represent?\n\nHow are these variables distributed?\n\nplot_df <- pivot_longer(new_df, PC1:PC9)\nggplot(plot_df, aes(x=value, fill=name)) + geom_histogram() + theme_bw() + facet_wrap(~ name)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nCorrelation plot\nğŸ—£ï¸ CLASSROOM DISCUSSIONS: Observe the correlation plot below. Why do you think there is no correlation between any predictor?\n\nnew_df %>% \n  select(-country_name, -period) %>% \n  cor() %>% \n  ggcorrplot(outline.col = \"white\", lab=TRUE, \n             type=\"lower\", lab_size=2.2, tl.cex=10)\n\n\n\n\nğŸ’¡ If you create a ggpairs() plot, you will see what zero correlation means. Each pairwise combination of features produces a scatter plot that look essentially like a random cloud of points."
  },
  {
    "objectID": "weeks/week10/lab.html#step-3.3.-looking-closer",
    "href": "weeks/week10/lab.html#step-3.3.-looking-closer",
    "title": "ğŸ’» Week 10 - Lab Roadmap (90 min)",
    "section": "Step 3.3. Looking closerâ€¦",
    "text": "Step 3.3. Looking closerâ€¦\nThese properties of the new data set were produced by design!\n\nğŸ’¡ PCA â€œrearrangesâ€ the original data matrix, producing a new data matrix where all features are intentionally completely uncorrelated to each other.\n\nThis is the key takeaway of PCA. If you remember only one thing about this technique, remember the sentence above!\nFor example, this is how PC1 is constructed from the other predictors:\n\\[\n\\text{PC1} = + 0.21799702 \\times \\text{sum_v2eltype_0}^* + 0.24465538 \\times \\text{sum_v2eltype_1}^* +0.51687063 \\times \\text{sum_v2eltype_1}^* + 0.49560420  \\times \\text{sum_v2eltype_7}^* +0.44277858 \\times \\text{mean_v2asuffrage}^* - 0.36757212 \\times \\text{mean_v2elage}^*-0.11972783 \\times \\text{mean_v2eltrnout}^* + 0.07999905 \\times \\text{pctg_v2elaccept_full}^* +0.16716920 \\times \\text{pctg_v2elasmoff_full}^*\n\\]\nwhere we marked predictors with an asterisk just to remind us that these are not the raw data, but the normalised values of those features.\nThe equation above looks familiar, right? Indeed, the way PCA calculates these weights (they are called loadings) bares some similarity to how a linear regression problem is solved.\nWith the code below, we get to see how PCA calculated the other Principal Components.\n\npca_prep$steps[[2]]$res\n\nStandard deviations (1, .., p=9):\n[1] 1.4023368 1.3007266 1.0920261 0.9766697 0.9119316 0.8625327 0.8404571\n[8] 0.6997937 0.6507657\n\nRotation (n x k) = (9 x 9):\n                             PC1         PC2         PC3         PC4\nsum_v2eltype_0        0.21799702 -0.53291377 -0.22914212  0.13663944\nsum_v2eltype_1        0.24465538  0.16727953 -0.23620529 -0.73134871\nsum_v2eltype_6        0.51687063  0.22326016 -0.07746218  0.22722123\nsum_v2eltype_7        0.49560420  0.08295731 -0.19709763  0.26956753\nmean_v2asuffrage      0.44277858 -0.25885010  0.33984115 -0.02374631\nmean_v2elage         -0.36757212 -0.31040794 -0.42191240 -0.01312946\nmean_v2eltrnout      -0.11972783 -0.16864188  0.69229053  0.09535488\npctg_v2elaccept_full  0.07999905 -0.57603673 -0.16594779  0.09195054\npctg_v2elasmoff_full  0.16716920 -0.32814417  0.21574560 -0.55125208\n                              PC5         PC6         PC7         PC8\nsum_v2eltype_0        0.054907792 -0.49128600 -0.09356601 -0.41886831\nsum_v2eltype_1        0.539181349  0.04632277 -0.06176915 -0.15736453\nsum_v2eltype_6       -0.091587177 -0.23055309 -0.40810152 -0.14944996\nsum_v2eltype_7        0.005748874  0.56282577 -0.23848959  0.20309696\nmean_v2asuffrage      0.238040294 -0.28970122  0.28978589  0.62790608\nmean_v2elage          0.154719253 -0.06468444 -0.53412624  0.50688248\nmean_v2eltrnout       0.408344599  0.17219205 -0.47740314 -0.20914392\npctg_v2elaccept_full  0.135679655  0.50324045  0.31887335 -0.20027917\npctg_v2elasmoff_full -0.657336929  0.12471072 -0.25523550  0.03395968\n                              PC9\nsum_v2eltype_0        0.410757481\nsum_v2eltype_1        0.007488321\nsum_v2eltype_6       -0.612327279\nsum_v2eltype_7        0.470158352\nmean_v2asuffrage     -0.045342913\nmean_v2elage         -0.141465088\nmean_v2eltrnout       0.028567184\npctg_v2elaccept_full -0.460799723\npctg_v2elasmoff_full  0.002797427\n\n\nWhat else is there? Letâ€™s look at the summary() of PCA:\n\nsummary(pca_prep$steps[[2]]$res)\n\nImportance of components:\n                          PC1    PC2    PC3    PC4    PC5     PC6     PC7\nStandard deviation     1.4023 1.3007 1.0920 0.9767 0.9119 0.86253 0.84046\nProportion of Variance 0.2185 0.1880 0.1325 0.1060 0.0924 0.08266 0.07849\nCumulative Proportion  0.2185 0.4065 0.5390 0.6450 0.7374 0.82005 0.89853\n                           PC8     PC9\nStandard deviation     0.69979 0.65077\nProportion of Variance 0.05441 0.04706\nCumulative Proportion  0.95294 1.00000\n\n\nThe table below indicates that if we chose to use only the first 5 Principal Components (PC1, PC2, PC3, PC4, PC5) instead of all of them and instead of the original dataframe, we would still have preserved 70% of the variance in this data.\nğŸ  TAKE-HOME ACTIVITY: Check out this tutorial on Dimensionality Reduction"
  },
  {
    "objectID": "slides/week01_slides_part1.html#the-data-science-institute",
    "href": "slides/week01_slides_part1.html#the-data-science-institute",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "The Data Science Institute",
    "text": "The Data Science Institute\n\n\n\n\n\nThis course is offered by the LSE Data Science Institute (DSI).\nDSI is the hub for LSEâ€™s interdisciplinary collaboration in data science\n\n\n\n\nSign up for DSI events at lse.ac.uk/DSI/Events"
  },
  {
    "objectID": "slides/week01_slides_part1.html#the-data-science-institute-1",
    "href": "slides/week01_slides_part1.html#the-data-science-institute-1",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "The Data Science Institute",
    "text": "The Data Science Institute\n\n\n\n\nActivities of interest to you:\n\nCIVICA Seminar Series\nCareers in Data Science\nSocial events\nIndustry â€œfield tripsâ€\nSummer projects\n\n\n\n\nSign up for DSI events at lse.ac.uk/DSI/Events"
  },
  {
    "objectID": "slides/week01_slides_part1.html#our-courses",
    "href": "slides/week01_slides_part1.html#our-courses",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "Our courses",
    "text": "Our courses\nDSI offer accessible introductions to Data Science:\n\n\nDS101\nFundamentals of  Data Science\nğŸ¯ Focus:  theoretical concepts of data science\nğŸ“‚ How:  reflections through reading and writing\n\nDS105\nData for  Data Scientists\nğŸ¯ Focus: collection and handling of real data\nğŸ“‚ How: hands-on coding exercises and a group project\n\nDS202\nData Science for  Social Scientists\nğŸ¯ Focus: fundamental machine learning algorithms\nğŸ“‚ How: practical use of ML techniques and metrics"
  },
  {
    "objectID": "slides/week01_slides_part1.html#your-lecturer",
    "href": "slides/week01_slides_part1.html#your-lecturer",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "Your lecturer",
    "text": "Your lecturer\n\n\n\n\n\n \nDr.Â Jonathan Cardoso-Silva\n\nPhD in Computer Science\nBackground: Engineering, Bio & Health Informatics\nFormer Lead Data Scientist\nResearch:\n\nNetworks\nOptimisation\nMachine Learning applications\nData Science Workflow"
  },
  {
    "objectID": "slides/week01_slides_part1.html#teaching-assistants",
    "href": "slides/week01_slides_part1.html#teaching-assistants",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "Teaching Assistants",
    "text": "Teaching Assistants\n\n\n\nDr.Â Stuart Bramwell  ESRC Postdoctoral Fellow  Department of Methodology PhD in Politics (Oxford)\n\n\nYijun Wang  Guest Teacher at the DSI PhD cand. in Health Informatics (KCL)  MSc in Data Science (KCL)\n\n\nMustafa Can Ozkan  Guest Teacher at the DSI PhD cand. in the Spacetime Lab (UCL)  MSc in Transport (Imperial/UCL)\n\n\n\n\n\n\n\nXiaowei Gao  Guest Teacher at the DSI PhD cand. in the Spacetime Lab (UCL)  MSc in Data Science (KCL)\n\n\nAnton Boichenko  Guest Teacher at the DSI Product Developer at Decoded  MSc in Applied Social Data Science (LSE)"
  },
  {
    "objectID": "slides/week01_slides_part1.html#who-are-you",
    "href": "slides/week01_slides_part1.html#who-are-you",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "Who are you",
    "text": "Who are you\n\n\n\n\n\n\n\n\n\n\nProgramme\nFreq\n\n\n\n\nBSc in Economics\n34\n\n\nBSc in Pyschological and Behavioural Science\n32\n\n\nGeneral Course\n11\n\n\nBSc in Politics and Economics\n4\n\n\nLLB in Laws\n3\n\n\nBSc in International Relations\n2\n\n\nBSc in Philosophy and Economics\n2\n\n\nBSc in Philosophy, Politics and Economics\n2\n\n\nBSc in Economic History and Geography\n1\n\n\nBSc in Economics and Economic History\n1\n\n\nBSc in Geography with Economics\n1\n\n\nBSc in International Relations and History\n1\n\n\nBSc in Mathematics, Statistics and Business\n1\n\n\nBSc in Philosophy, Logic and Scientific Method\n1\n\n\nErasmus Reciprocal Programme of Study\n1\n\n\nExchange Programme for Students from University of California, Berkeley\n1\n\n\n\n\n\n\n\nSource: LSE For You. Last Updated: 30 September 2022"
  },
  {
    "objectID": "slides/week01_slides_part1.html#learning-objectives-cont.",
    "href": "slides/week01_slides_part1.html#learning-objectives-cont.",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "Learning Objectives (cont.)",
    "text": "Learning Objectives (cont.)\n\n\nKnow how to evaluate and compare fitted models, and to improve model performance.\nUse applied computer programming, including the hands-on use of programming through course exercises.\nApply the methods learned to real data through hands-on exercises.\nIntegrate the insights from data analytics into knowledge generation and decision-making;"
  },
  {
    "objectID": "slides/week01_slides_part1.html#learning-objectives-cont.-1",
    "href": "slides/week01_slides_part1.html#learning-objectives-cont.-1",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "Learning Objectives (cont.)",
    "text": "Learning Objectives (cont.)\n\nUnderstand an introductory framework for working with natural language (text) data using techniques of machine learning.\n\n\n\nLearn how data science methods have been applied to a particular domain of study (applications)."
  },
  {
    "objectID": "slides/week01_slides_part1.html#philosophy-of-this-course-cont.",
    "href": "slides/week01_slides_part1.html#philosophy-of-this-course-cont.",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "Philosophy of this course (cont.)",
    "text": "Philosophy of this course (cont.)\n\nThis is an exciting research area, having important applications in science, industry and policy.\nMachine learning is a fundamental ingredient in the training of a modern data scientist.\n\n\nContent borrowed from ME314 Day 1"
  },
  {
    "objectID": "slides/week01_slides_part1.html#the-basics-of-statistics",
    "href": "slides/week01_slides_part1.html#the-basics-of-statistics",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "The basics of statistics",
    "text": "The basics of statistics\nBasic concepts of Statistics you might want to recap:\n\n\nExpected value, mean, median, variance, standard deviation\nProbabilities and simple probability distributions\nTypes of data\n\ndiscrete vs continuous\ncategorical vs numerical vs ordinal"
  },
  {
    "objectID": "slides/week01_slides_part1.html#resources-stats",
    "href": "slides/week01_slides_part1.html#resources-stats",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "Resources (Stats)",
    "text": "Resources (Stats)\nA few references that might be useful to read or skim through:\n\n(Warne 2018, chaps. 1-3,5,6,11-12)\n(Gelman, Hill, and Vehtari 2020, chaps. 1â€“4)\nIf you are a PBS student, you can revisit the content of PB130 (MT3, MT4, MT8-MT11)"
  },
  {
    "objectID": "slides/week01_slides_part1.html#the-basics-of-r-programming",
    "href": "slides/week01_slides_part1.html#the-basics-of-r-programming",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "The basics of R programming",
    "text": "The basics of R programming\nBasic concepts of programming in R to recap:\n\n\ndata structures (vectors, matrices, data frames)\nhow to manipulate data (filter, subset, select)\nread/write data files (for example: CSV, JSON, TXT)\n(optional but encouraged) some knowledge tidyverse can give you a productive boost\n\nthe official website (tidyverse.org) has some good tutorials."
  },
  {
    "objectID": "slides/week01_slides_part1.html#resources-r",
    "href": "slides/week01_slides_part1.html#resources-r",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "Resources (R)",
    "text": "Resources (R)\n\nCheck out â€œR for Data Scienceâ€ (Wickham and Grolemund 2016, chaps. 1â€“21). The online version is free.\nâ€œStatistical inference via data scienceâ€ (Ismay and Kim 2020, chaps. 4â€“6) is another great free resource"
  },
  {
    "objectID": "slides/week01_slides_part1.html#what-if-i-struggle-with-r",
    "href": "slides/week01_slides_part1.html#what-if-i-struggle-with-r",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "â€˜What if I struggle with Râ€™?",
    "text": "â€˜What if I struggle with Râ€™?\nâ¡ï¸ Our first lab (Week 02) is a recap of some basic R commands, plus some ggplot2.\n\n\nIf you are not confident with your R skills, I strongly encourage you invest in studying the basics in the next couple of weeks.\nContact LSE Digital Skills Lab to attend in-person workshops or self-paced online R courses."
  },
  {
    "objectID": "slides/week01_slides_part1.html#any-questions",
    "href": "slides/week01_slides_part1.html#any-questions",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "Any questions?",
    "text": "Any questions?\n\n\n\n\n\n\nImage created with the DALLÂ·E algorithm using the prompt: â€˜35mm macro photography of a robot holding a question mark card, white backgroundâ€™"
  },
  {
    "objectID": "slides/week01_slides_part1.html#syllabus",
    "href": "slides/week01_slides_part1.html#syllabus",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "Syllabus",
    "text": "Syllabus\n\n\n\n\n\n\n\nIntro\n\n\n\n\n\nÂ Â Â Â Introduction, Context & Key Concepts\nWeek 01\n\n\nSupervised Learning\n\n\n\nÂ Â Â Â Simple and Multiple Linear Regression  Â Â Â Â Classifiers (Logistic Regression & Naive Bayes)  Â Â Â Â Resampling methods  Â Â Â Â  Non-linear algorithms (SVM & tree-based models)\nWeek 02  Week 03  Week 04  Week 05\n\n\nUnsupervised Learning\n\n\n\nÂ Â Â Â Unsupervised Learning: Clustering Â Â Â Â Unsupervised Learning: PCAÂ Â Â Â Â Â Â Â Â \nWeek 07  Week 08\n\n\nApplications\n\n\n\nÂ Â Â Â Applications: Predictive Modelling on Tabular DataÂ Â Â  Applications: Text as Data & Topic Modelling Â Â Â  Applications: Social Media Data\nWeek 09  Week 10  Week 11"
  },
  {
    "objectID": "slides/week01_slides_part1.html#structure-of-lectures",
    "href": "slides/week01_slides_part1.html#structure-of-lectures",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "Structure of lectures ğŸ‘¨ğŸ»â€ğŸ«",
    "text": "Structure of lectures ğŸ‘¨ğŸ»â€ğŸ«\nOur lectures will be split in two parts:\n\n\nPart I (~ 50 min): Traditional exposition of theoretical content\nbreak (~ 10 min): Grab coffee â˜• or relax ğŸ§˜\nPart II (~ 50 min): Live demo\n\nTypically, an exploratory analysis or application of an algorithm\nFeel free to follow along in your own laptops."
  },
  {
    "objectID": "slides/week01_slides_part1.html#structure-of-classes",
    "href": "slides/week01_slides_part1.html#structure-of-classes",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "Structure of classes ğŸ‘©â€ğŸ’»",
    "text": "Structure of classes ğŸ‘©â€ğŸ’»\n\n\nStudents will work on weekly, structured problem sets in the staff-led class sessions.\nTips to get the most of classes:\n\nBring your own laptops ğŸ’» (most tablets are not suitable for programming)\nRead the recommended reading prior to the class\nSkim through the problem set before class"
  },
  {
    "objectID": "slides/week01_slides_part1.html#class-groups",
    "href": "slides/week01_slides_part1.html#class-groups",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "Class groups",
    "text": "Class groups\n\n\nGroup 01\n\nğŸ“† Mondays\nâŒš 09:00 â€” 10:30\nğŸ“ PAN.1.03\n\n\nGroup 02\n\nğŸ“† Mondays\nâŒš 10:30 â€” 12:00\nğŸ“ PAN.1.03\n\n\nGroup 03\n\nğŸ“† Mondays\nâŒš 13:00 â€” 14:30\nğŸ“ MAR.1.09\n\n\nGroup 04\n\nğŸ“† Fridays\nâŒš 16:00 â€” 17:30\nğŸ“ NAB.1.04\n\n\nGroup 05\n\nğŸ“† Mondays\nâŒš 09:00 â€” 10:30\nğŸ“ 32L.LG.11\n\n\nGroup 06\n\nğŸ“† Mondays\nâŒš 10:30 â€” 12:00\nğŸ“ 32L.LG.11\n\n\nGroup 07\n\nğŸ“† Fridays\nâŒš 09:30 â€” 11:00\nğŸ“ CBG.2.06\n\n\n\n\nğŸ—ºï¸ Check LSE campus map"
  },
  {
    "objectID": "slides/week01_slides_part1.html#your-background-knowledge",
    "href": "slides/week01_slides_part1.html#your-background-knowledge",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "Your background knowledge",
    "text": "Your background knowledge\n\nPlease, help our teaching team understand your needs as we prepare for the first labs next week.\nFind the link to the survey on our Slack group or point your phone to the QR code below"
  },
  {
    "objectID": "slides/week01_slides_part1.html#assessments",
    "href": "slides/week01_slides_part1.html#assessments",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "Assessments ğŸ“”",
    "text": "Assessments ğŸ“”\nThe breakdown of assessment for this class will be as follows:"
  },
  {
    "objectID": "slides/week01_slides_part1.html#assessments-1",
    "href": "slides/week01_slides_part1.html#assessments-1",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "Assessments ğŸ“”",
    "text": "Assessments ğŸ“”\n\nProblem sets (60%)\n\n\nSummative problem sets released on Weeks 5, 8 & 11.\nThese will have a similar style to the formative problem sets, a mix of R tasks and your written interpretation of the analyses.\nYou will have 4-6 days to submit your solutions.\nEach of the three summative problem sets is worth 20% of the final mark, and will be graded on a 100 point scale."
  },
  {
    "objectID": "slides/week01_slides_part1.html#assessments-2",
    "href": "slides/week01_slides_part1.html#assessments-2",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "Assessments ğŸ“”",
    "text": "Assessments ğŸ“”\n\nTake-home exam (40%)\n\n\nAn open-book take-home exam, taken during the January exams period.\nExam questions will be comparable in style to the problem sets.\nThe exam questions will be released on Moodle on 5 January 2023. (tentative)\nThe exam is due on 11 January at 4pm (tentative)\nâš ï¸ Update 11/10/2022: Last year, DS202 exam was performed entirely online due to COVID-19 mitigation procedures. We want to run it online via our own Moodle page again this academic term, we just need to understand LSE regulations about exams for this year. We will update you on this very soon (hopefully by the end of W04)."
  },
  {
    "objectID": "slides/week01_slides_part1.html#office-hours",
    "href": "slides/week01_slides_part1.html#office-hours",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "Office hours",
    "text": "Office hours\n\n\nIt is probably a good idea to book office hours if:\n\nyou struggled with a technical or theoretical aspect of a problem set in the previous week,\nyou have queries about careers in data science,\nyou want guidance in how to apply data science to other things you are studying outside this course.\n\nCome prepared. You only have 15 minutes.\nAsk for help sooner rather than later.\nBook slots via StudentHub up to 12 hours in advance."
  },
  {
    "objectID": "slides/week01_slides_part1.html#communication",
    "href": "slides/week01_slides_part1.html#communication",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "Communication",
    "text": "Communication\n\n\nJoin our Slack group (more info here).\nUse the public Slack channels to talk to share links, content (or memes) with your colleagues.\nOur teaching team will dedicate some time during the week to answer questions or other interactions on Slack.\nReserve ğŸ“§ e-mail for formal requests: extensions, deferrals, etc.\n\nNo need to e-mail to inform you will skip a class, for example."
  },
  {
    "objectID": "slides/week01_slides_part1.html#any-questions-1",
    "href": "slides/week01_slides_part1.html#any-questions-1",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "Any questions?",
    "text": "Any questions?\n\n\n\n\n\n\nImage created with the DALLÂ·E algorithm using the prompt: â€˜35mm macro photography of a robot holding a question mark card, white backgroundâ€™"
  },
  {
    "objectID": "slides/week01_slides_part1.html#we-changed-how-we-consume-music",
    "href": "slides/week01_slides_part1.html#we-changed-how-we-consume-music",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "We changed how we consume music ğŸ§",
    "text": "We changed how we consume music ğŸ§\n\n\n\n\n\n\nTo interact with this plot, check reference (Fischer-Baum 2017) at the end of this presentation."
  },
  {
    "objectID": "slides/week01_slides_part1.html#we-changed-how-we-consume-video",
    "href": "slides/week01_slides_part1.html#we-changed-how-we-consume-video",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "We changed how we consume video ğŸï¸",
    "text": "We changed how we consume video ğŸï¸\n\n\n\n\n\n\nTo interact with this plot, check reference (Fischer-Baum 2017) at the end of this presentation."
  },
  {
    "objectID": "slides/week01_slides_part1.html#smartphones-are-a-very-recent-thing",
    "href": "slides/week01_slides_part1.html#smartphones-are-a-very-recent-thing",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "Smartphones ğŸ“± are a very recent thing",
    "text": "Smartphones ğŸ“± are a very recent thing\n\n\n\n\n\n\nTo interact with this plot, check reference (Fischer-Baum 2017) at the end of this presentation."
  },
  {
    "objectID": "slides/week01_slides_part1.html#we-spend-a-lot-more-time-connected",
    "href": "slides/week01_slides_part1.html#we-spend-a-lot-more-time-connected",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "We spend a lot more time connected",
    "text": "We spend a lot more time connected"
  },
  {
    "objectID": "slides/week01_slides_part1.html#references",
    "href": "slides/week01_slides_part1.html#references",
    "title": "ğŸ—“ï¸ Week 01 Structure of this course",
    "section": "References",
    "text": "References\n\n\nFischer-Baum, Reuben. 2017. â€œWhat â€˜Tech Worldâ€™ Did You Grow up In?â€ Washington Post. https://www.washingtonpost.com/graphics/2017/entertainment/tech-generations/.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. 1st ed. Cambridge University Press. https://doi.org/10.1017/9781139161879.\n\n\nIsmay, Chester, and Albert Young-Sun Kim. 2020. Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. Chapman & Hall/CRC the R Series. Boca Raton: CRC Press / Taylor & Francis Group. https://moderndive.com/.\n\n\nKolawole, Emi. 2013. â€œAbout Those 2005 and 2013 Photos of the Crowds in St. Peterâ€™s Square.â€ Washington Post. http://wapo.st/WKKTMh.\n\n\nWarne, Russell T. 2018. Statistics for the Social Sciences: A General Linear Model Approach. https://www.cambridge.org/highereducation/books/statistics-for-the-social-sciences/716FF25785A6154CC6822D067A959445.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. First edition. Sebastopol, CA: Oâ€™Reilly. https://r4ds.had.co.nz/.\n\n\n\n\n\nDS202 - Data Science for Social Scientists ğŸ¤– ğŸ¤¹"
  },
  {
    "objectID": "slides/week01_slides_part2.html#data-science-is",
    "href": "slides/week01_slides_part2.html#data-science-is",
    "title": "ğŸ—“ï¸ Week 01Overview of core concepts",
    "section": "Data science isâ€¦",
    "text": "Data science isâ€¦\n\n\nâ€œ[â€¦] a field of study and practice that involves the collection, storage, and processing of data in order to derive important ğŸ’¡ insights into a problem or a phenomenon.\n\n\n\n\nSuch data may be generated by humans (surveys, logs, etc.) or machines (weather data, road vision, etc.),\n\n\n\n\nand could be in different formats (text, audio, video, augmented or virtual reality, etc.).â€\n\n\n\n\n(Shah 2020, chap. 1) - Emphasis and emojis are of my own making."
  },
  {
    "objectID": "slides/week01_slides_part2.html#the-mythical-unicorn",
    "href": "slides/week01_slides_part2.html#the-mythical-unicorn",
    "title": "ğŸ—“ï¸ Week 01Overview of core concepts",
    "section": "The mythical unicorn ğŸ¦„",
    "text": "The mythical unicorn ğŸ¦„\n\n\nknows everything about statistics\n\n\nable to communicate insights perfectly\n\n\nfully understands businesses like no one\n\n\nis a fluent computer programmer\n\n\n\nOf course, such a person does not exist!\n\n\nSee (Davenport 2020) for a more in-depth discussion about this"
  },
  {
    "objectID": "slides/week01_slides_part2.html#in-reality",
    "href": "slides/week01_slides_part2.html#in-reality",
    "title": "ğŸ—“ï¸ Week 01Overview of core concepts",
    "section": "In realityâ€¦",
    "text": "In realityâ€¦\n\n\nWe are all jugglers ğŸ¤¹\n\n\nEveryone brings a different skill set.\nWe need multi-disciplinary teams.\nGood data scientists know a bit of everything.\n\nNot fluent in all things\nUnderstands their strenghts and weaknessess\nThey know when and where to interface with others\n\n\n\n\n\n\n\n\n\nSee (Schutt and Oâ€™Neil 2013, chap. 1) for more on this."
  },
  {
    "objectID": "slides/week01_slides_part2.html#the-data-science-workflow-1",
    "href": "slides/week01_slides_part2.html#the-data-science-workflow-1",
    "title": "ğŸ—“ï¸ Week 01Overview of core concepts",
    "section": "The Data Science Workflow",
    "text": "The Data Science Workflow\n\n\n\n\n\n   \n\nstart\n\n Start   \n\ngather\n\nGather data Â    \n\nstart->gather\n\n    \n\nstore\n\nStore it Â Â Â Â Â Â Â Â  somewhere   \n\ngather->store\n\n   Â Â Â Â Â    \n\nclean\n\nClean & Â Â Â Â Â Â Â  pre-process   \n\nstore->clean\n\n   Â Â Â Â Â    \n\nbuild\n\nBuild a  dataset   \n\nclean->build\n\n   Â Â Â Â Â    \n\neda\n\nExploratory Â Â Â  data analysis   \n\nbuild->eda\n\n    \n\nml\n\nMachine learning   \n\neda->ml\n\n   Â Â Â Â Â    \n\ninsight\n\nObtain Â Â  insights   \n\nml->insight\n\n   Â Â Â Â Â    \n\ncommunicate\n\nCommunicate results Â Â Â Â Â Â Â Â    \n\ninsight->communicate\n\n   Â Â Â Â Â    \n\nend\n\n End   \n\ncommunicate->end\n\n   \n\n\n\n\n\n\n\nâš ï¸ Note that this is a simplified version of what happens in a data science project.  In practice, the process is not linear and there are many feedback loops."
  },
  {
    "objectID": "slides/week01_slides_part2.html#the-data-science-workflow-2",
    "href": "slides/week01_slides_part2.html#the-data-science-workflow-2",
    "title": "ğŸ—“ï¸ Week 01Overview of core concepts",
    "section": "The Data Science Workflow",
    "text": "The Data Science Workflow\n\n\n\n\n\n   \n\nstart\n\n Start   \n\ngather\n\n Gather data Â    \n\nstart->gather\n\n    \n\nend\n\n End   \n\nstore\n\n Store it Â Â Â Â Â Â Â Â  somewhere   \n\ngather->store\n\n   Â Â Â Â Â    \n\nclean\n\n Clean & Â Â Â Â Â Â Â  pre-process   \n\nstore->clean\n\n   Â Â Â Â Â    \n\nbuild\n\n Build a  dataset   \n\nclean->build\n\n   Â Â Â Â Â    \n\neda\n\n Exploratory Â Â Â  data analysis   \n\nbuild->eda\n\n    \n\nml\n\n Machine learning   \n\neda->ml\n\n   Â Â Â Â Â    \n\ninsight\n\n Obtain Â Â  insights   \n\nml->insight\n\n   Â Â Â Â Â    \n\ncommunicate\n\n Communicate results Â Â Â Â Â Â Â Â    \n\ninsight->communicate\n\n   Â Â Â Â Â    \n\ncommunicate->end\n\n   \n\n\n\n\n\nIt is often said that 80% of the time and effort spent on a data science project goes to the tasks highlighted above."
  },
  {
    "objectID": "slides/week01_slides_part2.html#the-data-science-workflow-3",
    "href": "slides/week01_slides_part2.html#the-data-science-workflow-3",
    "title": "ğŸ—“ï¸ Week 01Overview of core concepts",
    "section": "The Data Science Workflow",
    "text": "The Data Science Workflow\n\n\n\n\n\n   \n\nstart\n\n Start   \n\ngather\n\n Gather data Â    \n\nstart->gather\n\n    \n\nend\n\n End   \n\nstore\n\n Store it Â Â Â Â Â Â Â Â  somewhere   \n\ngather->store\n\n   Â Â Â Â Â    \n\nclean\n\n Clean & Â Â Â Â Â Â Â  pre-process   \n\nstore->clean\n\n   Â Â Â Â Â    \n\nbuild\n\n Build a  dataset   \n\nclean->build\n\n   Â Â Â Â Â    \n\neda\n\n Exploratory Â Â Â  data analysis   \n\nbuild->eda\n\n    \n\nml\n\n Machine learning   \n\neda->ml\n\n   Â Â Â Â Â    \n\ninsight\n\n Obtain Â Â  insights   \n\nml->insight\n\n   Â Â Â Â Â    \n\ncommunicate\n\n Communicate results Â Â Â Â Â Â Â Â    \n\ninsight->communicate\n\n   Â Â Â Â Â    \n\ncommunicate->end\n\n   \n\n\n\n\n\nThis course is about Machine Learning. So, in most examples and tutorials, we will assume that we already have good quality data."
  },
  {
    "objectID": "slides/week01_slides_part2.html#data-science-and-social-science",
    "href": "slides/week01_slides_part2.html#data-science-and-social-science",
    "title": "ğŸ—“ï¸ Week 01Overview of core concepts",
    "section": "Data Science and Social Science",
    "text": "Data Science and Social Science\n\nIn reality, data scientists work as a multidisciplinary group, collaborating towards a common goal.\nContent borrowed from ME314 Day 1\n\n\n\nSocial science: The goal is typically explanation\nData science: The goal is frequently prediction, or data exploration\nMany of the same methods are used for both objectives\n\n\n\n\nCheck (Shmueli 2010) for a discussion about this topic."
  },
  {
    "objectID": "slides/week01_slides_part2.html#what-does-it-mean-to-learn-something",
    "href": "slides/week01_slides_part2.html#what-does-it-mean-to-learn-something",
    "title": "ğŸ—“ï¸ Week 01Overview of core concepts",
    "section": "What does it mean to learn something?",
    "text": "What does it mean to learn something?\n\n\n\n\n\n\nImage created with the DALLÂ·E algorithm using the prompt: â€˜35mm macro photography of a robot holding a question mark card, white backgroundâ€™"
  },
  {
    "objectID": "slides/week01_slides_part2.html#predicting-a-sequence-intuitively",
    "href": "slides/week01_slides_part2.html#predicting-a-sequence-intuitively",
    "title": "ğŸ—“ï¸ Week 01Overview of core concepts",
    "section": "Predicting a sequence intuitively",
    "text": "Predicting a sequence intuitively\n\n\nSay our data is the following simple sequence:  \\(6, 9, 12, 15, 18, 21, 24, ...\\) \nWhat number do you expect to come next? Why?\nIt is very likely that you guessed that \\(\\operatorname{next number}=27\\)\nWe spot that the sequence follows a pattern\nFrom this, we notice â€” we learn â€” that the sequence is governed by: \\(\\operatorname{next number} = \\operatorname{previous number} + 3\\)"
  },
  {
    "objectID": "slides/week01_slides_part2.html#predicting-a-sequence-formula",
    "href": "slides/week01_slides_part2.html#predicting-a-sequence-formula",
    "title": "ğŸ—“ï¸ Week 01Overview of core concepts",
    "section": "Predicting a sequence (formula)",
    "text": "Predicting a sequence (formula)\nThe next number is a function of the previous one:\n \\[\n\\operatorname{next number} = f(\\operatorname{previous number})\n\\]"
  },
  {
    "objectID": "slides/week01_slides_part2.html#predicting-a-sequence-generic-formula",
    "href": "slides/week01_slides_part2.html#predicting-a-sequence-generic-formula",
    "title": "ğŸ—“ï¸ Week 01Overview of core concepts",
    "section": "Predicting a sequence (generic formula)",
    "text": "Predicting a sequence (generic formula)\nIn general terms, we can represented it as:\n \\[\n\\operatorname{Y} = f(\\operatorname{X})\n\\] \n\nwhere:\n\n\\(Y\\): a quantitative response.  It goes by many names: dependent variable, response, target, outcome\n\\(X\\): a set of predictors,  also called inputs, regressors, covariates, features, independent variables.\n\\(f\\): the systematic information that \\(X\\) provides about \\(Y\\)"
  },
  {
    "objectID": "slides/week01_slides_part2.html#predicting-a-sequence-generic-formula-1",
    "href": "slides/week01_slides_part2.html#predicting-a-sequence-generic-formula-1",
    "title": "ğŸ—“ï¸ Week 01Overview of core concepts",
    "section": "Predicting a sequence (generic formula)",
    "text": "Predicting a sequence (generic formula)\nIn general terms, we can represented it as:\n\n\\[\n\\operatorname{Y} = f(\\operatorname{X}) + \\epsilon\n\\]\n\nwhere:\n\n\\(Y\\): the output\n\\(X\\): a set of inputs\n\\(f\\): the systematic information that \\(X\\) provides about \\(Y\\)\n\\(\\epsilon~~\\): a random error term\n\n\nIn reality, there is some error \\(\\epsilon\\) that cannot be reduced."
  },
  {
    "objectID": "slides/week01_slides_part2.html#approximating-f",
    "href": "slides/week01_slides_part2.html#approximating-f",
    "title": "ğŸ—“ï¸ Week 01Overview of core concepts",
    "section": "Approximating \\(f\\)",
    "text": "Approximating \\(f\\)\n\n\n\\(f\\) is almost always unknown\nWe aim to find an approximation (a model). Letâ€™s call it \\(\\hat{f}\\)\nthat can then use it to predict values of \\(Y\\) for whatever \\(X\\).\nThat is: \\(\\hat{Y} = \\hat{f}(X)\\)"
  },
  {
    "objectID": "slides/week01_slides_part2.html#what-is-machine-learning",
    "href": "slides/week01_slides_part2.html#what-is-machine-learning",
    "title": "ğŸ—“ï¸ Week 01Overview of core concepts",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\n\n\nStatistical learning, or Machine learning, refers to a set of approaches for estimating \\(f\\).\nEach algorithm you will learn on this course has its own way to determine \\(\\hat{f}\\) given data"
  },
  {
    "objectID": "slides/week01_slides_part2.html#types-of-learning",
    "href": "slides/week01_slides_part2.html#types-of-learning",
    "title": "ğŸ—“ï¸ Week 01Overview of core concepts",
    "section": "Types of learning",
    "text": "Types of learning\n\nIn general terms, there are two main ways to learn from data:\n\n\nSupervised Learning\n\nEach observation (\\(x_i\\)) has an outcome associated with it (\\(y_i\\)).\nYour goal is to find a \\(\\hat{f}\\) that produces \\(\\hat{Y}\\) value close to the true \\(Y\\) values.\nOur focus on ğŸ—“ï¸ Weeks 2, 3, 4 & 5.\n\n\nUnsupervised Learning\n\nYou have observations (\\(x_i\\)) but there is no response variable.\nYour goal is to find a \\(\\hat{f}\\), focused only on \\(X\\) that best represents the patterns in the data.\nOur focus on ğŸ—“ï¸ Weeks 7 & 8."
  },
  {
    "objectID": "slides/week01_slides_part2.html#data-structure",
    "href": "slides/week01_slides_part2.html#data-structure",
    "title": "ğŸ—“ï¸ Week 01Overview of core concepts",
    "section": "Data Structure",
    "text": "Data Structure\nLetâ€™s go back to our example:\n\n\n\nOur simple sequence:\n \\(6, 9, 12, 15, 18, 21, 24\\) \n\n\n\nBecomes:\n\n\n\n\\(X\\)\n\\(Y\\)\n\n\n\n\n6\n9\n\n\n9\n12\n\n\n12\n15\n\n\n15\n18\n\n\n18\n21\n\n\n21\n24\n\n\n\n\n\n\nAnd for prediction:\n\n\n\n\n\n\n\n\\(X\\)\n\\(\\hat{Y}\\)\n\n\n\n\n24\n?\n\n\n\nwe present the \\(X\\) values and ask the fitted model to give us \\(\\hat{Y}\\).\n\n\n\n\nSame data but now in tabular format\na few other terms: - training data/test data - fitted model"
  },
  {
    "objectID": "slides/week01_slides_part2.html#the-ground-truth",
    "href": "slides/week01_slides_part2.html#the-ground-truth",
    "title": "ğŸ—“ï¸ Week 01Overview of core concepts",
    "section": "The ground truth",
    "text": "The ground truth\nLetâ€™s create a dataframe to illustrate the process of training an algorithm:\n\nlibrary(tidyverse)\n\ndf = tibble(X=as.integer(seq(6, 21, 3)),\n            Y=as.integer(seq(6+3, 21+3, 3)))\nprint(df)\n\n\n\n# A tibble: 6 Ã— 2\n      X     Y\n  <int> <int>\n1     6     9\n2     9    12\n3    12    15\n4    15    18\n5    18    21\n6    21    24"
  },
  {
    "objectID": "slides/week01_slides_part2.html#adding-noise",
    "href": "slides/week01_slides_part2.html#adding-noise",
    "title": "ğŸ—“ï¸ Week 01Overview of core concepts",
    "section": "Adding noise",
    "text": "Adding noise\nLetâ€™s simulate the introduction of some random error:\n\n# Let's simulate some noise\ngaussian_noise = rnorm(n=nrow(df), mean=0, sd=1.5)\n\n# Call it \"observed Y\"\ndf$obsY = df$Y + gaussian_noise\nprint(df)\n\n\n\n# A tibble: 6 Ã— 3\n      X     Y  obsY\n  <int> <int> <dbl>\n1     6     9  6.72\n2     9    12  9.05\n3    12    15 15.9 \n4    15    18 16.5 \n5    18    21 18.8 \n6    21    24 24.9"
  },
  {
    "objectID": "slides/week01_slides_part2.html#visualizing-the-data",
    "href": "slides/week01_slides_part2.html#visualizing-the-data",
    "title": "ğŸ—“ï¸ Week 01Overview of core concepts",
    "section": "Visualizing the data",
    "text": "Visualizing the data"
  },
  {
    "objectID": "slides/week01_slides_part2.html#visualizing-the-data-w-noise",
    "href": "slides/week01_slides_part2.html#visualizing-the-data-w-noise",
    "title": "ğŸ—“ï¸ Week 01Overview of core concepts",
    "section": "Visualizing the data (w/ noise)",
    "text": "Visualizing the data (w/ noise)"
  },
  {
    "objectID": "slides/week01_slides_part2.html#visualizing-the-data-w-noise-1",
    "href": "slides/week01_slides_part2.html#visualizing-the-data-w-noise-1",
    "title": "ğŸ—“ï¸ Week 01Overview of core concepts",
    "section": "Visualizing the data (w/ noise)",
    "text": "Visualizing the data (w/ noise)\n\n\n\n\n\n\n\nWhich line is closer to the â€œtruthâ€?"
  },
  {
    "objectID": "slides/week01_slides_part2.html#visualizing-the-data-w-noise-2",
    "href": "slides/week01_slides_part2.html#visualizing-the-data-w-noise-2",
    "title": "ğŸ—“ï¸ Week 01Overview of core concepts",
    "section": "Visualizing the data (w/ noise)",
    "text": "Visualizing the data (w/ noise)\n\n\n\n\n\n\n\nHow much error can we accept?"
  },
  {
    "objectID": "slides/week01_slides_part2.html#assessing-error",
    "href": "slides/week01_slides_part2.html#assessing-error",
    "title": "ğŸ—“ï¸ Week 01Overview of core concepts",
    "section": "Assessing error",
    "text": "Assessing error\nHow much error was introduced by \\(\\epsilon\\) per sample?\n\ndf$error    <- df$Y - df$obsY  # Calculate the error\ndf$absError <- abs(df$error)   # Ignore the sign of error\ndf\n\n\n\n# A tibble: 6 Ã— 5\n      X     Y  obsY  error absError\n  <int> <int> <dbl>  <dbl>    <dbl>\n1     6     9  6.72  2.28     2.28 \n2     9    12  9.05  2.95     2.95 \n3    12    15 15.9  -0.936    0.936\n4    15    18 16.5   1.51     1.51 \n5    18    21 18.8   2.24     2.24 \n6    21    24 24.9  -0.860    0.860\n\n\n\nOn average, what is the error?\n\nmean(df$absError)\n\n\n\n[1] 1.794542\n\n\n\n\nThis measure is called the Mean Absolute Error."
  },
  {
    "objectID": "slides/week01_slides_part2.html#measures-of-error",
    "href": "slides/week01_slides_part2.html#measures-of-error",
    "title": "ğŸ—“ï¸ Week 01Overview of core concepts",
    "section": "Measures of error",
    "text": "Measures of error\nThis is what we computed:\n\\[\n\\operatorname{MAE} = \\frac{\\sum_{i=1}^n{|(y_i + \\epsilon) - y_i|}}{n}\n\\]\n\n\nWe were able to compute this error because we knew what the ground truth \\(Y\\), we knew what its real value was.\nIt was only possible because it was a simulation, not real data.\nIn practice, we will almost never be able to assess the impact of \\(\\epsilon\\).\nWe will use this same way of thinking to assess how good and accurate our models are. ğŸ”œ"
  },
  {
    "objectID": "slides/week01_slides_part2.html#references",
    "href": "slides/week01_slides_part2.html#references",
    "title": "ğŸ—“ï¸ Week 01Overview of core concepts",
    "section": "References",
    "text": "References\n\n\nDavenport, Thomas. 2020. â€œBeyond Unicorns: Educating, Classifying, and Certifying Business Data Scientists.â€ Harvard Data Science Review 2 (2). https://doi.org/10.1162/99608f92.55546b4a.\n\n\nSchutt, Rachel, and Cathy Oâ€™Neil. 2013. Doing Data Science. First edition. Beijing ; Sebastopol: Oâ€™Reilly Media. https://ebookcentral.proquest.com/lib/londonschoolecons/detail.action?docID=1465965.\n\n\nShah, Chirag. 2020. A Hands-on Introduction to Data Science. Cambridge, United Kingdom ; New York, NY, USA: Cambridge University Press. https://librarysearch.lse.ac.uk/permalink/f/1n2k4al/TN_cdi_askewsholts_vlebooks_9781108673907.\n\n\nShmueli, Galit. 2010. â€œTo Explain or to Predict?â€ Statistical Science 25 (3). https://doi.org/10.1214/10-STS330.\n\n\n\n\n\nDS202 - Data Science for Social Scientists ğŸ¤– ğŸ¤¹"
  },
  {
    "objectID": "slides/week02_slides_part1.html#the-basic-models",
    "href": "slides/week02_slides_part1.html#the-basic-models",
    "title": "ğŸ—“ï¸ Week 02:Linear Regression",
    "section": "The basic models",
    "text": "The basic models\n\nLinear regression is a simple approach to supervised learning.\n\n\n\n\nThe generic supervised model:\n\\[\nY = \\operatorname{f}(X) + \\epsilon\n\\]\nis defined more explicitly as follows â¡ï¸\n\n\n\n\nSimple linear regression\n\n\\[\n\\begin{align}\nY = \\beta_0 +& \\beta_1 X + \\epsilon, \\\\\n\\\\\n\\\\\n\\end{align}\n\\] \nwhen we use a single predictor, \\(X\\).\n\n\n\n\nMultiple linear regression\n\n\\[\n\\begin{align}\nY = \\beta_0 &+ \\beta_1 X_1 + \\beta_2 X_2 \\\\\n   &+ \\dots \\\\\n   &+ \\beta_p X_p + \\epsilon\n\\end{align}\n\\]\n\nwhen there are multiple predictors, \\(X_p\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nTrue regression functions are never linear!\nAlthough it may seem overly simplistic, linear regression is extremely useful both conceptually and practically.\n\n\n\n\n\n\n\n\n\nSee ğŸ“º Regression: Crash Course Statistics on YouTube for inspiration on how to present linear regression to students.  \nWe will talk about both types of models, how we can estimate the values of all \\(\\beta\\) and assess how good our models are."
  },
  {
    "objectID": "slides/week02_slides_part1.html#linear-regression-with-a-single-predictor",
    "href": "slides/week02_slides_part1.html#linear-regression-with-a-single-predictor",
    "title": "ğŸ—“ï¸ Week 02:Linear Regression",
    "section": "Linear Regression with a single predictor",
    "text": "Linear Regression with a single predictor\n\n\nWe assume a model:\n\n\\[\nY = \\beta_0 + \\beta_1 X + \\epsilon ,\n\\]\n\n\n\n\n\n\n\n\n\n\nwhere:\n\n\\(\\beta_0\\): an unknown constant that represents the intercept of the line.\n\\(\\beta_1\\): an unknown constant that represents the slope of the line\n\\(\\epsilon\\): the random error term (irreducible)\n\n\n\n\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are also known as coefficients or parameters of the model."
  },
  {
    "objectID": "slides/week02_slides_part1.html#linear-regression-with-a-single-predictor-1",
    "href": "slides/week02_slides_part1.html#linear-regression-with-a-single-predictor-1",
    "title": "ğŸ—“ï¸ Week 02:Linear Regression",
    "section": "Linear Regression with a single predictor",
    "text": "Linear Regression with a single predictor\n\n\nWe want to estimate:\n\\[\n\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} x\n\\]\n\n\n\n\n\n\n\n\n\nwhere:\n\n\\(\\hat{y}\\): is a prediction of \\(Y\\) on the basis of \\(X = x\\).\n\\(\\hat{\\beta_0}\\): is an estimate of the â€œtrueâ€ \\(\\beta_0\\).\n\\(\\hat{\\beta_1}\\): is an estimate of the â€œtrueâ€ \\(\\beta_1\\).\n\n\n\n\n\nThe hat symbol denotes an estimated value."
  },
  {
    "objectID": "slides/week02_slides_part1.html#different-estimators-different-equations",
    "href": "slides/week02_slides_part1.html#different-estimators-different-equations",
    "title": "ğŸ—“ï¸ Week 02:Linear Regression",
    "section": "Different estimators, different equations",
    "text": "Different estimators, different equations\n\n\n\nThere are multiple ways to estimate the coefficients.\n\nIf you use different techniques, you might get different equations\nThe most common algorithm is called  Ordinary Least Squares (OLS)\nJust to name a few other estimators (Karafiath 2009):\n\nLeast Absolute Deviation (LAD)\nWeighted Least Squares (WLS)\nGeneralized Least Squares (GLS)\nHeteroskedastic-Consistent (HC) variants\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will only cover OLS in this course."
  },
  {
    "objectID": "slides/week02_slides_part1.html#the-concept-of-residuals",
    "href": "slides/week02_slides_part1.html#the-concept-of-residuals",
    "title": "ğŸ—“ï¸ Week 02:Linear Regression",
    "section": "The concept of residuals",
    "text": "The concept of residuals\nSuppose you came across some data:\n\n\n\n\n\n\n\nFirst, letâ€™s think of the concept of residualsâ€¦\n\n\nAnd you suspect there is a linear relationship between X and Y."
  },
  {
    "objectID": "slides/week02_slides_part1.html#the-concept-of-residuals-1",
    "href": "slides/week02_slides_part1.html#the-concept-of-residuals-1",
    "title": "ğŸ—“ï¸ Week 02:Linear Regression",
    "section": "The concept of residuals",
    "text": "The concept of residuals\nSo, you decide to fit a line to it.\n\n\n\n\n\n\n\nA line that goes right through the middle of the cloud of data."
  },
  {
    "objectID": "slides/week02_slides_part1.html#the-concept-of-residuals-2",
    "href": "slides/week02_slides_part1.html#the-concept-of-residuals-2",
    "title": "ğŸ—“ï¸ Week 02:Linear Regression",
    "section": "The concept of residuals",
    "text": "The concept of residuals\n\nResiduals are the distances from each data point to this line. \n\n\n\n\n\n\n\n\\(e_i\\)\\(=y_i-\\hat{y}_i\\) represents the \\(i\\)th residual"
  },
  {
    "objectID": "slides/week02_slides_part1.html#residual-sum-of-squares-rss",
    "href": "slides/week02_slides_part1.html#residual-sum-of-squares-rss",
    "title": "ğŸ—“ï¸ Week 02:Linear Regression",
    "section": "Residual Sum of Squares (RSS)",
    "text": "Residual Sum of Squares (RSS)\nFrom this, we can define the  Residual Sum of Squares  (RSS) as\n\\[\n\\mathrm{RSS}= e_1^2 + e_2^2 + \\dots + e_n^2,\n\\]\n\nor equivalently as\n\\[\n\\mathrm{RSS}= (y_1 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_1)^2 + (y_2 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_2)^2 + \\dots + (y_n - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_n)^2.\n\\]\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe (ordinary) least squares approach chooses \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) to minimize the RSS.\n\n\n\n\n\nThat is how it does its job."
  },
  {
    "objectID": "slides/week02_slides_part1.html#a-question-for-you",
    "href": "slides/week02_slides_part1.html#a-question-for-you",
    "title": "ğŸ—“ï¸ Week 02:Linear Regression",
    "section": "A question for you",
    "text": "A question for you\n\n\n\n\nWhy the squares and not, say, just the sum of residuals?\n\nImage created with the DALLÂ·E algorithm using the prompt: â€˜35mm macro photography of a robot holding a question mark card, white backgroundâ€™\n\n\n\n\nExplain that the sum penalizes individual large errors a lot more Consider adding a visualisation to illustrate this point."
  },
  {
    "objectID": "slides/week02_slides_part1.html#the-objective-function",
    "href": "slides/week02_slides_part1.html#the-objective-function",
    "title": "ğŸ—“ï¸ Week 02:Linear Regression",
    "section": "The objective function",
    "text": "The objective function\nWe treat this as an optimisation problem. We want to minimize RSS: \\[\n\\begin{align}\n\\min \\mathrm{RSS} =& \\sum_i^n{e_i^2} \\\\\n             =& \\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2} \\\\\n             =& \\sum_i^n{\\left(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i\\right)^2}\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/week02_slides_part1.html#estimating-hatbeta_0",
    "href": "slides/week02_slides_part1.html#estimating-hatbeta_0",
    "title": "ğŸ—“ï¸ Week 02:Linear Regression",
    "section": "Estimating \\(\\hat{\\beta}_0\\)",
    "text": "Estimating \\(\\hat{\\beta}_0\\)\nTo find \\(\\hat{\\beta}_0\\), we have to solve the following partial derivative:\n\\[\n\\frac{\\partial ~\\mathrm{RSS}}{\\partial \\hat{\\beta}_0}{\\sum_i^n{(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2}} = 0\n\\]\n\nâ€¦ which will lead you to:\n\n\n\\[\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x},\n\\]\n\n\nwhere we made use of the sample means:\n\n\\(\\bar{y} \\equiv \\frac{1}{n} \\sum_{i=1}^n y_i\\)\n\\(\\bar{x} \\equiv \\frac{1}{n} \\sum_{i=1}^n x_i\\)\n\n\n\n\nFull derivation if needed:\n\\[\n\\begin{align}\n0 &= \\frac{\\partial ~\\mathrm{RSS}}{\\partial \\hat{\\beta}_0}{\\sum_i^n{(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2}} & (\\text{chain rule})\\\\\n0 &= \\sum_i^n{-2 (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)} & (\\text{take $-2$ out})\\\\\n0 &= -2 \\sum_i^n{ (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)}  & (\\div -2) \\\\\n0 &=\\sum_i^n{ (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)}  & (\\text{sep. sums}) \\\\\n0 &=\\sum_i^n{y_i} - \\sum_i^n{\\hat{\\beta}_0} - \\sum_i^n{\\hat{\\beta}_1 x_i}  & (\\text{simplify}) \\\\\n0 &=\\sum_i^n{y_i} - n\\hat{\\beta}_0 - \\hat{\\beta}_1\\sum_i^n{ x_i}  & (+ n\\hat{\\beta}_0) \\\\\nn\\hat{\\beta}_0 &= \\sum_i^n{y_i} - \\hat{\\beta}_1\\sum_i^n{ x_i} & (\\text{isolate }\\hat{\\beta}_0 ) \\\\\n\\hat{\\beta}_0 &= \\frac{\\sum_i^n{y_i} - \\hat{\\beta}_1\\sum_i^n{ x_i}}{n} & (\\text{rearranging}) \\\\\n\\hat{\\beta}_0 &= \\frac{\\sum_i^n{y_i}}{n} - \\hat{\\beta}_1\\frac{\\sum_i^n{x_i}}{n} & (\\text{or simply}) \\\\\n\\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x} & \\blacksquare\n\\end{align}\n\\]\n\n\n\nğŸ“ Give it a go! Pretend \\(\\hat{\\beta}_1\\) is constant and use the power rule to solve the equation and reach the same result."
  },
  {
    "objectID": "slides/week02_slides_part1.html#estimating-hatbeta_1",
    "href": "slides/week02_slides_part1.html#estimating-hatbeta_1",
    "title": "ğŸ—“ï¸ Week 02:Linear Regression",
    "section": "Estimating \\(\\hat{\\beta}_1\\)",
    "text": "Estimating \\(\\hat{\\beta}_1\\)\nSimilarly, to find \\(\\hat{\\beta}_1\\) we solve:\n\\[\n\\frac{\\partial ~\\mathrm{RSS}}{\\partial \\hat{\\beta}_1}{[\\sum_i^n{y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i}]} = 0\n\\]\n\nâ€¦ which will lead you to:\n\n\n\\[\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})^2}\n\\]\n\n\n\nFull derivation if needed:\n\\[\n\\begin{align}\n0 &= \\frac{\\partial ~\\mathrm{RSS}}{\\partial \\hat{\\beta}_1}{\\sum_i^n{(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2}} & (\\text{chain rule})\\\\\n0 &= \\sum_i^n{\\left(-2x_i~ (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\\right)} & (\\text{take $-2$ out})\\\\\n0 &= -2\\sum_i^n{\\left( x_i~ (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\\right)} & (\\div -2) \\\\\n0 &= \\sum_i^n{\\left(x_i~ (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\\right)} & (\\text{distribute } x_i) \\\\\n0 &= \\sum_i^n{\\left(y_ix_i - \\hat{\\beta}_0x_i - \\hat{\\beta}_1 x_i^2\\right)} & (\\text{replace } \\hat{\\beta}_0) \\\\\n0 &= \\sum_i^n{\\left(y_ix_i - (\\bar{y} - \\hat{\\beta}_1 \\bar{x})x_i - \\hat{\\beta}_1 x_i^2\\right)} & (\\text{rearrange}) \\\\\n0 &= \\sum_i^n{\\left(y_ix_i - \\bar{y}x_i + \\hat{\\beta}_1 \\bar{x}x_i - \\hat{\\beta}_1 x_i^2\\right)} & (\\text{separate sums}) \\\\\n0 &= \\sum_i^n{\\left(y_ix_i - \\bar{y}x_i\\right)} + \\sum_i^n{\\left(\\hat{\\beta}_1 \\bar{x}x_i - \\hat{\\beta}_1 x_i^2\\right)} & (\\text{take $\\hat{\\beta}_1$ out}) \\\\\n0 &= \\sum_i^n{\\left(y_ix_i - \\bar{y}x_i\\right)} + \\hat{\\beta}_1\\sum_i^n{\\left(\\bar{x}x_i - x_i^2\\right)} & (\\text{isolate}) \\\\\n\\hat{\\beta}_1 &= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})^2}\n\\end{align}\n\\]\n\n\n\nğŸ“ Give it a go! Use the same method as before to solve the equation and isolate \\(\\hat{\\beta}_1\\). Tip: Use the previous formula to substitute \\(\\hat{\\beta}_0\\)."
  },
  {
    "objectID": "slides/week02_slides_part1.html#parameter-estimation-ols",
    "href": "slides/week02_slides_part1.html#parameter-estimation-ols",
    "title": "ğŸ—“ï¸ Week 02:Linear Regression",
    "section": "Parameter Estimation (OLS)",
    "text": "Parameter Estimation (OLS)\nAnd that is how OLS works!\n\\[\n\\begin{align}\n\\hat{\\beta}_1 &= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})^2} \\\\\n\\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/week02_slides_part1.html#estimates-for-multiple-regression",
    "href": "slides/week02_slides_part1.html#estimates-for-multiple-regression",
    "title": "ğŸ—“ï¸ Week 02:Linear Regression",
    "section": "Estimates for Multiple Regression",
    "text": "Estimates for Multiple Regression\n\nThe process of estimation is similar when we have more than one predictor. To estimate:\n\\[\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\dots + \\hat{\\beta}_p x_p.\n\\]\n\n\nWe aim to minimize Residual Sum of Squares as before:\n\\[\n\\min \\mathrm{RSS} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_{i1} - \\hat{\\beta}_2 x_{i2} - \\dots - \\hat{\\beta}_p x_{ip})^2.\n\\]\nThis is done using standard statistical software â€” you need a good linear algebra solver.\n\n\nThe values \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_p\\) that minimize RSS are the multiple least squares regression coefficient estimates."
  },
  {
    "objectID": "slides/week02_slides_part1.html#example-advertising-data",
    "href": "slides/week02_slides_part1.html#example-advertising-data",
    "title": "ğŸ—“ï¸ Week 02:Linear Regression",
    "section": "Example: Advertising data",
    "text": "Example: Advertising data\n\n\nA sample of the data:\n\nlibrary(tidyverse)\n\nfile = \"https://www.statlearning.com/s/Advertising.csv\"\nadvertising <- read_csv(file) %>% select(-1)\nhead(advertising, 11)\n\n\n\n# A tibble: 11 Ã— 4\n      TV radio newspaper sales\n   <dbl> <dbl>     <dbl> <dbl>\n 1 230.   37.8      69.2  22.1\n 2  44.5  39.3      45.1  10.4\n 3  17.2  45.9      69.3   9.3\n 4 152.   41.3      58.5  18.5\n 5 181.   10.8      58.4  12.9\n 6   8.7  48.9      75     7.2\n 7  57.5  32.8      23.5  11.8\n 8 120.   19.6      11.6  13.2\n 9   8.6   2.1       1     4.8\n10 200.    2.6      21.2  10.6\n11  66.1   5.8      24.2   8.6\n\n\n\nHow the data is spread:\n\nsummary(advertising$TV)\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.70   74.38  149.75  147.04  218.82  296.40 \n\n\n\nsummary(advertising$radio)\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   9.975  22.900  23.264  36.525  49.600 \n\n\n\nsummary(advertising$newspaper)\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.30   12.75   25.75   30.55   45.10  114.00 \n\n\n\nsummary(advertising$sales)\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.60   10.38   12.90   14.02   17.40   27.00"
  },
  {
    "objectID": "slides/week02_slides_part1.html#simple-linear-regression-models",
    "href": "slides/week02_slides_part1.html#simple-linear-regression-models",
    "title": "ğŸ—“ï¸ Week 02:Linear Regression",
    "section": "Simple linear regression models",
    "text": "Simple linear regression models\n\n\n\nTV ğŸ“º\n\n\ntv_model <- lm(sales ~ TV, data=advertising)\ncat(sprintf(\"Sales (1k units) = %.4f %+.4f TV ($ 1k)\\n\", \n            tv_model$coefficients[\"(Intercept)\"], \n            tv_model$coefficients[\"TV\"]))\n\nSales (1k units) = 7.0326 +0.0475 TV ($ 1k)\n\n\n\nRadio ğŸ“»\n\n\nradio_model <- lm(sales ~ radio, data=advertising)\ncat(sprintf(\"Sales (1k units) = %.4f %+.4f Radio ($ 1k)\\n\", \n            radio_model$coefficients[\"(Intercept)\"], \n            radio_model$coefficients[\"radio\"]))\n\nSales (1k units) = 9.3116 +0.2025 Radio ($ 1k)\n\n\n\n\nNewspaper ğŸ“°\n\n\nnewspaper_model <- lm(sales ~ newspaper, data=advertising)\ncat(sprintf(\"Sales (1k units) = %.4f %+.4f Newspaper ($ 1k)\\n\", \n            newspaper_model$coefficients[\"(Intercept)\"], \n            newspaper_model$coefficients[\"newspaper\"]))\n\nSales (1k units) = 12.3514 +0.0547 Newspaper ($ 1k)\n\n\n\nğŸ—¨ï¸ How should we interpret these models?\n\n\n\n\n\nGather answers from students.\nFor every 1k dollars spent in advertising on a particular media channel, we expect more \\(\\hat{\\beta}_1\\) thousand units of the product to be sold.\nWhy donâ€™t the models agree about the intercept?"
  },
  {
    "objectID": "slides/week02_slides_part1.html#confidence-interval-of-coefficients",
    "href": "slides/week02_slides_part1.html#confidence-interval-of-coefficients",
    "title": "ğŸ—“ï¸ Week 02:Linear Regression",
    "section": "Confidence Interval of coefficients",
    "text": "Confidence Interval of coefficients\n\n\n\nThe confidence interval of an estimate has the form: \\[\n\\hat{\\beta}_1 \\pm 2 \\times \\mathrm{SE}(\\hat{\\beta}_1).\n\\] where \\(SE\\) is the standard error and reflects how the estimate varies under repeated sampling.\n\n\n\n\nThat is, there is approximately a 95% chance that the interval \\[\n\\biggl[ \\hat{\\beta}_1 - 2 \\times \\mathrm{SE}(\\hat{\\beta}_1), \\hat{\\beta}_1 + 2 \\times \\mathrm{SE}(\\hat{\\beta}_1) \\biggr]\n\\] will contain the true value of \\(\\beta_1\\).\n\n\n\nHow SE differs from STD? See (Altman and Bland 2005)"
  },
  {
    "objectID": "slides/week02_slides_part1.html#standard-errors",
    "href": "slides/week02_slides_part1.html#standard-errors",
    "title": "ğŸ—“ï¸ Week 02:Linear Regression",
    "section": "Standard Errors",
    "text": "Standard Errors\n\n\nThe standard error of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) is shown below:\n\\[\n\\begin{align}\n  \\mathrm{SE}(\\hat{\\beta}_1)^2 &= \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}, \\\\\n  \\mathrm{SE}(\\hat{\\beta}_0)^2 &= \\sigma^2 \\biggl[ \\frac{1}{n} +  \\frac{\\bar{x}^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\biggr],\n\\end{align}\n\\]\nwhere \\(\\sigma^2 = \\operatorname{Var}(\\epsilon)\\).\n\n\n\nBut, wait, we donâ€™t know \\(\\epsilon\\)! How would we compute \\(\\sigma^2\\)?\nIn practice, we aproximate \\(\\sigma^2 \\approx \\mathrm{RSE} = \\sqrt{\\mathrm{RSS}/(n-2)}\\).\n\n\n\n\n\n\n\n\n\nImportant\n\n\nğŸ’¡ Standard errors are a type of standard deviation but are not the same! See (Altman and Bland 2005) for more on this.\n\n\n\n\n\n\nThese formulas are only valid if we assume the errors \\(\\epsilon_i\\) have common variance \\(\\sigma^2\\) and are uncorrelated.\nRSE makes a comeback in Section 3.1.3"
  },
  {
    "objectID": "slides/week02_slides_part1.html#back-to-our-advertising-linear-models",
    "href": "slides/week02_slides_part1.html#back-to-our-advertising-linear-models",
    "title": "ğŸ—“ï¸ Week 02:Linear Regression",
    "section": "Back to our Advertising linear models",
    "text": "Back to our Advertising linear models\nWhat are the confidence intervals of our independent linear models?\n\n\n\nTV ğŸ“º\n\n\nconfint(tv_model)\n\n\n\n                 2.5 %     97.5 %\n(Intercept) 6.12971927 7.93546783\nTV          0.04223072 0.05284256\n\n\n\nRadio ğŸ“»\n\n\nconfint(radio_model)\n\n\n\n                2.5 %     97.5 %\n(Intercept) 8.2015885 10.4216877\nradio       0.1622443  0.2427472\n\n\n\n\nNewspaper ğŸ“°\n\n\nconfint(newspaper_model)\n\n\n\n                  2.5 %      97.5 %\n(Intercept) 11.12595560 13.57685854\nnewspaper    0.02200549  0.08738071\n\n\n\nğŸ—¨ï¸ What does it mean?\n\n\n\n\n\n\nFor every additional $1000 invested in Radio, we can expect an increase in sales of between 162 and 242 units.\n\n\n\nUse the function confint to compute confidence intervals in R."
  },
  {
    "objectID": "slides/week02_slides_part1.html#p-values",
    "href": "slides/week02_slides_part1.html#p-values",
    "title": "ğŸ—“ï¸ Week 02:Linear Regression",
    "section": "p-values",
    "text": "p-values\n\nTo test the null hypothesis, we compute a t-statistic, given by \\[\nt = \\frac{\\hat{\\beta}_1 - 0}{\\mathrm{SE}(\\hat{\\beta}_1)},\n\\]\nThis will have a t-distribution1 with \\(n - 2\\) degrees of freedom, assuming \\(\\beta_1 = 0\\).\nUsing statistical software, it is easy to compute the probability of observing any value equal to \\(\\mid t \\mid\\) or larger.\nWe call this probability the p-value.\n\nğŸ¤” How are the t-distribution and the Normal distribution related? Check this link to find out."
  },
  {
    "objectID": "slides/week02_slides_part1.html#back-to-our-advertising-linear-models-1",
    "href": "slides/week02_slides_part1.html#back-to-our-advertising-linear-models-1",
    "title": "ğŸ—“ï¸ Week 02:Linear Regression",
    "section": "Back to our Advertising linear models",
    "text": "Back to our Advertising linear models\nHow significant are the linear models?\n\n\n\nTV ğŸ“º\n\n\nout <- capture.output(summary(tv_model))\ncat(paste(out[9:15]), sep=\"\\n\")\n\n\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 7.032594   0.457843   15.36   <2e-16 ***\nTV          0.047537   0.002691   17.67   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nRadio ğŸ“»\n\n\nout <- capture.output(summary(radio_model))\ncat(paste(out[9:15]), sep=\"\\n\")\n\n\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  9.31164    0.56290  16.542   <2e-16 ***\nradio        0.20250    0.02041   9.921   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nNewspaper ğŸ“°\n\n\nout <- capture.output(summary(newspaper_model))\ncat(paste(out[9:15]), sep=\"\\n\")\n\n\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 12.35141    0.62142   19.88  < 2e-16 ***\nnewspaper    0.05469    0.01658    3.30  0.00115 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nğŸ—¨ï¸ What does it mean?"
  },
  {
    "objectID": "slides/week02_slides_part1.html#references",
    "href": "slides/week02_slides_part1.html#references",
    "title": "ğŸ—“ï¸ Week 02:Linear Regression",
    "section": "References",
    "text": "References\n\n\nAltman, Douglas G, and J Martin Bland. 2005. â€œStandard Deviations and Standard Errors.â€ BMJ 331 (7521): 903. https://doi.org/10.1136/bmj.331.7521.903.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York NY: Springer. https://www.statlearning.com/.\n\n\nKarafiath, Imre. 2009. â€œIs There a Viable Alternative to Ordinary Least Squares Regression When Security Abnormal Returns Are the Dependent Variable?â€ Review of Quantitative Finance and Accounting 32 (1): 17â€“31. https://doi.org/10.1007/s11156-007-0079-y.\n\n\n\n\n\nDS202 - Data Science for Social Scientists ğŸ¤– ğŸ¤¹"
  },
  {
    "objectID": "slides/week02_slides_part2.html#residual-standard-errors-rse",
    "href": "slides/week02_slides_part2.html#residual-standard-errors-rse",
    "title": "ğŸ—“ï¸ Week 02:Multiple Linear Regression",
    "section": "Residual Standard Errors (RSE)",
    "text": "Residual Standard Errors (RSE)\n\n\nRecall the â€œtrue modelâ€: \\(Y = f(X) + \\epsilon\\)\nEven if we knew the true values of \\(\\beta_0\\) and \\(\\beta_1\\) â€” not just the estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) â€” our predictions of sales might still be off.\nBy how much?"
  },
  {
    "objectID": "slides/week02_slides_part2.html#residual-standard-errors-rse-1",
    "href": "slides/week02_slides_part2.html#residual-standard-errors-rse-1",
    "title": "ğŸ—“ï¸ Week 02:Multiple Linear Regression",
    "section": "Residual Standard Errors (RSE)",
    "text": "Residual Standard Errors (RSE)\n\n\nThis can be estimated by the variance of errors: \\(\\sigma^2 = \\operatorname{Var}(\\epsilon)\\).\nAs said earlier, this quantity can be approximated, for the simple linear regression case, by the Residual Standard Errors (\\(\\mathrm{RSE}\\)) formula below:\n\n\n\n\\[\n\\sigma^2 \\approx \\mathrm{RSE} = \\sqrt{\\frac{\\mathrm{RSS}}{(n-\\mathrm{df})}}\n\\]\nwhere \\(\\mathrm{RSS} = \\sum_i^n{(y_i - \\hat{y}_i)^2}\\) represents the residual sum of squares and \\(\\mathrm{df}\\) represents the degrees of freedom in our model.\n\n\n\n\nâ¡ï¸ It turns out that \\(\\mathrm{RSE}\\) is a good way to assess the goodness-of-fit of a model."
  },
  {
    "objectID": "slides/week02_slides_part2.html#back-to-our-advertising-linear-models",
    "href": "slides/week02_slides_part2.html#back-to-our-advertising-linear-models",
    "title": "ğŸ—“ï¸ Week 02:Multiple Linear Regression",
    "section": "Back to our Advertising linear models",
    "text": "Back to our Advertising linear models\nLetâ€™s compare the linear models we fitted earlier:\n\n\n\n\n\n\nTV ğŸ“º\n\n\nout <- capture.output(summary(tv_model))\ncat(paste(out[16:16]), sep=\"\\n\")\n\n\n\nResidual standard error: 3.259 on 198 degrees of freedom\n\n\n\nRadio ğŸ“»\n\n\nout <- capture.output(summary(radio_model))\ncat(paste(out[16:16]), sep=\"\\n\")\n\n\n\nResidual standard error: 4.275 on 198 degrees of freedom\n\n\n\n\nNewspaper ğŸ“°\n\n\nout <- capture.output(summary(newspaper_model))\ncat(paste(out[16:16]), sep=\"\\n\")\n\n\n\nResidual standard error: 5.092 on 198 degrees of freedom\n\n\n\nğŸ—¨ï¸ What does it mean?"
  },
  {
    "objectID": "slides/week02_slides_part2.html#the-r2-statistic",
    "href": "slides/week02_slides_part2.html#the-r2-statistic",
    "title": "ğŸ—“ï¸ Week 02:Multiple Linear Regression",
    "section": "The \\(R^2\\) statistic",
    "text": "The \\(R^2\\) statistic\n\nR-squared or fraction of variance explained is defined as:\n\n\\[\nR^2 = \\frac{\\mathrm{TSS - RSS}}{\\mathrm{TSS}} = 1 - \\frac{\\mathrm{RSS}}{\\mathrm{TSS}}\n\\]\nwhere TSS = \\(\\sum_{i=1}^n (y_i - \\bar{y})^2\\) is the total sum of squares. \n\n\n\n\n\n\n\nTip\n\n\nIntuitively, \\(R^2\\) measures the proportion of variability in \\(Y\\) that can be explained using \\(X\\).\n\n\\(R^2\\) close to 1 means that a large proportion of the variance in \\(Y\\) is explained by the regression.\n\\(R^2\\) close to 0 means that the regression does not explain much of the variability in \\(Y\\)."
  },
  {
    "objectID": "slides/week02_slides_part2.html#sample-correlation-coefficient",
    "href": "slides/week02_slides_part2.html#sample-correlation-coefficient",
    "title": "ğŸ—“ï¸ Week 02:Multiple Linear Regression",
    "section": "Sample correlation coefficient",
    "text": "Sample correlation coefficient\nBy the way, in the simple linear regression setting, it can be shown that \\(R^2 = (\\operatorname{Cor}(X, Y))^2\\), where \\(\\operatorname{Cor}(X, Y)\\) is the correlation between \\(X\\) and \\(Y\\):\n\\[\n\\operatorname{Cor}(X, Y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}}.\n\\]\n\n\nğŸ“ Give it a go! Play around with the definition of \\(R^2\\) shown in the previous slide and verify that \\(R^2 = (\\operatorname{Cor}(X, Y))^2\\)."
  },
  {
    "objectID": "slides/week02_slides_part2.html#f-statistic",
    "href": "slides/week02_slides_part2.html#f-statistic",
    "title": "ğŸ—“ï¸ Week 02:Multiple Linear Regression",
    "section": "F-statistic",
    "text": "F-statistic\nWe used t-statistic to compute p-values for the coefficients (\\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\)).Now how do I test whether the model, as a whole, makes sense?\n\n\n\n\nFor this, we perform the hypothesis test: \\[\n\\begin{align}\n&~~~~H_0:&\\beta_1 = \\beta_2 = \\ldots = \\beta_j = 0 \\\\\n&\\text{vs} \\\\\n&~~~~H_A:& \\text{at least one } \\beta_j \\neq 0.\n\\end{align}\n\\]\n\n\n\n\nwhich is performed by computing the F-statistic: \\[\nF = \\frac{(TSS - RSS) / p}{RSS/(n - p - 1)} \\sim F_{p, n-p-1}\n\\]\n\n\n\n\n\nIf F is close to 1, there is no relationship between the response and the predictor(s).\nIf \\(H_A\\) is true, then we expect \\(F\\) to be greater than 1.\nCheck (James et al. 2021, 75â€“77) for an in-depth explanation of this test.\n\n\n\n\n\n\n\n\nNote that the F-statistic applies to both simple and multiple linear regression models.\nCheck this link if you want to understand the difference between the t-test and the the F-test."
  },
  {
    "objectID": "slides/week02_slides_part2.html#back-to-our-advertising-linear-models-1",
    "href": "slides/week02_slides_part2.html#back-to-our-advertising-linear-models-1",
    "title": "ğŸ—“ï¸ Week 02:Multiple Linear Regression",
    "section": "Back to our Advertising linear models",
    "text": "Back to our Advertising linear models\nHow well do our models explain the variability of the response?\n\n\n\nTV ğŸ“º\n\n\nout <- capture.output(summary(tv_model))\ncat(paste(out[17:18]), sep=\"\\n\")\n\n\n\nMultiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: < 2.2e-16\n\n\n\nRadio ğŸ“»\n\n\nout <- capture.output(summary(radio_model))\ncat(paste(out[17:18]), sep=\"\\n\")\n\n\n\nMultiple R-squared:  0.332, Adjusted R-squared:  0.3287 \nF-statistic: 98.42 on 1 and 198 DF,  p-value: < 2.2e-16\n\n\n\n\nNewspaper ğŸ“°\n\n\nout <- capture.output(summary(newspaper_model))\ncat(paste(out[17:18]), sep=\"\\n\")\n\n\n\nMultiple R-squared:  0.05212,   Adjusted R-squared:  0.04733 \nF-statistic: 10.89 on 1 and 198 DF,  p-value: 0.001148\n\n\n\nğŸ—¨ï¸ What does it mean?"
  },
  {
    "objectID": "slides/week02_slides_part2.html#a-multiple-linear-regression-to-advertising",
    "href": "slides/week02_slides_part2.html#a-multiple-linear-regression-to-advertising",
    "title": "ğŸ—“ï¸ Week 02:Multiple Linear Regression",
    "section": "A multiple linear regression to Advertising",
    "text": "A multiple linear regression to Advertising\n\n\nWhen you run a linear model in R, you can call the summary function to see and check all of these statistics weâ€™ve covered so far.\nBy now, you should be able to understand its full output\n\n\nFitting all predictors:\n\n\nTV ğŸ“º + Radio ğŸ“» + Newspaper ğŸ“°\n\n\nfull_model <- lm(sales ~ ., data=advertising)\nsummary(full_model)\n\n\n\nCall:\nlm(formula = sales ~ ., data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.938889   0.311908   9.422   <2e-16 ***\nTV           0.045765   0.001395  32.809   <2e-16 ***\nradio        0.188530   0.008611  21.893   <2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: < 2.2e-16\n\n\n\n\nConfidence Intervals\n\n\nconfint(full_model)\n\n\n                  2.5 %     97.5 %\n(Intercept)  2.32376228 3.55401646\nTV           0.04301371 0.04851558\nradio        0.17154745 0.20551259\nnewspaper   -0.01261595 0.01054097"
  },
  {
    "objectID": "slides/week02_slides_part2.html#interpreting-the-coefficients",
    "href": "slides/week02_slides_part2.html#interpreting-the-coefficients",
    "title": "ğŸ—“ï¸ Week 02:Multiple Linear Regression",
    "section": "Interpreting the coefficients",
    "text": "Interpreting the coefficients\n\nRecall the multiple regression model:\n\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p + \\epsilon ,\n\\]\n\n\nWe interpret \\(\\beta_j\\) as the average effect on \\(Y\\) of a one unit increase in \\(X_j\\), holding all other predictors fixed. In the advertising example, the model becomes\n\n\\[\n\\mathrm{sales} = \\beta_0 + \\beta_1 \\times \\mathrm{TV} + \\beta_2 \\times \\mathrm{radio} + \\beta_3 \\times \\mathrm{newspaper} + \\epsilon .\n\\]"
  },
  {
    "objectID": "slides/week02_slides_part2.html#interpreting-the-coefficients-1",
    "href": "slides/week02_slides_part2.html#interpreting-the-coefficients-1",
    "title": "ğŸ—“ï¸ Week 02:Multiple Linear Regression",
    "section": "Interpreting the coefficients",
    "text": "Interpreting the coefficients\n\n\nThe ideal scenario is when the predictors are uncorrelated â€“ a balanced design:\n\nEach coefficient can be estimated and tested separately.\nInterpretations such as â€œa unit change in \\(X_j\\) is associated with a \\(\\beta_j\\) change in \\(Y\\), while all the other variables stay fixedâ€, are possible.\n\nCorrelations amongst predictors cause problems:\n\nThe variance of all coefficients tends to increase, sometimes dramatically\nInterpretations become hazardous â€“ when \\(X_j\\) changes, everything else changes.\n\nClaims of causality should be avoided for observational data."
  },
  {
    "objectID": "slides/week02_slides_part2.html#references",
    "href": "slides/week02_slides_part2.html#references",
    "title": "ğŸ—“ï¸ Week 02:Multiple Linear Regression",
    "section": "References",
    "text": "References\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York NY: Springer. https://www.statlearning.com/.\n\n\n\n\n\nDS202 - Data Science for Social Scientists ğŸ¤– ğŸ¤¹"
  },
  {
    "objectID": "slides/week03_slides_part1.html#classification",
    "href": "slides/week03_slides_part1.html#classification",
    "title": "ğŸ—“ï¸ Week 03 Classifiers - Part I",
    "section": "Classification",
    "text": "Classification\n\n\nWe have so far only modelled quantitative responses.\nToday, we focus on predicting categorical, or qualitative, responses.\n\n\n\n\n\nThe generic supervised model:\n\\[\nY = \\operatorname{f}(X) + \\epsilon\n\\]\nstill applies, only this time \\(Y\\) is categorical. â¡ï¸\n\n\n\nOur categorical variables of interest take values in an unordered set \\(\\mathcal{C}\\), such as:\n\n\\(\\text{eye color} \\in \\mathcal{C} = \\{\\color{brown}{brown},\\color{blue}{blue},\\color{green}{green}\\}\\)\n\\(\\text{email} \\in \\mathcal{C} = \\{spam, ham\\}\\)\n\\(\\text{football results} \\in \\mathcal{C} \\{away\\ win,draw,home\\ win\\}\\)\n\n\n\n\n\nOpening slides - Unordered here is an important distinction. - We can also call it a class"
  },
  {
    "objectID": "slides/week03_slides_part1.html#why-cant-i-use-linear-regression",
    "href": "slides/week03_slides_part1.html#why-cant-i-use-linear-regression",
    "title": "ğŸ—“ï¸ Week 03 Classifiers - Part I",
    "section": "Why canâ€™t I use linear regression?",
    "text": "Why canâ€™t I use linear regression?\n\n\nWhat if I just coded each category as a number?\n\\[\nY =\n    \\begin{cases}\n        1 &\\text{if}~\\color{brown}{brown},\\\\\n        2 &\\text{if}~\\color{blue}{blue},\\\\\n        3 &\\text{if}~\\color{green}{green}.\n    \\end{cases}\n\\]\n\nWhat could go wrong?\n\n\n\n\n\n\nHow would you interpret a particular prediction if your model returned:\n\n\\(\\hat{y} = ~~1.5\\) or\n\\(\\hat{y} = ~~0.1\\) or\n\\(\\hat{y} = 20.0\\)?\n\n\n\n\n\n\n\nKey takeaway: Regression is not suitable for all problems. - regression cannot accommodate a qualitative response with more than two classes - regression will not provide meaningful estaimtes of Pr(Y|X)"
  },
  {
    "objectID": "slides/week03_slides_part1.html#more-on-classification",
    "href": "slides/week03_slides_part1.html#more-on-classification",
    "title": "ğŸ—“ï¸ Week 03 Classifiers - Part I",
    "section": "More on Classification",
    "text": "More on Classification\n\n\nOften we are more interested in estimating the probabilities that \\(X\\) belongs to each category in \\(\\mathcal{C}\\).\n\n\n\n\nFor example, it is sometimes more valuable to have an estimate of the probability that an insurance claim is fraudulent, than a classification fraudulent or not.\n\n\n\n\n\n\n\nA successful gambling strategy, for instance, requires placing bets on outcomes to which you believe the bookmakers have assigned incorrect probabilities. Knowing the most likely outcome is not enough!\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nStatistical models for ordinal response, when sets are discrete but have an order, are outside the scope of this course. Should you need to create models for ordinal variables, consult â€œordinal logistic regressionâ€. A good reference about this is (Agresti 2019, chap. 6).\n\n\n\n\n\nKey takeaway: normally, we estimate"
  },
  {
    "objectID": "slides/week03_slides_part1.html#speaking-of-probabilities",
    "href": "slides/week03_slides_part1.html#speaking-of-probabilities",
    "title": "ğŸ—“ï¸ Week 03 Classifiers - Part I",
    "section": "Speaking of Probabilitiesâ€¦",
    "text": "Speaking of Probabilitiesâ€¦\nLetâ€™s talk about three possible interpretations of probability:\n\n\n\n\nClassical\n\n\n\n\nFrequentist\n\n\n\n\nBayesian\n\n\n\n\n\nEvents of the same kind can be reduced to a certain number of equally possible cases.\nExample: coin tosses lead to either heads or tails \\(1/2\\) of the time ( \\(50\\%/50\\%\\))\n\n\n\n\nWhat would be the outcome if I repeat the process many times?\nExample: if I toss a coin \\(1,000,000\\) times, I expect \\(\\approx 50\\%\\) heads and \\(\\approx 50\\%\\) tails outcome.\n\n\n\n\nWhat is your judgement of the likelihood of the outcome? Based on previous information.\nExample: if I know that this coin has symmetric weight, I expect a \\(50\\%/50\\%\\) outcome.\n\n\n\n\n\n\nSource: (DeGroot and Schervish 2003)"
  },
  {
    "objectID": "slides/week03_slides_part1.html#speaking-of-probabilities-1",
    "href": "slides/week03_slides_part1.html#speaking-of-probabilities-1",
    "title": "ğŸ—“ï¸ Week 03 Classifiers - Part I",
    "section": "Speaking of Probabilitiesâ€¦",
    "text": "Speaking of Probabilitiesâ€¦\nFor our purposes:\n\nProbabilities are numbers between 0 and 1\nThe sum of all possible outcomes of an event must sum to 1.\nIt is useful to think of things as probabilities\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nğŸ’¡ Although there is no such thing as â€œa probability of \\(120\\%\\)â€ or â€œa probability of \\(-23\\%\\)â€, you could still use this language to refer to increase or decrease in an outcome."
  },
  {
    "objectID": "slides/week03_slides_part1.html#the-logistic-regression-model",
    "href": "slides/week03_slides_part1.html#the-logistic-regression-model",
    "title": "ğŸ—“ï¸ Week 03 Classifiers - Part I",
    "section": "The Logistic Regression model",
    "text": "The Logistic Regression model\n\nConsider a binary response:\n\\[\nY = \\begin{cases}\n0 \\\\\n1\n\\end{cases}\n\\]\n\n\nWe model the probability that \\(Y = 1\\) using the logistic function (aka. sigmoid curve):\n\\[\nPr(Y = 1|X) = p(X) = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1 X}}\n\\]\n\n\n\n\n\nSource of illustration: TIBCO\n\n\n\nThis is how this function looks like"
  },
  {
    "objectID": "slides/week03_slides_part1.html#the-logistic-function",
    "href": "slides/week03_slides_part1.html#the-logistic-function",
    "title": "ğŸ—“ï¸ Week 03 Classifiers - Part I",
    "section": "The Logistic function",
    "text": "The Logistic function\n\nChanging \\(\\beta_0\\) while keeping \\(\\beta_1 = 1\\):"
  },
  {
    "objectID": "slides/week03_slides_part1.html#the-logistic-function-cont.",
    "href": "slides/week03_slides_part1.html#the-logistic-function-cont.",
    "title": "ğŸ—“ï¸ Week 03 Classifiers - Part I",
    "section": "The Logistic function (cont.)",
    "text": "The Logistic function (cont.)\n\nKeep \\(\\beta_0 = 0\\) but vary \\(\\beta_1\\):"
  },
  {
    "objectID": "slides/week03_slides_part1.html#maximum-likelihood-estimate",
    "href": "slides/week03_slides_part1.html#maximum-likelihood-estimate",
    "title": "ğŸ—“ï¸ Week 03 Classifiers - Part I",
    "section": "Maximum likelihood estimate",
    "text": "Maximum likelihood estimate\n\nAs with linear regression, the coefficients are unknown and need to be estimated from training data:\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1X}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}}\n\\]\n\n\nWe estimate these by maximising the likelihood function:\n\\[\n\\max \\ell(\\beta_0, \\beta_1) = \\prod_{i:y_i=1}{p(x_i)} \\prod_{i':y_{i'}=0} (1 - p(x_{i'})),\n\\]\nand we call this method the Maximum Likelihood Estimate (MLE).\n\n\nâ¡ï¸ As usual, there are multiple ways to solve this equation!\n\n\n\nKey takeaway of this slide: MLE (logistic regression) is analogous to OLS (linear regression).\nIntuition: What are the values for \\(\\alpha\\) and \\(\\beta\\) that generate predicted probabilities, \\(\\hat{Y}_i\\) for each training observation that are as close as possible to the realised outcomes, \\(Y_i\\)?\n\n\n\nğŸ’¡ If you want to read on how exactly this is solved check this link"
  },
  {
    "objectID": "slides/week03_slides_part1.html#solutions-to-mle",
    "href": "slides/week03_slides_part1.html#solutions-to-mle",
    "title": "ğŸ—“ï¸ Week 03 Classifiers - Part I",
    "section": "Solutions to MLE",
    "text": "Solutions to MLE\n\n\nMLE is much more difficult to solve than the least squares formulations.\nMost solutions rely on a variant of the Hill Climbing algorithm\n\n\n\n\n\n\n\n\n\nHow do you find the latitude and longitude of a mountain peak if you canâ€™t see very far?\n\n\n\nStart somewhere.\nLook around for the best way to go up.\nGo a small distance in that direction.\nLook around for the best way to go up.\nGo a small distance in that direction.\n\\(\\cdots\\)\n\n\n\n\n\n\n\nAdvanced: If for whatever random reason, you find yourself enamored with the Maximum Likelihood Estimate, check (Agresti 2019) for a recent take on the statistical properties of this method."
  },
  {
    "objectID": "slides/week03_slides_part1.html#the-concept-of-odds",
    "href": "slides/week03_slides_part1.html#the-concept-of-odds",
    "title": "ğŸ—“ï¸ Week 03 Classifiers - Part I",
    "section": "The concept of odds",
    "text": "The concept of odds\nThe quantity below is called the odds:\n\\[\n\\frac{p(X)}{1 - p(X)} = e^{\\beta_0 + \\beta_1 X}\n\\]\n\n\n\n\n\n\nExample\n\n\nIf the odds are 9, then \\(\\frac{p(X)}{(1-p(X))} = 9 \\Rightarrow p(X) = 0.9\\).\nThis means that 9 out of 10 people will default.\n\n\n\n\n\n\n\n\n\nTip\n\n\nHow to interpret \\(\\beta_1\\)\nIf X increases one unit then the odds increase by a factor of \\(e^{\\beta_1}\\)\n\n\n\n\n\nğŸ“ Give it a go! Using algebra, can you re-arrange the equation for \\(p(X)\\) presented in the Logistic regression model slides to arrive at the odds quantity shown above?"
  },
  {
    "objectID": "slides/week03_slides_part1.html#log-odds-or-logit",
    "href": "slides/week03_slides_part1.html#log-odds-or-logit",
    "title": "ğŸ—“ï¸ Week 03 Classifiers - Part I",
    "section": "Log odds or logit",
    "text": "Log odds or logit\n\nIt is also useful to think of the odds in log terms.\n\n\\[\nlog\\left(\\frac{p(X)}{1 - p(X)}\\right) = \\beta_0 + \\beta_1 X\n\\]\n\nWe call the quantity above the log odds or logit\n\n\n\n\n\n\n\nTip\n\n\nHow to interpret \\(\\beta_1\\)\nIf X increases one unit then the log odds increase by \\(\\beta_1\\)"
  },
  {
    "objectID": "slides/week03_slides_part1.html#example-default-data",
    "href": "slides/week03_slides_part1.html#example-default-data",
    "title": "ğŸ—“ï¸ Week 03 Classifiers - Part I",
    "section": "Example: Default data",
    "text": "Example: Default data\n\n\nA sample of the data:\n\nlibrary(ISLR2)\n\nhead(ISLR2::Default, n=15)\n\n\n\n   default student   balance    income\n1       No      No  729.5265 44361.625\n2       No     Yes  817.1804 12106.135\n3       No      No 1073.5492 31767.139\n4       No      No  529.2506 35704.494\n5       No      No  785.6559 38463.496\n6       No     Yes  919.5885  7491.559\n7       No      No  825.5133 24905.227\n8       No     Yes  808.6675 17600.451\n9       No      No 1161.0579 37468.529\n10      No      No    0.0000 29275.268\n11      No     Yes    0.0000 21871.073\n12      No     Yes 1220.5838 13268.562\n13      No      No  237.0451 28251.695\n14      No      No  606.7423 44994.556\n15      No      No 1112.9684 23810.174\n\n\n\nHow the data is spread:\n\nsummary(ISLR2::Default$default)\n\n\n\n  No  Yes \n9667  333 \n\n\n\nsummary(ISLR2::Default$balance)\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    0.0   481.7   823.6   835.4  1166.3  2654.3 \n\n\n\nsummary(ISLR2::Default$income)\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    772   21340   34553   33517   43808   73554 \n\n\n\nsummary(ISLR2::Default$student)\n\n\n\n  No  Yes \n7056 2944"
  },
  {
    "objectID": "slides/week03_slides_part1.html#simple-logistic-regression-models",
    "href": "slides/week03_slides_part1.html#simple-logistic-regression-models",
    "title": "ğŸ—“ï¸ Week 03 Classifiers - Part I",
    "section": "Simple logistic regression models",
    "text": "Simple logistic regression models\n\n\n\nIncome ğŸ’°\n\n\nincome_model <- \n  glm(default ~ income, data=ISLR2::Default, family=binomial)\ncat(sprintf(\"beta_0 = %.5f | beta_1 = %e\",\n            income_model$coefficients[\"(Intercept)\"],\n            income_model$coefficients[\"income\"]))\n\nbeta_0 = -3.09415 | beta_1 = -8.352575e-06\n\n\n\nBalance ğŸ’¸\n\n\nbalance_model <- \n  glm(default ~ balance, data=ISLR2::Default, family=binomial)\ncat(sprintf(\"beta_0 = %.5f | beta_1 = %.4f\",\n            balance_model$coefficients[\"(Intercept)\"],\n            balance_model$coefficients[\"balance\"]))\n\nbeta_0 = -10.65133 | beta_1 = 0.0055\n\n\n\n\nStudent ğŸ§‘â€ğŸ“\n\n\nstudent_model <- \n  glm(default ~ student, data=ISLR2::Default, family=binomial)\ncat(sprintf(\"beta_0 = %.5f | beta_1 = %.4f\",\n            student_model$coefficients[\"(Intercept)\"],\n            student_model$coefficients[\"studentYes\"]))\n\nbeta_0 = -3.50413 | beta_1 = 0.4049\n\n\n\n\n\n\n\n\n\nNote\n\n\nLogistic regression coefficients are a bit trickier to interpret when compared to those of linear regression. Letâ€™s look at how it works â¡ï¸\n\n\n\n\n\n\n\n\nGather answers from students."
  },
  {
    "objectID": "slides/week03_slides_part1.html#example-default-vs-balance",
    "href": "slides/week03_slides_part1.html#example-default-vs-balance",
    "title": "ğŸ—“ï¸ Week 03 Classifiers - Part I",
    "section": "Example: Default vs Balance",
    "text": "Example: Default vs Balance\n\n\n\nModel: Default vs Balance ğŸ’¸\n\n\\[\n\\hat{y} = \\frac{e^{-10.65133 + 0.005498917X}}{1 + e^{-10.65133 + 0.005498917X}}\n\\]\n\n  That is:\n\\[\n\\begin{align}\n\\hat{\\beta}_0 &= -10.65133\\\\\n\\hat{\\beta}_1 &= 0.005498917\n\\end{align}\n\\]\n\n\n\n\n\nInterpreting \\(\\hat{\\beta}_0\\):\n\nIn the absence of balance information:\n\nLog odds: \\(-10.65133\\)\nOdds : \\(e^{-10.65133} = 2.366933 \\times 10^{-5}\\) \\[\n\\begin{align}\np(\\text{default}=\\text{Yes}) &= \\frac{\\text{odds}}{(1 + \\text{odds})} \\\\\n                           &= 2.366877 \\times 10^{-5}\\end{align}\n\\]\n\n\n\n\n\nInterpreting \\(\\hat{\\beta}_1\\):\n\nWith balance information:\n\nLog odds: \\(0.005498917\\)\nOdds : \\(e^{0.005498917} = 1.005514\\)\nThat is, for every \\(\\$1\\) increase in balance, the probability of default increases \n\n\n\n\n\n\n\nNote that the increase is cumulative, not linear. It depends on where X is."
  },
  {
    "objectID": "slides/week03_slides_part1.html#example-default-vs-income",
    "href": "slides/week03_slides_part1.html#example-default-vs-income",
    "title": "ğŸ—“ï¸ Week 03 Classifiers - Part I",
    "section": "Example: Default vs Income",
    "text": "Example: Default vs Income\n\n\n\nModel: Default vs Income ğŸ’°\n\n\\[\n\\hat{y} = \\frac{e^{-3.094149 - 8.352575 \\times 10^{-6} X}}{1 + e^{-3.094149 - 8.352575 \\times 10^{-6} X}}\n\\]\n\n  That is:\n\\[\n\\begin{align}\n\\hat{\\beta}_0 &= - 3.094149\\\\\n\\hat{\\beta}_1 &= - 8.352575 \\times 10^{-6}\n\\end{align}\n\\]\n\n\n\n\n\nInterpreting \\(\\hat{\\beta}_0\\):\n\nIn the absence of balance information:\n\nLog odds: \\(- 3.094149\\)\nOdds : \\(e^{- 3.094149} = 0.04531355\\) \\[\n\\begin{align}\np(\\text{default}=\\text{Yes}) &= \\frac{\\text{odds}}{(1 + \\text{odds})} \\\\\n                           &= 0.04334924 = 4.33\\%\\end{align}\n\\]\n\n\n\n\n\nInterpreting \\(\\hat{\\beta}_1\\):\n\nWith balance information:\n\nLog odds: \\(- 8.352575\\)\nOdds : \\(e^{- 8.352575} = 0.9999916\\)\nThat is, for every \\(\\$1\\) increase in income, the probability of default decreases \n\n\n\n\n\n\n\nNote that the increase is cumulative, not linear. It depends on where X is."
  },
  {
    "objectID": "slides/week03_slides_part1.html#example-default-vs-is-student",
    "href": "slides/week03_slides_part1.html#example-default-vs-is-student",
    "title": "ğŸ—“ï¸ Week 03 Classifiers - Part I",
    "section": "Example: Default vs Is Student?",
    "text": "Example: Default vs Is Student?\n\n\n\nModel: Default vs Student ğŸ§‘â€ğŸ“\n\n\\[\n\\hat{y} = \\frac{e^{-3.504128 + 0.4048871 X}}{1 + e^{-3.504128 + 0.4048871 X}}\n\\]\n\n  That is:\n\\[\n\\begin{align}\n\\hat{\\beta}_0 &= -3.504128\\\\\n\\hat{\\beta}_1 &= +0.4048871\n\\end{align}\n\\]\n\n\n\n\n\nInterpreting \\(\\hat{\\beta}_0\\):\n\nIn the absence of balance information:\n\nLog odds: \\(-3.504128\\)\nOdds : \\(e^{- 3.504128} = 0.03007299\\) \\[\n\\begin{align}\np(\\text{default}=\\text{Yes}) &= \\frac{\\text{odds}}{(1 + \\text{odds})} \\\\\n                           &= 0.02919501 \\approx 2.92\\%\\end{align}\n\\]\n\n\n\n\n\nInterpreting \\(\\hat{\\beta}_1\\):\n\nWith balance information:\n\nLog odds: \\(0.4048871\\)\nOdds : \\(e^{0.4048871} = 1.499133\\)\n\nIf person is a student, then the probability of default increases"
  },
  {
    "objectID": "slides/week03_slides_part1.html#model-info",
    "href": "slides/week03_slides_part1.html#model-info",
    "title": "ğŸ—“ï¸ Week 03 Classifiers - Part I",
    "section": "Model info",
    "text": "Model info\nThe output of summary is similar to that of linear regression:\n\n\n\nModel: Default vs Balance ğŸ’¸\n\n\n\nsummary(balance_model)\n\n\n\nCall:\nglm(formula = default ~ balance, family = binomial, data = ISLR2::Default)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.2697  -0.1465  -0.0589  -0.0221   3.7589  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.065e+01  3.612e-01  -29.49   <2e-16 ***\nbalance      5.499e-03  2.204e-04   24.95   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1596.5  on 9998  degrees of freedom\nAIC: 1600.5\n\nNumber of Fisher Scoring iterations: 8\n\n\n\n\nConfidence Intervals\n\n\nconfint(balance_model)\n\n\n                    2.5 %       97.5 %\n(Intercept) -11.383288936 -9.966565064\nbalance       0.005078926  0.005943365"
  },
  {
    "objectID": "slides/week03_slides_part1.html#wraping-up-on-coefficients",
    "href": "slides/week03_slides_part1.html#wraping-up-on-coefficients",
    "title": "ğŸ—“ï¸ Week 03 Classifiers - Part I",
    "section": "Wraping up on coefficients:",
    "text": "Wraping up on coefficients:\n\n\n\nPay attention to the sign of the coefficient. The sign of the coefficients indicate the direction of the association.\nIf the value of a predictor increases, we look at the sign of its coefficient:\n\nIf it is a â• positive coefficient, we predict an increase in the probability of the class\nIf it is a â– negative coefficient, we predict a decrease in the probability of the class"
  },
  {
    "objectID": "slides/week03_slides_part1.html#multiple-logistic-regression-1",
    "href": "slides/week03_slides_part1.html#multiple-logistic-regression-1",
    "title": "ğŸ—“ï¸ Week 03 Classifiers - Part I",
    "section": "Multiple Logistic Regression",
    "text": "Multiple Logistic Regression\n\nIt is straightforward to extend the logistic model to include multiple predictors:\n\n\\[\nlog \\left( \\frac{p(X)}{1-p(X)} \\right)=\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p\n\\]\n\\[\np(X) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p}}\n\\]\n\nMost things are still available (hypothesis test, confidence intervals, etc.)\nLetâ€™s explore the output and summary of the full model â­ï¸"
  },
  {
    "objectID": "slides/week03_slides_part1.html#fitting-all-predictors-of-default",
    "href": "slides/week03_slides_part1.html#fitting-all-predictors-of-default",
    "title": "ğŸ—“ï¸ Week 03 Classifiers - Part I",
    "section": "Fitting all predictors of Default",
    "text": "Fitting all predictors of Default\n\n\nFull Model\n\n\nfull_model <- glm(default ~ ., data=ISLR2::Default, family=binomial)\nsummary(full_model)\n\n\n\nCall:\nglm(formula = default ~ ., family = binomial, data = ISLR2::Default)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.4691  -0.1418  -0.0557  -0.0203   3.7383  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.087e+01  4.923e-01 -22.080  < 2e-16 ***\nstudentYes  -6.468e-01  2.363e-01  -2.738  0.00619 ** \nbalance      5.737e-03  2.319e-04  24.738  < 2e-16 ***\nincome       3.033e-06  8.203e-06   0.370  0.71152    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1571.5  on 9996  degrees of freedom\nAIC: 1579.5\n\nNumber of Fisher Scoring iterations: 8\n\n\n\n\nConfidence Intervals\n\n\nconfint(full_model)\n\n\n                    2.5 %        97.5 %\n(Intercept) -1.185902e+01 -9.928174e+00\nstudentYes  -1.109018e+00 -1.822147e-01\nbalance      5.294898e-03  6.204587e-03\nincome      -1.304712e-05  1.912447e-05"
  },
  {
    "objectID": "slides/week03_slides_part1.html#references",
    "href": "slides/week03_slides_part1.html#references",
    "title": "ğŸ—“ï¸ Week 03 Classifiers - Part I",
    "section": "References",
    "text": "References\n\n\nAgresti, Alan. 2019. An Introduction to Categorical Data Analysis. Third edition. Wiley Series in Probability and Statistics. Hoboken, NJ: John Wiley & Sons.\n\n\nDeGroot, Morris H., and Mark J. Schervish. 2003. Probability and Statistics. 3. ed., international edition. Boston Munich: Addison-Wesley.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York NY: Springer. https://www.statlearning.com/.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDS202 - Data Science for Social Scientists ğŸ¤– ğŸ¤¹"
  },
  {
    "objectID": "slides/week03_slides_part2.html#bayes-theorem",
    "href": "slides/week03_slides_part2.html#bayes-theorem",
    "title": "ğŸ—“ï¸ Week 03:Classifiers - Part II",
    "section": "Bayesâ€™ Theorem",
    "text": "Bayesâ€™ Theorem\n\nBefore we go on to explain what Naive Bayes is about, we need to understand the formula below.\n\n\\[\nP(\\mathbf{Y} = k | \\mathbf{X} = x) = \\frac{P(k)P(\\mathbf{X}|\\mathbf{Y}=k)}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}\n\\]\n\n\nLetâ€™s look at it step-by-step â­ï¸"
  },
  {
    "objectID": "slides/week03_slides_part2.html#bayes-theorem-1",
    "href": "slides/week03_slides_part2.html#bayes-theorem-1",
    "title": "ğŸ—“ï¸ Week 03:Classifiers - Part II",
    "section": "Bayesâ€™ Theorem",
    "text": "Bayesâ€™ Theorem\n\\[\nP(\\mathbf{Y} = k | \\mathbf{X} = x) = \\frac{P(k)P(\\mathbf{X}|\\mathbf{Y}=k)}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}\n\\]\nNew variables\n\n\n\\(K \\Rightarrow\\) is the set of classes. In the binary case, \\(K = \\{0, 1\\}\\).\n\\(P(k) \\Rightarrow\\) is the probability that a random sample belongs to class \\(k\\).\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe textbook uses a slightly different notation."
  },
  {
    "objectID": "slides/week03_slides_part2.html#bayes-theorem-2",
    "href": "slides/week03_slides_part2.html#bayes-theorem-2",
    "title": "ğŸ—“ï¸ Week 03:Classifiers - Part II",
    "section": "Bayesâ€™ Theorem",
    "text": "Bayesâ€™ Theorem\n\\[\n\\color{blue}{P(\\mathbf{Y} = k | \\mathbf{X} = x)} \\color{Gainsboro}{= \\frac{P(k)P(\\mathbf{X}|\\mathbf{Y}=k)}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}}\n\\]\n\n\nThe quantity above (in blue) is called the posterior distribution\nIt is what we are interested in when making inferences/predictions\n\n\n\nRead it as:\n\nWhat is the probability that the class is \\(k\\) given that the sample is \\(x\\)?"
  },
  {
    "objectID": "slides/week03_slides_part2.html#bayes-theorem-3",
    "href": "slides/week03_slides_part2.html#bayes-theorem-3",
    "title": "ğŸ—“ï¸ Week 03:Classifiers - Part II",
    "section": "Bayesâ€™ Theorem",
    "text": "Bayesâ€™ Theorem\n\\[\n\\color{Gainsboro}{P(\\mathbf{Y} = k | \\mathbf{X} = x) =} \\frac{\\color{blue}{P(k)}\\color{Gainsboro}{P(\\mathbf{X}|\\mathbf{Y}=k)}}{\\color{Gainsboro}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}}\n\\]\n\n\nThe quantity above (in blue) is called the prior distribution\nIt represents the proportion of samples of class \\(k\\) we believe (estimate) we would find if sampling at random.\n\n\n\nRead it as:\n\nWhat is the probability that the class is \\(k\\) given a random sample?"
  },
  {
    "objectID": "slides/week03_slides_part2.html#bayes-theorem-4",
    "href": "slides/week03_slides_part2.html#bayes-theorem-4",
    "title": "ğŸ—“ï¸ Week 03:Classifiers - Part II",
    "section": "Bayesâ€™ Theorem",
    "text": "Bayesâ€™ Theorem\n\\[\n\\color{Gainsboro}{P(\\mathbf{Y} = k | \\mathbf{X} = x) =} \\frac{\\color{Gainsboro}{P(k)}\\color{blue}{P(\\mathbf{X}|\\mathbf{Y}=k)}}{\\color{Gainsboro}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}}\n\\]\n\n\nThe quantity above (in blue) is often called the likelihood\nIt represents the density function of \\(\\mathbf{X}\\) for samples of class \\(k\\).\n\n\n\nThink of it as:\n\nWhat values would I expect \\(X\\) to take when the class is \\(\\mathbf{Y} = k\\)?"
  },
  {
    "objectID": "slides/week03_slides_part2.html#bayes-theorem-5",
    "href": "slides/week03_slides_part2.html#bayes-theorem-5",
    "title": "ğŸ—“ï¸ Week 03:Classifiers - Part II",
    "section": "Bayesâ€™ Theorem",
    "text": "Bayesâ€™ Theorem\n\\[\n\\color{Gainsboro}{P(\\mathbf{Y} = k | \\mathbf{X} = x) =} \\frac{\\color{Gainsboro}{P(k)}\\color{Gainsboro}{P(\\mathbf{X}|\\mathbf{Y}=k)}}{\\color{blue}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}}\n\\]\n\n\nThe quantity above (in blue) represents the density function of \\(\\mathbf{X}\\) regardless of the class\nIt is often called the marginal probability of \\(\\mathbf{X}\\).\n\nNote that \\(\\color{blue}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}} = P(\\mathbf{X})\\)\n\n\n\n\nThink of it as:\n\nWhat values would I expect \\(X\\) if ignored the class?"
  },
  {
    "objectID": "slides/week03_slides_part2.html#bayes-theorem-6",
    "href": "slides/week03_slides_part2.html#bayes-theorem-6",
    "title": "ğŸ—“ï¸ Week 03:Classifiers - Part II",
    "section": "Bayesâ€™ Theorem",
    "text": "Bayesâ€™ Theorem\n\\[\nP(\\mathbf{Y} = k | \\mathbf{X} = x) = \\frac{P(k)P(\\mathbf{X}|\\mathbf{Y}=k)}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}\n\\]\n\nLetâ€™s look at how different algorithms explore this rule â­ï¸"
  },
  {
    "objectID": "slides/week03_slides_part2.html#linear-discriminant-analysis-lda",
    "href": "slides/week03_slides_part2.html#linear-discriminant-analysis-lda",
    "title": "ğŸ—“ï¸ Week 03:Classifiers - Part II",
    "section": "Linear Discriminant Analysis (LDA)",
    "text": "Linear Discriminant Analysis (LDA)\n\n\nAssumptions:\n\nLikelihood follows a Gaussian distribution\n\nEach class has its own mean, \\(\\mu_k\\)\nAll classes have the same standard deviation\n\nThat is, \\(\\sigma^2_1 = \\sigma^2_2 = \\ldots = \\sigma^2_K\\), or simply \\(\\sigma^2\\)\n\n\nWe denote this as: \\(P(\\mathbf{X}|\\mathbf{Y}=k) \\sim N(\\mu_k, \\sigma^2)\\)"
  },
  {
    "objectID": "slides/week03_slides_part2.html#lda---estimates",
    "href": "slides/week03_slides_part2.html#lda---estimates",
    "title": "ğŸ—“ï¸ Week 03:Classifiers - Part II",
    "section": "LDA - Estimates",
    "text": "LDA - Estimates\n\nWe estimate the mean per class and the shared standard deviation as follows:\n\n\n\\[\n\\begin{align}\n\\hat{\\mu}_k &= \\frac{1}{n_k}\\sum_{i:y_i=k}{x_i}\\\\\n\\hat{\\sigma}^2 &= \\frac{1}{n - K}\\sum_{k=1}^K{\\sum_{i:y_i=k}{\\left(x_i - \\hat{\\mu}_k\\right)^2}} \\\\\n\\hat{P}(k) &= \\frac{n_k}{n}\n\\end{align}\n\\]\n\n\n\nwhere:\n\n\\(n\\) is the total number of training observations\n\\(n_k\\) is the number of training observations in the \\(k\\)th class\n\n\n\n\n\nRead (James et al. 2021, sec. 4.4) to understand why these estimates are the way they are.\n\n\n\n\nMention that priors could come from prior knowledge"
  },
  {
    "objectID": "slides/week03_slides_part2.html#naive-bayes-classifier-1",
    "href": "slides/week03_slides_part2.html#naive-bayes-classifier-1",
    "title": "ğŸ—“ï¸ Week 03:Classifiers - Part II",
    "section": "Naive Bayes Classifier",
    "text": "Naive Bayes Classifier\n\n\nMain Assumption:\n\n\nWithin the \\(k\\)th class, the \\(p\\) predictors are independent\n\n\n\n\nAssuming features are not associated (not correlated), the likelihood becomes: \\[\nP(\\mathbf{X}|\\mathbf{Y}=k) = \\underbrace{P(x_1 |\\mathbf{Y}=k)}_{1\\text{st} \\text{ predictor}} \\times \\underbrace{P(x_2 |\\mathbf{Y}=k)}_{2\\text{nd} \\text{ predictor}} \\times \\ldots \\times \\underbrace{P(x_p |\\mathbf{Y}=k)}_{p\\text{-th} \\text{ predictor}}\n\\]\n\n\n\n\nThis means the posterior is given by: \\[\nP(\\mathbf{Y} = k| \\mathbf{X} = x) = \\frac{\\quad\\quad P(k) \\times P(x_1 |\\mathbf{Y}=k) \\times P(x_2 |\\mathbf{Y}=k) \\times \\ldots \\times P(x_p |\\mathbf{Y}=k)}{\\sum_{l=1}^K{P(l) \\times P(x_1 |\\mathbf{Y}=l) \\times P(x_2 |\\mathbf{Y}=l) \\times \\ldots \\times P(x_p |\\mathbf{Y}=l)}}\n\\]"
  },
  {
    "objectID": "slides/week03_slides_part2.html#a-naive-approach-indeed",
    "href": "slides/week03_slides_part2.html#a-naive-approach-indeed",
    "title": "ğŸ—“ï¸ Week 03:Classifiers - Part II",
    "section": "A naive approach indeed",
    "text": "A naive approach indeed\n\n\nThis may all look very complicated but it is actually quite simple\n\n\n\n\nIf data is discrete (categorical), you just count the proportion of each category.\n\nExample:\n\\[\nP(\\mathbf{Y} = k| \\mathbf{X}_j = x_j) =\n\\begin{cases}\n0.32 & \\text{if } x_j = 1\\\\\n0.55 & \\text{if } x_j = 2\\\\\n0.13 & \\text{if } x_j = 3\n\\end{cases}\n\\]\n\n\n\nIf data is continuous, use a histogram as an estimate for the true density of \\(x_p\\)\n\nAlternatively, use a kernel density estimator"
  },
  {
    "objectID": "slides/week03_slides_part2.html#default-yes-or-no",
    "href": "slides/week03_slides_part2.html#default-yes-or-no",
    "title": "ğŸ—“ï¸ Week 03:Classifiers - Part II",
    "section": "Default: Yes or No?",
    "text": "Default: Yes or No?\n\n\nWe have looked at how the probabilities (risk of default) change according to the value of predictors\nBut in practice we need to decide whether the risk is too high or tolerable\nIn our example, we might want to ask:\n\n\n\n\nâ€œWill this person default on their credit card? YES or NO?â€"
  },
  {
    "objectID": "slides/week03_slides_part2.html#default-yes-or-no-1",
    "href": "slides/week03_slides_part2.html#default-yes-or-no-1",
    "title": "ğŸ—“ï¸ Week 03:Classifiers - Part II",
    "section": "Default: Yes or No?",
    "text": "Default: Yes or No?\n\n\nHow would you classify the following customers?\n\n\nCode\nlibrary(tidyverse)\n\nfull_model <- \n  glm(default ~ ., data=ISLR2::Default, family=binomial)\n\nset.seed(40)\nsample_customers <- \n  ISLR2::Default %>% \n  slice(9986, 9908, 6848, 9762, 9979, 7438)\npred <- predict(full_model, sample_customers, type=\"response\")\n# Format it as percentage\nsample_customers$prediction <- \n  sapply(pred, function(x){sprintf(\"%.2f %%\", 100*x)})\nsample_customers\n\n\n  default student   balance   income prediction\n1      No      No  842.9494 39957.13     0.27 %\n2      No      No 1500.5721 39891.86    10.53 %\n3     Yes     Yes 1957.1203 18805.95    44.23 %\n4      No      No 1902.1499 35008.67    53.71 %\n5     Yes      No 2202.4624 47287.26    87.09 %\n6     Yes     Yes 2461.5070 11878.56    93.34 %\n\n\n\n\n\n\n\n\n\nImage created with the DALLÂ·E algorithm using the prompt: â€˜35mm macro photography of a robot holding a question mark card, white backgroundâ€™\n\n\nFull model expression: \\[\n\\hat{y} \\approxeq \\frac{e^{-10.87 - 0.65\\times\\text{student[Yes]} + 5.74 \\times 10^{-3}\\times\\text{balance} + 3\\times 10^{-6}\\times\\text{income}}}{1 + e^{-10.87 - 0.65\\times\\text{student[Yes]} + 5.74 \\times 10^{-3}\\times\\text{balance} + 3\\times 10^{-6}\\times\\text{income}}}\n\\]"
  },
  {
    "objectID": "slides/week03_slides_part2.html#default-yes-or-no-2",
    "href": "slides/week03_slides_part2.html#default-yes-or-no-2",
    "title": "ğŸ—“ï¸ Week 03:Classifiers - Part II",
    "section": "Default: Yes or No?",
    "text": "Default: Yes or No?\n\n\nHow would you classify the following customers?\n\n\nCode\nlibrary(tidyverse)\n\nfull_model <- \n  glm(default ~ ., data=ISLR2::Default, family=binomial)\n\nset.seed(40)\nsample_customers <- \n  ISLR2::Default %>% \n  slice(9986, 9908, 6848, 9762, 9979, 7438)\npred <- predict(full_model, sample_customers, type=\"response\")\n# Format it as percentage\nsample_customers$prediction <- \n  sapply(pred, function(x){sprintf(\"%.2f %%\", 100*x)})\nsample_customers\n\n\n  default student   balance   income prediction\n1      No      No  842.9494 39957.13     0.27 %\n2      No      No 1500.5721 39891.86    10.53 %\n3     Yes     Yes 1957.1203 18805.95    44.23 %\n4      No      No 1902.1499 35008.67    53.71 %\n5     Yes      No 2202.4624 47287.26    87.09 %\n6     Yes     Yes 2461.5070 11878.56    93.34 %\n\n\n\n\nIf we set our threshold \\(= 50\\%\\), we get the following confusion matrix:\n\n\n\n\n\nActual\n\n\n\n\n\nPredicted\nNo\nYes\n\n\nNo\n2\n1\n\n\nYes\n1\n2\n\n\n\n\n\n\nIf we set our threshold \\(= 40\\%\\), we get the following confusion matrix:\n\n\n\n\n\nActual\n\n\n\n\n\nPredicted\nNo\nYes\n\n\nNo\n2\n0\n\n\nYes\n1\n3\n\n\n\n\n\n\n\n\n\nWhich of the two is more accurate?"
  },
  {
    "objectID": "slides/week03_slides_part2.html#thresholds",
    "href": "slides/week03_slides_part2.html#thresholds",
    "title": "ğŸ—“ï¸ Week 03:Classifiers - Part II",
    "section": "Thresholds",
    "text": "Thresholds\n\nWhen making predictions about classes, we always have to make decisions.\nThresholds, applied to the predicted probability scores, are a way to decide whether to favour a particular class over another\nâ­ï¸ Next, we will explore several metrics that can help us decide whether our classification model is good or bad."
  },
  {
    "objectID": "slides/week03_slides_part2.html#confusion-matrix",
    "href": "slides/week03_slides_part2.html#confusion-matrix",
    "title": "ğŸ—“ï¸ Week 03:Classifiers - Part II",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\nLetâ€™s take another look at the confusion matrix. We can think of the numbers in each cell as the following: \n\n\n\n\n\n\n\n\n\n\nActual\n\n\n\n\n\nPredicted\nNo\nYes\n\n\nNo\nTrue Negative (TN)\nFalse Negative (FN)\n\n\nYes\nFalse Positive (FP)\nTrue Positive (TP)\n\n\n\n\n\nIdeally, we would have no False Negatives and no False Positives but, of course, that is never the case."
  },
  {
    "objectID": "slides/week03_slides_part2.html#classification-metrics-1",
    "href": "slides/week03_slides_part2.html#classification-metrics-1",
    "title": "ğŸ—“ï¸ Week 03:Classifiers - Part II",
    "section": "Classification metrics",
    "text": "Classification metrics\n\n\nIt is convenient to aggregate those quantities into a few other metrics\nTwo of the most common ones are called sensitivity and specificity\n\n\n\n\\[\n\\begin{align}\n\\text{Sensitivity} &= \\text{True Positive Rate (TPR)} = \\frac{TP}{P} \\\\\n\\text{Specificity} &= \\text{True Negative Rate (TNR)} = \\frac{TN}{N}\n\\end{align}\n\\]\n\n\n\nAnother common one is accuracy:\n\n\\[\n\\text{Accuracy} = \\frac{TP + TN}{P + N}\n\\]\n\n\nA good model has high sensitivity and high specificity and high accuracy.\n\n\n\nThere are many other ways to assess the results of a classification model"
  },
  {
    "objectID": "slides/week03_slides_part2.html#which-threshold-is-better",
    "href": "slides/week03_slides_part2.html#which-threshold-is-better",
    "title": "ğŸ—“ï¸ Week 03:Classifiers - Part II",
    "section": "Which threshold is better?",
    "text": "Which threshold is better?\n\n\n\nğŸ“ Now, looking at the logistic regression model we built for the entire dataset, work out the sensitivity, specificity and accuracy of the following confusion matrices:\n\n\n\n\n\n\n\nPractice\n\n\n\nâ²ï¸ 5 min to work out the math\nğŸ—³ï¸ Vote on your preferred threshold (on  Slack)\n\n\n\n\n\n\\(\\text{Threshold} = 50\\%\\):\n\n\n\n\nActual\n\n\n\n\n\nPredicted\nNo\nYes\n\n\nNo\n9627\n228\n\n\nYes\n40\n105\n\n\n\n\n\\(\\text{Threshold} = 40\\%\\):\n\n\n\n\nActual\n\n\n\n\n\nPredicted\nNo\nYes\n\n\nNo\n9588\n199\n\n\nYes\n79\n134"
  },
  {
    "objectID": "slides/week03_slides_part2.html#meet-the-roc-curve",
    "href": "slides/week03_slides_part2.html#meet-the-roc-curve",
    "title": "ğŸ—“ï¸ Week 03:Classifiers - Part II",
    "section": "Meet the ROC curve",
    "text": "Meet the ROC curve\n\n\n\nThe Receiver Operating Characteristic (ROC) curve is another way to assess the model.\nIt shows how sensitivity and specificity change as we vary the threshold from 0 to 1 (threshold not shown).\n\n\n\n\n\n\n\n\nAsk: how would an ideal curve look like?"
  },
  {
    "objectID": "slides/week03_slides_part2.html#generalisation-problems",
    "href": "slides/week03_slides_part2.html#generalisation-problems",
    "title": "ğŸ—“ï¸ Week 03:Classifiers - Part II",
    "section": "Generalisation problems",
    "text": "Generalisation problems\n\n\nThe data used to train algorithms is called training data\nOften, we want to use the fitted models to make predictions on new previously unseen data\n\n\n\n\n\n\n\n\n\nImportant\n\n\nâš ï¸ A model that performs well on training data will not necessarily perform well on new data âš ï¸\n\n\n\n\n\n\nTo make a robust assessment of our model, we have to split the data in two:\n\nthe training data and\nthe test data\n\nWe do NOT use the test data to fit the model\nWe will come back to this next week, this is the topic of ğŸ—“ï¸ Week 04."
  },
  {
    "objectID": "slides/week03_slides_part2.html#inappropriate-reliance-on-metrics",
    "href": "slides/week03_slides_part2.html#inappropriate-reliance-on-metrics",
    "title": "ğŸ—“ï¸ Week 03:Classifiers - Part II",
    "section": "Inappropriate reliance on metrics",
    "text": "Inappropriate reliance on metrics\n\n\nAccuracy can be very misleading when classes are imbalanced\nConsider the following model: \\(\\hat{y} = \\text{Yes}\\) (always)\n\nOnly \\(3\\%\\) of customers default on their credit cards\nTherefore, this model would have a \\(97\\%\\) accuracy!\nIt is correct ninety-seven percent of times. But is it a good model?\n\nğŸ™…â€â™‚ï¸ NO!\n\n\nSimilarly, you have to ask yourself about the usefulness of any other metric\n\nIs True Positive Rate more or less important than True Negative Rate for the classification problem at hand?\nWhy? Why not?\n\nUltimately, it boils down to how you plan to use this model afterwards."
  },
  {
    "objectID": "slides/week03_slides_part2.html#references",
    "href": "slides/week03_slides_part2.html#references",
    "title": "ğŸ—“ï¸ Week 03:Classifiers - Part II",
    "section": "References",
    "text": "References\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York NY: Springer. https://www.statlearning.com/.\n\n\n\n\n\nDS202 - Data Science for Social Scientists ğŸ¤– ğŸ¤¹"
  },
  {
    "objectID": "slides/week05_slides_part1.html#regression-analysis-in-real-life",
    "href": "slides/week05_slides_part1.html#regression-analysis-in-real-life",
    "title": "ğŸ—“ï¸ Week 05: Decision Trees",
    "section": "Regression analysis in real life",
    "text": "Regression analysis in real life\n\n\nFollowing current trends, the next PM will be in office for approximately minus 200 days pic.twitter.com/avLQE9i1yy\n\nâ€” Rob Sansom (@Sansom_Rob) October 20, 2022"
  },
  {
    "objectID": "slides/week05_slides_part1.html#the-limits-of-classic-regression-models-1",
    "href": "slides/week05_slides_part1.html#the-limits-of-classic-regression-models-1",
    "title": "ğŸ—“ï¸ Week 05: Decision Trees",
    "section": "The limits of classic regression models",
    "text": "The limits of classic regression models\nLinear and logistic regression are a good first shot for building ML models\n\n\nEasy-to-interpret coefficients\nIntuitive (ish) ways to assess variable importance\nOften good out-of-the-box predictions"
  },
  {
    "objectID": "slides/week05_slides_part1.html#however",
    "href": "slides/week05_slides_part1.html#however",
    "title": "ğŸ—“ï¸ Week 05: Decision Trees",
    "section": "Howeverâ€¦",
    "text": "Howeverâ€¦\n\n\nAssumption that the predictors are linearly related to the outcome is restrictive\nWe have seen, for instance, that accounting for higher order polynomial relationships can produce better model fit"
  },
  {
    "objectID": "slides/week05_slides_part1.html#enter-non-linear-methods",
    "href": "slides/week05_slides_part1.html#enter-non-linear-methods",
    "title": "ğŸ—“ï¸ Week 05: Decision Trees",
    "section": "Enter non-linear methods",
    "text": "Enter non-linear methods\n\n\nThese algorithms do not make (strong) statistical assumptions about the data\nThe focus is more on predictive rather than explanatory power"
  },
  {
    "objectID": "slides/week05_slides_part1.html#decision-tree-for-a-regression-task",
    "href": "slides/week05_slides_part1.html#decision-tree-for-a-regression-task",
    "title": "ğŸ—“ï¸ Week 05: Decision Trees",
    "section": "Decision Tree for a Regression task",
    "text": "Decision Tree for a Regression task\nUsing the Auto dataset, predict mpg with a tree-based model using weight and year as features."
  },
  {
    "objectID": "slides/week05_slides_part1.html#decision-tree-for-a-classification-task",
    "href": "slides/week05_slides_part1.html#decision-tree-for-a-classification-task",
    "title": "ğŸ—“ï¸ Week 05: Decision Trees",
    "section": "Decision Tree for a Classification task",
    "text": "Decision Tree for a Classification task\nUsing the Boston dataset, predict whether medv is above the median using crim and tax:"
  },
  {
    "objectID": "slides/week05_slides_part1.html#whats-going-on-behind-the-scenes",
    "href": "slides/week05_slides_part1.html#whats-going-on-behind-the-scenes",
    "title": "ğŸ—“ï¸ Week 05: Decision Trees",
    "section": "Whatâ€™s going on behind the scenes?",
    "text": "Whatâ€™s going on behind the scenes?\nHow decision trees work:\n\n\nDivide the predictor space into \\(\\mathbf{J}\\) distinct regions \\(R_1\\), \\(R_2\\),â€¦,\\(R_j\\).\nTake the mean of the response values in each region\n\n\n\nHereâ€™s how the regions were created in our regression/classification examples â­ï¸"
  },
  {
    "objectID": "slides/week05_slides_part1.html#how-are-regions-created",
    "href": "slides/week05_slides_part1.html#how-are-regions-created",
    "title": "ğŸ—“ï¸ Week 05: Decision Trees",
    "section": "How are regions created?",
    "text": "How are regions created?\nRecursive binary splitting\n\n\nTop down\n\nStart from the top of the tree\nThen perform splits at a current level of depth\n\n\nGreedy\n\nSplits are â€œlocalâ€ not global\nOnly cares about data in the current branch"
  },
  {
    "objectID": "slides/week05_slides_part1.html#when-trees-run-amock",
    "href": "slides/week05_slides_part1.html#when-trees-run-amock",
    "title": "ğŸ—“ï¸ Week 05: Decision Trees",
    "section": "When trees run amock",
    "text": "When trees run amock\n\n\nTrees can become too complex if we are not careful\nIt can lead to something called overfitting\n\nHigh training set predictive power\nLow test set predictive power\n\nLetâ€™s see one example â­ï¸"
  },
  {
    "objectID": "slides/week05_slides_part1.html#pruning-the-tree",
    "href": "slides/week05_slides_part1.html#pruning-the-tree",
    "title": "ğŸ—“ï¸ Week 05: Decision Trees",
    "section": "Pruning the tree",
    "text": "Pruning the tree\n\n\nHyperparameters are model-specific dials that we can tune\n\nThings like max tree depth, or min samples per leaf\n\nAs with model selection, there is no one one-size-fits-all approach to hyperparameter tuning.\nInstead, we experiment with resampling\n\nMost frequently, k-fold cross-validation\n\n\n\n\n\n\n\n\n\n\nk-fold cross-validation\n\n\n\nWe experimented with k-fold CV in ğŸ—“ï¸ Week 04â€™s lecture/workshop\nWe will revisit this topic in ğŸ—“ï¸ Week 07â€™s lab\nNot compulsory for âœï¸ Summmative Problem Set (01) | W05-W07"
  },
  {
    "objectID": "slides/week05_slides_part1.html#cost-complexity",
    "href": "slides/week05_slides_part1.html#cost-complexity",
    "title": "ğŸ—“ï¸ Week 05: Decision Trees",
    "section": "Cost Complexity",
    "text": "Cost Complexity\n\nWe apply \\(\\alpha\\) which is a non-negative value to prune the tree.\nFor example, when \\(\\alpha = 0.02\\) we can create a less complex tree."
  },
  {
    "objectID": "slides/week05_slides_part2.html#support-vector-machines-for-classification",
    "href": "slides/week05_slides_part2.html#support-vector-machines-for-classification",
    "title": "ğŸ—“ï¸ Week 05:Support Vector Machines (SVM)",
    "section": "Support Vector Machines for Classification",
    "text": "Support Vector Machines for Classification\n\nConsidered one of the best out-of-the-box classifiers (ISLR)\nAble to accommodate non-linear decision boundaries"
  },
  {
    "objectID": "slides/week05_slides_part2.html#building-intuition-the-hyperplane",
    "href": "slides/week05_slides_part2.html#building-intuition-the-hyperplane",
    "title": "ğŸ—“ï¸ Week 05:Support Vector Machines (SVM)",
    "section": "Building Intuition: the Hyperplane",
    "text": "Building Intuition: the Hyperplane"
  },
  {
    "objectID": "slides/week05_slides_part2.html#building-intuition-the-hyperplane-cont.",
    "href": "slides/week05_slides_part2.html#building-intuition-the-hyperplane-cont.",
    "title": "ğŸ—“ï¸ Week 05:Support Vector Machines (SVM)",
    "section": "Building Intuition: the Hyperplane (cont.)",
    "text": "Building Intuition: the Hyperplane (cont.)\nWhen \\(1 + 2x_1 + 3x_2 < 0\\)"
  },
  {
    "objectID": "slides/week05_slides_part2.html#building-intuition-the-hyperplane-cont.-1",
    "href": "slides/week05_slides_part2.html#building-intuition-the-hyperplane-cont.-1",
    "title": "ğŸ—“ï¸ Week 05:Support Vector Machines (SVM)",
    "section": "Building Intuition: the Hyperplane (cont.)",
    "text": "Building Intuition: the Hyperplane (cont.)\nWhen \\(1 + 2x_1 + 3x_2 > 0\\)"
  },
  {
    "objectID": "slides/week05_slides_part2.html#building-intuition-the-hyperplane-cont.-2",
    "href": "slides/week05_slides_part2.html#building-intuition-the-hyperplane-cont.-2",
    "title": "ğŸ—“ï¸ Week 05:Support Vector Machines (SVM)",
    "section": "Building Intuition: the Hyperplane (cont.)",
    "text": "Building Intuition: the Hyperplane (cont.)\nWhen \\(1 + 2x_1 + 3x_2 = 0\\)"
  },
  {
    "objectID": "slides/week05_slides_part2.html#the-maximal-marginal-classifier",
    "href": "slides/week05_slides_part2.html#the-maximal-marginal-classifier",
    "title": "ğŸ—“ï¸ Week 05:Support Vector Machines (SVM)",
    "section": "The Maximal Marginal Classifier",
    "text": "The Maximal Marginal Classifier\nThe linearly separable case:"
  },
  {
    "objectID": "slides/week05_slides_part2.html#identifying-support-vectors",
    "href": "slides/week05_slides_part2.html#identifying-support-vectors",
    "title": "ğŸ—“ï¸ Week 05:Support Vector Machines (SVM)",
    "section": "Identifying support vectors",
    "text": "Identifying support vectors\nThe SVâ€™s represent the so-called support vectors:"
  },
  {
    "objectID": "slides/week05_slides_part2.html#when-data-is-not-linearly-separable",
    "href": "slides/week05_slides_part2.html#when-data-is-not-linearly-separable",
    "title": "ğŸ—“ï¸ Week 05:Support Vector Machines (SVM)",
    "section": "When data is not linearly separable",
    "text": "When data is not linearly separable\nSuppose we have a case that is not linearly separable like this. We have two classes but class 1 is â€œsandwichedâ€ in between class 2."
  },
  {
    "objectID": "slides/week05_slides_part2.html#enter-support-vector-machines",
    "href": "slides/week05_slides_part2.html#enter-support-vector-machines",
    "title": "ğŸ—“ï¸ Week 05:Support Vector Machines (SVM)",
    "section": "Enter support vector machines",
    "text": "Enter support vector machines\nLetâ€™s start with a (linear) support vector classifier function\n\\[\nf(x) = \\beta_0 + \\sum_{i\\in\\mathcal{S}}^{} \\alpha_i\\left\\langle x_i, x_{i'} \\right\\rangle\n\\]\nA couple of points:\n\n\\(\\left\\langle x_i, x_{i'} \\right\\rangle\\) is the inner product 1 of two observations\n\\(\\alpha_i\\) is a parameter fitted for all pairs of training observations\n\\(i\\in\\mathcal{S}\\) indicates observations that are support vectors (all other observations are ignored by setting all \\(\\alpha_i \\notin \\mathcal{S}\\) to zero.)\n\nRead about inner products"
  },
  {
    "objectID": "slides/week05_slides_part2.html#enter-support-vector-machines-cont.",
    "href": "slides/week05_slides_part2.html#enter-support-vector-machines-cont.",
    "title": "ğŸ—“ï¸ Week 05:Support Vector Machines (SVM)",
    "section": "Enter support vector machines (cont.)",
    "text": "Enter support vector machines (cont.)\n\n\nWe can replace \\(\\left\\langle x_i, x_{i'} \\right\\rangle\\) with a of the form \\(K(x_i, x_{i'})\\) where \\(K\\) is a kernel.\n\n\n\nTwo well-known kernels:\n\nPolynomial \\(K(x_i, x_{i'}) = (1 + \\sum_{j = 1}^{p} x_{ij}x_{i'j})^d\\) where \\(d > 1\\).\nRadial or â€œGaussianâ€ \\(K(x_i, x_{i'}) = \\text{exp}(-\\gamma\\sum_{j=1}^{p}(x_{ij}-x_{i'j})^2)\\) where \\(\\gamma\\) is a positive constant."
  },
  {
    "objectID": "slides/week05_slides_part2.html#svm-with-linear-kernel",
    "href": "slides/week05_slides_part2.html#svm-with-linear-kernel",
    "title": "ğŸ—“ï¸ Week 05:Support Vector Machines (SVM)",
    "section": "SVM with Linear Kernel",
    "text": "SVM with Linear Kernel"
  },
  {
    "objectID": "slides/week05_slides_part2.html#svm-with-polynomial-kernel",
    "href": "slides/week05_slides_part2.html#svm-with-polynomial-kernel",
    "title": "ğŸ—“ï¸ Week 05:Support Vector Machines (SVM)",
    "section": "SVM with Polynomial Kernel",
    "text": "SVM with Polynomial Kernel"
  },
  {
    "objectID": "slides/week05_slides_part2.html#svm-with-sigmoid-kernel",
    "href": "slides/week05_slides_part2.html#svm-with-sigmoid-kernel",
    "title": "ğŸ—“ï¸ Week 05:Support Vector Machines (SVM)",
    "section": "SVM with Sigmoid Kernel",
    "text": "SVM with Sigmoid Kernel"
  },
  {
    "objectID": "slides/week05_slides_part2.html#svm-with-radial-kernel",
    "href": "slides/week05_slides_part2.html#svm-with-radial-kernel",
    "title": "ğŸ—“ï¸ Week 05:Support Vector Machines (SVM)",
    "section": "SVM with Radial Kernel",
    "text": "SVM with Radial Kernel"
  },
  {
    "objectID": "slides/week05_slides_part2.html#source-code",
    "href": "slides/week05_slides_part2.html#source-code",
    "title": "ğŸ—“ï¸ Week 05:Support Vector Machines (SVM)",
    "section": "Source Code",
    "text": "Source Code\n\n\n\n\n\n\nTip\n\n\n\nUse the code below to replicate the plot from the previous slide.\nFound a bug? Report it on Slack.\n\n\n\n\nlibrary(datasets)   # to load the iris  data\nlibrary(tidyverse)  # to use things like the pipe (%>%), mutate and if_else\nlibrary(ggsci)      # just for pretty colours! It enables functions scale_fill_lancet() and scale_colour_lancet().\nlibrary(e1071)      # to load the SVM algorithm\ndata(iris)          # load the dataset `iris`\n\n# Train the model! change the parameter `kernel`. It accepts 'linear', 'polynomial', 'radial' and 'sigmoid'\nmodel <- svm(Species ~ Sepal.Length + Sepal.Width, data = iris, kernel = 'linear')\n\n# Generate all possible combinations of Sepal.Length and Sepal.Width\nkernel.points <- crossing(Sepal.Length = seq(4, 8, 0.1), Sepal.Width = seq(2, 5, 0.1)) %>%  mutate(pred = predict(model, .))\n\n# Create a dataframe just for plotting (with predictions)\nplot_df <- iris %>% mutate(pred=predict(model, iris), correct = if_else(pred == Species, TRUE, FALSE))\n\nplot_df %>%   \n  ggplot() +\n  geom_tile(data = kernel.points, aes(x=Sepal.Length, y=Sepal.Width, fill = pred), alpha = 0.25) +\n  geom_point(aes(x=Sepal.Length, y=Sepal.Width, colour = Species, shape = correct), size = 4) +\n  scale_shape_manual(values = c(4, 1)) +\n  scale_colour_lancet() +\n  scale_fill_lancet() +\n  theme_minimal() +\n  theme(panel.grid = element_blank(), legend.position = 'bottom', plot.title = element_text(hjust = 0.5)) +\n  labs(x = 'Sepal.Length', y = 'Sepal.Width', fill = 'Species', colour = 'Species', shape = 'Correct prediction?',\n       title = sprintf('Overall Training Accuracy = %.2f %%', 100*(sum(plot_df$correct)/nrow(plot_df))))"
  },
  {
    "objectID": "slides/week05_slides_part2.html#overfitting-illustrated",
    "href": "slides/week05_slides_part2.html#overfitting-illustrated",
    "title": "ğŸ—“ï¸ Week 05:Support Vector Machines (SVM)",
    "section": "Overfitting illustrated",
    "text": "Overfitting illustrated\nSimple models can sometimes be better than complex models."
  },
  {
    "objectID": "slides/week05_slides_part2.html#simple-model-on-training-set",
    "href": "slides/week05_slides_part2.html#simple-model-on-training-set",
    "title": "ğŸ—“ï¸ Week 05:Support Vector Machines (SVM)",
    "section": "Simple model on training set",
    "text": "Simple model on training set"
  },
  {
    "objectID": "slides/week05_slides_part2.html#complex-model-on-training-set",
    "href": "slides/week05_slides_part2.html#complex-model-on-training-set",
    "title": "ğŸ—“ï¸ Week 05:Support Vector Machines (SVM)",
    "section": "Complex model on training set",
    "text": "Complex model on training set"
  },
  {
    "objectID": "slides/week05_slides_part2.html#now-lets-create-some-testing-data",
    "href": "slides/week05_slides_part2.html#now-lets-create-some-testing-data",
    "title": "ğŸ—“ï¸ Week 05:Support Vector Machines (SVM)",
    "section": "Now letâ€™s create some testing data",
    "text": "Now letâ€™s create some testing data\nLook what happens when I set a different seed (nothing else changes) to construct a test set."
  },
  {
    "objectID": "slides/week05_slides_part2.html#simple-model-on-test-set",
    "href": "slides/week05_slides_part2.html#simple-model-on-test-set",
    "title": "ğŸ—“ï¸ Week 05:Support Vector Machines (SVM)",
    "section": "Simple model on test set",
    "text": "Simple model on test set"
  },
  {
    "objectID": "slides/week05_slides_part2.html#complex-model-on-test-set",
    "href": "slides/week05_slides_part2.html#complex-model-on-test-set",
    "title": "ğŸ—“ï¸ Week 05:Support Vector Machines (SVM)",
    "section": "Complex model on test set",
    "text": "Complex model on test set"
  },
  {
    "objectID": "slides/week05_slides_part2.html#back-to-iris",
    "href": "slides/week05_slides_part2.html#back-to-iris",
    "title": "ğŸ—“ï¸ Week 05:Support Vector Machines (SVM)",
    "section": "Back to iris",
    "text": "Back to iris\nSVM with Radial Kernel but tweaking parameters, namely cost and gamma:"
  },
  {
    "objectID": "slides/week05_slides_part2.html#source-code-1",
    "href": "slides/week05_slides_part2.html#source-code-1",
    "title": "ğŸ—“ï¸ Week 05:Support Vector Machines (SVM)",
    "section": "Source Code",
    "text": "Source Code\n\n\n\n\n\n\nTip\n\n\n\nThe only difference is that we added gamma and cost:\n\n\n\n\nlibrary(datasets)   # to load the iris  data\nlibrary(tidyverse)  # to use things like the pipe (%>%), mutate and if_else\nlibrary(ggsci)      # just for pretty colours! It enables functions scale_fill_lancet() and scale_colour_lancet().\nlibrary(e1071)      # to load the SVM algorithm\ndata(iris)          # load the dataset `iris`\n\n# Train the model! change the parameter `kernel`. It accepts 'linear', 'polynomial', 'radial' and 'sigmoid'\nmodel <- svm(Species ~ Sepal.Length + Sepal.Width, data = iris, kernel = 'radial', gamma = 10^2, cost = 10^4)\n\n# Generate all possible combinations of Sepal.Length and Sepal.Width\nkernel.points <- crossing(Sepal.Length = seq(4, 8, 0.1), Sepal.Width = seq(2, 5, 0.1)) %>%  mutate(pred = predict(model, .))\n\n# Create a dataframe just for plotting (with predictions)\nplot_df <- iris %>% mutate(pred=predict(model, iris), correct = if_else(pred == Species, TRUE, FALSE))\n\nplot_df %>%   \n  ggplot() +\n  geom_tile(data = kernel.points, aes(x=Sepal.Length, y=Sepal.Width, fill = pred), alpha = 0.25) +\n  geom_point(aes(x=Sepal.Length, y=Sepal.Width, colour = Species, shape = correct), size = 4) +\n  scale_shape_manual(values = c(4, 1)) +\n  scale_colour_lancet() +\n  scale_fill_lancet() +\n  theme_minimal() +\n  theme(panel.grid = element_blank(), legend.position = 'bottom', plot.title = element_text(hjust = 0.5)) +\n  labs(x = 'Sepal.Length', y = 'Sepal.Width', fill = 'Species', colour = 'Species', shape = 'Correct prediction?',\n       title = sprintf('Overall Training Accuracy = %.2f %%', 100*(sum(plot_df$correct)/nrow(plot_df))))"
  },
  {
    "objectID": "slides/week05_slides_part2.html#example",
    "href": "slides/week05_slides_part2.html#example",
    "title": "ğŸ—“ï¸ Week 05:Support Vector Machines (SVM)",
    "section": "Example",
    "text": "Example\n\nA 5-fold cross-validation:\n\n\n\nSplit 1:\n\n\nTest\n\n\nTrain\n\n\nTrain\n\n\nTrain\n\n\nTrain\n\n\n\n\nSplit 2:\n\n\nTrain\n\n\nTest\n\n\nTrain\n\n\nTrain\n\n\nTrain\n\n\n\n\nSplit 3:\n\n\nTrain\n\n\nTrain\n\n\nTest\n\n\nTrain\n\n\nTrain\n\n\n\n\nSplit 4:\n\n\nTrain\n\n\nTrain\n\n\nTrain\n\n\nTest\n\n\nTrain\n\n\n\n\nSplit 5:\n\n\nTrain\n\n\nTrain\n\n\nTrain\n\n\nTrain\n\n\nTest\n\n\n\nWe experimented with k-fold CV in ğŸ—“ï¸ Week 04â€™s lecture/workshop"
  },
  {
    "objectID": "slides/week05_slides_part2.html#recommendations",
    "href": "slides/week05_slides_part2.html#recommendations",
    "title": "ğŸ—“ï¸ Week 05:Support Vector Machines (SVM)",
    "section": "Recommendations",
    "text": "Recommendations\n\nReadings - Decision Trees: (James et al. 2021, sec. 8.1) (ignore section 8.2) - SVMs: (James et al. 2021, chap. 9) - Read it all once just to retain main concepts - Focus your attention on the concepts on the margins of the book\nPractice the code that is contained in these slides! Several times! - There are many datasets in the ISLR2 package (link) - Load any dataset and explore it with these new algorithms"
  },
  {
    "objectID": "slides/week07_slides_part1.html#the-rationale",
    "href": "slides/week07_slides_part1.html#the-rationale",
    "title": "ğŸ—“ï¸ Week 07: Data Transformation & Iterations",
    "section": "The rationale",
    "text": "The rationale\nOur expectations for this problem set:\n\n\nwe knew it would require somewhat considerable time effort\nencourage people to collaborate and work together\nput what you learned from the R pre-sessionals to practice\nwe wanted to allow for some freedom"
  },
  {
    "objectID": "slides/week07_slides_part1.html#the-r-pre-sessional-course",
    "href": "slides/week07_slides_part1.html#the-r-pre-sessional-course",
    "title": "ğŸ—“ï¸ Week 07: Data Transformation & Iterations",
    "section": "The R pre-sessional course",
    "text": "The R pre-sessional course"
  },
  {
    "objectID": "slides/week07_slides_part1.html#our-blind-spots",
    "href": "slides/week07_slides_part1.html#our-blind-spots",
    "title": "ğŸ—“ï¸ Week 07: Data Transformation & Iterations",
    "section": "Our blind spots",
    "text": "Our blind spots\nWhat we could have done better:\n\n\nwe assumed the pre-sessional chapter above would have prepared you even for the challenging questions\n\ndata types & data frames\nif-else statements\nfor-loops\ncreation of functions\nvectorized functions like apply() and sapply\n\nwe could have provided a cheatsheet for common R tasks\nwe could have given better marking criteria for questions where more freedom was allowed"
  },
  {
    "objectID": "slides/week07_slides_part1.html#next-summative",
    "href": "slides/week07_slides_part1.html#next-summative",
    "title": "ğŸ—“ï¸ Week 07: Data Transformation & Iterations",
    "section": "Next summative",
    "text": "Next summative\nOperational changes we plan to introduce:\n\n\nLess code writing and more code reading\nReduce ambiguity.\n\nWhere freedom/creativity is allowed, explain if and how this is rewarded\n\nElements of randomness. Each student will be assigned a unique combination of:\n\nselected variables and\nselected metrics to consider"
  },
  {
    "objectID": "slides/week07_slides_part1.html#next-summative-1",
    "href": "slides/week07_slides_part1.html#next-summative-1",
    "title": "ğŸ—“ï¸ Week 07: Data Transformation & Iterations",
    "section": "Next summative",
    "text": "Next summative\nTopics of Summative W08-W10:\n\nRegression & Classification\nDecision Tree\nSupport Vector Machine\nk-fold Cross-Validation"
  },
  {
    "objectID": "slides/week07_slides_part1.html#explore",
    "href": "slides/week07_slides_part1.html#explore",
    "title": "ğŸ—“ï¸ Week 07: Data Transformation & Iterations",
    "section": "Explore",
    "text": "Explore\nLetâ€™s explore together using content from two sources:\n\n\nR for Data Science Book\n\n\nTidy Data Tutor\n\n\n\n\n\nLinks:\n\nR for Data Science\nTidy Data Tutor"
  },
  {
    "objectID": "slides/week07_slides_part2.html#example-of-supervised-models",
    "href": "slides/week07_slides_part2.html#example-of-supervised-models",
    "title": "ğŸ—“ï¸ Week 07:Unsupervised Learning",
    "section": "Example of Supervised Models",
    "text": "Example of Supervised Models\nFrom ğŸ—“ï¸ Week 02:\n\n\nThe generic supervised model:\n\\[\nY = \\operatorname{f}(X) + \\epsilon\n\\]\nis defined more explicitly according to each algorithm â¡ï¸\n\n\n\nMultiple Linear Regression\n\\[\n\\begin{align}\nY = \\beta_0 &+ \\beta_1 X_1 + \\beta_2 X_2 \\\\\n   &+ \\dots \\\\\n   &+ \\beta_p X_p + \\epsilon\n\\end{align}\n\\]\nwhen there are multiple predictors, \\(X_p\\).\n\n\n\n\nMultiple Logistic Regression\n\\[\n\\begin{align}\nY \\propto p(X) &= \\\\\n               &= \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p}}\n\\end{align}\n\\]\nwhen there are multiple predictors, \\(X_p\\)."
  },
  {
    "objectID": "slides/week07_slides_part2.html#how-is-it-different",
    "href": "slides/week07_slides_part2.html#how-is-it-different",
    "title": "ğŸ—“ï¸ Week 07:Unsupervised Learning",
    "section": "How is it different?",
    "text": "How is it different?\n\n\nSupervised Learning\n\n\nOur main goal is to predict future values of \\(Y\\)\n\n\n\n\nWe have historic \\(X\\) and \\(Y\\) data\n\n\n\n\nAlgorithms fit the data and supervise themselves objectively (e.g.: residuals)\n\n\n\n\nWe can validate how well the model fits training data and how it generalises beyond that.\n\n\n\nUnsupervised Learning\n\n\nThe main goal is to observe (dis-)similarities in \\(X\\)\n\n\n\n\nWe only have \\(X\\) data\n\n\n\n\nThere is no \\(Y\\) variable to â€œsuperviseâ€ how models should fit the data\n\n\n\n\nValidation is a lot more subjective. There is no objective way to check our work."
  },
  {
    "objectID": "slides/week07_slides_part2.html#why-clustering",
    "href": "slides/week07_slides_part2.html#why-clustering",
    "title": "ğŸ—“ï¸ Week 07:Unsupervised Learning",
    "section": "Why clustering?",
    "text": "Why clustering?\n\n\nAre there clusters (subgroups) in my data?\nWhich samples are most similar to each other?\nAre there samples that do not fall in any subgroup?"
  },
  {
    "objectID": "slides/week07_slides_part2.html#each-algorithm-is-different",
    "href": "slides/week07_slides_part2.html#each-algorithm-is-different",
    "title": "ğŸ—“ï¸ Week 07:Unsupervised Learning",
    "section": "Each algorithm is different",
    "text": "Each algorithm is different\n\n\nSource: Scikit Learn | Comparing different clustering algorithms on toy datasets"
  },
  {
    "objectID": "slides/week07_slides_part2.html#penguins-data",
    "href": "slides/week07_slides_part2.html#penguins-data",
    "title": "ğŸ—“ï¸ Week 07:Unsupervised Learning",
    "section": "ğŸ§ penguins data",
    "text": "ğŸ§ penguins data\n\n\nCode\npenguins_cleaned <-\n  penguins %>% \n  na.omit() %>% \n  select(species, where(is.numeric), - year)\nhead(penguins_cleaned, 15)\n\n\n# A tibble: 15 Ã— 5\n   species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   <fct>            <dbl>         <dbl>             <int>       <int>\n 1 Adelie            39.1          18.7               181        3750\n 2 Adelie            39.5          17.4               186        3800\n 3 Adelie            40.3          18                 195        3250\n 4 Adelie            36.7          19.3               193        3450\n 5 Adelie            39.3          20.6               190        3650\n 6 Adelie            38.9          17.8               181        3625\n 7 Adelie            39.2          19.6               195        4675\n 8 Adelie            41.1          17.6               182        3200\n 9 Adelie            38.6          21.2               191        3800\n10 Adelie            34.6          21.1               198        4400\n11 Adelie            36.6          17.8               185        3700\n12 Adelie            38.7          19                 195        3450\n13 Adelie            42.5          20.7               197        4500\n14 Adelie            34.4          18.4               184        3325\n15 Adelie            46            21.5               194        4200\n\n\n\n\n\nData collected and made available by Dr.Â Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network."
  },
  {
    "objectID": "slides/week07_slides_part2.html#correlation-plot",
    "href": "slides/week07_slides_part2.html#correlation-plot",
    "title": "ğŸ—“ï¸ Week 07:Unsupervised Learning",
    "section": "Correlation plot",
    "text": "Correlation plot\nIt is easy to distinguish the species using colour because we have a species column:"
  },
  {
    "objectID": "slides/week07_slides_part2.html#source-code",
    "href": "slides/week07_slides_part2.html#source-code",
    "title": "ğŸ—“ï¸ Week 07:Unsupervised Learning",
    "section": "Source Code",
    "text": "Source Code\n\n\n\n\n\n\nTip\n\n\n\nUse the code below to replicate the plot from the previous slide.\nFound a bug? Report it on Slack.\n\n\n\n\nlibrary(GGally)         # pretty correlation plots\nlibrary(tidyverse)      # to use things like the pipe (%>%), mutate and if_else\nlibrary(palmerpenguins) # for penguin data \n\npenguins_cleaned <-\n  penguins %>% \n  na.omit() %>%  \n  select(species, where(is.numeric), - year) \n\n# View(penguins_cleaned) or do head(penguins_cleaned)\n\ng <-\n  ggpairs(penguins_cleaned, \n        aes(colour = species, fill = species, alpha = 0.875),\n        columns = 2:5, \n        upper = list(continuous = 'blankDiag')) +\n  theme_minimal() +\n  theme(panel.grid.minor = element_blank()) +\n  scale_colour_viridis_d() +\n  scale_fill_viridis_d() +\n  labs(colour = 'Species', fill = 'Species')\n\ng"
  },
  {
    "objectID": "slides/week07_slides_part2.html#penguins-data-without-the-species",
    "href": "slides/week07_slides_part2.html#penguins-data-without-the-species",
    "title": "ğŸ—“ï¸ Week 07:Unsupervised Learning",
    "section": "Penguins data without the species",
    "text": "Penguins data without the species\nWhat if we did not have the species column?\n\n\nCode\npenguins %>% \n  na.omit() %>% \n  select(species, where(is.numeric), - year, -species) %>%\n  head(15)\n\n\n# A tibble: 15 Ã— 4\n   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n            <dbl>         <dbl>             <int>       <int>\n 1           39.1          18.7               181        3750\n 2           39.5          17.4               186        3800\n 3           40.3          18                 195        3250\n 4           36.7          19.3               193        3450\n 5           39.3          20.6               190        3650\n 6           38.9          17.8               181        3625\n 7           39.2          19.6               195        4675\n 8           41.1          17.6               182        3200\n 9           38.6          21.2               191        3800\n10           34.6          21.1               198        4400\n11           36.6          17.8               185        3700\n12           38.7          19                 195        3450\n13           42.5          20.7               197        4500\n14           34.4          18.4               184        3325\n15           46            21.5               194        4200"
  },
  {
    "objectID": "slides/week07_slides_part2.html#objective-function",
    "href": "slides/week07_slides_part2.html#objective-function",
    "title": "ğŸ—“ï¸ Week 07:Unsupervised Learning",
    "section": "Objective function:",
    "text": "Objective function:\nFind a partition \\(C_1 \\cup C_2 \\cup C_3 \\cup \\ldots \\cup C_K = \\{1, \\ldots, n\\}\\) for the data such that: \\[\n\\text{minimize}_{C_1, \\ldots, C_K} \\left\\{ \\sum_{k=1}^{K}{\\frac{1}{|C_k|} \\sum_{i,i' \\in C_k}{\\sum_{j=1}^p{(x_{ij} - x_{i'j})^2}} }\\right\\}\n\\]"
  },
  {
    "objectID": "slides/week07_slides_part2.html#k-means-algorithm",
    "href": "slides/week07_slides_part2.html#k-means-algorithm",
    "title": "ğŸ—“ï¸ Week 07:Unsupervised Learning",
    "section": "K-Means algorithm",
    "text": "K-Means algorithm\nStep by step of one clustering algorithm:\n\n\nYou have to inform \\(K\\), the number of clusters you wish to recover\nThe algorithm randomly assign each observation to a random cluster\nIterate until cluster assignments stop changing\n\nFor each of the \\(K\\) clusters, compute the cluster centroid\nRe-assign samples to their closest centroid (euclidean distance)\n\n\n\n\n\nIndicative reading: (James et al. 2021, sec. 12.4.1)"
  },
  {
    "objectID": "slides/week07_slides_part2.html#clustering-penguins",
    "href": "slides/week07_slides_part2.html#clustering-penguins",
    "title": "ğŸ—“ï¸ Week 07:Unsupervised Learning",
    "section": "Clustering Penguins",
    "text": "Clustering Penguins\n\n\nCode\nset.seed(1)\n\nK=3\n\nkmeans_model <- kmeans(penguins_cleaned %>% select(-species), K)\nkmeans_model\n\n\nK-means clustering with 3 clusters of sizes 113, 80, 140\n\nCluster means:\n  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n1       44.24336      17.44779          201.5487    4310.619\n2       48.66250      15.39750          219.9875    5365.938\n3       41.12214      17.94643          189.6286    3461.250\n\nClustering vector:\n  [1] 3 3 3 3 3 3 1 3 3 1 3 3 1 3 1 3 3 3 1 3 3 3 3 3 1 3 1 3 1 3 1 1 3 3 1 3 1\n [38] 3 1 3 1 3 3 1 3 1 3 1 3 3 3 3 3 3 3 1 3 1 3 1 3 1 3 1 3 1 3 1 3 1 3 1 3 1\n [75] 3 1 3 1 3 3 3 3 1 3 3 1 3 1 3 1 3 1 3 1 3 1 3 1 3 3 3 1 3 1 3 1 3 1 1 1 3\n[112] 3 3 3 3 3 3 3 3 1 3 1 3 1 3 3 3 1 3 1 3 1 3 1 3 3 3 3 3 3 1 3 3 3 3 1 1 2\n[149] 1 2 2 1 1 2 1 2 1 2 1 2 1 2 1 2 1 2 2 2 1 2 2 2 2 1 2 2 1 2 2 2 2 2 2 1 2\n[186] 1 2 1 1 2 2 1 2 2 2 2 2 1 2 2 2 1 2 1 2 1 2 1 2 1 2 2 1 2 1 2 2 2 1 2 1 2\n[223] 1 2 1 2 1 2 1 2 1 2 2 2 2 2 1 2 2 2 2 2 1 2 2 2 2 2 2 1 2 1 2 2 2 1 2 1 2\n[260] 2 2 2 2 2 2 3 1 3 3 3 1 3 3 1 3 3 3 3 1 3 1 3 3 3 1 3 3 3 3 3 1 3 3 3 1 3\n[297] 1 3 1 3 1 3 1 3 1 1 3 3 3 3 1 3 1 3 3 3 1 3 1 3 3 3 1 3 3 1 3 3 1 3 3 1 3\n\nWithin cluster sum of squares by cluster:\n[1] 9318036 9718829 9724809\n (between_SS / total_SS =  86.6 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\""
  },
  {
    "objectID": "slides/week07_slides_part2.html#inspect-the-centroids",
    "href": "slides/week07_slides_part2.html#inspect-the-centroids",
    "title": "ğŸ—“ï¸ Week 07:Unsupervised Learning",
    "section": "Inspect the centroids",
    "text": "Inspect the centroids\n\n\nCode\nknitr::kable(kmeans_model$centers)\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\n44.24336\n17.44779\n201.5487\n4310.619\n\n\n48.66250\n15.39750\n219.9875\n5365.938\n\n\n41.12214\n17.94643\n189.6286\n3461.250"
  },
  {
    "objectID": "slides/week07_slides_part2.html#how-does-it-compare-to-the-real-one",
    "href": "slides/week07_slides_part2.html#how-does-it-compare-to-the-real-one",
    "title": "ğŸ—“ï¸ Week 07:Unsupervised Learning",
    "section": "How does it compare to the real one?",
    "text": "How does it compare to the real one?\nWe can cross-tabulate species and cluster membership:\n\n\nCode\nplot_df <- penguins_cleaned\nplot_df$cluster <- factor(kmeans_model$cluster)\nlevels(plot_df$cluster) <- c(\"Cluster 1\", \"Cluster 2\", \"Cluster 3\")\n\nknitr::kable(table(plot_df$species, plot_df$cluster))\n\n\n\n\n\n\nCluster 1\nCluster 2\nCluster 3\n\n\n\n\nAdelie\n52\n0\n94\n\n\nChinstrap\n22\n0\n46\n\n\nGentoo\n39\n80\n0"
  },
  {
    "objectID": "slides/week07_slides_part2.html#visual-comparison",
    "href": "slides/week07_slides_part2.html#visual-comparison",
    "title": "ğŸ—“ï¸ Week 07:Unsupervised Learning",
    "section": "Visual comparison",
    "text": "Visual comparison"
  },
  {
    "objectID": "slides/week07_slides_part2.html#visual-comparison-1",
    "href": "slides/week07_slides_part2.html#visual-comparison-1",
    "title": "ğŸ—“ï¸ Week 07:Unsupervised Learning",
    "section": "Visual comparison",
    "text": "Visual comparison"
  },
  {
    "objectID": "slides/week07_slides_part2.html#whats-next",
    "href": "slides/week07_slides_part2.html#whats-next",
    "title": "ğŸ—“ï¸ Week 07:Unsupervised Learning",
    "section": "Whatâ€™s Next",
    "text": "Whatâ€™s Next\n\nNext week, Week 08, the labs will be about SVM & cross-validation\n\nRevisit ğŸ—“ï¸ Week 05 lecture\nRead â€œThe cross-validation setupâ€ section in Week 04 page\n\nYou will explore k-means clustering in Week 09 labs"
  },
  {
    "objectID": "slides/week07_slides_part2.html#references",
    "href": "slides/week07_slides_part2.html#references",
    "title": "ğŸ—“ï¸ Week 07:Unsupervised Learning",
    "section": "References",
    "text": "References\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York NY: Springer. https://www.statlearning.com/.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDS202 - Data Science for Social Scientists ğŸ¤– ğŸ¤¹"
  },
  {
    "objectID": "slides/week08_slides_part1.html#comparison",
    "href": "slides/week08_slides_part1.html#comparison",
    "title": "ğŸ—“ï¸ Week 08: Principal Component Analysis",
    "section": "Comparison",
    "text": "Comparison\n\n\n\nSupervised Learning\n\nWe want to predict \\(Y\\)\n\n\n\n\n\n\nX1\nX2\nX3\nX4\nX5\nY\n\n\n\n\n0.73\n2.86\n1.35\n0.01\n0.36\nB\n\n\n0.17\n2.39\n1.30\n0.01\n0.84\nB\n\n\n0.25\n9.53\n0.68\n0.00\n2.40\nA\n\n\n0.21\n3.38\n0.68\n0.00\n1.39\nA\n\n\n0.53\n9.73\n1.24\n0.01\n1.31\nA\n\n\n0.52\n6.92\n1.13\n0.00\n0.99\nC\n\n\n0.55\n1.62\n0.78\n0.00\n1.61\nA\n\n\n0.09\n2.45\n1.02\n0.00\n0.25\nC\n\n\n0.72\n0.91\n1.54\n0.01\n4.77\nC\n\n\n\n\n\n\n\nUnsupervised Learning\n\nWe work with \\(X\\) only\n\n\n\n\n\n\nX1\nX2\nX3\nX4\nX5\nY\n\n\n\n\n0.73\n2.86\n1.35\n0.01\n0.36\n-\n\n\n0.17\n2.39\n1.30\n0.01\n0.84\n-\n\n\n0.25\n9.53\n0.68\n0.00\n2.40\n-\n\n\n0.21\n3.38\n0.68\n0.00\n1.39\n-\n\n\n0.53\n9.73\n1.24\n0.01\n1.31\n-\n\n\n0.52\n6.92\n1.13\n0.00\n0.99\n-\n\n\n0.55\n1.62\n0.78\n0.00\n1.61\n-\n\n\n0.09\n2.45\n1.02\n0.00\n0.25\n-\n\n\n0.72\n0.91\n1.54\n0.01\n4.77\n-"
  },
  {
    "objectID": "slides/week08_slides_part1.html#how-is-it-different",
    "href": "slides/week08_slides_part1.html#how-is-it-different",
    "title": "ğŸ—“ï¸ Week 08: Principal Component Analysis",
    "section": "How is it different?",
    "text": "How is it different?\n\n\nSupervised Learning\n\nWe have historic \\(X\\) and \\(Y\\) data\nOur main goal is to predict future values of \\(Y\\)\nAlgorithms fit the data and supervise themselves objectively (e.g.: residuals)\nWe can validate how well the model fits training data and how it generalises beyond that.\n\n\nUnsupervised Learning\n\nWe only have \\(X\\) data\nThe main goal is to observe (dis-)similarities in \\(X\\)\nThere is no \\(Y\\) variable to â€œsuperviseâ€ how models should fit the data\nValidation is a lot more subjective. There is no objective way to check our work."
  },
  {
    "objectID": "slides/week08_slides_part1.html#what-the-data-looks-like",
    "href": "slides/week08_slides_part1.html#what-the-data-looks-like",
    "title": "ğŸ—“ï¸ Week 08: Principal Component Analysis",
    "section": "What the data looks like",
    "text": "What the data looks like\n\nHow to visualise this data?\n\n\nknitr::kable(USArrests)\n\n\n\n\n\nMurder\nAssault\nUrbanPop\nRape\n\n\n\n\nAlabama\n13.2\n236\n58\n21.2\n\n\nAlaska\n10.0\n263\n48\n44.5\n\n\nArizona\n8.1\n294\n80\n31.0\n\n\nArkansas\n8.8\n190\n50\n19.5\n\n\nCalifornia\n9.0\n276\n91\n40.6\n\n\nColorado\n7.9\n204\n78\n38.7\n\n\nConnecticut\n3.3\n110\n77\n11.1\n\n\nDelaware\n5.9\n238\n72\n15.8\n\n\nFlorida\n15.4\n335\n80\n31.9\n\n\nGeorgia\n17.4\n211\n60\n25.8\n\n\nHawaii\n5.3\n46\n83\n20.2\n\n\nIdaho\n2.6\n120\n54\n14.2\n\n\nIllinois\n10.4\n249\n83\n24.0\n\n\nIndiana\n7.2\n113\n65\n21.0\n\n\nIowa\n2.2\n56\n57\n11.3\n\n\nKansas\n6.0\n115\n66\n18.0\n\n\nKentucky\n9.7\n109\n52\n16.3\n\n\nLouisiana\n15.4\n249\n66\n22.2\n\n\nMaine\n2.1\n83\n51\n7.8\n\n\nMaryland\n11.3\n300\n67\n27.8\n\n\nMassachusetts\n4.4\n149\n85\n16.3\n\n\nMichigan\n12.1\n255\n74\n35.1\n\n\nMinnesota\n2.7\n72\n66\n14.9\n\n\nMississippi\n16.1\n259\n44\n17.1\n\n\nMissouri\n9.0\n178\n70\n28.2\n\n\nMontana\n6.0\n109\n53\n16.4\n\n\nNebraska\n4.3\n102\n62\n16.5\n\n\nNevada\n12.2\n252\n81\n46.0\n\n\nNew Hampshire\n2.1\n57\n56\n9.5\n\n\nNew Jersey\n7.4\n159\n89\n18.8\n\n\nNew Mexico\n11.4\n285\n70\n32.1\n\n\nNew York\n11.1\n254\n86\n26.1\n\n\nNorth Carolina\n13.0\n337\n45\n16.1\n\n\nNorth Dakota\n0.8\n45\n44\n7.3\n\n\nOhio\n7.3\n120\n75\n21.4\n\n\nOklahoma\n6.6\n151\n68\n20.0\n\n\nOregon\n4.9\n159\n67\n29.3\n\n\nPennsylvania\n6.3\n106\n72\n14.9\n\n\nRhode Island\n3.4\n174\n87\n8.3\n\n\nSouth Carolina\n14.4\n279\n48\n22.5\n\n\nSouth Dakota\n3.8\n86\n45\n12.8\n\n\nTennessee\n13.2\n188\n59\n26.9\n\n\nTexas\n12.7\n201\n80\n25.5\n\n\nUtah\n3.2\n120\n80\n22.9\n\n\nVermont\n2.2\n48\n32\n11.2\n\n\nVirginia\n8.5\n156\n63\n20.7\n\n\nWashington\n4.0\n145\n73\n26.2\n\n\nWest Virginia\n5.7\n81\n39\n9.3\n\n\nWisconsin\n2.6\n53\n66\n10.8\n\n\nWyoming\n6.8\n161\n60\n15.6\n\n\n\n\n\n\n\nThis data set contains statistics, in arrests per 100,000 residents for assault, murder, and rape in each of the 50 US states in 1973. Also given is the percent of the population living in urban areas.\nA data frame with 50 observations on 4 variables. [,1] Murder numeric Murder arrests (per 100,000) [,2] Assault numeric Assault arrests (per 100,000) [,3] UrbanPop numeric Percent urban population [,4] Rape numeric Rape arrests (per 100,000)"
  },
  {
    "objectID": "slides/week08_slides_part1.html#eda-with-our-friend-the-ggpairs-plot",
    "href": "slides/week08_slides_part1.html#eda-with-our-friend-the-ggpairs-plot",
    "title": "ğŸ—“ï¸ Week 08: Principal Component Analysis",
    "section": "EDA with our friend the ggpairs plot",
    "text": "EDA with our friend the ggpairs plot"
  },
  {
    "objectID": "slides/week08_slides_part1.html#when-ggpairs-becomes-impractical",
    "href": "slides/week08_slides_part1.html#when-ggpairs-becomes-impractical",
    "title": "ğŸ—“ï¸ Week 08: Principal Component Analysis",
    "section": "When ggpairs becomes impractical",
    "text": "When ggpairs becomes impractical\n\n\nIf we have \\(p\\) features, there will be \\(\\frac{p \\times (p - 1)}{2}\\) different pair plots!\nThink about it:\n\n10 features -> 45 pair plots\n100 features -> 4950 pair plots\n1000 features -> 499500 pair plots (nearly 500k combinations!)"
  },
  {
    "objectID": "slides/week08_slides_part1.html#when-you-have-too-many-features-1",
    "href": "slides/week08_slides_part1.html#when-you-have-too-many-features-1",
    "title": "ğŸ—“ï¸ Week 08: Principal Component Analysis",
    "section": "When you have too many featuresâ€¦",
    "text": "When you have too many featuresâ€¦\nâ€¦ you might want to reduce the dimensionality of your dataset\n\n\n\n\n\n\n\n\nTip\n\n\nWhat if I told you you can reduce a matrix of 10 features into just 2?"
  },
  {
    "objectID": "slides/week08_slides_part1.html#what-is-pca",
    "href": "slides/week08_slides_part1.html#what-is-pca",
    "title": "ğŸ—“ï¸ Week 08: Principal Component Analysis",
    "section": "What is PCA?",
    "text": "What is PCA?\nLetâ€™s start with terminology:\n\n\nPrincipal Component Analysis (PCA) recombines the matrix of features \\(\\mathbf{X}\\)\nIf \\(\\mathbf{X}\\) has \\(n\\) observations and \\(p\\) features (\\(n \\times p\\)), PCA will produce a new \\(n \\times p\\) matrix \\(\\mathbf{Z}\\)\n\nInstead of â€œfeaturesâ€ or â€œpredictorsâ€, we call these new columns principal components\n\nYou might be thinking: ğŸ¤” â€œwait, didnâ€™t you just say that PCA would help to reduce the number of features?!â€\nThe thing is: because of the way this matrix is constructed, it is often enough to just use the first few columns of \\(\\mathbf{Z}\\)."
  },
  {
    "objectID": "slides/week08_slides_part1.html#the-first-principal-component",
    "href": "slides/week08_slides_part1.html#the-first-principal-component",
    "title": "ğŸ—“ï¸ Week 08: Principal Component Analysis",
    "section": "The first principal component",
    "text": "The first principal component\n\nThe first column of \\(\\mathbf{Z}\\) is a linear combination of the \\(p\\) features of \\(\\mathbf{X}\\):\n\n\\[\nZ_1 = \\phi_{11}X_1 + \\phi_{21}X2 + \\ldots + \\phi_{p1}X_p\n\\]\nsubject to:\n\\[\n\\sum_{j=1}^p \\phi_{j1}^2 = 1\n\\]\n\n\nWe refer to the elements \\(\\phi_{11}, \\phi_{21}, \\ldots, \\phi_{p1}\\) as the loadings of the first principal component.\nWe can also refer to them collective, as the loading vector \\(\\phi_1 = (\\phi_{11}, \\phi_{21}, \\ldots, \\phi_{p1})\\).\nEach one of the new elements, say \\(z_{11}\\) are referred to as scores"
  },
  {
    "objectID": "slides/week08_slides_part1.html#a-closer-look-at-the-optimisation-task",
    "href": "slides/week08_slides_part1.html#a-closer-look-at-the-optimisation-task",
    "title": "ğŸ—“ï¸ Week 08: Principal Component Analysis",
    "section": "A closer look at the optimisation task:",
    "text": "A closer look at the optimisation task:\n\\[\n\\begin{eqnarray}\n&\\operatorname{maximize}_{\\phi_{11}, \\ldots, \\phi_{p1}}&\\left\\{\\frac{1}{n}\\sum_i^{n}\\left(\\sum_i^p{\\phi_{j1}x_{ij}}\\right)\\right\\}\n\\\\\n&\\text{subject to} &\\\\\n&&\\sum_{j=1}^p \\phi_{j1}^2 = 1\n\\end{eqnarray}\n\\]\n\nIn practice, this maximizes the sample variance of the \\(n\\) values of \\(z_{i1}\\)\nCollective, the loading vector \\(\\phi_1\\) points to the direction of largest variance in the data\n\nWe will see an example soon!"
  },
  {
    "objectID": "slides/week08_slides_part1.html#the-second-principal-component",
    "href": "slides/week08_slides_part1.html#the-second-principal-component",
    "title": "ğŸ—“ï¸ Week 08: Principal Component Analysis",
    "section": "The second principal component",
    "text": "The second principal component\n\n\nAs I said, \\(\\mathbf{Z}\\) has \\(p\\) dimensions. We only talked about the first one.\n\n\n\n\nThe second column of \\(\\mathbf{Z}\\) is also linear combination of the \\(p\\) features of \\(\\mathbf{X}\\):\n\n\\[\nZ_2 = \\phi_{12}X_1 + \\phi_{22}X2 + \\ldots + \\phi_{p2}X_p\n\\]\nsubject to:\n\\[\n\\sum_{j=1}^p \\phi_{j2}^2 = 1\n\\]\nbut also:\n\n\\(\\phi_2\\) should be orthogonal to \\(\\phi_1\\)"
  },
  {
    "objectID": "slides/week08_slides_part1.html#example-of-two-pcs",
    "href": "slides/week08_slides_part1.html#example-of-two-pcs",
    "title": "ğŸ—“ï¸ Week 08: Principal Component Analysis",
    "section": "Example of two PCs",
    "text": "Example of two PCs\n\n\n\nYou will find this as Figure 6.14 of our Textbook"
  },
  {
    "objectID": "slides/week08_slides_part1.html#example-usarrests-1",
    "href": "slides/week08_slides_part1.html#example-usarrests-1",
    "title": "ğŸ—“ï¸ Week 08: Principal Component Analysis",
    "section": "Example: USArrests",
    "text": "Example: USArrests"
  },
  {
    "objectID": "slides/week08_slides_part1.html#youtube-recommendation",
    "href": "slides/week08_slides_part1.html#youtube-recommendation",
    "title": "ğŸ—“ï¸ Week 08: Principal Component Analysis",
    "section": "ğŸ“¹ YouTube Recommendation",
    "text": "ğŸ“¹ YouTube Recommendation\n\n3Blue1Brown is fantastic! Do check it out.\nThe video below helps to visualize how linear matrix transformations work"
  },
  {
    "objectID": "slides/week08_slides_part1.html#whats-next",
    "href": "slides/week08_slides_part1.html#whats-next",
    "title": "ğŸ—“ï¸ Week 08: Principal Component Analysis",
    "section": "Whatâ€™s Next",
    "text": "Whatâ€™s Next\n\nI will show you your Summative 02!\n\n\n\n\nDS202 - Data Science for Social Scientists ğŸ¤– ğŸ¤¹"
  }
]