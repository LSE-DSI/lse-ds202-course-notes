[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LSE DS202 (2022/2023)",
    "section": "",
    "text": "üéØ Learning Objectives\n\nUnderstand the fundamentals of the data science approach, with an emphasis on social scientific analysis and the study of the social, political, and economic worlds;\nUnderstand how classical methods such as regression analysis or principal components analysis can be treated as machine learning approaches for prediction or for data mining.\nKnow how to fit and apply supervised machine learning models for classification and prediction.\nKnow how to evaluate and compare fitted models, and to improve model performance.\nUse applied computer programming, including the hands-on use of programming through course exercises.\nApply the methods learned to real data through hands-on exercises.\nIntegrate the insights from data analytics into knowledge generation and decision-making.\nUnderstand an introductory framework for working with natural language (text) data using techniques of machine learning.\nLearn how data science methods have been applied to a particular domain of study (applications).\n\n\n\nüßëüèª‚Äçüè´ Our Team\n\nTeacher ResponsibleTeaching StaffStudentsAdministrative Support\n\n\n\nDr.¬†Jonathan Cardoso-Silva  Assistant Professorial Lecturer  LSE Data Science Institute üìß J.Cardoso-Silva at lse dot ac dot uk\nOffice Hours:\n\n15-min slots, on Wednesdays 13:00 ‚Äì 15:00 during Term Time\nRoom: PEL 9.01c (check out the üó∫Ô∏è campus map)\nBook via Student Hub up to 12 hours in advance\n\n\n\n\nDr.¬†Stuart Bramwell  ESRC Postdoctoral Fellow  Department of Methodology PhD in Politics (Oxford)\n\nYijun Wang  Guest Teacher at the LSE Data Science Institute PhD candidate in Health Informatics (KCL)  MSc in Data Science (KCL)\n\nMustafa Can Ozkan  Guest Teacher at the LSE Data Science Institute PhD candidate in the Spacetime Lab (UCL)  MSc in Transport (Imperial/UCL)\n\nXiaowei Gao  Guest Teacher at the LSE Data Science Institute PhD candidate in the Spacetime Lab (UCL)  MSc in Data Science (KCL)\n\nAnton Boichenko  Guest Teacher at the LSE Data Science Institute Product Developer at Decoded  MSc in Applied Social Data Science (LSE)\n\n\n\nZhang Ruishan (Yoyo)  1st Year BSc Economics Student  Course Representative for DS202\n\nRachitha Raghuram  2nd Year BSc Economics Student  Course Representative for DS202\n\n\n\nNathaniel Ocquaye  Teaching Support and Events Officer Office: PEL 9.01 Email: DSI.UG at lse dot ac dot uk\n\nJill Beattie  Institute Coordinator Office: PEL 9.01E Tel: +44 (0) 20 7955 7759 Email: DSI.Admin at lse dot ac dot uk\n\n\n\n\n\nClass Groups\n\nGroup 01\n\nüìÜ Mondays\n‚åö 09:00 ‚Äî 10:30\nüìç PAN.1.03\nüßë‚Äçüè´ Xiaowei\n\n\n\nGroup 02\n\nüìÜ Mondays\n‚åö 10:30 ‚Äî 12:00\nüìç PAN.1.03\nüßë‚Äçüè´ Xiaowei\n\n\n\nGroup 03\n\nüìÜ Mondays\n‚åö 13:00 ‚Äî 14:30\nüìç MAR.1.09\nüßë‚Äçüè´ Stuart/Jon\n\n\n\nGroup 04\n\nüìÜ Fridays\n‚åö 16:00 ‚Äî 17:30\nüìç NAB.1.04\nüßë‚Äçüè´ Stuart/Yijun\n\n\n\nGroup 05\n\nüìÜ Mondays\n‚åö 09:00 ‚Äî 10:30\nüìç 32L.LG.11\nüßë‚Äçüè´ Mustafa\n\n\n\nGroup 06\n\nüìÜ Mondays\n‚åö 10:30 ‚Äî 12:00\nüìç 32L.LG.11\nüßë‚Äçüè´ Mustafa\n\n\n\nGroup 07\n\nüìÜ Fridays\n‚åö 09:30 ‚Äî 11:00\nüìç CBG.2.06\nüßë‚Äçüè´ Stuart/Yijun"
  },
  {
    "objectID": "blog/main.html",
    "href": "blog/main.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n1 min\n\n\n\nweek04\n\n\np-values\n\n\n\n\n\n\n\n19 October 2022\n\n\n\n\n\n\n\n\n\n\n\n\n0 min\n\n\n\nweek03\n\n\nbayesian statistics\n\n\nequations\n\n\n\n\n\n\n\n17 October 2022\n\n\n\n\n\n\n\n\n\n\n\n\n3 min\n\n\n\nweek02\n\n\nR-squared\n\n\nequations\n\n\n\n\n\n\n\n10 October 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/bayes-theorem.html",
    "href": "blog/posts/bayes-theorem.html",
    "title": "Understanding Bayes‚Äô Theorem",
    "section": "",
    "text": "On Week 03 (Part II), I showed you the following equation and mentioned that it forms the basis for one of the algorithms we explore in this course, Naive Bayes.\n\\[\nP(\\mathbf{Y} = k | \\mathbf{X} = x) = \\frac{P(k)P(\\mathbf{X}|\\mathbf{Y}=k)}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}\n\\]\nThe following video might help you understand the intuition behind the equation. Enjoy!"
  },
  {
    "objectID": "blog/posts/p-values.html",
    "href": "blog/posts/p-values.html",
    "title": "Don‚Äôt give p-values more credit than they deserve",
    "section": "",
    "text": "When you run a linear or logistic regression and find out that a regression coefficient has a low associated p-value, it is tempting to scream THIS FEATURE IS SIGNIFICANT AND I CAN PROVE!\nIn reality, although p-values might suggest a non-zero relationship between variables, you shouldn‚Äôt judge the performance or explainability of a model simply by the p-values of coefficients, nor the p-value associated with the full model (say, the F-statistic).\nWhen assessing a model, look beyond goodness-of-fit. Perform train/test splits, cross-validation, bootstrap, and use appropriate measures of success to the problem you have at hand. Come to üóìÔ∏è Week 04 workshop (the lecture) this Friday 21 October to learn more about this.\nThe reason I am saying all this is because p-values are very easy to hack. In fact, there is even a term for misuse of p-values in the scientific literature: p-hacking.\nWhere do I inform myself about this?\nI have separated a list of articles and commentaries about this topic. Check them out:\n\nNahm, Francis Sahngun. 2017. ‚ÄúWhat the P Values Really Tell Us.‚Äù The Korean Journal of Pain 30 (4): 241.\nAmrhein, Valentin, Sander Greenland, and Blake McShane. 2019. ‚ÄúScientists Rise up Against Statistical Significance.‚Äù Nature 567 (7748): 305‚Äì7.\nAschwanden, Christie. 2015. ‚ÄúScience Isn‚Äôt Broken.‚Äù FiveThirtyEight.\nSterne, Jonathan A C, and George Davey Smith. 2001. ‚ÄúSifting the Evidence‚ÄîWhat‚Äôs Wrong with Significance Tests?‚Äù BMJ : British Medical Journal 322 (7280): 226‚Äì31.\n\nüí° If you are in a hurry and want to read just ONE thing, read the ‚ÄúScience Isn‚Äôt Broken.‚Äù piece at FiveThirtyEight. They have a cool visualisation to illustrate the problem."
  },
  {
    "objectID": "blog/posts/r-squared.html",
    "href": "blog/posts/r-squared.html",
    "title": "What happens to R-squared when we add more predictors?",
    "section": "",
    "text": "TLDR\n\n\n\nWhen using OLS, \\(R^2\\) will always stay the same or increase you add more features to a linear regression, even if those features are ‚Äúuseless‚Äù."
  },
  {
    "objectID": "blog/posts/r-squared.html#about-r2",
    "href": "blog/posts/r-squared.html#about-r2",
    "title": "What happens to R-squared when we add more predictors?",
    "section": "About \\(R^2\\)",
    "text": "About \\(R^2\\)\nAs we saw briefly on Week 02 and as defined in Chapter 3 of the textbook (James et al. 2021), \\(R^2\\) represents the proportion of variability in the response variable that can be explained using the dependent variables (features). More specifically:\n\\[\nR^2 = 1 - \\frac{\\operatorname{RSS}}{\\operatorname{TSS}}\n\\]\nwhere \\(\\operatorname{TSS}\\) represents the Total Sum of Squares:\n\\[\n\\operatorname{TSS} =  \\sum_i^n{\\left(y_i - \\bar{y}\\right)^2},\n\\]\nand \\(\\bar{y}\\) is the mean of \\(\\mathbf{y}\\).\nAs for the \\(\\operatorname{RSS}\\), it represents the Residual Sum of Squares.\nTo understand why \\(R^2\\) can never decrease if we add more features, I think it is useful to visualise linear regression in matrix format."
  },
  {
    "objectID": "blog/posts/r-squared.html#data-and-variables",
    "href": "blog/posts/r-squared.html#data-and-variables",
    "title": "What happens to R-squared when we add more predictors?",
    "section": "Data and Variables",
    "text": "Data and Variables\nSuppose you have the following data:\n\\[\n\\mathbf{X} = \\left[\n\\begin{array}{cc}\nx_{11}\\\\\nx_{21}\\\\\n\\vdots\\\\\nx_{n1}\n\\end{array}\\right],\n\\]\nthat is, there i only \\(p=1\\) feature for the \\(n\\) observations of data, and you want to fit a linear regression to attempt to predict the \\(\\mathbf{y}\\) column below:\n\\[\n\\mathbf{y} = \\left[\n\\begin{array}{c}\ny_{1}\\\\\ny_2\\\\\n\\vdots\\\\\ny_n\n\\end{array}\\right].\n\\]"
  },
  {
    "objectID": "blog/posts/r-squared.html#simple-linear-regression-in-matrix-format",
    "href": "blog/posts/r-squared.html#simple-linear-regression-in-matrix-format",
    "title": "What happens to R-squared when we add more predictors?",
    "section": "Simple Linear Regression in matrix format",
    "text": "Simple Linear Regression in matrix format\nWe can also represent the regression coefficients obtained by Ordinary Least Squares (OLS) in matrix format. \\(\\hat{\\boldsymbol{\\beta}}\\) is a \\(1 \\times (p+1)\\) matrix:\n\\[\n\\hat{\\boldsymbol{\\beta}} = \\left[\n\\begin{array}{c}\n\\hat{\\beta}_{0}\\\\\n\\hat{\\beta}_1\n\\end{array}\\right].\n\\]\nWhen thinking about regression this way, it is often useful to add an additional column of 1s to \\(\\mathbf{X}\\):\n\\[\n\\mathbf{X} = \\left[\n\\begin{array}{cc}\n1 & x_{11}\\\\\n1 & x_{21}\\\\\n1 & \\vdots\\\\\n1 & x_{n1}\n\\end{array}\\right],\n\\]\nso that:\n\\[\n\\hat{\\boldsymbol{\\beta}}\\mathbf{X} =\\left[\n\\begin{array}{c}\n\\hat{\\beta}_{0} \\times 1 + \\hat{\\beta}_1 \\times x_{11} \\\\\n\\hat{\\beta}_{0} \\times 1 + \\hat{\\beta}_1 \\times x_{21} \\\\\n\\vdots  \\\\\n\\hat{\\beta}_{0} \\times 1 + \\hat{\\beta}_1 \\times x_{n1}\\\\\n\\end{array}\\right] = \\hat{\\mathbf{y}}\n\\]\nrepresents the estimated values \\(\\hat{\\mathbf{y}}\\).\nResiduals can then be represented as \\(\\mathbf{e} = (\\mathbf{y} - \\hat{\\mathbf{y}})\\), or\n\\[\n\\mathbf{e} =\\left[\n\\begin{array}{c}\ny_{1} - (\\hat{\\beta}_{0} \\times 1 + \\hat{\\beta}_1 \\times x_{11}) \\\\\ny_{2} - (\\hat{\\beta}_{0} \\times 1 + \\hat{\\beta}_1 \\times x_{21} )\\\\\n\\vdots  \\\\\ny_{n} - (\\hat{\\beta}_{0} \\times 1 + \\hat{\\beta}_1 \\times x_{n1})\\\\\n\\end{array}\\right]\n\\]\nOLS provides an optimal way to find coefficients \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes \\(\\operatorname{RSS}\\).\nIn matrix format, the \\(\\operatorname{RSS}\\) can be represented as:\n\\[\n\\operatorname{RSS} = \\mathbf{e}^T\\mathbf{e}.\n\\]\nOLS will find an optimal solution to \\(\\text{minimise } \\operatorname{RSS}\\). Let‚Äôs call it \\(\\operatorname{RSS}_{(p=1)}\\)."
  },
  {
    "objectID": "blog/posts/r-squared.html#what-if-i-add-another-feature",
    "href": "blog/posts/r-squared.html#what-if-i-add-another-feature",
    "title": "What happens to R-squared when we add more predictors?",
    "section": "What if I add another feature?",
    "text": "What if I add another feature?\nüí° What would happen if I added a second feature and ran a regression with \\(p=2\\)?\nThink of the implications this has for the matrix of coefficients \\(\\hat{\\boldsymbol{\\beta}}\\) and matrix of residuals \\(\\mathbf{e}\\) if I decided to add a second column of data to \\(\\mathbf{X}\\).\nThe new extended feature matrix, let‚Äôs call it \\(\\mathbf{X}'\\), would look like the following:\n\\[\n\\mathbf{X} = \\left[\n\\begin{array}{ccc}\n1 & x_{11}&x_{12}\\\\\n1 & x_{21}&x_{22}\\\\\n1 & \\vdots&\\vdots\\\\\n1 & x_{n1}&x_{n2}\n\\end{array}\\right],\n\\]\nOLS would find new coefficients, let‚Äôs call them \\(\\hat{\\boldsymbol{\\beta}}'\\):\n\\[\n\\hat{\\boldsymbol{\\beta}} = \\left[\n\\begin{array}{c}\n\\hat{\\beta}'_{0}\\\\\n\\hat{\\beta}'_1\\\\\n\\hat{\\beta}'_2\n\\end{array}\\right].\n\\]\nSimilarly, the new \\(\\mathbf{e}'\\) could be represented as:\n\\[\n\\mathbf{e}' =\\left[\n\\begin{array}{c}\ny_{1} - (\\hat{\\beta}'_{0} \\times 1 + \\hat{\\beta}'_1 \\times x_{11} + \\hat{\\beta}'_2 \\times x_{12})\\\\\ny_{2} - (\\hat{\\beta}'_{0} \\times 1 + \\hat{\\beta}'_1 \\times x_{21} + \\hat{\\beta}'_2 \\times x_{22})\\\\\n\\vdots  \\\\\ny_{n} - (\\hat{\\beta}'_{0} \\times 1 + \\hat{\\beta}'_1 \\times x_{n1} + \\hat{\\beta}'_2 \\times x_{n2})\\\\\n\\end{array}\\right]\n\\]\nAgain, if we run OLS, the algorithm will find an optimal solution to the the residuals above, a minimum \\(\\operatorname{RSS}_{(p=2)}\\). What can we expect of \\(\\operatorname{RSS}_{(p=2)}\\) in relation to \\(\\operatorname{RSS}_{(p=1)}\\)? Let‚Äôs think through some scenarios:\nIf it turned out that \\(\\hat{\\beta}'_0 = \\hat{\\beta}_0\\) and \\(\\hat{\\beta}'_1 = \\hat{\\beta}_1\\):\n\nIf \\(\\hat{\\beta}'_2 = 0\\), then we could conclude that \\(\\mathbf{e}' = \\mathbf{e}\\) and \\(\\operatorname{RSS}_{(p=2)} = \\operatorname{RSS}_{(p=1)}\\). That is, by minimising the sum of squares, OLS cannot find a better solution that involves \\(\\hat{\\beta}'_2\\).\nIf \\(\\hat{\\beta}'_2 \\neq 0\\) then notice that the new residuals are related to the previous ones: \\({e_i}' = (e_i - \\hat{\\beta}'_2x_{i2}) ~~\\forall i\\). That means \\(({e_i}')^2 \\geq e_i^2~~\\forall i\\) and therefore \\(\\operatorname{RSS}_{(p=2)} \\geq \\operatorname{RSS}_{(p=1)}\\).\n\nA similar logic applies to the case where \\(\\hat{\\beta}'_0 \\neq \\hat{\\beta}_0\\) and \\(\\hat{\\beta}'_1 \\neq \\hat{\\beta}_1\\) ‚Äî check 1 for a more formal proof."
  },
  {
    "objectID": "blog/posts/r-squared.html#conclusion",
    "href": "blog/posts/r-squared.html#conclusion",
    "title": "What happens to R-squared when we add more predictors?",
    "section": "Conclusion",
    "text": "Conclusion\nBy adding a new feature, given the fact that OLS uses squares of errors, it is inevitable that \\(({e_i}')^2 \\geq e_i^2~~\\forall i\\). OLS will always find an equivalent or a lower \\(\\operatorname{RSS}\\) value. If you check the \\(R^2\\) definition once again, you will realise that \\(\\operatorname{TSS}\\) will never change ‚Äî as it is not related to \\(\\mathbf{X}\\), only to \\(\\mathbf{y}\\) ‚Äî only \\(\\operatorname{RSS}\\) can vary when you add new features. Since \\(\\operatorname{RSS}\\) can only decrease or stay the same, \\(R^2\\) will always increase or stay the same, never decrease, if you add more features.\nTo correct this misleading tendency of R-squared, an adjusted index has been proposed. The ajusted R-squared takes the number of features into account and it is what you should rely on when assessing the goodness-of-fit of a linear regression."
  },
  {
    "objectID": "main/assessments.html",
    "href": "main/assessments.html",
    "title": "Assessments",
    "section": "",
    "text": "Click on the assignments below to go to the Moodle page and submit your responses:\n\n‚úçÔ∏è Formative Problem Set (01): Moodle link & associated RMarkdown file."
  },
  {
    "objectID": "main/assessments.html#problem-sets-60",
    "href": "main/assessments.html#problem-sets-60",
    "title": "Assessments",
    "section": "üìù Problem Sets (60%)",
    "text": "üìù Problem Sets (60%)\n\nSummative problem sets released on Weeks 5, 8 & 11.\nThese will have a similar style to the formative problem sets, a mix of R tasks and your written interpretation of the analyses.\nTypically, you will have 4-6 days to submit your solutions.\nEach of the three summative problem sets is worth 20% of the final mark, and will be graded on a 100 point scale."
  },
  {
    "objectID": "main/assessments.html#exam-40",
    "href": "main/assessments.html#exam-40",
    "title": "Assessments",
    "section": "‚úçÔ∏è Exam (40%)",
    "text": "‚úçÔ∏è Exam (40%)\n\nAn open-book take-home exam, taken during the January exams period.\nExam questions will be comparable in style to the problem sets.\nThe exam questions will be released on Moodle\n\n\n\n\n\n\n\n‚ö†Ô∏è Import Update 11/10/2022\n\n\n\nLast year, DS202 exam was performed entirely online due to COVID-19 mitigation procedures. We want to run it online via our own Moodle page again this academic term, we just need to understand LSE regulations about exams for this year.\nWe will update you on this very soon (hopefully by the end of W04)."
  },
  {
    "objectID": "main/communication.html",
    "href": "main/communication.html",
    "title": "LSE DS202",
    "section": "",
    "text": "Find out how to reach out to your peers, teaching and administrative staff during this course.\n\n\n\nMost of our ‚Äúinformal‚Äù communication and interactions will happen through Slack.\nSlack is a platform used by many companies and institutions where teams can collaborate and communicate about a specific project.\nThere will be channels dedicated to discussing each week‚Äôs content, a channel for sharing useful links and events, plus a random channel to share random stuff about data science.\n\nYou will receive an invitation to join our Slack group via e-mail. Send an e-mail to Jon (J.Cardoso-Silva at lse dot ac dot uk) if you have not received an invite by the time of the first lecture.\nCheck out the following links to understand more about this tool:\n\nSlack tutorials\nCollaborate effectively in channels\n\n\n\n\n\nIt is probably a good idea to book office hours if:\n\nyou struggled with a technical or theoretical aspect of a problem set in the previous week,\nyou have queries about careers in data science,\nyou want guidance in how to apply data science to other things you are studying outside this course.\n\nCome prepared. You only have 15 minutes.\nAsk for help sooner rather than later.\nBook slots via StudentHub up to 12 hours in advance.\n\n‚ö†Ô∏è Reserve üìß e-mail for formal requests: extensions, deferrals, etc. No need to e-mail to inform you will skip a class, for example."
  },
  {
    "objectID": "main/courserep.html",
    "href": "main/courserep.html",
    "title": "Course Representative",
    "section": "",
    "text": "The Data Science Institute (DSI) is excited to announce that the elections for Student Academic Representatives have now gone live! You can nominate yourself if you are taking DS105M or DS202 this Term, and all students get to to cast their votes anonymously on the candidates. The nomination and voting process will be conducted via Slack.\nAs a Student Academic Representative (Course Rep), you‚Äôll work with staff to ensure your peers‚Äô feedback is seen and acted on, and that student voices are represented in institutional decision-making. It is also a fantastic way to develop new skills, get to know the Institute better, and bolster your CV.\nYou can see the positions available in the table below:"
  },
  {
    "objectID": "main/courserep.html#engagement",
    "href": "main/courserep.html#engagement",
    "title": "Course Representative",
    "section": "Engagement",
    "text": "Engagement\n\nYou will have a direct channel of communication to voice suggestions, concerns and recommendations to the teaching and administrative staff at the DSI.\nYou will be allowed to use the DSI space on certain periods during the week.\nYou will have direct contact to PhD students and researchers who are visiting the DSI.\nYou will be the first to know about internal research projects available to undergraduate students."
  },
  {
    "objectID": "main/courserep.html#teaching-committees",
    "href": "main/courserep.html#teaching-committees",
    "title": "Course Representative",
    "section": "Teaching Committees",
    "text": "Teaching Committees\nAs a course rep, you will be invited to attend two Teaching Committees at the DSI during Michaelmas Term:\n\nthe first on Week 07\nthe second on Week 11\n\n\n\n\n\n\n\nWhat are Teaching Committees?\n\n\n\n\n\nTeaching Committees are official meetings where academic and professional service staff at the DSI discuss how the courses are going, devise adjustments for what is not going as planned, and plan new ways to boost recognition of innovative work of students enrolled in DSI courses. These meetings typically involve Lecturers, the Teaching Support Officer, the Communications Officer, the Institute Manager, and the Director of the DSI."
  },
  {
    "objectID": "main/courserep.html#nominations",
    "href": "main/courserep.html#nominations",
    "title": "Course Representative",
    "section": "1. Nominations",
    "text": "1. Nominations\n\nWrite down a short paragraph (max. ~50 words) explaining why you feel you would be great for the role!\nHead to your Slack group and post your short paragraph on the #student-rep channel.\nYou can nominate yourself anytime but no later than Monday 10 October 12 p.m.."
  },
  {
    "objectID": "main/courserep.html#voting",
    "href": "main/courserep.html#voting",
    "title": "Course Representative",
    "section": "2. Voting",
    "text": "2. Voting\n\nWe will set up an (anonymous) poll on the #student-rep channel using the app Polly\nNo one will be able to see who is winning/losing. Only when the poll closes will we know the results.\nAll students will be able to cast their votes anonymously on their favourite candidates from Tuesday 11 October ‚Äì Thursday 13 October (end of Week 03).\nResults will be announced on the #student-rep channel."
  },
  {
    "objectID": "main/syllabus.html",
    "href": "main/syllabus.html",
    "title": "LSE DS202",
    "section": "",
    "text": "References\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York NY: Springer. https://www.statlearning.com/.\n\n\nSumner, Chris, Alison Byers, Rachel Boochever, and Gregory J. Park. 2012. ‚ÄúPredicting Dark Triad Personality Traits from Twitter Usage and a Linguistic Analysis of Tweets.‚Äù In 2012 11th International Conference on Machine Learning and Applications, 2:386‚Äì93. https://doi.org/10.1109/ICMLA.2012.218."
  },
  {
    "objectID": "weeks/week01.html",
    "href": "weeks/week01.html",
    "title": "üóìÔ∏è Week 01 - Introduction, Context & Key Concepts",
    "section": "",
    "text": "In this first week, we will cover what you can expect to learn from this course and the course logistics: all you need to know about the structure of the lectures, classes, assessments and how we will interact throughout this course.\nWe will also cover some of the basics: what do we mean by data science & machine learning, and what do you need to do to get the most of this course?\nJoin the lecture on 30 September 2022 at 2pm at NAB LG.01 (Wolfson Theatre)."
  },
  {
    "objectID": "weeks/week01.html#links",
    "href": "weeks/week01.html#links",
    "title": "üóìÔ∏è Week 01 - Introduction, Context & Key Concepts",
    "section": "Links",
    "text": "Links\n\nüë®‚Äçüè´ Lecture slides\nüìí Preparing for next week‚Äôs lab\nüîñ Appendix"
  },
  {
    "objectID": "weeks/week02.html",
    "href": "weeks/week02.html",
    "title": "üóìÔ∏è Week 02 - Simple and Multiple Linear Regression",
    "section": "",
    "text": "If you have already taken stats level at university, it is likely that you have met the linear regression method.\nIt is the most fundamental algorithm for regression and if you truly master it, you will be way ahead of the curve when it comes to the most advanced Machine Learning algorithms.\nJoin the lecture 7 October 2022 2pm at NAB LG.01 (Wolfson Theatre).."
  },
  {
    "objectID": "weeks/week02.html#links",
    "href": "weeks/week02.html#links",
    "title": "üóìÔ∏è Week 02 - Simple and Multiple Linear Regression",
    "section": "Links",
    "text": "Links\n\nüë®‚Äçüè´ Lecture slides\n‚úÖ Take a look at this week‚Äôs checklist\nüíª This week‚Äôs lab"
  },
  {
    "objectID": "weeks/week03.html",
    "href": "weeks/week03.html",
    "title": "üóìÔ∏è Week 03 - Classifiers",
    "section": "",
    "text": "On üóìÔ∏è Week 02, we learned how to make predictions about numerical variables. But what if you wanted to predict whether someone will perform an action (a Yes or No question)? Or, say, you were interested in assessing how the risk of fraud increases depending on the behaviour of a customer? These problems can be modelled using classifiers, a type of supervised learning.\nThis week, we will explore two classifier algorithms: the Logistic Regression and the Naive Bayes classifiers. We will learn how those methods relate (or not) to linear regression, and how to interpret its results. You will also meet a few new metrics and will learn of new ways to assess the ‚Äòaccuracy‚Äô of models. These metrics will be incrediblly important on Week 04!\nJoin the lecture 14 October 2022 2pm at NAB LG.01 (Wolfson Theatre)."
  },
  {
    "objectID": "weeks/week03.html#links",
    "href": "weeks/week03.html#links",
    "title": "üóìÔ∏è Week 03 - Classifiers",
    "section": "Links",
    "text": "Links\n\nüë®‚Äçüè´ Lecture slides\n‚úÖ Take a look at this week‚Äôs checklist\nüíª This week‚Äôs lab"
  },
  {
    "objectID": "weeks/week04.html",
    "href": "weeks/week04.html",
    "title": "üóìÔ∏è Week 04 - Resampling Methods",
    "section": "",
    "text": "We have made it to Week 04! In the labs, you will get a chance to explore different Classification algorithms (Logistic Regression and Naive Bayes) and how to think about their output. What can we say about predicted probabilities?\nThis week‚Äôs lecture (on Friday 21 October 2022, Wolfson Theatre NAB.LG.01 2pm-4pm) will be a bit different. We will not learn about new algorithms. Instead, we will learn how to compare models built from the three algorithms we learned about (linear regression, logistic regression and naive bayes). You will learn about the different metrics one could use to assess the predictive performance on a test set rather than just the goodness-of-fit of the training data.\nThe structure of the lecture will also be a bit different, it will resemble a workshop more than a taught lecture. See you there!"
  },
  {
    "objectID": "weeks/week04.html#links",
    "href": "weeks/week04.html#links",
    "title": "üóìÔ∏è Week 04 - Resampling Methods",
    "section": "Links",
    "text": "Links\n\nüë®‚Äçüè´ Lecture\n‚úÖ Take a look at this week‚Äôs checklist\nüíª This week‚Äôs lab"
  },
  {
    "objectID": "weeks/week05.html",
    "href": "weeks/week05.html",
    "title": "üóìÔ∏è Week 05 - Non-linear algorithms",
    "section": "",
    "text": "üë®‚Äçüè´ Lecture\n‚úÖ Take a look at this week‚Äôs checklist\nüíª This week‚Äôs lab"
  },
  {
    "objectID": "weeks/week06.html",
    "href": "weeks/week06.html",
    "title": "üóìÔ∏è Week 06 - Reading Week",
    "section": "",
    "text": "Check out this¬†blog post¬†from Students@LSE blog.\nI would definitely recommend you take a break during Week 06. After all, rest and¬†sleep can help you consolidate memories¬†and improve your learning process.\nHowever, if you feel like revisiting concepts you have learned so far, check out the Recommended Learning sections present in each week‚Äôs sections here on our Moodle Page.\nWe will return with our lectures and classes on Week 07."
  },
  {
    "objectID": "weeks/week01/appendix.html",
    "href": "weeks/week01/appendix.html",
    "title": "üîñ Week 01 - Appendix",
    "section": "",
    "text": "This week‚Äôs indicative reading: (James et al. 2021, chaps. 2, 2.1‚Äì2.2)\n\n\nNeed to recap probability and statistics concepts? Check the suggested readings below:\n\n(Warne 2018, chaps. 1-3,5,6,11-12)\n(Gelman, Hill, and Vehtari 2020, chaps. 1‚Äì4)\nIf you are a PBS student, you can revisit the content of PB130 (MT3, MT4, MT8-MT11)"
  },
  {
    "objectID": "weeks/week01/appendix.html#recommended-additional-reading",
    "href": "weeks/week01/appendix.html#recommended-additional-reading",
    "title": "üîñ Week 01 - Appendix",
    "section": "Recommended (additional) reading",
    "text": "Recommended (additional) reading\nWhat are different ways one can approach a modelling problem?\nCheckout the upcoming book ‚ÄòModeling Mindsets‚Äô (Molnar 2022, chaps. 2‚Äì3) (it‚Äôs free to read online) to learn about the traditional frequentist statistics vs Bayesian statistics vs Machine Learning approaches.\nThe following twitter thread also summarises the main points of these different paradigms:\n\n\nIn a perfect world, you could effortlessly switch between modeling mindsets (statistics, machine learning, causal inference, ‚Ä¶).Realistically, you only have time to master a few mindsets.So what to do? A thread üßµ\n\n‚Äî Christoph Molnar (@ChristophMolnar) August 30, 2022"
  },
  {
    "objectID": "weeks/week01/appendix.html#lse-digital-skills-lab",
    "href": "weeks/week01/appendix.html#lse-digital-skills-lab",
    "title": "üîñ Week 01 - Appendix",
    "section": "LSE Digital Skills Lab",
    "text": "LSE Digital Skills Lab\nLSE Digital Skills Lab offers R and python workshops during Term time and they will also give DSI students access to self-paced programming courses via Dataquest.\nFollow the links below to take the pre-sessional self-paced courses:\n\nR for Data Science Pre-sessional Course 22/23\n\nAlso, keep an eye on the following pages for news of the in-person workshops:\n\nR workshops"
  },
  {
    "objectID": "weeks/week01/appendix.html#other-resources",
    "href": "weeks/week01/appendix.html#other-resources",
    "title": "üîñ Week 01 - Appendix",
    "section": "Other resources",
    "text": "Other resources\n\nCheckout this summer‚Äôs LSE Careers Skill Accelerator programme. Some of the self-paced courses will remain open to LSE students until the end of the year.\n\n\n\nReady to develop the key skills employers are looking for in 2022?Join this summer's LSE Careers Skills Accelerator programme to expand your skillset!‚≠êApply on CareerHub by 11.59pm, Wed 15 June for your chance to join the programme‚≠êhttps://t.co/J5sI1NRaMA\n\n‚Äî LSE Careers (@LSECareers) June 9, 2022\n\n\n\nThe book R for Data Science is free to read online and is a great resource to advance your R skills."
  },
  {
    "objectID": "weeks/week01/lecture.html",
    "href": "weeks/week01/lecture.html",
    "title": "üë®‚Äçüè´ Week 01 - Slides",
    "section": "",
    "text": "Either click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides."
  },
  {
    "objectID": "weeks/week01/lecture.html#coffee-break-10-min",
    "href": "weeks/week01/lecture.html#coffee-break-10-min",
    "title": "üë®‚Äçüè´ Week 01 - Slides",
    "section": "‚òï Coffee Break (10 min)",
    "text": "‚òï Coffee Break (10 min)\nUse this time to chat, stretch, drink some coffee or just relax for a bit by yourself."
  },
  {
    "objectID": "weeks/week01/lecture.html#part-ii---key-concepts-45-50-min",
    "href": "weeks/week01/lecture.html#part-ii---key-concepts-45-50-min",
    "title": "üë®‚Äçüè´ Week 01 - Slides",
    "section": "Part II - Key Concepts (45-50 min)",
    "text": "Part II - Key Concepts (45-50 min)\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides."
  },
  {
    "objectID": "weeks/week01/prep.html",
    "href": "weeks/week01/prep.html",
    "title": "üìí Week 01 - Preparing for next week‚Äôs lab",
    "section": "",
    "text": "We do not have classes this week. Instead, we recommend you use this time to learn or revisit the basics of the programming language R.\nSee sections below for advice on how to do that."
  },
  {
    "objectID": "weeks/week01/prep.html#preparing-for-the-next-week",
    "href": "weeks/week01/prep.html#preparing-for-the-next-week",
    "title": "üìí Week 01 - Preparing for next week‚Äôs lab",
    "section": "üìí Preparing for the next week",
    "text": "üìí Preparing for the next week\n\nJoin the Slack group of this course (more info)\nUse this time to learn or revisit basic R programming skills.\nWrite down your R questions. Next week‚Äôs lab, we have a roadmap for a revision of basic commands but we can tailor it to address any questions you might have with R."
  },
  {
    "objectID": "weeks/week01/prep.html#recommended-reading-other-resources",
    "href": "weeks/week01/prep.html#recommended-reading-other-resources",
    "title": "üìí Week 01 - Preparing for next week‚Äôs lab",
    "section": "üîñ Recommended reading & other resources",
    "text": "üîñ Recommended reading & other resources\nCheck out the Appendix page."
  },
  {
    "objectID": "weeks/week02/checklist.html",
    "href": "weeks/week02/checklist.html",
    "title": "‚úÖ Week 02 - Checklist",
    "section": "",
    "text": "Your Checklist:\nYour Checklist:\n\nüñ•Ô∏è Before you come to the class, skim through the W02 lab roadmap page to have an idea of what we are going to do.\nüë®‚Äçüíª New to R? Or perhaps you are in one of the Monday sessions and struggled to follow the lab? There is still time to take the R pre-sessional course. (Read the Getting access and using Dataquest session carefully)\nüìö Before you attend the lecture on Friday, try to catch up on the recommended reading of last week.\nüíª Assess your own understanding: did you understand all the exercises of the lab?\nüìù Take note of anything that is still not clear to you.\nüìü Share your conceptual or programming-related questions on #week02 channel on Slack.\nüì§ Have anything else to share? If came across an interesting resource for R beginners, or curious articles about exploratory data analysis, feel free to share it on /week02 or /random channels in our Slack group.\n\nFollowing this will keep you well prepared for the Linear Regression lab of Week 03."
  },
  {
    "objectID": "weeks/week02/lab.html",
    "href": "weeks/week02/lab.html",
    "title": "üíª Week 02 - Lab Roadmap (90 min)",
    "section": "",
    "text": "This week, we will recap some basic R commands for social data science and then apply these commands to a practical case. We will learn about data structures and some simple data visualisation skills in R .\nIt is expected that R has been downloaded locally. We recommend that you run R within an integrated development environment (IDE) such as RStudio, which can be freely downloaded."
  },
  {
    "objectID": "weeks/week02/lab.html#step-1-basic-commands",
    "href": "weeks/week02/lab.html#step-1-basic-commands",
    "title": "üíª Week 02 - Lab Roadmap (90 min)",
    "section": "Step 1: Basic commands",
    "text": "Step 1: Basic commands\nWe will follow the instructions below step by step together while answering whatever questions you might encounter along the way.\n\nOpen R or RStudio. You can either run the folllowing commands in a R script or in the console window.\nCreate a vetor of numbers with the function c() and name it x. When we type x, it gives us back the vector:\n> x <- c(1, 3, 2, 5)\n> x\n[1] 1 3 2 5\nNote that the > is not part of the command; rather, it is printed by R to indicate that it is ready for another command to be entered. We can also save things using = rather than <-:\n> x = c(1, 3, 2, 5)\nCheck the length of vector x using the length() function:\n> length(x)\n[1] 4\nCreate a matrix of numbers with the function matrix() and name it y. When we type y, it gives us back the matrix:\n> y <- matrix(data = c(1:16), nrow = 4, ncol = 4)\n> y\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\nIf you want to learn about the meaning of some arguments like nrow or ncol:\n> ?matrix\nSelect one element in the matrix y:\n> y[2,3]\n[1] 10\nThe first number after the open-bracket symbol [ always refers to the row, and the second number always refers to the column\nSelect multiple rows and column at a time in the matrix y:\n> y[c(1, 3), c(2, 4)]\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n> y[1:3, 2:4]\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n> y[1:2, ]\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n> y[-c(1, 3), ]\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\nNo index for the columns or the rows indicates that R should include all columns or all rows, respectively. The use of a negative sign - in the index tells R to keep all rows or columns except those indicated in the index.\nCheck the number of rows and columns in a matrix:\n> dim(y)\n[1] 4 4\nGenerate a vector of random normal variables:\n> set.seed(1303)\n> x <- rnorm(50)\n> y <- x + rnorm(50, mean = 50, sd = .1)\n> cor(x, y)\n[1] 0.9942128\nBy default, rnorm() creates standard normal random variables with a mean of 0 and a standard deviation of 1. However, the mean and standard deviation can be altered as illustrated above.\nEach time we call the function rnorm(), we will get a different answer. However, sometimes we want our code to reproduce the exact same set of random numbers; we can use the set.seed() function to do this. We use set.seed() throughout the labs whenever we perform calculations involving random quantities.\nLet‚Äôs check some descriptive statistics of these vectors:\n> mean(y)\n[1] 50.18446\n> var(y)\n[1] 0.8002002\n> sqrt ( var (y))\n[1] 0.8945391\n> sd(y)\n[1] 0.8945391\n> cor (x, y)\n[1] 0.9942128\nThe mean() and var() functions can be used to compute the mean and variance of a vector of numbers. Applying sqrt() to the output of var() will give the standard deviation. Or we can simply use the sd() function. The cor() function is to compute the correlation between vector x and y."
  },
  {
    "objectID": "weeks/week02/lab.html#step-2-graphics",
    "href": "weeks/week02/lab.html#step-2-graphics",
    "title": "üíª Week 02 - Lab Roadmap (90 min)",
    "section": "Step 2: Graphics",
    "text": "Step 2: Graphics\nWe will plot and save plots in R.\n\nProduce a scatterplot between two vectors of numbers using the function plot():\n> set.seed(1303)\n> x <- rnorm(100)\n> y <- rnorm(100)\n> plot(x,y)\n> plot(x, y, xlab = \" this is the x- axis \",\n       ylab = \" this is the y- axis \",\n       main = \" Plot of X vs Y\")\nBy default, the output plot will show in Plots window in the lower right cornor.\nSave the scatterplot in a pdf or a jpeg file:\n> pdf(\"Figure.pdf\")\n> plot(x, y, col = \"green\")\n> dev.off()\nnull device\n        1    \nTo create a jpeg, we use the function jpeg() instaed of pdf(). The function dev.off() indicates to R that we are done creating the plot.\nProduce a contour plot (like a topographical map) to represent 3-Dimentional data using the function contour():\n> x <- seq(1, 10)\n> y <- x\n> f <- outer(x, y, function (x, y) cos(y) / (1 + x^2))\n> contour(x, y, f)\n> contour(x, y, f, nlevels = 45, add = T)\n> fa <- (f - t(f)) / 2\n> contour(x, y, fa, nlevels = 15)\nThe image() function works the same way as contour(). Explore it if you are interested.\nUsing ggplot2 package for graphic:\nIn R, the data is stored in a structure called dataframe. Dataframe can be seen as a 2-dimensional table consisting of rows and columns and their values. These values might be in different types such as numeric, character or logical. However, each column should have the exactly same data type.\nWe can use the open-source data visualization package - ggplot2 to construct aesthetic mappings based on our data.\n\nSince tidyverse library includes ggplot2, if you install tidyverse you will have access to ggplot2; installation can be done;\n\n> install.packages(\"tidyverse\")\n\nAlternatively, ggplot2 package can be installed\n\n> install.packages(\"ggplot2\")\nAfter the installation is completed, it should be called in R environment:\n> library(ggplot2)\nThere are some ready datasets to play with in the package ggplot2. Let‚Äôs explore and plot a dataset called diamonds showing the prices and some features of over 50000 diamonds. You can explore the meanings of the variables with ?diamonds command.\nPlease type:\n> View(diamonds)\nthe View() function can be used to view it in a spreadsheet-like window.\nwe can plot this dataset with desired variables.\n > ggplot(diamonds[0:50,], aes(x=carat, y=price)) +\ngeom_point() + \ngeom_text(label=diamonds[0:50,]$cut)\nx and y in aes shows the axis which are the carat and the price info each diamond. diamonds is the dataframe used in the plot and We used only the first 50 lines for clear visualisation. geom_point defines the shape of data to be plot and geom_text adds the labels. With $ sign, you can access a column in your dataset.\nWe can also plot a histogram showing price\n > ggplot(diamonds,aes(x=price)) + geom_histogram(binwidth=100)\nThis time all dataset is used for the visualisation.. For more detailed information and some examples you can use ?ggplot and ?aes\n\n\n\n\n\n\n\nFurther Study - Heatmap Example\n\n\n\n\n\nCreating a heatmap with ggplot2 package:\nThis time we will create a dummy dataframe with country names, a time period and random GDP for each country.\ncountries <- c(\"Canada\", \"France\",\"Greece\",\"Libya\",\"Malta\")\nyears <- c(2012:2021)\nLet‚Äôs gather them together and see what our dataframe looks like:\ndata <- expand.grid(Country=countries, Year=years)\ndata\nexpand.grid creates a dataframe from all combinations of the supplied vectors.\nto create random GPD for each country and for each year, and to add these values into our dataframe as GDP column::\ngdps  <- runif(50, min=20000, max=500000)\ndata$GDP = gdps\nrunif generates a certain number of random values between min and maximum values with a uniform distribution. Since we have 5 countries and 10 year, we generated 50 random GPD value.\nTo check the data and type of the variable data:\nView(data)\nclass(data)\nWe can plot now a very basic heatmap\nggplot(data, aes(Year, Country, fill= GDP)) + geom_tile()\nTo create a heatmap, our dataframe should look like a tabular dataset with three columns. aes defines X,Y axis and the values filling these pairs in the heatmap. geom_tile creates a heatmap with rectangulars with different options. For detailed information ?geom_tile"
  },
  {
    "objectID": "weeks/week02/lab.html#step-3-loading-data",
    "href": "weeks/week02/lab.html#step-3-loading-data",
    "title": "üíª Week 02 - Lab Roadmap (90 min)",
    "section": "Step 3: Loading data",
    "text": "Step 3: Loading data\nNow, we will learn how to import a data set into R and explore the data set. For this lab session, we will use a ready-to-use dataset AUTO in the book ‚ÄúIntroduction to Statistical Learning, with Applications in R‚Äù. With the package ISLR2, we can use all the datasets in the book.\n\nFirst, we need to install ISLR2 into our R environment for future use.\n> install.packages(\"ISLR2\")\nTo use ISLR2 package and the datasets in our analyses, we need to call it in each R session with;\n> library(ISLR2)\nThat‚Äôs it! We now can use all datasets by calling them by their names. The package includes numerous datasets and you can explore them with R.\nAUTO dataset is ready to be used in the analyse. You can explore the dataset by using:\n> View(Auto)\n> head(Auto)\nThe head() function can also be used to view the first few rows of the data\nYou may want to save this dataset on a local computer, which is useful for your future analyses while doing some changes on it. To save a dataset as a csv file:\n> write.csv(DataFrameName, file=\"Path to save the DataFrame//File Name.csv\", row.names = FALSE)\nThe option row.names = FALSE deletes the row names when you are saving the dataset. In this case, it will remove basic incremental indexes such as 1,2,‚Ä¶ from the data. A detailed explanation of write.csv and its options could be found by typing ?write.csv\n\n\n\n\n\n\nExample\n\n\n\nYou need to include the path where you would like to save the dataset on your computer. For example, if you work in a folder called Test in your desktop in a Windows machine. The code:\n> write.csv(Auto, \"C:Users//LSE//Desktop//Test//autodataset.csv\", row.names = FALSE )\n\n\nTo use the dataset in the future, you need to load it into a dataframe by importing the csv file.\nWe will load this dataset in a dataframe called Auto. Dataframe name is changable, however we would like to use words understandable and readable.\n> Auto <- read.csv(\"C://Users//LSE//Desktop//Test//autodataset.csv\", na.strings = \"?\")\nUsing the option na.strings tells R that any time it sees a particular character or set of characters (such as a question mark), it should be treated as a missing element of the data matrix.\nYou can check the dataset:\n> View(Auto)\n> head(Auto)\nDeal with the missing data by removing rows with missing observations:\n> Auto <- na.omit(Auto)\n> dim(Auto)\n[1] 392 9\nThe function dim() is to check the size of the data frame.\nProduce a numerical summary of each variable in the particular data frame:\n> summary(Auto)"
  },
  {
    "objectID": "weeks/week02/lab.html#step-4-practical-exercises-in-pairs",
    "href": "weeks/week02/lab.html#step-4-practical-exercises-in-pairs",
    "title": "üíª Week 02 - Lab Roadmap (90 min)",
    "section": "Step 4: Practical exercises (in pairs)",
    "text": "Step 4: Practical exercises (in pairs)\nSo far, we have learnt some basic commands in R. In this practical case, we will continues with the data set Auto studied in Step 3. Make sure that the missing values have been removed from the data.\nSix questions are listed below. You are required to try to answer these questions in pair using R commands. We will go over the solutions once everyone has finished these questions.\nüéØ Questions\n\nWhich of the predictors are quantitative, and which are qualitative?\nWhat is the range of each quantitative predictor? (hint: You can answer this using the range() function)\nWhat is the mean and standard deviation of each quantitative predictor?\nNow remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?\nUsing the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings.\nSuppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer."
  },
  {
    "objectID": "weeks/week02/lab_solutions.html",
    "href": "weeks/week02/lab_solutions.html",
    "title": "üíª Week 02 - Lab Roadmap (90 min)",
    "section": "",
    "text": "Use the function View()to identify the type of a variable (quantitative or qualitative):\nView(Auto)\nVariables mpg, cylinders, horsepower, weight, accelation, year are quantitative variable. Variables origin, name are qualitative varibale.\nUse the function range() to check the range of each quantitative predictor:\nrange(Auto$mpg)\n[1]  9.0 46.6\nTo refer to a variable, we must type the data set and the variable name joined with a $ symbol.\nUsing summary() to have an overall look at all variables and statistical features (like mean and standard deviation) are included in the outputs:\nsummary(Auto)\nor\nmean(Auto$mpg)\nsd(Auto$mpg)\nRemove the 10th through 85th observations from the original data frame and store it as another new data frame:\nAuto_tmp = Auto[-c(10:85), ]\nsummary(Auto_tmp)\nmean(Auto_tmp$mpg)\nsd(Auto_tmp$mpg)\nCreate a scatterplot matrix using the function pairs():\npairs( ~ mpg + displacement + horsepower + weight + \n        acceleration + year + origin + cylinders, \n        data = Auto)\nNotice the linear or non-linear trends in the scatterplots.Then create a histogram of the variable mpg:\nhist (Auto$mpg , col = 2, breaks = 15)\nUse the hist() function to produce some histograms with differing numbers of bins for a few of the quantitative variables. You may find the command par(mfrow = c(2, 2)) useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways.\nAfter observing the first row of the scatterplot matrix which indicates the relationship between gas mileage (mpg) and other variables, you will find evident linear or non-linear trends exist in the scatterplots with variables displacement, horsepower, weight, year and origin. Therefore, these varibles might be useful in predicting mpg.\n\nIf you want to achieve ststistical robust when exploring the relationship between variables, you need to culculate some statistics (like the correlation using the function cor()) and conduct statistical tests. This will be further illustrated in Week 03."
  },
  {
    "objectID": "weeks/week02/lecture.html",
    "href": "weeks/week02/lecture.html",
    "title": "üë®‚Äçüè´ Week 02 - Slides",
    "section": "",
    "text": "Either click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides. For a PDF copy, check Moodle."
  },
  {
    "objectID": "weeks/week02/lecture.html#coffee-break-10-min",
    "href": "weeks/week02/lecture.html#coffee-break-10-min",
    "title": "üë®‚Äçüè´ Week 02 - Slides",
    "section": "‚òï Coffee Break (10 min)",
    "text": "‚òï Coffee Break (10 min)\nUse this time to chat, stretch, drink some coffee or just relax for a bit by yourself."
  },
  {
    "objectID": "weeks/week02/lecture.html#part-ii---multiple-linear-regression-45-50-min",
    "href": "weeks/week02/lecture.html#part-ii---multiple-linear-regression-45-50-min",
    "title": "üë®‚Äçüè´ Week 02 - Slides",
    "section": "Part II - Multiple Linear Regression (45-50 min)",
    "text": "Part II - Multiple Linear Regression (45-50 min)\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides. For a PDF copy, check Moodle."
  },
  {
    "objectID": "weeks/week03/checklist.html",
    "href": "weeks/week03/checklist.html",
    "title": "‚úÖ Week 03 - Checklist",
    "section": "",
    "text": "Your Checklist:\n\nüìô Read (James et al. 2021, chap. 3) to reinforce your theoretical knowledge of Linear Regression. The textbook is available online for free.\nüßë‚Äçüíª If you already know linear regression from previous courses you have taken, why not take this knowledge to next level?\n\nTry to find a dataset online that contains a numerical variable you could predict by fitting a linear regression to it. I will be curious to see what you find. Share your findings on the #week03 channel in our Slack.\n\nüñ•Ô∏è Before you come to the class, skim the W03 lab roadmap page to have an idea of what we are going to do.\n\nThis week, instead of just typing things in the terminal, we will use R Markdown. You can read about it here. This is also how you will be submitting solutions to formative and summative assignments in the future.\nI will post solutions to the practical exercises at the end of the week.\n\nüíª Assess yourself: did you understand all the exercises in the lab?\n\nIf you are new to linear regression and you are enrolled in the Monday sessions, it is likely that you will struggle a bit in the lab. During the week, reserve some time to read about Linear Regression and then practice the exercises again.\n\nüìü Struggling with something? Don‚Äôt know what a particular R command do? Share your questions on the #week03 channel in our Slack.\n\nI will also be posting follow up questions on Slack during the week.\n\nüìù Keep in mind that: after the lecture on Friday, 14 October 2022, we will post the first formative assignment on Moodle.\n\nYou will have until Thursday of the following week (20 October 2022) to submit your solutions.\nThis assignment is not marked, it doesn‚Äôt count towards your final grade, but you will receive feedback if you submit.\nThe assignment will have a similar format as the questions we explore in the lab.\n\nüë®‚Äçüè´ Attend the lecture. It will help you remember concepts more easily when revising later.\n\n\n\n\n\n\nReferences\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York NY: Springer. https://www.statlearning.com/."
  },
  {
    "objectID": "weeks/week03/formative.html",
    "href": "weeks/week03/formative.html",
    "title": "üìù Week 03 - Formative homework",
    "section": "",
    "text": "Use the Carseats data set in the ISLR2 package to answer the following questions:\n\nFit a multiple linear regression model to predict Sales using Price, Urban, and US. Show the summary output.\nProvide an interpretation of each coefficient in the model. Be careful‚Äîsome of the variables in the model are qualitative!\nWrite the model in equation form, carefully handling the qualitative variables properly.\nFor which of the predictors can you reject the null hypothesis \\(H_0: \\beta_j = 0\\)?\nBased on your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of an association with the outcome. Justify your choices.\nHow well do the models in Questions 1 & 5 fit the data?\nUsing the model from question 5, obtain 95% confidence intervals for the coefficient(s).\nUse the * and : symbols to fit linear regression models with interaction effects. Could you find any model with interactions that fit better than the models you built in Questions 1 & 5? Justify your answer."
  },
  {
    "objectID": "weeks/week03/formative_solutions.html",
    "href": "weeks/week03/formative_solutions.html",
    "title": "üìù Week 03 - Formative homework",
    "section": "",
    "text": "Use the Carseats data set in the ISLR2 package to answer the following questions:\n\nFit a multiple linear regression model to predict Sales using Price, Urban, and US. Show the summary output.\n> library(ISLR2)\n> lm.fit <- lm(Sales ~ Price + Urban + US, data = Carseats)\n> summary(lm.fit)\nProvide an interpretation of each coefficient in the model. Be careful‚Äîsome of the variables in the model are qualitative!\nŒ≤1: Holding Urban and US fixed, the Sales decrease 54.459 units on average when the Price company charges for car seats at each site increases 1000 units.\nŒ≤2: Holding Price and US fixed, the Sales decrease 0.021916 units on average when the store is in urban area.\nŒ≤3: Holding Price and Urban fixed, the Sales increase 1.200573 units on average when the store is in US.\nWrite the model in equation form, carefully handling the qualitative variables properly.\nSales = Œ≤0 + Œ≤1 x Price + Œ≤2 + Œ≤3, when the store is in urban and in US\nSales = Œ≤0 + Œ≤1 x Price + Œ≤3, when the store is not in urban and but in US\nSales = Œ≤0 + Œ≤1 x Price + Œ≤2, when the store is in urban and but not in US\nSales = Œ≤0 + Œ≤1 x Price, when the store is not in urban and not in US\nFor which of the predictors can you reject the null hypothesis H0: Œ≤j = 0?\nFrom the p-values of t-test, we could reject the null hypothesis H0: Œ≤1 = 0 for the predictor Price and the null hypothesis H0: Œ≤3 = 0 for the predictor US.\nBased on your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of an association with the outcome. Justify your choices.\n> lm.fit1 <- lm(Sales ~ Price + US, data = Carseats)\n> summary(lm.fit1)\nHow well do the models in Questions 1 & 5 fit the data?\nNeither model fits the data well according to the small value of R2 and Adjusted R2. It means that the predictors included in both models can only interpret a small part of the change pattern of the response.\nUsing the model from question 5, obtain 95 % confidence intervals for the coefficient(s).\n> confint(lm.fit1)\n    2.5 %      97.5 %\n(Intercept) 11.79032020 14.27126531\nPrice       -0.06475984 -0.04419543\nUSYes        0.69151957  1.70776632\n95% confidence interval for Œ≤0 = [11.79032020, 14.27126531]; 95% confidence interval for Œ≤1 = [-0.06475984 -0.04419543]; 95% confidence interval for Œ≤2 = [0.69151957 1.70776632].\nUse the * and : symbols to fit linear regression models with interaction effects. Could you find any model with interactions that fit better than the models you built in Questions 1 & 5? Justify your answer.\n> lm.fit2 <- lm(Sales ~ Price * US, data = Carseats)\n> summary(lm.fit2)\n> lm.fit3 <- lm(Sales ~ Price * Urban, data = Carseats)\n> summary(lm.fit3)\n> lm.fit4 <- lm(Sales ~ Price * US + Urban, data = Carseats)\n> summary(lm.fit4)\n> lm.fit5 <- lm(Sales ~ Price * Urban + US, data = Carseats)\n> summary(lm.fit5)\n> lm.fit6 <- lm(Sales ~ Price + US + Urban + Price:US + Price:Urban, data = Carseats)\n> summary(lm.fit6)\n> lm.fit7 <- lm(Sales ~ Price + US + Urban + Price:US + Price:Urban + Urban:US, data = Carseats)\n> summary(lm.fit7)\nDifferent multiple linear regression models with interactions have been built. However, after comparision there is almost no difference between the values of the \\(R^2\\) and adjusted \\(R^2\\), which means the goodness of fit is not significantly improved. The reason is that there is no interaction effects between Price and US, and between Price and Urban, and between US and Urban which is supported by the significance of these coeficients‚Äô t-tests."
  },
  {
    "objectID": "weeks/week03/lab.html",
    "href": "weeks/week03/lab.html",
    "title": "üíª Week 03 - Lab Roadmap (90 min)",
    "section": "",
    "text": "This week, we will fit simple and multiple linear regression models in R and learn to interpret the R output. We will apply this method to practical cases and deal with problems that commonly arise during this process."
  },
  {
    "objectID": "weeks/week03/lab.html#step-1-simple-linear-regression",
    "href": "weeks/week03/lab.html#step-1-simple-linear-regression",
    "title": "üíª Week 03 - Lab Roadmap (90 min)",
    "section": "Step 1: Simple linear regression",
    "text": "Step 1: Simple linear regression\nWe will follow the instructions below step by step together while answering whatever questions you might encounter along the way.\n\nInstall and load the ISLR2 package, which contains a large collection of data sets and functions.\ninstall.packages(\"ISLR2\").\nlibrary (ISLR2)\nThe function install.packages() is used to download packages that don‚Äôt come with R. This installation only needs to be done the first time you use a package. However, the library() function must be called within each R session to load packages.\nUse the Boston data set in the ISLR2 library. It records medv (median house value) for 506 census tracts in Boston. Have a look at the first few rows of the Boston data set:\nhead (Boston)\n    crim zn indus chas   nox    rm  age    dis rad tax ptratio lstat medv\n1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3  4.98 24.0\n2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8  9.14 21.6\n3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8  4.03 34.7\n4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7  2.94 33.4\n5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7  5.33 36.2\n6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7  5.21 28.7\nWe want to predict medv using the available predictors, such as rm (average number of rooms per house), age (average age of houses), and lstat (percentage of households with low socioeconomic status). To find out more about the data set, we can type ?Boston.\nFit a simple linear regression lm() model, with medv as the response and lstat as the predictor:\n> lm.fit <- lm(medv ~ lstat , data = Boston)\nThe basic syntax is lm(y ‚àº x, data), where y is the response, x is the predictor, and data is the data set in which we keep these two variables.\nUse the tidy function to create a dataframe with columns for the estimate, standard error, f-statistic (estimate/standard error), p-values, and 95 percent confidence intervals:\ninstalling/loading broom:\ninstall.packages(\"broom\")\nlibrary(broom)\n> tidy(lm.fit, conf.int = TRUE)\n\n\n# A tibble: 2 √ó 7\nterm        estimate std.error statistic   p.value conf.low conf.high\n<chr>          <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)   34.6      0.563       61.4 3.74e-236    33.4     35.7  \n2 lstat         -0.950    0.0387     -24.5 5.08e- 88    -1.03    -0.874\nBecause lm.fit is a simple linear regression model, there are only two coefficients: \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). The goodness-of-fit of the model can be measured by the \\(R^2\\) in the output, which can be obtained (along with other model statistics) using the glance function.\n> glance(lm.fit)$r.squared\n[1] 0.5441463\nPlot medv and lstat along with the least squares regression line using the geom_point() and geom_abline() functions.::\nlibrary(tidyverse)\n\n> ggplot(data = Boston, aes(x = lstat, y = medv)) +\ngeom_point() + \ngeom_abline(intercept = lm.fit$coefficients[1], slope = lm.fit$coefficients[2])"
  },
  {
    "objectID": "weeks/week03/lab.html#step-2-multiple-linear-regression",
    "href": "weeks/week03/lab.html#step-2-multiple-linear-regression",
    "title": "üíª Week 03 - Lab Roadmap (90 min)",
    "section": "Step 2: Multiple linear regression",
    "text": "Step 2: Multiple linear regression\nWe will still use the Boston data set to fit multiple linear regression. The fitting process is similar to simple linear regression.\n\nFit a multiple linear regression lm() model, with medv as the response, lstat and age as the predictors:\n> lm.fit <- lm(medv ~ lstat + age , data = Boston)\n> tidy(lm.fit, conf.int = TRUE)\n\n# A tibble: 3 √ó 7\nterm        estimate std.error statistic   p.value conf.low conf.high\n<chr>          <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)  33.2       0.731      45.5  2.94e-180  31.8      34.7   \n2 lstat        -1.03      0.0482    -21.4  8.42e- 73  -1.13     -0.937 \n3 age           0.0345    0.0122      2.83 4.91e-  3   0.0105    0.0586\nThe syntax lm(y ~ x1 + x2 + x3) is used to fit a model with three predictors, x1, x2, and x3. The tidy() function now outputs the regression coefficients for all the predictors.\nFit a multiple linear regression lm() model, with medv as the response, all rest variables as the predictors:\n> lm.fit <- lm(medv ~ ., data = Boston)\n> tidy(lm.fit, conf.int = TRUE)\n\n# A tibble: 13 √ó 7\nterm         estimate std.error statistic  p.value conf.low conf.high\n<chr>           <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)  41.6       4.94        8.43  3.79e-16  31.9     51.3    \n2 crim         -0.121     0.0330     -3.68  2.61e- 4  -0.186   -0.0565 \n3 zn            0.0470    0.0139      3.38  7.72e- 4   0.0197   0.0742 \n4 indus         0.0135    0.0621      0.217 8.29e- 1  -0.109    0.136  \n5 chas          2.84      0.870       3.26  1.17e- 3   1.13     4.55   \n6 nox         -18.8       3.85       -4.87  1.50e- 6 -26.3    -11.2    \n7 rm            3.66      0.420       8.70  4.81e-17   2.83     4.48   \n8 age           0.00361   0.0133      0.271 7.87e- 1  -0.0226   0.0298 \n9 dis          -1.49      0.202      -7.39  6.17e-13  -1.89    -1.09   \n10 rad           0.289     0.0669      4.33  1.84e- 5   0.158    0.421  \n11 tax          -0.0127    0.00380    -3.34  9.12e- 4  -0.0202  -0.00521\n12 ptratio      -0.938     0.132      -7.09  4.63e-12  -1.20    -0.678  \n13 lstat        -0.552     0.0507    -10.9   6.39e-25  -0.652   -0.452  \n\nWe can access the individual components of a summary object by name (type ?glance to see what is available). Hence glance(lm.fit)$r.squared gives us the \\(R^2\\).\n\nSelect variables:\nIn these two multiple linear regression models, the t-tests and F-test results suggest that many of the predictors are significant for the response variable. However, some do not achieve statistical significance. Can you see which variables these are?\nWe call the process of determining which predictors are associated with the response as variable selection.\nIf the number of predictors is very small, we could perform the variable selection by trying out a lot of different models, each containing a different subset of the predictors. We can then select the best model out of all of the models we have considered.\nUsing the template below, try figuring out the model which produces the highest adjusted \\(R^2\\). The adjusted \\(R^2\\) has a similar interpretation to \\(R^2\\), only it is an advantage here as it penalises models that include insignificant parameters.\nlm.fit <- lm(medv ~ ., data = Boston)\nglance(lm.fit)$adj.r.squared\n[1] 0.7278399\nWe found that if you remove indus and age, the adjusted \\(R^2\\) becomes slightly larger compared to including all predictors.\nlm.fit <- lm(medv ~ ., data = Boston[,-c(3,7)])\nglance(lm.fit)$adj.r.squared\n[1] 0.7288734"
  },
  {
    "objectID": "weeks/week03/lab.html#step-3-some-potential-problems",
    "href": "weeks/week03/lab.html#step-3-some-potential-problems",
    "title": "üíª Week 03 - Lab Roadmap (90 min)",
    "section": "Step 3: Some potential problems",
    "text": "Step 3: Some potential problems\nMany problems may occur when we fit a linear regression model to a particular data set. These problems will lead to inaccurate estimation. In this step, we will identify and overcome potential problems such as outliers, collinearity and interaction effects.\nWe present a few of the many methods available, but those interested can explore more after class.\n\nHandle interaction terms:\nIn regression, an interaction effect exists when the effect of an independent variable on the response variable changes, depending on the values of one or more independent variables. When you believe there is an interaction effect, it is easy to include interaction terms in a linear model using the lm() function.\n> tidy(lm(medv ~ lstat * age , data = Boston))\n\n# A tibble: 4 √ó 5\nterm         estimate std.error statistic  p.value\n<chr>           <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept) 36.1        1.47      24.6    4.91e-88\n2 lstat       -1.39       0.167     -8.31   8.78e-16\n3 age         -0.000721   0.0199    -0.0363 9.71e- 1\n4 lstat:age    0.00416    0.00185    2.24   2.52e- 2\nThe syntax lstat:age tells R to include an interaction term between lstat and age. The syntax lstat*age simultaneously includes lstat, age, and the interaction term lstat√óage as predictors; it is a shorthand for lstat+age+lstat:age.\nIdentify outliers through residual plots:\nAn outlier is a point for which \\(\\hat{y}_i\\) is far from the value predicted by the model. Outliers can arise for a variety of reasons, such as incorrect recording of observation during data collection. Outliers could be identified through residual plots:\n> par(mfrow = c(2, 2))\n> plot(lm.fit)\nThe plot function automatically produces four diagnostic plots when you pass the output from lm(). Plots on the left column are residual plots, indicating the relationship between residuals and fitted values.\nIn practice, it can be difficult to decide how large a residual needs to be before we consider the point to be an outlier. Instead of plotting the residuals, we can address this problem by plotting the studentized residuals. These are computed by dividing each residual ei by its estimated standard studentized residual error. Observations with studentized residuals greater than 3 in absolute value are possible outliers. Using the plot() function to plot the studentized residuals:\n> plot(predict(lm.fit), rstudent(lm.fit)\nHandle outliers:\nIf we believe an outlier is due to an error in data collection or recording, then one solution is to simply remove the observation. However, care should be taken, as an outlier may instead signal a deficiency with our model, such as a missing predictor.\nDetect multicollinearity using the correlation matrix:\nMulticollinearity refers to the situation in which two or more predictor variables are highly correlated to one another. It can be detected through the correlation matrix:\ncor(Boston)\nIgnoring the last row and the last column in the matrix, which indicate the relationship with response variable medv, an element of this matrix that is large in absolute value indicates a pair of highly correlated variables, and therefore a collinearity problem in the data.\nWe can detect multicollinearity quantitatively using vif() function in the `car‚Äô package:\ninstall.packages(\"car\"))\nlibrary(car)\n> vif(lm.fit)\nInstead of inspecting the correlation matrix, a better way to assess multicollinearity is to compute the variance inflation factor (VIF). As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity.\n\n\n\n\n\n\n\nRead more about VIF\n\n\n\n\n\nCheck out our textbook (James et al. 2021, 99‚Äì103) for a description of the Variance Inflation Factor (VIF).\n\n\n\n\nHandle collinearity:\nWhen faced with the problem of multicollinearity, there are two simple solutions.\n-The first is to drop one of the problematic variables from the regression.\n-The second solution is to combine the collinear variables into a single predictor, where such combination makes theoretical sense."
  },
  {
    "objectID": "weeks/week03/lab.html#step-4-practical-exercises-in-pairs",
    "href": "weeks/week03/lab.html#step-4-practical-exercises-in-pairs",
    "title": "üíª Week 03 - Lab Roadmap (90 min)",
    "section": "Step 4: Practical exercises (in pairs)",
    "text": "Step 4: Practical exercises (in pairs)\nSo far, we have learnt to fit simple and multiple linear regression models in R. In this practical case, we will continue to use the data set Auto studied in the last lab. Make sure that the missing values have been removed from the data.\nEight questions are listed below. You are required to try to answer these questions in pairs using R commands. We will go over the solutions once everyone has finished these questions.\nüéØ Questions\n\nUse the lm() function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the tidy() function to print the results. Comment on the output. For example:\n\nIs there a relationship between the predictor and the response?\nHow strong is the relationship between the predictor and the response?\nIs the relationship between the predictor and the response positive or negative?\nWhat is the predicted mpg associated with a horsepower of 98? What are the associated 95 % confidence intervals?\n\nPlot the response and the predictor. Use the geom_abline() function to display the least squares regression line.\nUse the plot() function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.\nProduce a scatterplot matrix that includes all the variables in the data set.\nCompute the matrix of correlations between the variables using the function cor(). You will need to exclude the name and origin variable, which are qualitative.\nUse the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the tidy() function to print the results. Comment on the output. For instance:\n\nIs there a relationship between the predictors and the response?\nWhich predictors appear to have a statistically significant relationship to the response?\nWhat does the coefficient for the year variable suggest?\n\nUse the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers?\nUse the * and : symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?\n\n\n\n\n\n\n\nTip\n\n\n\nIf you could not finish all eight questions during the lab, take that as a home exercise.\nUse the #week03 channel on Slack if you have any questions."
  },
  {
    "objectID": "weeks/week03/lab.html#solutions-to-exercises",
    "href": "weeks/week03/lab.html#solutions-to-exercises",
    "title": "üíª Week 03 - Lab Roadmap (90 min)",
    "section": "üîë Solutions to exercises",
    "text": "üîë Solutions to exercises\n\nlibrary(tidyverse)\nlibrary(ISLR2)\n\n\nQ1\nUse the lm() function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the tidy() function to print the results. Comment on the output.\nFor example:\n\nIs there a relationship between the predictor and the response?\nHow strong is the relationship between the predictor and the response?\nIs the relationship between the predictor and the response positive or negative?\nWhat is the predicted mpg associated with a horsepower of 98? What are the associated 95 % confidence intervals?\n\n\nAuto <- na.omit(Auto)\nlm.fit <- lm(mpg ~ horsepower, data = Auto)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = mpg ~ horsepower, data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5710  -3.2592  -0.3435   2.7630  16.9240 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 39.935861   0.717499   55.66   <2e-16 ***\nhorsepower  -0.157845   0.006446  -24.49   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.906 on 390 degrees of freedom\nMultiple R-squared:  0.6059,    Adjusted R-squared:  0.6049 \nF-statistic: 599.7 on 1 and 390 DF,  p-value: < 2.2e-16\n\n\nRegarding to the p-values of t-test and F-test, there is a strong relationship between the predictor horsepower and the response mpg. From the sign of coefficients, the relationship between the predictor and the response is negative. Using the function predict() to predict the value of response and the confidence interval, we get:\n\npredict(lm.fit, data.frame(horsepower = 98), interval = \"confidence\")\n\n       fit      lwr      upr\n1 24.46708 23.97308 24.96108\n\n\nTherefore, the predicted mpg associated with a horsepower of 98 is 24.47, and the associated 95 % confidence interval is [23.97308, 24.96108].\n\n\nQ2\nPot the response and the predictor. Use the geom_abline() function to display the least squares regression line.\nIn base R (without using any tidverse or any other package):\n\nplot(Auto$horsepower, Auto$mpg, xlim = c(0, 250))\nabline (lm.fit, lwd = 3, col = \"red\")\n\n\n\n\nUsing ggplot (from tidyverse):\n\nggplot(data = Auto, aes(x = horsepower, y = mpg)) +\n  geom_point(alpha=0.6, size=2.5) +\n  geom_abline(intercept = lm.fit$coefficients[1], \n              slope = lm.fit$coefficients[2],\n              color=\"red\", size=1.2) +\n  theme_bw()\n\n\n\n\n\n\nQ3\nUse the plot() function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.*\n\npar(mfrow = c(2, 2))\nplot(lm.fit)\n\n\n\n\nBy observing four diagnostic plots, we could find non-linear pattern in residual plots. The quadratic trend of the residuals could be a problem. Then we plot studentized residuals to identify outliers:\n\nplot(predict(lm.fit), rstudent(lm.fit))\n\n\n\n\nThere are possible outliers as seen in the plot of studentized residuals because there are data with a value greater than 3.\n\n\nQ4\nProduce a scatterplot matrix that includes all the variables in the data set.\n\npairs(Auto)\n\n\n\n\n\n\nQ5\nCompute the matrix of correlations between the variables using the function cor(). You will need to exclude the name and origin variable, which is qualitative.\n\ncor(subset(Auto, select = -name))\n\n                    mpg  cylinders displacement horsepower     weight\nmpg           1.0000000 -0.7776175   -0.8051269 -0.7784268 -0.8322442\ncylinders    -0.7776175  1.0000000    0.9508233  0.8429834  0.8975273\ndisplacement -0.8051269  0.9508233    1.0000000  0.8972570  0.9329944\nhorsepower   -0.7784268  0.8429834    0.8972570  1.0000000  0.8645377\nweight       -0.8322442  0.8975273    0.9329944  0.8645377  1.0000000\nacceleration  0.4233285 -0.5046834   -0.5438005 -0.6891955 -0.4168392\nyear          0.5805410 -0.3456474   -0.3698552 -0.4163615 -0.3091199\norigin        0.5652088 -0.5689316   -0.6145351 -0.4551715 -0.5850054\n             acceleration       year     origin\nmpg             0.4233285  0.5805410  0.5652088\ncylinders      -0.5046834 -0.3456474 -0.5689316\ndisplacement   -0.5438005 -0.3698552 -0.6145351\nhorsepower     -0.6891955 -0.4163615 -0.4551715\nweight         -0.4168392 -0.3091199 -0.5850054\nacceleration    1.0000000  0.2903161  0.2127458\nyear            0.2903161  1.0000000  0.1815277\norigin          0.2127458  0.1815277  1.0000000\n\n\nA nicer way to plot correlations is through the package ggcorrplot:\n\nlibrary(ggcorrplot)\n\nggcorrplot(cor(Auto %>% select(-c(name))))\n\n\n\n\n\n\nQ6\nUse the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the tidy() function to print the results. Comment on the output.\nFor instance:\n\nIs there a relationship between the predictors and the response?\nWhich predictors appear to have a statistically significant relationship to the response?\nWhat does the coefficient for the year variable suggest?\n\n\nlm.fit1 <- lm(mpg ~ . -name, data = Auto)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = mpg ~ . - name, data = Auto)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5903 -2.1565 -0.1169  1.8690 13.0604 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -17.218435   4.644294  -3.707  0.00024 ***\ncylinders     -0.493376   0.323282  -1.526  0.12780    \ndisplacement   0.019896   0.007515   2.647  0.00844 ** \nhorsepower    -0.016951   0.013787  -1.230  0.21963    \nweight        -0.006474   0.000652  -9.929  < 2e-16 ***\nacceleration   0.080576   0.098845   0.815  0.41548    \nyear           0.750773   0.050973  14.729  < 2e-16 ***\norigin         1.426141   0.278136   5.127 4.67e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.328 on 384 degrees of freedom\nMultiple R-squared:  0.8215,    Adjusted R-squared:  0.8182 \nF-statistic: 252.4 on 7 and 384 DF,  p-value: < 2.2e-16\n\n\n\nYes, there is a relationship between the predictors and the response by testing the null hypothesis of whether all the regression coefficients are zero. The F-statistic is far from 1 (with a small p-value), indicating evidence against the null hypothesis.\nObserving the p-values associated with each predictor‚Äôs t-statistic, we see that displacement, weight, year, and origin have a statistically significant relationship, while cylinders, horsepower and acceleration do not.\nThe regression coefficient for year is 0.75. This suggests that, considering all other predictors fixed, mpg increases by additional 0.75 unit. In other words, cars become more fuel efficient every year by almost 1 mpg/year.\n\n\n\nQ7\nUse the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers?\n\npar(mfrow = c(2, 2))\nplot(lm.fit1)\n\n\n\n\nFrom the leverage plot, we see that point 14 appears to have a high leverage, although not a high magnitude residual. Besides, the quadratic trend of the residuals could be a problem. Maybe linear regression is not the best fit for this prediction.\nWe plot studentized residuals to identify outliers:\n\nplot(predict(lm.fit1), rstudent(lm.fit1))\n\n\n\n\nThere are possible outliers as seen in the plot of studentized residuals because there are data with a value greater than 3.\n\n\nQ8\nUse the * and : symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?\n\nlm.fit2 <-  lm(mpg ~ cylinders * displacement + displacement * weight, data = Auto)\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = mpg ~ cylinders * displacement + displacement * \n    weight, data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.2934  -2.5184  -0.3476   1.8399  17.7723 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             5.262e+01  2.237e+00  23.519  < 2e-16 ***\ncylinders               7.606e-01  7.669e-01   0.992    0.322    \ndisplacement           -7.351e-02  1.669e-02  -4.403 1.38e-05 ***\nweight                 -9.888e-03  1.329e-03  -7.438 6.69e-13 ***\ncylinders:displacement -2.986e-03  3.426e-03  -0.872    0.384    \ndisplacement:weight     2.128e-05  5.002e-06   4.254 2.64e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.103 on 386 degrees of freedom\nMultiple R-squared:  0.7272,    Adjusted R-squared:  0.7237 \nF-statistic: 205.8 on 5 and 386 DF,  p-value: < 2.2e-16\n\n\nInteraction between displacement and weight is statistically signifcant, while the interaction between cylinders and displacement is not."
  },
  {
    "objectID": "weeks/week03/lab_solutions.html",
    "href": "weeks/week03/lab_solutions.html",
    "title": "üíª Week 03 - Lab Roadmap (90 min)",
    "section": "",
    "text": "Use the lm() function to perform a simple linear regressionÔºåand use the summary() function to print the results:\n> library(ISLR2)\n> Auto <- na.omit(Auto)\n> lm.fit <- lm(mpg ~ horsepower, data = Auto)\n> summary(lm.fit)\n\nCall:\nlm(formula = mpg ~ horsepower, data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5710  -3.2592  -0.3435   2.7630  16.9240 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 39.935861   0.717499   55.66   <2e-16 ***\nhorsepower  -0.157845   0.006446  -24.49   <2e-16 ***\n---\n Signif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\nResidual standard error: 4.906 on 390 degrees of freedom\nMultiple R-squared:  0.6059,    Adjusted R-squared:  0.6049 \nF-statistic: 599.7 on 1 and 390 DF,  p-value: < 2.2e-16\nRegarding to the p-values of t-test and F-test, there is a strong relationship between the predictor horsepower and the reponse mpg. From the sign of coefficients, the relationship between the predicator and the response is negative. Using the function predict() to predict the value of reponse and the confidence interval, we get:\n> predict(lm.fit, data.frame(horsepower = 98), interval = \"confidence\")\n       fit      lwr      upr\n1 24.46708 23.97308 24.96108\nTherefore, the predicted mpg associated with a horsepower of 98 is 24.47, and the associated 95 % confidence interval is [23.97308, 24.96108].\nUse the function plot() and abline():\n> attach(Auto)\n> plot(mpg, horsepower, ylim = c(0, 250))\n> abline (lm.fit, lwd = 3, col = \"red\")\n> dev.off()\nUsing plot() function to produce diagnostic plots:\n> par(mfrow = c(2, 2))\n> plot (lm.fit)\n> dev.off()\nBy observing four diagnostic plots, we could find non-linear patttern in residual plots. The quadratic trend of the residuals could be a problem. Then we plot studentized residuals to identify outliers:\n> plot(predict(lm.fit), rstudent(lm.fit))\n> dev.off()\nThere are possible outliers as seen in the plot of studentized residuals because there are data with a value greater than 3.\nUse the pairs() function to produce a scatterplot matrix:\n> pairs(Auto)\n> dev.off()\nUse the cor() function to compute the matrix of correlations between the variables while excluding the name variable:\n> cor(subset(Auto, select = -name))\nUse the lm() function to perform a multiple linear regressionÔºåand use the summary() function to print the results:\n> lm.fit1 <- lm(mpg ~ . -name, data = Auto)\n> summary (lm.fit1)\n\nCall:\nlm(formula = mpg ~ . - name, data = Auto)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5903 -2.1565 -0.1169  1.8690 13.0604 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -17.218435   4.644294  -3.707  0.00024 ***\ncylinders     -0.493376   0.323282  -1.526  0.12780    \ndisplacement   0.019896   0.007515   2.647  0.00844 ** \nhorsepower    -0.016951   0.013787  -1.230  0.21963    \nweight        -0.006474   0.000652  -9.929  < 2e-16 ***\nacceleration   0.080576   0.098845   0.815  0.41548    \nyear           0.750773   0.050973  14.729  < 2e-16 ***\norigin         1.426141   0.278136   5.127 4.67e-07 ***\n---\nSignif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\nResidual standard error: 3.328 on 384 degrees of freedom\nMultiple R-squared:  0.8215,    Adjusted R-squared:  0.8182 \nF-statistic: 252.4 on 7 and 384 DF,  p-value: < 2.2e-16\n\nYes, there is a relationship between the predictors and the response by testing the null hypothesis of whether all the regression coefficients are zero. The F-statistic is far from 1 (with a small p-value), indicating evidence against the null hypothesis.\nObverving the p-values associated with each predictor‚Äôs t-statistic, we see that displacement, weight, year, and origin have a statistically significant relationship, while cylinders, horsepower and acceleration do not.\nThe regression coefficient for year is 0.75. This suggests that, considering all other predictors fixed, mpg increases by additional 0.75 unit. In other words, cars become more fuel efficient every year by almost 1 mpg/year.\n\nUse the plot() function to produce diagnostic plots:\n> par(mfrow = c(2, 2))\n> plot (lm.fit1)\n> dev.off()\nFrom the leverage plot, we see that point 14 appears to have a high leverage, although not a high magnitude residual. Besides, the quadratic trend of the residuals could be a problem. Maybe linear regression is not the best fit for this prediction.\nWe plot studentized residuals to identify outliers:\n> plot(predict(lm.fit1), rstudent(lm.fit1))\n> dev.off()\nThere are possible outliers as seen in the plot of studentized residuals because there are data with a value greater than 3.\nUse the * and : symbols to fit linear regression models with interaction effects:\n> lm.fit2 <-  lm(mpg ~ cylinders * displacement + displacement * weight, data = Auto)\n> summary(lm.fit2)\nInteraction between displacement and weight is statistically signifcant, while the interaction between cylinders and displacement is not."
  },
  {
    "objectID": "weeks/week03/lecture.html",
    "href": "weeks/week03/lecture.html",
    "title": "üë®‚Äçüè´ Week 03 - Slides",
    "section": "",
    "text": "Tip\n\n\n\nFeel free to browse the slides before the lecture, but it is probably safer to wait until the time of the lecture to download/save them, as we might make small changes to slides before then."
  },
  {
    "objectID": "weeks/week03/lecture.html#part-i---classifiers-logistic-regression-45-50-min",
    "href": "weeks/week03/lecture.html#part-i---classifiers-logistic-regression-45-50-min",
    "title": "üë®‚Äçüè´ Week 03 - Slides",
    "section": "Part I - Classifiers (Logistic Regression) (45-50 min)",
    "text": "Part I - Classifiers (Logistic Regression) (45-50 min)\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides. For a PDF copy, check Moodle."
  },
  {
    "objectID": "weeks/week03/lecture.html#coffee-break-10-min",
    "href": "weeks/week03/lecture.html#coffee-break-10-min",
    "title": "üë®‚Äçüè´ Week 03 - Slides",
    "section": "‚òï Coffee Break (10 min)",
    "text": "‚òï Coffee Break (10 min)\nUse this time to chat, stretch, drink some coffee or just relax for a bit by yourself."
  },
  {
    "objectID": "weeks/week03/lecture.html#part-ii---classifiers-naive-bayes-45-50-min",
    "href": "weeks/week03/lecture.html#part-ii---classifiers-naive-bayes-45-50-min",
    "title": "üë®‚Äçüè´ Week 03 - Slides",
    "section": "Part II - Classifiers (Naive Bayes) (45-50 min)",
    "text": "Part II - Classifiers (Naive Bayes) (45-50 min)\nEither click on the slide area below or click here to view it in fullscreen. Use your keypad to navigate the slides. For a PDF copy, check Moodle."
  },
  {
    "objectID": "weeks/week04/checklist.html",
    "href": "weeks/week04/checklist.html",
    "title": "‚úÖ Week 04 - Checklist",
    "section": "",
    "text": "Here is a suggestion of how to program your week in relation to this course:\nExtra:"
  },
  {
    "objectID": "weeks/week04/checklist.html#if-your-lab-is-on-monday",
    "href": "weeks/week04/checklist.html#if-your-lab-is-on-monday",
    "title": "‚úÖ Week 04 - Checklist",
    "section": "If your lab is on Monday:",
    "text": "If your lab is on Monday:\n\nüì• Download: Before or once you arrive at the classroom, download the DS202_2022MT_w04_lab_rmark.Rmd file that contains the lab roadmap or browse the webpage version here.\nüßë‚ÄçüíªParticipate: Actively engage with the material in the lab. Ask your class teacher for help if anything is unclear. Work with others whenever possible and take notes of theoretical concepts or practical coding skill you might want to revisit later in the week.\n\nIt is ok if you feel a bit lost in the lab. After all, you haven‚Äôt had much time to read the material to reinforce the concepts in your mind. That is why participation and note-taking is so important.\n\nüìô Read: Find some time before Thursday to read (James et al. 2021, chap. 4) and reinforce your theoretical knowledge of Classifiers (Logistic Regression & other methods, including Na√Øve Bayes); the textbook is available online for free.\n\nAs you go through the text, try to connect what you read to the things you heard about in the lecture or the examples you explored in the lab.\n\n‚úçÔ∏è Solve: Also before Thursday, take a look at the problem sets of the first formative assessment. Either try to complete it before Thursday or at least have a look to see what it contains. Take note of whatever questions you might have about R or linear regression.\n\nThe Formative Problem Set 01 is available on Moodle and you can submit your solutions until Tuesday 25 October 2022, 23:50 UK time.\n\n‚úãTeacher Support: Stuart will host a drop-in session on Thursday 20 October 2022 2pm-4pm (provisionally at the FAW 9.04 room). Bring your notes and questions or just simply use this shared space to work on your problem set.\nüßë‚Äçüè´ Attend the lecture: This week, the lecture will look more like a workshop. We won‚Äôt explore new algorithms but we will work on regression/classification metrics and explore the concepts of train/test splits and cross-validation. You will need to use those in your first summative problem set."
  },
  {
    "objectID": "weeks/week04/checklist.html#if-your-lab-is-on-friday",
    "href": "weeks/week04/checklist.html#if-your-lab-is-on-friday",
    "title": "‚úÖ Week 04 - Checklist",
    "section": "If your lab is on Friday:",
    "text": "If your lab is on Friday:\n\nüìô Read: Find some time before Thursday to read (James et al.¬†2021, chap.¬†4) and reinforce your theoretical knowledge of Classifiers (Logistic Regression & other methods, including Na√Øve Bayes); the textbook is available online for free. As you go through the text, try to connect what you read to the things you heard about in the lecture.\n\nYou can ask questions on Slack (#week04) or take them with you to the drop-in session on Thursday.\n\n‚úçÔ∏è Solve: Also before Thursday, take a look at the problem sets of the first formative assessment. Either try to complete it before Thursday or at least have a look to see what it contains. Take note of whatever questions you might have about R or linear regression.\n\nThe Formative Problem Set 01 is available on Moodle and you can submit your solutions until Tuesday 25 October 2022, 23:50 UK time.\n\n‚úãTeacher Support: Stuart will host a drop-in session on Thursday 20 October 2022 2pm-4pm (provisionally at the FAW 9.04 room). Bring your notes and questions or just simply use this shared space to work on your problem set.\nüì•Download: Before or once you arrive at the classroom, download the DS202_2022MT_w04_lab_rmark.Rmd file that contains the lab roadmap or browse the webpage version here.\nüßë‚ÄçüíªParticipate: Actively engage with the material in the lab. Ask your class teacher for help if anything is unclear. Keep your notes and the textbook handy so you can consult them during the session.\nüßë‚Äçüè´Attend the lecture: This week, the lecture will look more like a workshop. We won‚Äôt explore new algorithms but we will work on regression/classification metrics and explore the concepts of train/test splits and cross-validation. You will need to use those in your first summative problem set."
  },
  {
    "objectID": "weeks/week04/lab.html",
    "href": "weeks/week04/lab.html",
    "title": "üíª Week 04 - Lab Roadmap (90 min)",
    "section": "",
    "text": "This week, we will build diverse classification models to deal with situation when the response variable is qualitative in R. We predict these qualitative variables through some widely-used classifiers including Logistic Regression (one of many Generalized Linear Models) and Na√Øve Bayes in this lab session. We will also apply these classification models into practical practices and compare their performance on different data sets.\nWe will follow the instructions below step by step together while answering whatever questions you might encounter along the way.\nR packages you will need:"
  },
  {
    "objectID": "weeks/week04/lab.html#step-1-explore-the-dataset",
    "href": "weeks/week04/lab.html#step-1-explore-the-dataset",
    "title": "üíª Week 04 - Lab Roadmap (90 min)",
    "section": "Step 1: Explore the dataset",
    "text": "Step 1: Explore the dataset\n\nStep 1.1 Load the Data\nLoad the ISLR2 package, which contains a large collection of data sets and functions. We will begin by examining some numerical and graphical summaries of the Smarket data, which is part of the ISLR2 library.\n\nlibrary(\"ISLR2\")\nhead(Smarket)\n\n  Year   Lag1   Lag2   Lag3   Lag4   Lag5 Volume  Today Direction\n1 2001  0.381 -0.192 -2.624 -1.055  5.010 1.1913  0.959        Up\n2 2001  0.959  0.381 -0.192 -2.624 -1.055 1.2965  1.032        Up\n3 2001  1.032  0.959  0.381 -0.192 -2.624 1.4112 -0.623      Down\n4 2001 -0.623  1.032  0.959  0.381 -0.192 1.2760  0.614        Up\n5 2001  0.614 -0.623  1.032  0.959  0.381 1.2057  0.213        Up\n6 2001  0.213  0.614 -0.623  1.032  0.959 1.3491  1.392        Up\n\n\nThis data set consists of percentage returns for the S&P 500 stock index over 1250 days, from the beginning of 2001 until the end of 2005. We use the command names() to obtain the variable names of this data set:\n\nnames(Smarket)\n\n[1] \"Year\"      \"Lag1\"      \"Lag2\"      \"Lag3\"      \"Lag4\"      \"Lag5\"     \n[7] \"Volume\"    \"Today\"     \"Direction\"\n\n\nHow many rows and columns do we have in this dataset?\n\ndim(Smarket)\n\n[1] 1250    9\n\n\nLet‚Äôs add another column day to index the number of days in this dataset:\n\nSmarket$day <- 1:nrow(Smarket)\nhead(Smarket)\n\n  Year   Lag1   Lag2   Lag3   Lag4   Lag5 Volume  Today Direction day\n1 2001  0.381 -0.192 -2.624 -1.055  5.010 1.1913  0.959        Up   1\n2 2001  0.959  0.381 -0.192 -2.624 -1.055 1.2965  1.032        Up   2\n3 2001  1.032  0.959  0.381 -0.192 -2.624 1.4112 -0.623      Down   3\n4 2001 -0.623  1.032  0.959  0.381 -0.192 1.2760  0.614        Up   4\n5 2001  0.614 -0.623  1.032  0.959  0.381 1.2057  0.213        Up   5\n6 2001  0.213  0.614 -0.623  1.032  0.959 1.3491  1.392        Up   6\n\n\nFor each date, we have recorded the percentage returns for each of the five previous trading days, Lag1 through Lag5. We have also recorded Volume (the number of shares traded on the previous day, in billions), Today (the percentage return on the date in question) and Direction (whether the market was Up or Down on this date). Our goal is to predict Direction (a qualitative response) using the other features.\nLet‚Äôs look at a generic summary of this data and how each pair of variables are related:\n\nsummary(Smarket)\n\n      Year           Lag1                Lag2                Lag3          \n Min.   :2001   Min.   :-4.922000   Min.   :-4.922000   Min.   :-4.922000  \n 1st Qu.:2002   1st Qu.:-0.639500   1st Qu.:-0.639500   1st Qu.:-0.640000  \n Median :2003   Median : 0.039000   Median : 0.039000   Median : 0.038500  \n Mean   :2003   Mean   : 0.003834   Mean   : 0.003919   Mean   : 0.001716  \n 3rd Qu.:2004   3rd Qu.: 0.596750   3rd Qu.: 0.596750   3rd Qu.: 0.596750  \n Max.   :2005   Max.   : 5.733000   Max.   : 5.733000   Max.   : 5.733000  \n      Lag4                Lag5              Volume           Today          \n Min.   :-4.922000   Min.   :-4.92200   Min.   :0.3561   Min.   :-4.922000  \n 1st Qu.:-0.640000   1st Qu.:-0.64000   1st Qu.:1.2574   1st Qu.:-0.639500  \n Median : 0.038500   Median : 0.03850   Median :1.4229   Median : 0.038500  \n Mean   : 0.001636   Mean   : 0.00561   Mean   :1.4783   Mean   : 0.003138  \n 3rd Qu.: 0.596750   3rd Qu.: 0.59700   3rd Qu.:1.6417   3rd Qu.: 0.596750  \n Max.   : 5.733000   Max.   : 5.73300   Max.   :3.1525   Max.   : 5.733000  \n Direction       day        \n Down:602   Min.   :   1.0  \n Up  :648   1st Qu.: 313.2  \n            Median : 625.5  \n            Mean   : 625.5  \n            3rd Qu.: 937.8  \n            Max.   :1250.0  \n\n\n\npairs(Smarket)\n\n\n\n\n\n\nStep 1.2 Initial exploratory data analysis\nProduce a matrix that contains all of the pairwise correlations among the predictors in a data set:\n\ncor(Smarket[, -9])\n\n             Year         Lag1         Lag2         Lag3         Lag4\nYear   1.00000000  0.029699649  0.030596422  0.033194581  0.035688718\nLag1   0.02969965  1.000000000 -0.026294328 -0.010803402 -0.002985911\nLag2   0.03059642 -0.026294328  1.000000000 -0.025896670 -0.010853533\nLag3   0.03319458 -0.010803402 -0.025896670  1.000000000 -0.024051036\nLag4   0.03568872 -0.002985911 -0.010853533 -0.024051036  1.000000000\nLag5   0.02978799 -0.005674606 -0.003557949 -0.018808338 -0.027083641\nVolume 0.53900647  0.040909908 -0.043383215 -0.041823686 -0.048414246\nToday  0.03009523 -0.026155045 -0.010250033 -0.002447647 -0.006899527\nday    0.97977289  0.035414677  0.036022487  0.038988767  0.041437137\n               Lag5      Volume        Today        day\nYear    0.029787995  0.53900647  0.030095229 0.97977289\nLag1   -0.005674606  0.04090991 -0.026155045 0.03541468\nLag2   -0.003557949 -0.04338321 -0.010250033 0.03602249\nLag3   -0.018808338 -0.04182369 -0.002447647 0.03898877\nLag4   -0.027083641 -0.04841425 -0.006899527 0.04143714\nLag5    1.000000000 -0.02200231 -0.034860083 0.03502515\nVolume -0.022002315  1.00000000  0.014591823 0.54634793\nToday  -0.034860083  0.01459182  1.000000000 0.03527333\nday     0.035025152  0.54634793  0.035273325 1.00000000\n\n\nThe function cor() can only take quantitative variables. Because the Direction variable is qualitative, therefore we exclude it when calculating the correlation matrix.\nAs one would expect, the correlations between the lag variables and today‚Äôs returns are close to zero. In other words, there appears to be little correlation between today‚Äôs returns and previous days‚Äô returns. The only substantial correlation is between Year and Volume. We could explore how Volume changed chronologically.\n\nlibrary(tidyverse)\n\nggplot(data = Smarket, aes(x = day, y = Volume)) +\n    geom_col(fill=\"#665797\") +\n\n    ggtitle(\"Volume of shares traded over the period of this dataset\") +\n\n    xlab(\"Day\") + ylab(\"Volume (in billion US dollars)\") +\n\n    theme_minimal() +\n    theme(aspect.ratio = 2/5)\n\n\n\n\nBy plotting the data, which is ordered chronologically, we see that Volume is increasing over time. In other words, the average number of shares traded daily increased from 2001 to 2005."
  },
  {
    "objectID": "weeks/week04/lab.html#step-2-logistic-regression",
    "href": "weeks/week04/lab.html#step-2-logistic-regression",
    "title": "üíª Week 04 - Lab Roadmap (90 min)",
    "section": "Step 2: Logistic Regression",
    "text": "Step 2: Logistic Regression\nWe will still use the Smarket data set to fit a Logistic Regression Model to predict Direction.\n\nStep 2.1 Separate some data just for training\nBuild a training and a testing dataset. In practice we will be interested in our model‚Äôs performance not on the data that we used to fit the model, but rather on days in the future for which the market‚Äôs movements are unknown. Therefore, we will first create a training data set corresponding to the observations from 2001 through 2004. We will then create a testing data set of observations from 2005.:\n\ntrain <- (Smarket$Year < 2005)\nSmarket.before.2005 <- Smarket [ train , ]\n\nSmarket.2005 <- Smarket[ Smarket$Year==2005, ]\nDirection.2005 <- Smarket$Direction [ Smarket$Year==2005 ]\n\nSmarket$Year < 2005 returns True for the values satisfying <2005 (smaller than 2005) condition in Year column in Smarket dataset. The same logic can be applied to >, <= , >=, ==(equal) or != (not equal).\nTo access corresponding rows, we can create a vector (train) and put it in open brackets to make it more readable and reusable, or we can explicitly specify in the open brackets.\nUsing Smarket[ Smarket$Year==2005,] gives all the rows satisfying this condition with all columns. The same logic can be applied to < , >, <= , >= or != (not equal).\nSmarket$Direction [ Smarket$Year==2005 ] tells the direction values corresponding to the rows equal to 2005 in the year column are requested.\nThus, we can construct a training data set named Smarket.before.2005, and a testing data set named Smarket.2005.\nHow many observations we have in the training set (< 2005)?\n\nnrow(Smarket.before.2005)\n\n[1] 998\n\n\nWhat about the test set (2005)?\n\nnrow(Smarket.2005)\n\n[1] 252\n\n\n\n\nStep 2.2 Fit a logistic regression model\nFit a Logistic Regression Model in order to predict Direction using Lag1 through Lag5 and Volume based on training data set:\n\nglm.fits <- \n    glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, \n        data = Smarket.before.2005, family = binomial)\n\nThe generalized linear model syntax of the glm() function is similar to that of lm(), except that we must pass in the argument family = binomial in order to tell R to run a logistic regression rather than some other type of generalized linear model.\nWe now fit a logistic regression model using only the subset of the observations that correspond to dates before 2005, using the subset argument.\n\n\nStep 2.3 Inspect the model\nHave a look at p-values of this Logistic Regression Model:\n\nsummary(glm.fits)\n\n\nCall:\nglm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + \n    Volume, family = binomial, data = Smarket.before.2005)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.302  -1.190   1.079   1.160   1.350  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)\n(Intercept)  0.191213   0.333690   0.573    0.567\nLag1        -0.054178   0.051785  -1.046    0.295\nLag2        -0.045805   0.051797  -0.884    0.377\nLag3         0.007200   0.051644   0.139    0.889\nLag4         0.006441   0.051706   0.125    0.901\nLag5        -0.004223   0.051138  -0.083    0.934\nVolume      -0.116257   0.239618  -0.485    0.628\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1383.3  on 997  degrees of freedom\nResidual deviance: 1381.1  on 991  degrees of freedom\nAIC: 1395.1\n\nNumber of Fisher Scoring iterations: 3\n\n\nOr, alternatively, you can gather the same information using the broom package:\n\nlibrary(broom)\n\ntidy(glm.fits)\n\n# A tibble: 7 √ó 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)  0.191      0.334     0.573    0.567\n2 Lag1        -0.0542     0.0518   -1.05     0.295\n3 Lag2        -0.0458     0.0518   -0.884    0.377\n4 Lag3         0.00720    0.0516    0.139    0.889\n5 Lag4         0.00644    0.0517    0.125    0.901\n6 Lag5        -0.00422    0.0511   -0.0826   0.934\n7 Volume      -0.116      0.240    -0.485    0.628\n\n\nThe smallest p-value here is associated with Lag1. The negative coefficient for this predictor suggests that if the market had a positive return yesterday, then it is less likely to go up today. However, at a value of 0.295, the p-value is still relatively large, and so there is no clear evidence of a real association between Lag1 and Direction. So do other predictors.\n\n\nStep 2.4 Make predictions about the future (2005)\nObtain predicted probabilities of the stock market going up for each of the days in our testing data set, that is, for the days in 2005:\n\nglm.probs <- predict(glm.fits, Smarket.2005, type = \"response\") \nglm.probs[1:10]   \n\n      999      1000      1001      1002      1003      1004      1005      1006 \n0.5282195 0.5156688 0.5226521 0.5138543 0.4983345 0.5010912 0.5027703 0.5095680 \n     1007      1008 \n0.5040112 0.5106408 \n\n\nThe predict() function can be used to predict the probability that the market will go up, given values of the predictors. The type = \"response\" option tells R to output probabilities of the form P(Y = 1|X), as opposed to other information such as the logit. If no data set is supplied to the predict() function, then the probabilities are computed for the training data set that was used to fit the logistic regression model. Here we have printed only the first ten probabilities.\n\ncontrasts(Smarket$Direction)\n\n     Up\nDown  0\nUp    1\n\n\nWe know that these values correspond to the probability of the market going up, rather than down, because the contrasts() function indicates that R has created a dummy variable with a 1 for Up.\nIn order to make a prediction as to whether the market will go up or down on a particular day, we must convert these predicted probabilities into class labels, Up or Down. The following commands create a vector of class predictions based on whether the predicted probability of a market increase is greater than or less than 0.5.\n\nglm.pred <- rep(\"Down\", 252)\nglm.pred[glm.probs > .5] <- \"Up\"\n\nThe first command creates a vector of 252 Down elements. The second line transforms to Up all of the elements for which the predicted probability of a market increase exceeds 0.5.\n\n\nStep 2.5 Create a confusion matrix\nConstruct confusion matrix in order to determine how many observations in testing data set were correctly or incorrectly classified.\n\ntable(glm.pred, Direction.2005)    \n\n        Direction.2005\nglm.pred Down Up\n    Down   77 97\n    Up     34 44\n\n\nGiven the predictions, the table() function can be used to produce a confusion matrix in order to determine how many observations were correctly or incorrectly classified.\n\n\nStep 2.6 What is the error in the test set?\nCalculate the test set error rate:\n\nmean(glm.pred == Direction.2005)\n\n[1] 0.4801587\n\n\n\nmean(glm.pred != Direction.2005)\n\n[1] 0.5198413\n\n\nThe != notation means not equal to, and so the last command computes the test set error rate. The results are rather disappointing: the test error rate is 52%, which is worse than random guessing! Of course this result is not all that surprising, given that one would not generally expect to be able to use previous days‚Äô returns to predict future market performance.\n\n\nStep 2.7 Can we find a better combination of features?\nRemove the variables that appear not to be helpful in predicting Direction and fit a new Logistic Regression model. We recall that the logistic regression model had very underwhelming p-values associated with all of the predictors, and that the smallest p-value, though not very small, corresponded to Lag1. Perhaps by removing the variables that appear not to be helpful in predicting Direction, we can obtain a more effective model. After all, using predictors that have no relationship with the response tends to cause a deterioration in the test error rate (since such predictors cause an increase in variance without a corresponding decrease in bias), and so removing such predictors may in turn yield an improvement.\n\nglm.fits <- glm(Direction ~ Lag1 + Lag2, \n                data = Smarket.before.2005, family = binomial)\nsummary(glm.fits)\n\n\nCall:\nglm(formula = Direction ~ Lag1 + Lag2, family = binomial, data = Smarket.before.2005)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.345  -1.188   1.074   1.164   1.326  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept)  0.03222    0.06338   0.508    0.611\nLag1        -0.05562    0.05171  -1.076    0.282\nLag2        -0.04449    0.05166  -0.861    0.389\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1383.3  on 997  degrees of freedom\nResidual deviance: 1381.4  on 995  degrees of freedom\nAIC: 1387.4\n\nNumber of Fisher Scoring iterations: 3\n\n\n\nglm.probs <- predict(glm.fits, Smarket.2005, type = \"response\")\nglm.pred <- rep(\"Down\", 252)\nglm.pred[glm.probs > .5] <- \"Up\"\ntable(glm.pred , Direction.2005)\n\n        Direction.2005\nglm.pred Down  Up\n    Down   35  35\n    Up     76 106\n\n\n\nmean(glm.pred == Direction.2005)\n\n[1] 0.5595238\n\n\nCheck proportion:\n\n106 / (106 + 76)\n\n[1] 0.5824176\n\n\nAbove we have refit the logistic regression using just Lag1 and Lag2, which seemed to have the highest predictive power in the original logistic regression model.\nNow the results appear to be a little better: 56% of the daily movements have been correctly predicted. It is worth noting that in this case, a much simpler strategy of predicting that the market will increase every day will also be correct 56% of the time! Hence, in terms of overall error rate, the logistic regression method is no better than the naive approach However, the confusion matrix shows that on days when logistic regression predicts an increase in the market, it has a 58% accuracy rate. This suggests a possible trading strategy of buying on days when the model predicts an increasing market, and avoiding trades on days when a decrease is predicted. Of course one would need to investigate more carefully whether this small improvement was real or just due to random chance.\nSuppose that we want to predict the returns associated with particular values of Lag1 and Lag2. In particular, we want to predict Direction on a day when Lag1 and Lag2 equal 1.2 and 1.1, respectively, and on a day when they equal 1.5 and ‚àí0.8. We do this using the predict() function.\n\npredict(glm.fits, \n        newdata = data.frame (Lag1 = c(1.2, 1.5), Lag2 = c(1.1, -0.8)), \n        type = \"response\")\n\n        1         2 \n0.4791462 0.4960939 \n\n\n\n\nStep 2.8 Logistic regression siblings\nIn this lab we used the glm() function with family = binomial to perform logistic regression. Other choices for the family argument can be used to fit other types of GLMs. For instance, family = Gamma fits a gamma regression model. You can alwarys use the following command to explore more about family argument and possible choices.\n?glm()"
  },
  {
    "objectID": "weeks/week04/lab.html#step-3-naive-bayes",
    "href": "weeks/week04/lab.html#step-3-naive-bayes",
    "title": "üíª Week 04 - Lab Roadmap (90 min)",
    "section": "Step 3: Naive Bayes",
    "text": "Step 3: Naive Bayes\nThe Smarket data set will still be utilised to fit a naive Bayes classifier to predict Direction.\n\nStep 3.1 Let‚Äôs fit a Naive Bayes model\nFit a naive Bayes model to predict Direction using Lag1 and Lag2:\n\nlibrary(e1071)\nnb.fit <- naiveBayes (Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\nnb.fit\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n    Down       Up \n0.491984 0.508016 \n\nConditional probabilities:\n      Lag1\nY             [,1]     [,2]\n  Down  0.04279022 1.227446\n  Up   -0.03954635 1.231668\n\n      Lag2\nY             [,1]     [,2]\n  Down  0.03389409 1.239191\n  Up   -0.03132544 1.220765\n\n\nNaive Bayes is implemented in R using the naiveBayes() function, which is part of the e1071 library. By default, this implementation of the naive Bayes classifier models each quantitative feature using a Gaussian distribution. However, a kernel density method can also be used to estimate the distributions.\nThe output contains the estimated mean and standard deviation for each variable in each class. For example, the mean for Lag1 is 0.0428 for Direction=Down, and the standard deviation is 1.23. We can easily verify this:\n\nmean(Smarket$Lag1[train][Smarket$Direction[train] == \"Down\"])\n\n[1] 0.04279022\n\n\n\nsd(Smarket$Lag1[train][Smarket$Direction[train] == \"Down\"])\n\n[1] 1.227446\n\n\n\n\nStep 3.2 Predict the test set\nPredict Direction in testing data set:\n\nnb.class <- predict(nb.fit, Smarket.2005)\ntable(nb.class, Direction.2005)\n\n        Direction.2005\nnb.class Down  Up\n    Down   28  20\n    Up     83 121\n\n\n\nmean(nb.class == Direction.2005)\n\n[1] 0.5912698\n\n\nThe predict() function is straightforward. From the confusion matrix, Naive Bayes performs very well on this data, with accurate predictions over 59% of the time. This is better than Logistic Regression Model.\nThe predict() function can also generate estimates of the probability that each observation belongs to a particular class.\n\nnb.preds <- predict(nb.fit, Smarket.2005, type = \"raw\")\nnb.preds[1:5, ]\n\n          Down        Up\n[1,] 0.4873164 0.5126836\n[2,] 0.4762492 0.5237508\n[3,] 0.4653377 0.5346623\n[4,] 0.4748652 0.5251348\n[5,] 0.4901890 0.5098110"
  },
  {
    "objectID": "weeks/week04/lab.html#step-4-practical-exercises-in-pairs",
    "href": "weeks/week04/lab.html#step-4-practical-exercises-in-pairs",
    "title": "üíª Week 04 - Lab Roadmap (90 min)",
    "section": "Step 4: Practical exercises (in pairs)",
    "text": "Step 4: Practical exercises (in pairs)\nSo far, we have learnt to fit some kinds of classification models in R. In this practical case, we will continue to use the data set Auto. Make sure that the missing values have been removed from the data.\nSix questions are listed below. In this part, you will develop a model to predict whether a given car gets high or low gas mileage based on the Auto data set.\nüéØ Questions\n\nCreate a binary variable, mpg01, that contains a 1 if mpg contains a value above its median, and a 0 if mpg contains a value below its median. You can compute the median using the median() function. Note you may find it helpful to use the data.frame() function to create a single data set containing both mpg01 and the other Auto variables.\nExplore the data graphically in order to investigate the association between mpg01 and the other features. Which of the other features seem most likely to be useful in predicting mpg01? Scatterplots and boxplots may be useful tools to answer this question. Describe your findings.\nSplit the data into a training set and a test set. Train set contains observations before 1979. Test set contains the rest of the observations.\nPerform logistic regression on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in question 2. What is the test error of the model obtained?\nPerform naive Bayes on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in question 2. What is the test error of the model obtained?\nWhich of these two methods appears to provide the best results on this data? Justify your choice."
  },
  {
    "objectID": "weeks/week04/lab_solutions.html",
    "href": "weeks/week04/lab_solutions.html",
    "title": "‚úîÔ∏è Week 04 - Lab Solutions",
    "section": "",
    "text": "Create a binary variable, mpg01, that contains a 1 if mpg contains a value above its median, and a 0 if mpg contains a value below its median. You can compute the median using the median() function. Note you may find it helpful to use the data.frame() function to create a single data set containing both mpg01 and the other Auto variables.\nlibrary(ISLR2)\nAuto = na.omit(Auto)\nmpg01 = rep(0, dim(Auto)[1])\nmpg01[Auto$mpg > median(Auto$mpg)] = 1\nAuto = data.frame(Auto, mpg01)\nhead(Auto)\nBefore we move to next step, we need to lable mpg01 and orgin as factor so that R could recognized them as quanlitative variables instead of quantitative variables.\nAuto$mpg01 = as.factor(Auto$mpg01)\nAuto$origin = as.factor(Auto$origin)\nExplore the data graphically in order to investigate the association between mpg01 and the other features. Which of the other features seem most likely to be useful in predicting mpg01? Scatterplots and boxplots may be useful tools to answer this question. Describe your findings.\npar(mfrow = c(2, 3))\nplot(factor(Auto$mpg01), Auto$cylinders, ylab = \"Number of engine cylinders\")\nplot(factor(Auto$mpg01), Auto$displacement, ylab = \"Engine displacement (cubic inches)\")\nplot(factor(Auto$mpg01), Auto$horsepower, ylab = \"Horsepower\")\nplot(factor(Auto$mpg01), Auto$weight, ylab = \"Weight (pounds)\")\nplot(factor(Auto$mpg01), Auto$acceleration, ylab = \"Time to reach 60mpg (seconds)\")\nplot(factor(Auto$mpg01), Auto$year, ylab = \"Manufacture year\")\nmtext(\"Boxplots for cars with above(1) and below(0) median mpg\", outer = TRUE, line = -3)\nBoxplots were plotted to compare the distributions for each of the quantitative variables between cars with above-median mpg and those with below median-mpg. If the distribution of a predictor significantly varies with the response variable, then it may contribute to the prediction of response variable. If the distribution of a predictor does not significantly differ between different values of the response variable, then it may not contribute to the prediction of response variable. The boxplots suggest that cylinders, displacement, horsepower, and weight might be the most useful in predicting mpg01. The function par() is used to change the layout of output plots.\nTo visualise the association between mpg01, year and origin, scatterplot is used.\npar(mfrow = c(1, 1))\nplot(Auto$year, Auto$mpg)\nabline(h = median(Auto$mpg), lwd = 2, col = \"red\")\nThe above scatterplot of mpg vs year shows that the newer cars in the data set tend to be more fuel efficient. Therefore, while manufacture year might not be as useful as the other four quantitative variables, it still seems worth including.\nplot(Auto$origin, Auto$mpg, xlab = \"Origin\", ylab = \"mpg\")\nabline(h = median(Auto$mpg), lwd = 2, col = \"red\")\nLastly, when looking at a boxplot that compares the mpg values for each car, categorized by country of origin, we see that there is a clear difference between American cars, which tend to have below-median fuel efficiency, and European and Japanese cars, which tend to have above-median fuel efficiency. Thus, it seems that origin will also be useful in predicting mpg01.\nIn conclusion, all of the predictors except for acceleration and name will be used in fitting this classification model for trying to predict mpg01. Also, mpg will be excluded because that was directly used to create the classification label.\nSplit the data into a training set and a test set. Train set contains observations before 1979. Test set contains the rest of the observations.\nattach(Auto)\ntrain <- (year < 79)\nAuto_train <- Auto[train , ]\nAuto_test <- Auto[!train , ]\nPerform logistic regression on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in question 2. What is the test error of the model obtained?\nglm.fit = glm(mpg01 ~ cylinders + displacement + horsepower + weight + year + origin, data = Auto, subset = train, family = \"binomial\")\nsummary(glm.fit)\nglm.probs = predict(glm.fit, Auto_test, type = \"response\")\nglm.pred = rep(0, dim(Auto_test)[1])\nglm.pred[glm.probs > 0.5] = 1\ntable(glm.pred, Auto_test$mpg01, dnn = c(\"Predicted\", \"Actual\"))\nmean(glm.pred == Auto_test$mpg01)\n[1] 0.877193\nAs is shown, the test error of the model ontained is (1-0.877193) = 0.122807.\nPerform naive Bayes on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in question 2. What is the test error of the model obtained?\nlibrary (e1071)\nnb.fit = naiveBayes(mpg01 ~ cylinders + displacement + horsepower + weight + year + origin, data = Auto, subset = train)\nnb.fit\nnb.class <- predict(nb.fit, Auto_test)\ntable(nb.class, Auto_test$mpg01, dnn = c(\"Predicted\", \"Actual\"))\nmean(nb.class == Auto_test$mpg01)\nAs is shown, the test error of the model ontained is (1-0.877193) = 0.122807.\nWhich of these two methods appears to provide the best results on this data? Justify your choice.\nAfter comparing the test errors, these two classification models were equally good. To further compare the performance of these two model, we have to look at the confusion matrix and find these two classification models have identiical confusion matrix. It means that Precision, Recall, Accuracy and F-Score of these two models are all same. Therefore, we can conclude that these two classification models have equally good performace on this data set. One thing that should be cautious of is that the data set is imbalnced. Therefore, we cannot judge the performance of this data set based on test error. More detailed explaination about be found here: https://www.analyticsvidhya.com/blog/2021/06/5-techniques-to-handle-imbalanced-data-for-a-classification-problem/ ."
  },
  {
    "objectID": "weeks/week04/lecture.html",
    "href": "weeks/week04/lecture.html",
    "title": "üë®‚Äçüè´ Week 04 - Lecture",
    "section": "",
    "text": "References\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York NY: Springer. https://www.statlearning.com/."
  },
  {
    "objectID": "weeks/week05/lab.html",
    "href": "weeks/week05/lab.html",
    "title": "üíª Week 05 - Lab Roadmap (90 min)",
    "section": "",
    "text": "Last week we learnt how to do the Classification Methods in R. As data scientists, we need to know how to examine the results we generated and also to justify our results. So, we will focus on and practice the validation methods in the R language in this week‚Äôs lab.\nIf you feel confident at the current stage, free to explore more on your own. We have provided you with some supplementary online resources :\nR packages you will need:"
  },
  {
    "objectID": "weeks/week05/lab.html#step-1-the-validation-set-approach",
    "href": "weeks/week05/lab.html#step-1-the-validation-set-approach",
    "title": "üíª Week 05 - Lab Roadmap (90 min)",
    "section": "Step 1: The Validation Set Approach",
    "text": "Step 1: The Validation Set Approach\nIn this step, we will explore how to extract the subset of the whole dataset as a training dataset, and then estimate the test error rates of various linear models. The dataset is Auto from the ISLR2 package.\n\nStep 1.1: Training vs Testing splits\nSplit the Auto dataset into two halves, as randomly selecting 196 observations out of the original 392 observations. We performed splits using base R in the last lab. However, this can be done more easily using the rsample package. We first create a split object Auto.split using prop = 0.5 to specify a \\(50\\%/50\\%\\) train/test split. We also specify strata = mpg as we want our train/test split to have a similar mean/standard deviation for mpg. We then create dataframes using the training and testing functions.\n\nlibrary(ISLR2)\nlibrary(rsample)\n\nset.seed(1) # Just so you and I have the same \"random\" numbers.\n\nAuto.split <- initial_split(Auto, prop = 0.5, strata = mpg)\nAuto.train <- training(Auto.split)\nAuto.test <- testing(Auto.split)\n\n\n\n\n\n\n\nAbout set.seed()\n\n\n\n\n\nset.seed() is important here as it set a seed for the random number generator. Literally, the same results will be replicated in the following steps. Further information can be found in the official documentation.Seeding Random Variate Generators\n\n\n\n\n\nStep 1.2: How good is your model at predicting the test set?\nFit a linear regression model using the training dataset (Auto.train), making the mpg as the dependent variable(x) and horsepower as the independent variable(y). Then, using the fitted model to estimate the mpg from Auto.test. Finally, calculating the MSE of the 196 observations in the validation set.\n\n# use the lm() function to fit a linear regression model\nlm.fit <- lm(mpg ~ horsepower, data = Auto.train)\n\n# estimate the 'mpg' values by the lm.fit model\nlm.pred <- predict(lm.fit, Auto.test)\n\n# calculate MSE\nmean((Auto.test$mpg - lm.pred)^2)\n\n[1] 25.58657\n\n\nTherefore, we have estimated the test MSE for the linear regression model, \\(\\mathbf{25.59}\\). (Well Done! üí™ )\n\n\n\n\n\n\nHow to get help in R\n\n\n\n\n\nIf you are ever in doubt of what a particular R function do, you can consult the R documentation for it by using ?.\nFor example, you have been using lm and predict but you want to see what other arguments/parameters this function can take, just type ?lm or ?predict in your console.\n\n\n\n\n\n\n\n\n\nHow to access or create columns in R?\n\n\n\n\n\nIn base R, you generally use the $ sign to access columns or create new columns.\n\n#You can see all the values from the 'mpg' column.\nAuto$mpg\n\n  [1] 18.0 15.0 18.0 16.0 17.0 15.0 14.0 14.0 14.0 15.0 15.0 14.0 15.0 14.0 24.0\n [16] 22.0 18.0 21.0 27.0 26.0 25.0 24.0 25.0 26.0 21.0 10.0 10.0 11.0  9.0 27.0\n [31] 28.0 25.0 19.0 16.0 17.0 19.0 18.0 14.0 14.0 14.0 14.0 12.0 13.0 13.0 18.0\n [46] 22.0 19.0 18.0 23.0 28.0 30.0 30.0 31.0 35.0 27.0 26.0 24.0 25.0 23.0 20.0\n [61] 21.0 13.0 14.0 15.0 14.0 17.0 11.0 13.0 12.0 13.0 19.0 15.0 13.0 13.0 14.0\n [76] 18.0 22.0 21.0 26.0 22.0 28.0 23.0 28.0 27.0 13.0 14.0 13.0 14.0 15.0 12.0\n [91] 13.0 13.0 14.0 13.0 12.0 13.0 18.0 16.0 18.0 18.0 23.0 26.0 11.0 12.0 13.0\n[106] 12.0 18.0 20.0 21.0 22.0 18.0 19.0 21.0 26.0 15.0 16.0 29.0 24.0 20.0 19.0\n[121] 15.0 24.0 20.0 11.0 20.0 19.0 15.0 31.0 26.0 32.0 25.0 16.0 16.0 18.0 16.0\n[136] 13.0 14.0 14.0 14.0 29.0 26.0 26.0 31.0 32.0 28.0 24.0 26.0 24.0 26.0 31.0\n[151] 19.0 18.0 15.0 15.0 16.0 15.0 16.0 14.0 17.0 16.0 15.0 18.0 21.0 20.0 13.0\n[166] 29.0 23.0 20.0 23.0 24.0 25.0 24.0 18.0 29.0 19.0 23.0 23.0 22.0 25.0 33.0\n[181] 28.0 25.0 25.0 26.0 27.0 17.5 16.0 15.5 14.5 22.0 22.0 24.0 22.5 29.0 24.5\n[196] 29.0 33.0 20.0 18.0 18.5 17.5 29.5 32.0 28.0 26.5 20.0 13.0 19.0 19.0 16.5\n[211] 16.5 13.0 13.0 13.0 31.5 30.0 36.0 25.5 33.5 17.5 17.0 15.5 15.0 17.5 20.5\n[226] 19.0 18.5 16.0 15.5 15.5 16.0 29.0 24.5 26.0 25.5 30.5 33.5 30.0 30.5 22.0\n[241] 21.5 21.5 43.1 36.1 32.8 39.4 36.1 19.9 19.4 20.2 19.2 20.5 20.2 25.1 20.5\n[256] 19.4 20.6 20.8 18.6 18.1 19.2 17.7 18.1 17.5 30.0 27.5 27.2 30.9 21.1 23.2\n[271] 23.8 23.9 20.3 17.0 21.6 16.2 31.5 29.5 21.5 19.8 22.3 20.2 20.6 17.0 17.6\n[286] 16.5 18.2 16.9 15.5 19.2 18.5 31.9 34.1 35.7 27.4 25.4 23.0 27.2 23.9 34.2\n[301] 34.5 31.8 37.3 28.4 28.8 26.8 33.5 41.5 38.1 32.1 37.2 28.0 26.4 24.3 19.1\n[316] 34.3 29.8 31.3 37.0 32.2 46.6 27.9 40.8 44.3 43.4 36.4 30.0 44.6 33.8 29.8\n[331] 32.7 23.7 35.0 32.4 27.2 26.6 25.8 23.5 30.0 39.1 39.0 35.1 32.3 37.0 37.7\n[346] 34.1 34.7 34.4 29.9 33.0 33.7 32.4 32.9 31.6 28.1 30.7 25.4 24.2 22.4 26.6\n[361] 20.2 17.6 28.0 27.0 34.0 31.0 29.0 27.0 24.0 36.0 37.0 31.0 38.0 36.0 36.0\n[376] 36.0 34.0 38.0 32.0 38.0 25.0 38.0 26.0 22.0 32.0 36.0 27.0 27.0 44.0 32.0\n[391] 28.0 31.0\n\n\n\n\n\n\n\nStep 1.3: Does it get better if I modify the features using polynomial terms?\nRepeat the second part to estimate the test error for the quadratic and cubic regressions.\n\nlm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto.train)\n\n\nmean((Auto.test$mpg - predict(lm.fit2, Auto.test))^2)\n\n[1] 19.08374\n\n\n\nlm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto.train)\nmean((Auto.test$mpg - predict(lm.fit3, Auto.test))^2)\n\n[1] 19.00317\n\n\nüí° We can see a model that predicts mpg using a quadratic function of horsepower performs better than a model that involves only a linear function of horsepower. Furthermore, we see that adding a cubic function of horsepower actually increases MSE when compared to the quadratic function. Thus, the quadratic function of horsepower appears to perform the best out of all the functions considered.\n\n\nStep 1.4: Visualisation\nWant to see how well does the quadratic fit maps onto the raw data?\nLet‚Äôs create a scatter plot with horsepower on the x-axis and mpg on the y-axis. Now, we can predict mpg using lm.fit2 and specify interval = 'confidence' to get \\(95\\%\\) confidence intervals. Along with geom_line we can use geom_ribbon to plot the line of best fit and associated confidence intervals. Remember to specify alpha so that we can see the predicted value - otherwise the ribbon will not be translucent.\n\n    library(tidyverse)\n\n    sim.data <- data.frame(horsepower = 46:230)\n\n    sim.pred <- predict(lm.fit2, sim.data, interval = 'confidence') \n\n    sim.data <- cbind(sim.data, sim.pred)\n\n    ggplot(data = sim.data, aes(x = horsepower, y = fit)) +\n        geom_point(data = Auto, aes(x = horsepower, y = mpg)) +\n        geom_line() +\n        geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.125, fill = 'blue') +\n        theme_minimal() +\n        labs(x = 'Horsepower', y = 'MPG')"
  },
  {
    "objectID": "weeks/week05/lab.html#step-2-k-fold-cross-validation",
    "href": "weeks/week05/lab.html#step-2-k-fold-cross-validation",
    "title": "üíª Week 05 - Lab Roadmap (90 min)",
    "section": "Step 2: k-Fold Cross-Validation",
    "text": "Step 2: k-Fold Cross-Validation\nIt will be easy to follow the former procedure in Step 2, by the cv.glm to implement K-fold CV.\n\nStep 2.1: Using K=10 folds\nLet‚Äôs estimate Cross-Validation errors corresponding to the polynomial fits of orders one to ten using ten-fold cross-validation (via K = 10).\nThe cv.glm() function is part of the boot library. Meanwhile, you can explore the cv.err by yourself to see what call,K,delta and seed mean. This online webpage will be useful when interpreting the results.\n\nlibrary(boot)\n\n\nset.seed(17)\n\ncv.error.10 <- c() \n\nfor(i in 1:10){\n    glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)\n    cv.error.10[i] <- cv.glm(data = Auto, glmfit = glm.fit, K = 10)$delta[1]\n}\n\ncv.error.10\n\n [1] 24.27207 19.26909 19.34805 19.29496 19.03198 18.89781 19.12061 19.14666\n [9] 18.87013 20.95520\n\n\n\n# we can plot the results by passing a data frame to ggplot\n\ncv.data <- data.frame(poly = 1:10, cv.errs = cv.error.10)\n\nggplot(data = cv.data, aes(x = poly, y = cv.errs)) +\n    geom_point() +\n    geom_line(linetype = 'dashed') +\n    scale_x_continuous(breaks = 1:10) +\n    theme_minimal() +\n    labs(x = 'Degree of polynomial', y = 'Cross-validated MSE')\n\n\n\n\nNote that in the line of cv.error.10[i] <- cv.glm(data = Auto, glmfit = glm.fit, K=10)$delta[1], it will be very strict to K rather than k."
  },
  {
    "objectID": "weeks/week05/lab.html#step-3-the-bootstrap",
    "href": "weeks/week05/lab.html#step-3-the-bootstrap",
    "title": "üíª Week 05 - Lab Roadmap (90 min)",
    "section": "Step 3: The Bootstrap",
    "text": "Step 3: The Bootstrap\nWe have learnt the theoretical method regarding Bootstrap. I understand that it may be a bit difficult for beginners in statistics, but we will mainly focus on the coding implementation and visualisation here. Also, we will introduce how to create a function below.\nFunctions are ‚Äúself contained‚Äù modules of code that accomplish a specific task. referenced from Functions and theri argumens\nWith the help of a function, you can reuse the same pattern codes with a simple function name. In fact, you work with functions all the time in R - perhaps without even realising it!\n\nStep 3.1 Estimating the Accuracy of a Linear Regression Model\nIn this step, we will use the boostrap approach to assess the variability of a coefficient estimate. For the sake of simplicity we will look at the relationship between weight and horsepower which appears to be linear.\nCreate a function as boot.fn() which\n\nboot.fn <- function(data, index) {\n\n    lm(horsepower ~ weight, data = data[index,])$coefficients\n\n}  \n\nboot.fn simply returns a vector of coefficient estimates. It takes two parameters: data and index. data is a placeholder for the dataframe used in the model. index is a placeholder for the sample used to subset the dataframe. Other than this, the body of the function should look familiar. We are estimating a linear model where we are looking to predict horsepower by weight, and then extracting the coefficients.\nNow, compare the results from bootstrap estimates and the standard estimates\n\n# bootstrap with 1000 times\n\nboot(Auto, boot.fn, 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Auto, statistic = boot.fn, R = 1000)\n\n\nBootstrap Statistics :\n        original        bias    std. error\nt1* -12.18348470  1.659517e-01 3.221766853\nt2*   0.03917702 -6.479214e-05 0.001251433\n\n\n\nsummary(lm(horsepower ~ weight, data = Auto))$coef\n\n                Estimate  Std. Error   t value      Pr(>|t|)\n(Intercept) -12.18348470 3.570431493 -3.412328  7.115312e-04\nweight        0.03917702 0.001153214 33.972031 1.364347e-118\n\n\nWe can find that in the bootstrap estimation process, \\(\\mathrm{SE}(\\hat{\\beta}_{0}) = 3.5704\\) and \\(\\mathrm{SE}(\\hat{\\beta}_{1}) = 0.0012\\) , while in the standard estimation process, \\(\\hat{\\beta}_{Intercept}=-12.1835\\) and \\(\\hat{\\beta}_{horsepower}=0.0392\\).\nTo get a better intuition of what the bootstrap algorithm does let‚Äôs create a ggplot. We can get the intercepts and slopes estimated and overlay them on a scatterplot (weight on x-axis, horsepower on y-axis). We will create 50 bootstrap resamples for ease of visualisation and use geom_abline to overlay all the lines of best fit.\n\nboot.model <- boot(Auto, boot.fn, 50)\n\nboot.df <- as.data.frame(boot.model$t)\nnames(boot.df) <- c('b0','b1')\n\nggplot(data = Auto, aes(x = weight, y = horsepower)) +\n    geom_point() +\n    geom_abline(data = boot.df,\n                aes(intercept = b0, slope = b1), \n                alpha = 0.1, colour = 'blue') +\n    theme_minimal() +\n    labs(x = 'Weight (lbs.)', y = 'Engine horsepower')"
  },
  {
    "objectID": "weeks/week05/lab.html#step-4-practical-exercises",
    "href": "weeks/week05/lab.html#step-4-practical-exercises",
    "title": "üíª Week 05 - Lab Roadmap (90 min)",
    "section": "Step 4: üéØ Practical Exercises",
    "text": "Step 4: üéØ Practical Exercises\nSince then, we have known and implemented the coding with Cross-validation and Bootstrap. In this practical case, we will use the new dataset Default and also Weekly from the ISRL package. Do not forget to set a random seed before beginning your analysis.\nSome questions are listed below. You are required to try to answer these questions in pairs using R commands. We will go over the solutions once everyone has finished these questions.\n\nQ1: Train vs test sets\nFor the Default dataset, please split the sample set into a training set and a test set, then fit a logistic regression model that uses income and balance to predict default:\n\nQ1.1: Use three different splits of the observations into a training set and a test set.\nQ1.2: Fit three multiple logistic regression models using only the training observations.\nQ1.3: Based on the three models, obtain a prediction of default status for each individual in the test set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than 0.5.\nQ1.4: Based on the three models, compute the test set error, which is the fraction of the observations in the test set that are misclassified.\n\n\n\nQ2: Bootstrap\nFor the Default dataset, We continue to consider the use of a logistic regression model to predict the probability of default using income and balance on the Default data set.\nIn particular, we will now compute estimates for the standard errors of the income and balance logistic regression coefficients in two different ways: 2.1. Using the bootstrap, 2.2. Using the standard formula for computing the standard errors in the glm() function. As following,\n\nUsing the summary() and glm() functions, determine the estimated standard errors for the coefficients associated with income and balance in a multiple logistic regression model that uses both predictors.\nWrite a function,boot.fn(),that takes as input the Default data set as well as an index of the observations, and that outputs the coefficient estimates for income and balance in the multiple logistic regression model.\nUse the boot() function together with your boot.fn() function to estimate the standard errors of the logistic regression coefficients for income and balance. Then, Create a histogram of the bootstrap parameter estimates with ggplot2, and also set the bins=20, title as 1,000 Bootstrap Parameter Estimates - 'balance' & 'income."
  },
  {
    "objectID": "weeks/week05/lab_solution.html",
    "href": "weeks/week05/lab_solution.html",
    "title": "üíª Week 05 - Lab Roadmap (90 min)",
    "section": "",
    "text": "Step 5: Exercise (35mins)\n\n Since then, we have known and implemented the coding with Cross-validation and Bootstrap. In this practical case, we will use the new dataset Default and also Weekly from the ISRL package. Do not forget to set a random seed before beginning your analysis.\nSome questions are listed below. You are required to try to answer these questions in pairs using R commands. We will go over the solutions once everyone has finished these questions\nüéØ Questions\n\nFor the Default dataset, please Split the sample set into a training set and a validation set, then fit a logistic regression model that uses income and balance to predict default. As following,\n\nUse three different splits of the observations into a training set and a validation set. \n\n> train.1 = sample(dim(Default)[1], 0.70*dim(Default)[1])\n> train.2 = sample(dim(Default)[1], 0.70*dim(Default)[1])\n> train.3 = sample(dim(Default)[1], 0.70*dim(Default)[1])\n\nFit three multiple logistic regression models using only the training observations. \n\n#just give one sample here with train.1\n> glm.fit.1 = glm(default ~ income + balance, data = Default, subset = train.1, family = \"binomial\")\n\nBased on the three models,obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than 0.5. \n\n#just give one sample here with train.1\n> glm.probs = predict(glm.fit.1, Default[-train.1, ], type = \"response\")\n> glm.preds = rep(\"No\", dim(Default)[1])\n> glm.preds[glm.probs > 0.5] = \"Yes\"\n\nBased on the three models, compute the validation set error, which is the fraction of the observations in the validation set that are misclassified.\n\n#just give one sample here\n> mean(glm.preds != Default[-train, \"default\"])\nFor the Default dataset, We continue to consider the use of a logistic regression model to predict the probability of default using income and balance on the Default data set. In particular, we will now compute estimates for the standard errors of the income and balance logistic regression coefficients in two different ways:\n2.1. Using the bootstrap.\n2.2. Using the standard formula for computing the standard errors in the glm() function.\nAs following,\n\nUsing the summary() and glm() functions, determine the estimated standard errors for the coefficients associated with income and balance in a multiple logistic regression model that uses both predictors. \n> log_def <- glm(default ~ income + balance, data = Default, family = \"binomial\")\n> summary(log_def)$coefficients[, 2]\n[1]  (Intercept)       income      balance \n    4.347564e-01 4.985167e-06 2.273731e-04 \nWrite a function,boot.fn(),that takes as input the Default data set as well as an index of the observations, and that outputs the coefficient estimates for income and balance in the multiple logistic regression model. \n    > boot.fn <- function(data, index = 1:nrow(data)) {\n    +   coef(glm(default ~ income + balance, data = data, subset = index, family = \"binomial\"))[-1]\n    + }\n    > boot.fn(Default)\n   [1]  income      balance \n    2.080898e-05 5.647103e-03 \nUse the boot() function together with your boot.fn() function to estimate the standard errors of the logistic regression coefficients for income and balance. Then, Create a histogram of the bootstrap parameter estimates with ggplot2, and also set the bins=20, title as 1,000 Bootstrap Parameter Estimates - 'balance' & 'income. \n\n ```r\n > set.seed(101)\n > boot_results <- boot(data = Default, statistic = boot.fn, R = 1000)\n\n > as.data.frame(boot_results$t) %>%\n     rename(income = V1, balance = V2) %>%\n     gather(key = \"variable\", value = \"estimate\") %>%\n     ggplot(aes(x = estimate, fill = factor(variable))) + \n     geom_histogram(bins = 20) + \n     facet_wrap(~ variable, scales = \"free_x\") + \n     labs(title = \"1,000 Bootstrap Parameter Estimates - 'balance' & 'income'\", \n         subtitle = paste0(\"SE(balance) = \", formatC(sd(boot_results$t[ ,2]), format = \"e\", digits = 6), \n                             \", SE(income) = \", formatC(sd(boot_results$t[ ,1]), format = \"e\", digits = 6)), \n         x = \"Parameter Estimate\", \n         y = \"Count\") + \n     theme(legend.position = \"none\")\n ```\nWe saw that the cv.glm() function can be used in order to compute the LOOCV test error estimate. However, one could compute those quantities using just the glm() and predict.glm() functions, and a for loop. You will now take this approach in order to compute the LOOCV error for a simple logistic regression model on the Weekly data set. As following,\n\nLogistic Regression- Fit a logistic regression model that predicts Direction using Lag1 and Lag2. \n\n    > log_dir <- glm(Direction ~ Lag1 + Lag2, data = Weekly, family = \"binomial\")\n    > summary(log_dir)\n\nOmitting One Observation from Training- Fit a logistic regression model that predicts Direction using Lag1 and Lag2 using all but the first observation . \n\n    > log_dir_2 <- glm(Direction ~ Lag1 + Lag2, data = Weekly[-1, ], family = \"binomial\")\n    > summary(log_dir_2)\n\nPredicting the Omitted Observation- Use the model from ii. to predict the direction of the first observation. You can do this by predicting that the first observation will go up if \\(P(\\text { Direction }=' U p ' \\mid \\operatorname{Lag} 1, \\text { Lag2 })>0.5\\). Was this observation correctly classified? \n\n    # methods 1\n    > predict(log_dir_2, Weekly[1, ])\n    > Weekly[1, ]\n\n    # methods 2\n    > ifelse(predict(log_dir_2, newdata = Weekly[1, ], type = \"response\") > 0.5, \"Up\", \"Down\")\n\nWriting a LOOCV Loop- Write a for loop from i = 1 to i = n, where n is the number of observations in the data set, that performs each of the following steps:\n\nFit a logistic regression model using all but the ith observation to predict Direction using Lag1 and Lag2.\nCompute the posterior probability of the market moving up for the i th observation.\nUse the posterior probability for the i th observation in order to predict whether or not the market moves up.\nDetermine whether or not an error was made in predicting the direction for the i th observation. If an error was made, then indicate this as a 1 , and otherwise indicate it as a 0 . \n\n\n    # method 1\n    > error <- c()\n    > for (i in 1:nrow(Weekly)) {\n    log_dir <- glm(Direction ~ Lag1 + Lag2, data = Weekly[-i, ], family = \"binomial\") # 1.\n    prediction <- ifelse(predict(log_dir, newdata = Weekly[i, ], type = \"response\") > 0.5, \"Up\", \"Down\") # 2. & 3.\n    error[i] <- as.numeric(prediction != Weekly[i, \"Direction\"]) # 4.\n    }\n    > head(error)\n    > error[1:10]\n    [1]  1 1 0 1 0 1 0 0 0 1\n\n    #method 2\n    > n <- dim(Weekly)[1]\n    > errors <- rep(0, n)\n    > for (i in 1:n){\n        glm.fit.loo <- glm(Direction ~ Lag1 + Lag2, data = Weekly, family = \"binomial\", subset <- c(-i))\n        pred = \"Down\"\n        if (predict(glm.fit.loo, Weekly[i, ], type = \"response\") > 0.5){\n            pred = \"Up\"\n        }\n        if (pred != Weekly[i, \"Direction\"]){\n            errors[i] <- 1\n        }\n    }\n    > head(errors)\n    > errors[1:10]\n    [1]  1 1 0 1 0 1 0 0 0 1\n\nThe LOOCV Estimate for the Test Error- Take the average of the n numbers obtained in (iv)-4. in order to obtain the LOOCV estimate for the test error. Comment on the results.\n\n\n#method 1\n> mean(error)\n[1] 0.4499541\n> prop.table(table(Weekly$Direction))\n\n    Down        Up \n0.4444444 0.5555556\n\n#method 2\n> mean(errors)\n[1] 0.4499541\n> mean(Weekly[\"Direction\"] != \"Up\")\n[1] 0.4444444\n> mean(Weekly[\"Direction\"] == \"Up\")\n[1] 0.5555556"
  },
  {
    "objectID": "slides/week01_slides_part1.html#the-data-science-institute",
    "href": "slides/week01_slides_part1.html#the-data-science-institute",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "The Data Science Institute",
    "text": "The Data Science Institute\n\n\n\n\n\nThis course is offered by the LSE Data Science Institute (DSI).\nDSI is the hub for LSE‚Äôs interdisciplinary collaboration in data science\n\n\n\n\nSign up for DSI events at lse.ac.uk/DSI/Events"
  },
  {
    "objectID": "slides/week01_slides_part1.html#the-data-science-institute-1",
    "href": "slides/week01_slides_part1.html#the-data-science-institute-1",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "The Data Science Institute",
    "text": "The Data Science Institute\n\n\n\n\nActivities of interest to you:\n\nCIVICA Seminar Series\nCareers in Data Science\nSocial events\nIndustry ‚Äúfield trips‚Äù\nSummer projects\n\n\n\n\nSign up for DSI events at lse.ac.uk/DSI/Events"
  },
  {
    "objectID": "slides/week01_slides_part1.html#our-courses",
    "href": "slides/week01_slides_part1.html#our-courses",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "Our courses",
    "text": "Our courses\nDSI offer accessible introductions to Data Science:\n\n\nDS101\nFundamentals of  Data Science\nüéØ Focus:  theoretical concepts of data science\nüìÇ How:  reflections through reading and writing\n\nDS105\nData for  Data Scientists\nüéØ Focus: collection and handling of real data\nüìÇ How: hands-on coding exercises and a group project\n\nDS202\nData Science for  Social Scientists\nüéØ Focus: fundamental machine learning algorithms\nüìÇ How: practical use of ML techniques and metrics"
  },
  {
    "objectID": "slides/week01_slides_part1.html#your-lecturer",
    "href": "slides/week01_slides_part1.html#your-lecturer",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "Your lecturer",
    "text": "Your lecturer\n\n\n\n\n\n \nDr.¬†Jonathan Cardoso-Silva\n\nPhD in Computer Science\nBackground: Engineering, Bio & Health Informatics\nFormer Lead Data Scientist\nResearch:\n\nNetworks\nOptimisation\nMachine Learning applications\nData Science Workflow"
  },
  {
    "objectID": "slides/week01_slides_part1.html#teaching-assistants",
    "href": "slides/week01_slides_part1.html#teaching-assistants",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "Teaching Assistants",
    "text": "Teaching Assistants\n\n\n\nDr.¬†Stuart Bramwell  ESRC Postdoctoral Fellow  Department of Methodology PhD in Politics (Oxford)\n\n\nYijun Wang  Guest Teacher at the DSI PhD cand. in Health Informatics (KCL)  MSc in Data Science (KCL)\n\n\nMustafa Can Ozkan  Guest Teacher at the DSI PhD cand. in the Spacetime Lab (UCL)  MSc in Transport (Imperial/UCL)\n\n\n\n\n\n\n\nXiaowei Gao  Guest Teacher at the DSI PhD cand. in the Spacetime Lab (UCL)  MSc in Data Science (KCL)\n\n\nAnton Boichenko  Guest Teacher at the DSI Product Developer at Decoded  MSc in Applied Social Data Science (LSE)"
  },
  {
    "objectID": "slides/week01_slides_part1.html#who-are-you",
    "href": "slides/week01_slides_part1.html#who-are-you",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "Who are you",
    "text": "Who are you\n\n\n\n\n\n\n\n\n\n\nProgramme\nFreq\n\n\n\n\nBSc in Economics\n34\n\n\nBSc in Pyschological and Behavioural Science\n32\n\n\nGeneral Course\n11\n\n\nBSc in Politics and Economics\n4\n\n\nLLB in Laws\n3\n\n\nBSc in International Relations\n2\n\n\nBSc in Philosophy and Economics\n2\n\n\nBSc in Philosophy, Politics and Economics\n2\n\n\nBSc in Economic History and Geography\n1\n\n\nBSc in Economics and Economic History\n1\n\n\nBSc in Geography with Economics\n1\n\n\nBSc in International Relations and History\n1\n\n\nBSc in Mathematics, Statistics and Business\n1\n\n\nBSc in Philosophy, Logic and Scientific Method\n1\n\n\nErasmus Reciprocal Programme of Study\n1\n\n\nExchange Programme for Students from University of California, Berkeley\n1\n\n\n\n\n\n\n\nSource: LSE For You. Last Updated: 30 September 2022"
  },
  {
    "objectID": "slides/week01_slides_part1.html#learning-objectives-cont.",
    "href": "slides/week01_slides_part1.html#learning-objectives-cont.",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "Learning Objectives (cont.)",
    "text": "Learning Objectives (cont.)\n\n\nKnow how to evaluate and compare fitted models, and to improve model performance.\nUse applied computer programming, including the hands-on use of programming through course exercises.\nApply the methods learned to real data through hands-on exercises.\nIntegrate the insights from data analytics into knowledge generation and decision-making;"
  },
  {
    "objectID": "slides/week01_slides_part1.html#learning-objectives-cont.-1",
    "href": "slides/week01_slides_part1.html#learning-objectives-cont.-1",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "Learning Objectives (cont.)",
    "text": "Learning Objectives (cont.)\n\nUnderstand an introductory framework for working with natural language (text) data using techniques of machine learning.\n\n\n\nLearn how data science methods have been applied to a particular domain of study (applications)."
  },
  {
    "objectID": "slides/week01_slides_part1.html#philosophy-of-this-course-cont.",
    "href": "slides/week01_slides_part1.html#philosophy-of-this-course-cont.",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "Philosophy of this course (cont.)",
    "text": "Philosophy of this course (cont.)\n\nThis is an exciting research area, having important applications in science, industry and policy.\nMachine learning is a fundamental ingredient in the training of a modern data scientist.\n\n\nContent borrowed from ME314 Day 1"
  },
  {
    "objectID": "slides/week01_slides_part1.html#the-basics-of-statistics",
    "href": "slides/week01_slides_part1.html#the-basics-of-statistics",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "The basics of statistics",
    "text": "The basics of statistics\nBasic concepts of Statistics you might want to recap:\n\n\nExpected value, mean, median, variance, standard deviation\nProbabilities and simple probability distributions\nTypes of data\n\ndiscrete vs continuous\ncategorical vs numerical vs ordinal"
  },
  {
    "objectID": "slides/week01_slides_part1.html#resources-stats",
    "href": "slides/week01_slides_part1.html#resources-stats",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "Resources (Stats)",
    "text": "Resources (Stats)\nA few references that might be useful to read or skim through:\n\n(Warne 2018, chaps. 1-3,5,6,11-12)\n(Gelman, Hill, and Vehtari 2020, chaps. 1‚Äì4)\nIf you are a PBS student, you can revisit the content of PB130 (MT3, MT4, MT8-MT11)"
  },
  {
    "objectID": "slides/week01_slides_part1.html#the-basics-of-r-programming",
    "href": "slides/week01_slides_part1.html#the-basics-of-r-programming",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "The basics of R programming",
    "text": "The basics of R programming\nBasic concepts of programming in R to recap:\n\n\ndata structures (vectors, matrices, data frames)\nhow to manipulate data (filter, subset, select)\nread/write data files (for example: CSV, JSON, TXT)\n(optional but encouraged) some knowledge tidyverse can give you a productive boost\n\nthe official website (tidyverse.org) has some good tutorials."
  },
  {
    "objectID": "slides/week01_slides_part1.html#resources-r",
    "href": "slides/week01_slides_part1.html#resources-r",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "Resources (R)",
    "text": "Resources (R)\n\nCheck out ‚ÄúR for Data Science‚Äù (Wickham and Grolemund 2016, chaps. 1‚Äì21). The online version is free.\n‚ÄúStatistical inference via data science‚Äù (Ismay and Kim 2020, chaps. 4‚Äì6) is another great free resource"
  },
  {
    "objectID": "slides/week01_slides_part1.html#what-if-i-struggle-with-r",
    "href": "slides/week01_slides_part1.html#what-if-i-struggle-with-r",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "‚ÄòWhat if I struggle with R‚Äô?",
    "text": "‚ÄòWhat if I struggle with R‚Äô?\n‚û°Ô∏è Our first lab (Week 02) is a recap of some basic R commands, plus some ggplot2.\n\n\nIf you are not confident with your R skills, I strongly encourage you invest in studying the basics in the next couple of weeks.\nContact LSE Digital Skills Lab to attend in-person workshops or self-paced online R courses."
  },
  {
    "objectID": "slides/week01_slides_part1.html#any-questions",
    "href": "slides/week01_slides_part1.html#any-questions",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "Any questions?",
    "text": "Any questions?\n\n\n\n\n\n\nImage created with the DALL¬∑E algorithm using the prompt: ‚Äò35mm macro photography of a robot holding a question mark card, white background‚Äô"
  },
  {
    "objectID": "slides/week01_slides_part1.html#syllabus",
    "href": "slides/week01_slides_part1.html#syllabus",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "Syllabus",
    "text": "Syllabus\n\n\n\n\n\n\n\nIntro\n\n\n\n\n\n¬†¬†¬†¬†Introduction, Context & Key Concepts\nWeek 01\n\n\nSupervised Learning\n\n\n\n¬†¬†¬†¬†Simple and Multiple Linear Regression  ¬†¬†¬†¬†Classifiers (Logistic Regression & Naive Bayes)  ¬†¬†¬†¬†Resampling methods  ¬†¬†¬†¬† Non-linear algorithms (SVM & tree-based models)\nWeek 02  Week 03  Week 04  Week 05\n\n\nUnsupervised Learning\n\n\n\n¬†¬†¬†¬†Unsupervised Learning: Clustering ¬†¬†¬†¬†Unsupervised Learning: PCA¬†¬†¬†¬†¬†¬†¬†¬†¬†\nWeek 07  Week 08\n\n\nApplications\n\n\n\n¬†¬†¬†¬†Applications: Predictive Modelling on Tabular Data¬†¬†¬† Applications: Text as Data & Topic Modelling ¬†¬†¬† Applications: Social Media Data\nWeek 09  Week 10  Week 11"
  },
  {
    "objectID": "slides/week01_slides_part1.html#structure-of-lectures",
    "href": "slides/week01_slides_part1.html#structure-of-lectures",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "Structure of lectures üë®üèª‚Äçüè´",
    "text": "Structure of lectures üë®üèª‚Äçüè´\nOur lectures will be split in two parts:\n\n\nPart I (~ 50 min): Traditional exposition of theoretical content\nbreak (~ 10 min): Grab coffee ‚òï or relax üßò\nPart II (~ 50 min): Live demo\n\nTypically, an exploratory analysis or application of an algorithm\nFeel free to follow along in your own laptops."
  },
  {
    "objectID": "slides/week01_slides_part1.html#structure-of-classes",
    "href": "slides/week01_slides_part1.html#structure-of-classes",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "Structure of classes üë©‚Äçüíª",
    "text": "Structure of classes üë©‚Äçüíª\n\n\nStudents will work on weekly, structured problem sets in the staff-led class sessions.\nTips to get the most of classes:\n\nBring your own laptops üíª (most tablets are not suitable for programming)\nRead the recommended reading prior to the class\nSkim through the problem set before class"
  },
  {
    "objectID": "slides/week01_slides_part1.html#class-groups",
    "href": "slides/week01_slides_part1.html#class-groups",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "Class groups",
    "text": "Class groups\n\n\nGroup 01\n\nüìÜ Mondays\n‚åö 09:00 ‚Äî 10:30\nüìç PAN.1.03\n\n\nGroup 02\n\nüìÜ Mondays\n‚åö 10:30 ‚Äî 12:00\nüìç PAN.1.03\n\n\nGroup 03\n\nüìÜ Mondays\n‚åö 13:00 ‚Äî 14:30\nüìç MAR.1.09\n\n\nGroup 04\n\nüìÜ Fridays\n‚åö 16:00 ‚Äî 17:30\nüìç NAB.1.04\n\n\nGroup 05\n\nüìÜ Mondays\n‚åö 09:00 ‚Äî 10:30\nüìç 32L.LG.11\n\n\nGroup 06\n\nüìÜ Mondays\n‚åö 10:30 ‚Äî 12:00\nüìç 32L.LG.11\n\n\nGroup 07\n\nüìÜ Fridays\n‚åö 09:30 ‚Äî 11:00\nüìç CBG.2.06\n\n\n\n\nüó∫Ô∏è Check LSE campus map"
  },
  {
    "objectID": "slides/week01_slides_part1.html#your-background-knowledge",
    "href": "slides/week01_slides_part1.html#your-background-knowledge",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "Your background knowledge",
    "text": "Your background knowledge\n\nPlease, help our teaching team understand your needs as we prepare for the first labs next week.\nFind the link to the survey on our Slack group or point your phone to the QR code below"
  },
  {
    "objectID": "slides/week01_slides_part1.html#assessments",
    "href": "slides/week01_slides_part1.html#assessments",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "Assessments üìî",
    "text": "Assessments üìî\nThe breakdown of assessment for this class will be as follows:"
  },
  {
    "objectID": "slides/week01_slides_part1.html#assessments-1",
    "href": "slides/week01_slides_part1.html#assessments-1",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "Assessments üìî",
    "text": "Assessments üìî\n\nProblem sets (60%)\n\n\nSummative problem sets released on Weeks 5, 8 & 11.\nThese will have a similar style to the formative problem sets, a mix of R tasks and your written interpretation of the analyses.\nYou will have 4-6 days to submit your solutions.\nEach of the three summative problem sets is worth 20% of the final mark, and will be graded on a 100 point scale."
  },
  {
    "objectID": "slides/week01_slides_part1.html#assessments-2",
    "href": "slides/week01_slides_part1.html#assessments-2",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "Assessments üìî",
    "text": "Assessments üìî\n\nTake-home exam (40%)\n\n\nAn open-book take-home exam, taken during the January exams period.\nExam questions will be comparable in style to the problem sets.\nThe exam questions will be released on Moodle on 5 January 2023. (tentative)\nThe exam is due on 11 January at 4pm (tentative)\n‚ö†Ô∏è Update 11/10/2022: Last year, DS202 exam was performed entirely online due to COVID-19 mitigation procedures. We want to run it online via our own Moodle page again this academic term, we just need to understand LSE regulations about exams for this year. We will update you on this very soon (hopefully by the end of W04)."
  },
  {
    "objectID": "slides/week01_slides_part1.html#office-hours",
    "href": "slides/week01_slides_part1.html#office-hours",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "Office hours",
    "text": "Office hours\n\n\nIt is probably a good idea to book office hours if:\n\nyou struggled with a technical or theoretical aspect of a problem set in the previous week,\nyou have queries about careers in data science,\nyou want guidance in how to apply data science to other things you are studying outside this course.\n\nCome prepared. You only have 15 minutes.\nAsk for help sooner rather than later.\nBook slots via StudentHub up to 12 hours in advance."
  },
  {
    "objectID": "slides/week01_slides_part1.html#communication",
    "href": "slides/week01_slides_part1.html#communication",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "Communication",
    "text": "Communication\n\n\nJoin our Slack group (more info here).\nUse the public Slack channels to talk to share links, content (or memes) with your colleagues.\nOur teaching team will dedicate some time during the week to answer questions or other interactions on Slack.\nReserve üìß e-mail for formal requests: extensions, deferrals, etc.\n\nNo need to e-mail to inform you will skip a class, for example."
  },
  {
    "objectID": "slides/week01_slides_part1.html#any-questions-1",
    "href": "slides/week01_slides_part1.html#any-questions-1",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "Any questions?",
    "text": "Any questions?\n\n\n\n\n\n\nImage created with the DALL¬∑E algorithm using the prompt: ‚Äò35mm macro photography of a robot holding a question mark card, white background‚Äô"
  },
  {
    "objectID": "slides/week01_slides_part1.html#we-changed-how-we-consume-music",
    "href": "slides/week01_slides_part1.html#we-changed-how-we-consume-music",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "We changed how we consume music üéß",
    "text": "We changed how we consume music üéß\n\n\n\n\n\n\nTo interact with this plot, check reference (Fischer-Baum 2017) at the end of this presentation."
  },
  {
    "objectID": "slides/week01_slides_part1.html#we-changed-how-we-consume-video",
    "href": "slides/week01_slides_part1.html#we-changed-how-we-consume-video",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "We changed how we consume video üéûÔ∏è",
    "text": "We changed how we consume video üéûÔ∏è\n\n\n\n\n\n\nTo interact with this plot, check reference (Fischer-Baum 2017) at the end of this presentation."
  },
  {
    "objectID": "slides/week01_slides_part1.html#smartphones-are-a-very-recent-thing",
    "href": "slides/week01_slides_part1.html#smartphones-are-a-very-recent-thing",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "Smartphones üì± are a very recent thing",
    "text": "Smartphones üì± are a very recent thing\n\n\n\n\n\n\nTo interact with this plot, check reference (Fischer-Baum 2017) at the end of this presentation."
  },
  {
    "objectID": "slides/week01_slides_part1.html#we-spend-a-lot-more-time-connected",
    "href": "slides/week01_slides_part1.html#we-spend-a-lot-more-time-connected",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "We spend a lot more time connected",
    "text": "We spend a lot more time connected"
  },
  {
    "objectID": "slides/week01_slides_part1.html#references",
    "href": "slides/week01_slides_part1.html#references",
    "title": "üóìÔ∏è Week 01 Structure of this course",
    "section": "References",
    "text": "References\n\n\nFischer-Baum, Reuben. 2017. ‚ÄúWhat ‚ÄòTech World‚Äô Did You Grow up In?‚Äù Washington Post. https://www.washingtonpost.com/graphics/2017/entertainment/tech-generations/.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. 1st ed. Cambridge University Press. https://doi.org/10.1017/9781139161879.\n\n\nIsmay, Chester, and Albert Young-Sun Kim. 2020. Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. Chapman & Hall/CRC the R Series. Boca Raton: CRC Press / Taylor & Francis Group. https://moderndive.com/.\n\n\nKolawole, Emi. 2013. ‚ÄúAbout Those 2005 and 2013 Photos of the Crowds in St. Peter‚Äôs Square.‚Äù Washington Post. http://wapo.st/WKKTMh.\n\n\nWarne, Russell T. 2018. Statistics for the Social Sciences: A General Linear Model Approach. https://www.cambridge.org/highereducation/books/statistics-for-the-social-sciences/716FF25785A6154CC6822D067A959445.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. First edition. Sebastopol, CA: O‚ÄôReilly. https://r4ds.had.co.nz/.\n\n\n\n\n\nDS202 - Data Science for Social Scientists ü§ñ ü§π"
  },
  {
    "objectID": "slides/week01_slides_part2.html#data-science-is",
    "href": "slides/week01_slides_part2.html#data-science-is",
    "title": "üóìÔ∏è Week 01Overview of core concepts",
    "section": "Data science is‚Ä¶",
    "text": "Data science is‚Ä¶\n\n\n‚Äú[‚Ä¶] a field of study and practice that involves the collection, storage, and processing of data in order to derive important üí° insights into a problem or a phenomenon.\n\n\n\n\nSuch data may be generated by humans (surveys, logs, etc.) or machines (weather data, road vision, etc.),\n\n\n\n\nand could be in different formats (text, audio, video, augmented or virtual reality, etc.).‚Äù\n\n\n\n\n(Shah 2020, chap. 1) - Emphasis and emojis are of my own making."
  },
  {
    "objectID": "slides/week01_slides_part2.html#the-mythical-unicorn",
    "href": "slides/week01_slides_part2.html#the-mythical-unicorn",
    "title": "üóìÔ∏è Week 01Overview of core concepts",
    "section": "The mythical unicorn ü¶Ñ",
    "text": "The mythical unicorn ü¶Ñ\n\n\nknows everything about statistics\n\n\nable to communicate insights perfectly\n\n\nfully understands businesses like no one\n\n\nis a fluent computer programmer\n\n\n\nOf course, such a person does not exist!\n\n\nSee (Davenport 2020) for a more in-depth discussion about this"
  },
  {
    "objectID": "slides/week01_slides_part2.html#in-reality",
    "href": "slides/week01_slides_part2.html#in-reality",
    "title": "üóìÔ∏è Week 01Overview of core concepts",
    "section": "In reality‚Ä¶",
    "text": "In reality‚Ä¶\n\n\nWe are all jugglers ü§π\n\n\nEveryone brings a different skill set.\nWe need multi-disciplinary teams.\nGood data scientists know a bit of everything.\n\nNot fluent in all things\nUnderstands their strenghts and weaknessess\nThey know when and where to interface with others\n\n\n\n\n\n\n\n\n\nSee (Schutt and O‚ÄôNeil 2013, chap. 1) for more on this."
  },
  {
    "objectID": "slides/week01_slides_part2.html#the-data-science-workflow-1",
    "href": "slides/week01_slides_part2.html#the-data-science-workflow-1",
    "title": "üóìÔ∏è Week 01Overview of core concepts",
    "section": "The Data Science Workflow",
    "text": "The Data Science Workflow\n\n\n\n\n\n   \n\nstart\n\n Start   \n\ngather\n\nGather data ¬†   \n\nstart->gather\n\n    \n\nstore\n\nStore it ¬†¬†¬†¬†¬†¬†¬†¬† somewhere   \n\ngather->store\n\n   ¬†¬†¬†¬†¬†   \n\nclean\n\nClean & ¬†¬†¬†¬†¬†¬†¬† pre-process   \n\nstore->clean\n\n   ¬†¬†¬†¬†¬†   \n\nbuild\n\nBuild a  dataset   \n\nclean->build\n\n   ¬†¬†¬†¬†¬†   \n\neda\n\nExploratory ¬†¬†¬† data analysis   \n\nbuild->eda\n\n    \n\nml\n\nMachine learning   \n\neda->ml\n\n   ¬†¬†¬†¬†¬†   \n\ninsight\n\nObtain ¬†¬† insights   \n\nml->insight\n\n   ¬†¬†¬†¬†¬†   \n\ncommunicate\n\nCommunicate results ¬†¬†¬†¬†¬†¬†¬†¬†   \n\ninsight->communicate\n\n   ¬†¬†¬†¬†¬†   \n\nend\n\n End   \n\ncommunicate->end\n\n   \n\n\n\n\n\n\n\n‚ö†Ô∏è Note that this is a simplified version of what happens in a data science project.  In practice, the process is not linear and there are many feedback loops."
  },
  {
    "objectID": "slides/week01_slides_part2.html#the-data-science-workflow-2",
    "href": "slides/week01_slides_part2.html#the-data-science-workflow-2",
    "title": "üóìÔ∏è Week 01Overview of core concepts",
    "section": "The Data Science Workflow",
    "text": "The Data Science Workflow\n\n\n\n\n\n   \n\nstart\n\n Start   \n\ngather\n\n Gather data ¬†   \n\nstart->gather\n\n    \n\nend\n\n End   \n\nstore\n\n Store it ¬†¬†¬†¬†¬†¬†¬†¬† somewhere   \n\ngather->store\n\n   ¬†¬†¬†¬†¬†   \n\nclean\n\n Clean & ¬†¬†¬†¬†¬†¬†¬† pre-process   \n\nstore->clean\n\n   ¬†¬†¬†¬†¬†   \n\nbuild\n\n Build a  dataset   \n\nclean->build\n\n   ¬†¬†¬†¬†¬†   \n\neda\n\n Exploratory ¬†¬†¬† data analysis   \n\nbuild->eda\n\n    \n\nml\n\n Machine learning   \n\neda->ml\n\n   ¬†¬†¬†¬†¬†   \n\ninsight\n\n Obtain ¬†¬† insights   \n\nml->insight\n\n   ¬†¬†¬†¬†¬†   \n\ncommunicate\n\n Communicate results ¬†¬†¬†¬†¬†¬†¬†¬†   \n\ninsight->communicate\n\n   ¬†¬†¬†¬†¬†   \n\ncommunicate->end\n\n   \n\n\n\n\n\nIt is often said that 80% of the time and effort spent on a data science project goes to the tasks highlighted above."
  },
  {
    "objectID": "slides/week01_slides_part2.html#the-data-science-workflow-3",
    "href": "slides/week01_slides_part2.html#the-data-science-workflow-3",
    "title": "üóìÔ∏è Week 01Overview of core concepts",
    "section": "The Data Science Workflow",
    "text": "The Data Science Workflow\n\n\n\n\n\n   \n\nstart\n\n Start   \n\ngather\n\n Gather data ¬†   \n\nstart->gather\n\n    \n\nend\n\n End   \n\nstore\n\n Store it ¬†¬†¬†¬†¬†¬†¬†¬† somewhere   \n\ngather->store\n\n   ¬†¬†¬†¬†¬†   \n\nclean\n\n Clean & ¬†¬†¬†¬†¬†¬†¬† pre-process   \n\nstore->clean\n\n   ¬†¬†¬†¬†¬†   \n\nbuild\n\n Build a  dataset   \n\nclean->build\n\n   ¬†¬†¬†¬†¬†   \n\neda\n\n Exploratory ¬†¬†¬† data analysis   \n\nbuild->eda\n\n    \n\nml\n\n Machine learning   \n\neda->ml\n\n   ¬†¬†¬†¬†¬†   \n\ninsight\n\n Obtain ¬†¬† insights   \n\nml->insight\n\n   ¬†¬†¬†¬†¬†   \n\ncommunicate\n\n Communicate results ¬†¬†¬†¬†¬†¬†¬†¬†   \n\ninsight->communicate\n\n   ¬†¬†¬†¬†¬†   \n\ncommunicate->end\n\n   \n\n\n\n\n\nThis course is about Machine Learning. So, in most examples and tutorials, we will assume that we already have good quality data."
  },
  {
    "objectID": "slides/week01_slides_part2.html#data-science-and-social-science",
    "href": "slides/week01_slides_part2.html#data-science-and-social-science",
    "title": "üóìÔ∏è Week 01Overview of core concepts",
    "section": "Data Science and Social Science",
    "text": "Data Science and Social Science\n\nIn reality, data scientists work as a multidisciplinary group, collaborating towards a common goal.\nContent borrowed from ME314 Day 1\n\n\n\nSocial science: The goal is typically explanation\nData science: The goal is frequently prediction, or data exploration\nMany of the same methods are used for both objectives\n\n\n\n\nCheck (Shmueli 2010) for a discussion about this topic."
  },
  {
    "objectID": "slides/week01_slides_part2.html#what-does-it-mean-to-learn-something",
    "href": "slides/week01_slides_part2.html#what-does-it-mean-to-learn-something",
    "title": "üóìÔ∏è Week 01Overview of core concepts",
    "section": "What does it mean to learn something?",
    "text": "What does it mean to learn something?\n\n\n\n\n\n\nImage created with the DALL¬∑E algorithm using the prompt: ‚Äò35mm macro photography of a robot holding a question mark card, white background‚Äô"
  },
  {
    "objectID": "slides/week01_slides_part2.html#predicting-a-sequence-intuitively",
    "href": "slides/week01_slides_part2.html#predicting-a-sequence-intuitively",
    "title": "üóìÔ∏è Week 01Overview of core concepts",
    "section": "Predicting a sequence intuitively",
    "text": "Predicting a sequence intuitively\n\n\nSay our data is the following simple sequence:  \\(6, 9, 12, 15, 18, 21, 24, ...\\) \nWhat number do you expect to come next? Why?\nIt is very likely that you guessed that \\(\\operatorname{next number}=27\\)\nWe spot that the sequence follows a pattern\nFrom this, we notice ‚Äî we learn ‚Äî that the sequence is governed by: \\(\\operatorname{next number} = \\operatorname{previous number} + 3\\)"
  },
  {
    "objectID": "slides/week01_slides_part2.html#predicting-a-sequence-formula",
    "href": "slides/week01_slides_part2.html#predicting-a-sequence-formula",
    "title": "üóìÔ∏è Week 01Overview of core concepts",
    "section": "Predicting a sequence (formula)",
    "text": "Predicting a sequence (formula)\nThe next number is a function of the previous one:\n \\[\n\\operatorname{next number} = f(\\operatorname{previous number})\n\\]"
  },
  {
    "objectID": "slides/week01_slides_part2.html#predicting-a-sequence-generic-formula",
    "href": "slides/week01_slides_part2.html#predicting-a-sequence-generic-formula",
    "title": "üóìÔ∏è Week 01Overview of core concepts",
    "section": "Predicting a sequence (generic formula)",
    "text": "Predicting a sequence (generic formula)\nIn general terms, we can represented it as:\n \\[\n\\operatorname{Y} = f(\\operatorname{X})\n\\] \n\nwhere:\n\n\\(Y\\): a quantitative response.  It goes by many names: dependent variable, response, target, outcome\n\\(X\\): a set of predictors,  also called inputs, regressors, covariates, features, independent variables.\n\\(f\\): the systematic information that \\(X\\) provides about \\(Y\\)"
  },
  {
    "objectID": "slides/week01_slides_part2.html#predicting-a-sequence-generic-formula-1",
    "href": "slides/week01_slides_part2.html#predicting-a-sequence-generic-formula-1",
    "title": "üóìÔ∏è Week 01Overview of core concepts",
    "section": "Predicting a sequence (generic formula)",
    "text": "Predicting a sequence (generic formula)\nIn general terms, we can represented it as:\n\n\\[\n\\operatorname{Y} = f(\\operatorname{X}) + \\epsilon\n\\]\n\nwhere:\n\n\\(Y\\): the output\n\\(X\\): a set of inputs\n\\(f\\): the systematic information that \\(X\\) provides about \\(Y\\)\n\\(\\epsilon~~\\): a random error term\n\n\nIn reality, there is some error \\(\\epsilon\\) that cannot be reduced."
  },
  {
    "objectID": "slides/week01_slides_part2.html#approximating-f",
    "href": "slides/week01_slides_part2.html#approximating-f",
    "title": "üóìÔ∏è Week 01Overview of core concepts",
    "section": "Approximating \\(f\\)",
    "text": "Approximating \\(f\\)\n\n\n\\(f\\) is almost always unknown\nWe aim to find an approximation (a model). Let‚Äôs call it \\(\\hat{f}\\)\nthat can then use it to predict values of \\(Y\\) for whatever \\(X\\).\nThat is: \\(\\hat{Y} = \\hat{f}(X)\\)"
  },
  {
    "objectID": "slides/week01_slides_part2.html#what-is-machine-learning",
    "href": "slides/week01_slides_part2.html#what-is-machine-learning",
    "title": "üóìÔ∏è Week 01Overview of core concepts",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\n\n\nStatistical learning, or Machine learning, refers to a set of approaches for estimating \\(f\\).\nEach algorithm you will learn on this course has its own way to determine \\(\\hat{f}\\) given data"
  },
  {
    "objectID": "slides/week01_slides_part2.html#types-of-learning",
    "href": "slides/week01_slides_part2.html#types-of-learning",
    "title": "üóìÔ∏è Week 01Overview of core concepts",
    "section": "Types of learning",
    "text": "Types of learning\n\nIn general terms, there are two main ways to learn from data:\n\n\nSupervised Learning\n\nEach observation (\\(x_i\\)) has an outcome associated with it (\\(y_i\\)).\nYour goal is to find a \\(\\hat{f}\\) that produces \\(\\hat{Y}\\) value close to the true \\(Y\\) values.\nOur focus on üóìÔ∏è Weeks 2, 3, 4 & 5.\n\n\nUnsupervised Learning\n\nYou have observations (\\(x_i\\)) but there is no response variable.\nYour goal is to find a \\(\\hat{f}\\), focused only on \\(X\\) that best represents the patterns in the data.\nOur focus on üóìÔ∏è Weeks 7 & 8."
  },
  {
    "objectID": "slides/week01_slides_part2.html#data-structure",
    "href": "slides/week01_slides_part2.html#data-structure",
    "title": "üóìÔ∏è Week 01Overview of core concepts",
    "section": "Data Structure",
    "text": "Data Structure\nLet‚Äôs go back to our example:\n\n\n\nOur simple sequence:\n \\(6, 9, 12, 15, 18, 21, 24\\) \n\n\n\nBecomes:\n\n\n\n\\(X\\)\n\\(Y\\)\n\n\n\n\n6\n9\n\n\n9\n12\n\n\n12\n15\n\n\n15\n18\n\n\n18\n21\n\n\n21\n24\n\n\n\n\n\n\nAnd for prediction:\n\n\n\n\n\n\n\n\\(X\\)\n\\(\\hat{Y}\\)\n\n\n\n\n24\n?\n\n\n\nwe present the \\(X\\) values and ask the fitted model to give us \\(\\hat{Y}\\).\n\n\n\n\nSame data but now in tabular format\na few other terms: - training data/test data - fitted model"
  },
  {
    "objectID": "slides/week01_slides_part2.html#the-ground-truth",
    "href": "slides/week01_slides_part2.html#the-ground-truth",
    "title": "üóìÔ∏è Week 01Overview of core concepts",
    "section": "The ground truth",
    "text": "The ground truth\nLet‚Äôs create a dataframe to illustrate the process of training an algorithm:\n\nlibrary(tidyverse)\n\ndf = tibble(X=as.integer(seq(6, 21, 3)),\n            Y=as.integer(seq(6+3, 21+3, 3)))\nprint(df)\n\n\n\n# A tibble: 6 √ó 2\n      X     Y\n  <int> <int>\n1     6     9\n2     9    12\n3    12    15\n4    15    18\n5    18    21\n6    21    24"
  },
  {
    "objectID": "slides/week01_slides_part2.html#adding-noise",
    "href": "slides/week01_slides_part2.html#adding-noise",
    "title": "üóìÔ∏è Week 01Overview of core concepts",
    "section": "Adding noise",
    "text": "Adding noise\nLet‚Äôs simulate the introduction of some random error:\n\n# Let's simulate some noise\ngaussian_noise = rnorm(n=nrow(df), mean=0, sd=1.5)\n\n# Call it \"observed Y\"\ndf$obsY = df$Y + gaussian_noise\nprint(df)\n\n\n\n# A tibble: 6 √ó 3\n      X     Y  obsY\n  <int> <int> <dbl>\n1     6     9  8.14\n2     9    12 10.6 \n3    12    15 16.5 \n4    15    18 14.5 \n5    18    21 19.8 \n6    21    24 24.2"
  },
  {
    "objectID": "slides/week01_slides_part2.html#visualizing-the-data",
    "href": "slides/week01_slides_part2.html#visualizing-the-data",
    "title": "üóìÔ∏è Week 01Overview of core concepts",
    "section": "Visualizing the data",
    "text": "Visualizing the data"
  },
  {
    "objectID": "slides/week01_slides_part2.html#visualizing-the-data-w-noise",
    "href": "slides/week01_slides_part2.html#visualizing-the-data-w-noise",
    "title": "üóìÔ∏è Week 01Overview of core concepts",
    "section": "Visualizing the data (w/ noise)",
    "text": "Visualizing the data (w/ noise)"
  },
  {
    "objectID": "slides/week01_slides_part2.html#visualizing-the-data-w-noise-1",
    "href": "slides/week01_slides_part2.html#visualizing-the-data-w-noise-1",
    "title": "üóìÔ∏è Week 01Overview of core concepts",
    "section": "Visualizing the data (w/ noise)",
    "text": "Visualizing the data (w/ noise)\n\n\n\n\n\n\n\nWhich line is closer to the ‚Äútruth‚Äù?"
  },
  {
    "objectID": "slides/week01_slides_part2.html#visualizing-the-data-w-noise-2",
    "href": "slides/week01_slides_part2.html#visualizing-the-data-w-noise-2",
    "title": "üóìÔ∏è Week 01Overview of core concepts",
    "section": "Visualizing the data (w/ noise)",
    "text": "Visualizing the data (w/ noise)\n\n\n\n\n\n\n\nHow much error can we accept?"
  },
  {
    "objectID": "slides/week01_slides_part2.html#assessing-error",
    "href": "slides/week01_slides_part2.html#assessing-error",
    "title": "üóìÔ∏è Week 01Overview of core concepts",
    "section": "Assessing error",
    "text": "Assessing error\nHow much error was introduced by \\(\\epsilon\\) per sample?\n\ndf$error    <- df$Y - df$obsY  # Calculate the error\ndf$absError <- abs(df$error)   # Ignore the sign of error\ndf\n\n\n\n# A tibble: 6 √ó 5\n      X     Y  obsY  error absError\n  <int> <int> <dbl>  <dbl>    <dbl>\n1     6     9  8.14  0.863    0.863\n2     9    12 10.6   1.41     1.41 \n3    12    15 16.5  -1.55     1.55 \n4    15    18 14.5   3.50     3.50 \n5    18    21 19.8   1.20     1.20 \n6    21    24 24.2  -0.195    0.195\n\n\n\nOn average, what is the error?\n\nmean(df$absError)\n\n[1] 1.452126\n\n\n\n\nThis measure is called the Mean Absolute Error."
  },
  {
    "objectID": "slides/week01_slides_part2.html#measures-of-error",
    "href": "slides/week01_slides_part2.html#measures-of-error",
    "title": "üóìÔ∏è Week 01Overview of core concepts",
    "section": "Measures of error",
    "text": "Measures of error\nThis is what we computed:\n\\[\n\\operatorname{MAE} = \\frac{\\sum_{i=1}^n{|(y_i + \\epsilon) - y_i|}}{n}\n\\]\n\n\nWe were able to compute this error because we knew what the ground truth \\(Y\\), we knew what its real value was.\nIt was only possible because it was a simulation, not real data.\nIn practice, we will almost never be able to assess the impact of \\(\\epsilon\\).\nWe will use this same way of thinking to assess how good and accurate our models are. üîú"
  },
  {
    "objectID": "slides/week01_slides_part2.html#references",
    "href": "slides/week01_slides_part2.html#references",
    "title": "üóìÔ∏è Week 01Overview of core concepts",
    "section": "References",
    "text": "References\n\n\nDavenport, Thomas. 2020. ‚ÄúBeyond Unicorns: Educating, Classifying, and Certifying Business Data Scientists.‚Äù Harvard Data Science Review 2 (2). https://doi.org/10.1162/99608f92.55546b4a.\n\n\nSchutt, Rachel, and Cathy O‚ÄôNeil. 2013. Doing Data Science. First edition. Beijing ; Sebastopol: O‚ÄôReilly Media. https://ebookcentral.proquest.com/lib/londonschoolecons/detail.action?docID=1465965.\n\n\nShah, Chirag. 2020. A Hands-on Introduction to Data Science. Cambridge, United Kingdom ; New York, NY, USA: Cambridge University Press. https://librarysearch.lse.ac.uk/permalink/f/1n2k4al/TN_cdi_askewsholts_vlebooks_9781108673907.\n\n\nShmueli, Galit. 2010. ‚ÄúTo Explain or to Predict?‚Äù Statistical Science 25 (3). https://doi.org/10.1214/10-STS330.\n\n\n\n\n\nDS202 - Data Science for Social Scientists ü§ñ ü§π"
  },
  {
    "objectID": "slides/week02_slides_part1.html#the-basic-models",
    "href": "slides/week02_slides_part1.html#the-basic-models",
    "title": "üóìÔ∏è Week 02:Linear Regression",
    "section": "The basic models",
    "text": "The basic models\n\nLinear regression is a simple approach to supervised learning.\n\n\n\n\nThe generic supervised model:\n\\[\nY = \\operatorname{f}(X) + \\epsilon\n\\]\nis defined more explicitly as follows ‚û°Ô∏è\n\n\n\n\nSimple linear regression\n\n\\[\n\\begin{align}\nY = \\beta_0 +& \\beta_1 X + \\epsilon, \\\\\n\\\\\n\\\\\n\\end{align}\n\\] \nwhen we use a single predictor, \\(X\\).\n\n\n\n\nMultiple linear regression\n\n\\[\n\\begin{align}\nY = \\beta_0 &+ \\beta_1 X_1 + \\beta_2 X_2 \\\\\n   &+ \\dots \\\\\n   &+ \\beta_p X_p + \\epsilon\n\\end{align}\n\\]\n\nwhen there are multiple predictors, \\(X_p\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nTrue regression functions are never linear!\nAlthough it may seem overly simplistic, linear regression is extremely useful both conceptually and practically.\n\n\n\n\n\n\n\n\n\nSee üì∫ Regression: Crash Course Statistics on YouTube for inspiration on how to present linear regression to students.  \nWe will talk about both types of models, how we can estimate the values of all \\(\\beta\\) and assess how good our models are."
  },
  {
    "objectID": "slides/week02_slides_part1.html#linear-regression-with-a-single-predictor",
    "href": "slides/week02_slides_part1.html#linear-regression-with-a-single-predictor",
    "title": "üóìÔ∏è Week 02:Linear Regression",
    "section": "Linear Regression with a single predictor",
    "text": "Linear Regression with a single predictor\n\n\nWe assume a model:\n\n\\[\nY = \\beta_0 + \\beta_1 X + \\epsilon ,\n\\]\n\n\n\n\n\n\n\n\n\n\nwhere:\n\n\\(\\beta_0\\): an unknown constant that represents the intercept of the line.\n\\(\\beta_1\\): an unknown constant that represents the slope of the line\n\\(\\epsilon\\): the random error term (irreducible)\n\n\n\n\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are also known as coefficients or parameters of the model."
  },
  {
    "objectID": "slides/week02_slides_part1.html#linear-regression-with-a-single-predictor-1",
    "href": "slides/week02_slides_part1.html#linear-regression-with-a-single-predictor-1",
    "title": "üóìÔ∏è Week 02:Linear Regression",
    "section": "Linear Regression with a single predictor",
    "text": "Linear Regression with a single predictor\n\n\nWe want to estimate:\n\\[\n\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} x\n\\]\n\n\n\n\n\n\n\n\n\nwhere:\n\n\\(\\hat{y}\\): is a prediction of \\(Y\\) on the basis of \\(X = x\\).\n\\(\\hat{\\beta_0}\\): is an estimate of the ‚Äútrue‚Äù \\(\\beta_0\\).\n\\(\\hat{\\beta_1}\\): is an estimate of the ‚Äútrue‚Äù \\(\\beta_1\\).\n\n\n\n\n\nThe hat symbol denotes an estimated value."
  },
  {
    "objectID": "slides/week02_slides_part1.html#different-estimators-different-equations",
    "href": "slides/week02_slides_part1.html#different-estimators-different-equations",
    "title": "üóìÔ∏è Week 02:Linear Regression",
    "section": "Different estimators, different equations",
    "text": "Different estimators, different equations\n\n\n\nThere are multiple ways to estimate the coefficients.\n\nIf you use different techniques, you might get different equations\nThe most common algorithm is called  Ordinary Least Squares (OLS)\nJust to name a few other estimators (Karafiath 2009):\n\nLeast Absolute Deviation (LAD)\nWeighted Least Squares (WLS)\nGeneralized Least Squares (GLS)\nHeteroskedastic-Consistent (HC) variants\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will only cover OLS in this course."
  },
  {
    "objectID": "slides/week02_slides_part1.html#the-concept-of-residuals",
    "href": "slides/week02_slides_part1.html#the-concept-of-residuals",
    "title": "üóìÔ∏è Week 02:Linear Regression",
    "section": "The concept of residuals",
    "text": "The concept of residuals\nSuppose you came across some data:\n\n\n\n\n\n\n\nFirst, let‚Äôs think of the concept of residuals‚Ä¶\n\n\nAnd you suspect there is a linear relationship between X and Y."
  },
  {
    "objectID": "slides/week02_slides_part1.html#the-concept-of-residuals-1",
    "href": "slides/week02_slides_part1.html#the-concept-of-residuals-1",
    "title": "üóìÔ∏è Week 02:Linear Regression",
    "section": "The concept of residuals",
    "text": "The concept of residuals\nSo, you decide to fit a line to it.\n\n\n\n\n\n\n\nA line that goes right through the middle of the cloud of data."
  },
  {
    "objectID": "slides/week02_slides_part1.html#the-concept-of-residuals-2",
    "href": "slides/week02_slides_part1.html#the-concept-of-residuals-2",
    "title": "üóìÔ∏è Week 02:Linear Regression",
    "section": "The concept of residuals",
    "text": "The concept of residuals\n\nResiduals are the distances from each data point to this line. \n\n\n\n\n\n\n\n\\(e_i\\)\\(=y_i-\\hat{y}_i\\) represents the \\(i\\)th residual"
  },
  {
    "objectID": "slides/week02_slides_part1.html#residual-sum-of-squares-rss",
    "href": "slides/week02_slides_part1.html#residual-sum-of-squares-rss",
    "title": "üóìÔ∏è Week 02:Linear Regression",
    "section": "Residual Sum of Squares (RSS)",
    "text": "Residual Sum of Squares (RSS)\nFrom this, we can define the  Residual Sum of Squares  (RSS) as\n\\[\n\\mathrm{RSS}= e_1^2 + e_2^2 + \\dots + e_n^2,\n\\]\n\nor equivalently as\n\\[\n\\mathrm{RSS}= (y_1 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_1)^2 + (y_2 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_2)^2 + \\dots + (y_n - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_n)^2.\n\\]\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe (ordinary) least squares approach chooses \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) to minimize the RSS.\n\n\n\n\n\nThat is how it does its job."
  },
  {
    "objectID": "slides/week02_slides_part1.html#a-question-for-you",
    "href": "slides/week02_slides_part1.html#a-question-for-you",
    "title": "üóìÔ∏è Week 02:Linear Regression",
    "section": "A question for you",
    "text": "A question for you\n\n\n\n\nWhy the squares and not, say, just the sum of residuals?\n\nImage created with the DALL¬∑E algorithm using the prompt: ‚Äò35mm macro photography of a robot holding a question mark card, white background‚Äô\n\n\n\n\nExplain that the sum penalizes individual large errors a lot more Consider adding a visualisation to illustrate this point."
  },
  {
    "objectID": "slides/week02_slides_part1.html#the-objective-function",
    "href": "slides/week02_slides_part1.html#the-objective-function",
    "title": "üóìÔ∏è Week 02:Linear Regression",
    "section": "The objective function",
    "text": "The objective function\nWe treat this as an optimisation problem. We want to minimize RSS: \\[\n\\begin{align}\n\\min \\mathrm{RSS} =& \\sum_i^n{e_i^2} \\\\\n             =& \\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2} \\\\\n             =& \\sum_i^n{\\left(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i\\right)^2}\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/week02_slides_part1.html#estimating-hatbeta_0",
    "href": "slides/week02_slides_part1.html#estimating-hatbeta_0",
    "title": "üóìÔ∏è Week 02:Linear Regression",
    "section": "Estimating \\(\\hat{\\beta}_0\\)",
    "text": "Estimating \\(\\hat{\\beta}_0\\)\nTo find \\(\\hat{\\beta}_0\\), we have to solve the following partial derivative:\n\\[\n\\frac{\\partial ~\\mathrm{RSS}}{\\partial \\hat{\\beta}_0}{\\sum_i^n{(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2}} = 0\n\\]\n\n‚Ä¶ which will lead you to:\n\n\n\\[\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x},\n\\]\n\n\nwhere we made use of the sample means:\n\n\\(\\bar{y} \\equiv \\frac{1}{n} \\sum_{i=1}^n y_i\\)\n\\(\\bar{x} \\equiv \\frac{1}{n} \\sum_{i=1}^n x_i\\)\n\n\n\n\nFull derivation if needed:\n\\[\n\\begin{align}\n0 &= \\frac{\\partial ~\\mathrm{RSS}}{\\partial \\hat{\\beta}_0}{\\sum_i^n{(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2}} & (\\text{chain rule})\\\\\n0 &= \\sum_i^n{-2 (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)} & (\\text{take $-2$ out})\\\\\n0 &= -2 \\sum_i^n{ (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)}  & (\\div -2) \\\\\n0 &=\\sum_i^n{ (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)}  & (\\text{sep. sums}) \\\\\n0 &=\\sum_i^n{y_i} - \\sum_i^n{\\hat{\\beta}_0} - \\sum_i^n{\\hat{\\beta}_1 x_i}  & (\\text{simplify}) \\\\\n0 &=\\sum_i^n{y_i} - n\\hat{\\beta}_0 - \\hat{\\beta}_1\\sum_i^n{ x_i}  & (+ n\\hat{\\beta}_0) \\\\\nn\\hat{\\beta}_0 &= \\sum_i^n{y_i} - \\hat{\\beta}_1\\sum_i^n{ x_i} & (\\text{isolate }\\hat{\\beta}_0 ) \\\\\n\\hat{\\beta}_0 &= \\frac{\\sum_i^n{y_i} - \\hat{\\beta}_1\\sum_i^n{ x_i}}{n} & (\\text{rearranging}) \\\\\n\\hat{\\beta}_0 &= \\frac{\\sum_i^n{y_i}}{n} - \\hat{\\beta}_1\\frac{\\sum_i^n{x_i}}{n} & (\\text{or simply}) \\\\\n\\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x} & \\blacksquare\n\\end{align}\n\\]\n\n\n\nüìù Give it a go! Pretend \\(\\hat{\\beta}_1\\) is constant and use the power rule to solve the equation and reach the same result."
  },
  {
    "objectID": "slides/week02_slides_part1.html#estimating-hatbeta_1",
    "href": "slides/week02_slides_part1.html#estimating-hatbeta_1",
    "title": "üóìÔ∏è Week 02:Linear Regression",
    "section": "Estimating \\(\\hat{\\beta}_1\\)",
    "text": "Estimating \\(\\hat{\\beta}_1\\)\nSimilarly, to find \\(\\hat{\\beta}_1\\) we solve:\n\\[\n\\frac{\\partial ~\\mathrm{RSS}}{\\partial \\hat{\\beta}_1}{[\\sum_i^n{y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i}]} = 0\n\\]\n\n‚Ä¶ which will lead you to:\n\n\n\\[\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})^2}\n\\]\n\n\n\nFull derivation if needed:\n\\[\n\\begin{align}\n0 &= \\frac{\\partial ~\\mathrm{RSS}}{\\partial \\hat{\\beta}_1}{\\sum_i^n{(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2}} & (\\text{chain rule})\\\\\n0 &= \\sum_i^n{\\left(-2x_i~ (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\\right)} & (\\text{take $-2$ out})\\\\\n0 &= -2\\sum_i^n{\\left( x_i~ (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\\right)} & (\\div -2) \\\\\n0 &= \\sum_i^n{\\left(x_i~ (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\\right)} & (\\text{distribute } x_i) \\\\\n0 &= \\sum_i^n{\\left(y_ix_i - \\hat{\\beta}_0x_i - \\hat{\\beta}_1 x_i^2\\right)} & (\\text{replace } \\hat{\\beta}_0) \\\\\n0 &= \\sum_i^n{\\left(y_ix_i - (\\bar{y} - \\hat{\\beta}_1 \\bar{x})x_i - \\hat{\\beta}_1 x_i^2\\right)} & (\\text{rearrange}) \\\\\n0 &= \\sum_i^n{\\left(y_ix_i - \\bar{y}x_i + \\hat{\\beta}_1 \\bar{x}x_i - \\hat{\\beta}_1 x_i^2\\right)} & (\\text{separate sums}) \\\\\n0 &= \\sum_i^n{\\left(y_ix_i - \\bar{y}x_i\\right)} + \\sum_i^n{\\left(\\hat{\\beta}_1 \\bar{x}x_i - \\hat{\\beta}_1 x_i^2\\right)} & (\\text{take $\\hat{\\beta}_1$ out}) \\\\\n0 &= \\sum_i^n{\\left(y_ix_i - \\bar{y}x_i\\right)} + \\hat{\\beta}_1\\sum_i^n{\\left(\\bar{x}x_i - x_i^2\\right)} & (\\text{isolate}) \\\\\n\\hat{\\beta}_1 &= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})^2}\n\\end{align}\n\\]\n\n\n\nüìù Give it a go! Use the same method as before to solve the equation and isolate \\(\\hat{\\beta}_1\\). Tip: Use the previous formula to substitute \\(\\hat{\\beta}_0\\)."
  },
  {
    "objectID": "slides/week02_slides_part1.html#parameter-estimation-ols",
    "href": "slides/week02_slides_part1.html#parameter-estimation-ols",
    "title": "üóìÔ∏è Week 02:Linear Regression",
    "section": "Parameter Estimation (OLS)",
    "text": "Parameter Estimation (OLS)\nAnd that is how OLS works!\n\\[\n\\begin{align}\n\\hat{\\beta}_1 &= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})^2} \\\\\n\\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/week02_slides_part1.html#estimates-for-multiple-regression",
    "href": "slides/week02_slides_part1.html#estimates-for-multiple-regression",
    "title": "üóìÔ∏è Week 02:Linear Regression",
    "section": "Estimates for Multiple Regression",
    "text": "Estimates for Multiple Regression\n\nThe process of estimation is similar when we have more than one predictor. To estimate:\n\\[\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\dots + \\hat{\\beta}_p x_p.\n\\]\n\n\nWe aim to minimize Residual Sum of Squares as before:\n\\[\n\\min \\mathrm{RSS} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_{i1} - \\hat{\\beta}_2 x_{i2} - \\dots - \\hat{\\beta}_p x_{ip})^2.\n\\]\nThis is done using standard statistical software ‚Äî you need a good linear algebra solver.\n\n\nThe values \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_p\\) that minimize RSS are the multiple least squares regression coefficient estimates."
  },
  {
    "objectID": "slides/week02_slides_part1.html#example-advertising-data",
    "href": "slides/week02_slides_part1.html#example-advertising-data",
    "title": "üóìÔ∏è Week 02:Linear Regression",
    "section": "Example: Advertising data",
    "text": "Example: Advertising data\n\n\nA sample of the data:\n\nlibrary(tidyverse)\n\nfile = \"https://www.statlearning.com/s/Advertising.csv\"\nadvertising <- read_csv(file) %>% select(-1)\nhead(advertising, 11)\n\n# A tibble: 11 √ó 4\n      TV radio newspaper sales\n   <dbl> <dbl>     <dbl> <dbl>\n 1 230.   37.8      69.2  22.1\n 2  44.5  39.3      45.1  10.4\n 3  17.2  45.9      69.3   9.3\n 4 152.   41.3      58.5  18.5\n 5 181.   10.8      58.4  12.9\n 6   8.7  48.9      75     7.2\n 7  57.5  32.8      23.5  11.8\n 8 120.   19.6      11.6  13.2\n 9   8.6   2.1       1     4.8\n10 200.    2.6      21.2  10.6\n11  66.1   5.8      24.2   8.6\n\n\n\nHow the data is spread:\n\nsummary(advertising$TV)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.70   74.38  149.75  147.04  218.82  296.40 \n\n\n\nsummary(advertising$radio)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   9.975  22.900  23.264  36.525  49.600 \n\n\n\nsummary(advertising$newspaper)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.30   12.75   25.75   30.55   45.10  114.00 \n\n\n\nsummary(advertising$sales)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.60   10.38   12.90   14.02   17.40   27.00"
  },
  {
    "objectID": "slides/week02_slides_part1.html#simple-linear-regression-models",
    "href": "slides/week02_slides_part1.html#simple-linear-regression-models",
    "title": "üóìÔ∏è Week 02:Linear Regression",
    "section": "Simple linear regression models",
    "text": "Simple linear regression models\n\n\n\nTV üì∫\n\n\ntv_model <- lm(sales ~ TV, data=advertising)\ncat(sprintf(\"Sales (1k units) = %.4f %+.4f TV ($ 1k)\\n\", \n            tv_model$coefficients[\"(Intercept)\"], \n            tv_model$coefficients[\"TV\"]))\n\nSales (1k units) = 7.0326 +0.0475 TV ($ 1k)\n\n\n\nRadio üìª\n\n\nradio_model <- lm(sales ~ radio, data=advertising)\ncat(sprintf(\"Sales (1k units) = %.4f %+.4f Radio ($ 1k)\\n\", \n            radio_model$coefficients[\"(Intercept)\"], \n            radio_model$coefficients[\"radio\"]))\n\nSales (1k units) = 9.3116 +0.2025 Radio ($ 1k)\n\n\n\n\nNewspaper üì∞\n\n\nnewspaper_model <- lm(sales ~ newspaper, data=advertising)\ncat(sprintf(\"Sales (1k units) = %.4f %+.4f Newspaper ($ 1k)\\n\", \n            newspaper_model$coefficients[\"(Intercept)\"], \n            newspaper_model$coefficients[\"newspaper\"]))\n\nSales (1k units) = 12.3514 +0.0547 Newspaper ($ 1k)\n\n\n\nüó®Ô∏è How should we interpret these models?\n\n\n\n\n\nGather answers from students.\nFor every 1k dollars spent in advertising on a particular media channel, we expect more \\(\\hat{\\beta}_1\\) thousand units of the product to be sold.\nWhy don‚Äôt the models agree about the intercept?"
  },
  {
    "objectID": "slides/week02_slides_part1.html#confidence-interval-of-coefficients",
    "href": "slides/week02_slides_part1.html#confidence-interval-of-coefficients",
    "title": "üóìÔ∏è Week 02:Linear Regression",
    "section": "Confidence Interval of coefficients",
    "text": "Confidence Interval of coefficients\n\n\n\nThe confidence interval of an estimate has the form: \\[\n\\hat{\\beta}_1 \\pm 2 \\times \\mathrm{SE}(\\hat{\\beta}_1).\n\\] where \\(SE\\) is the standard error and reflects how the estimate varies under repeated sampling.\n\n\n\n\nThat is, there is approximately a 95% chance that the interval \\[\n\\biggl[ \\hat{\\beta}_1 - 2 \\times \\mathrm{SE}(\\hat{\\beta}_1), \\hat{\\beta}_1 + 2 \\times \\mathrm{SE}(\\hat{\\beta}_1) \\biggr]\n\\] will contain the true value of \\(\\beta_1\\).\n\n\n\nHow SE differs from STD? See (Altman and Bland 2005)"
  },
  {
    "objectID": "slides/week02_slides_part1.html#standard-errors",
    "href": "slides/week02_slides_part1.html#standard-errors",
    "title": "üóìÔ∏è Week 02:Linear Regression",
    "section": "Standard Errors",
    "text": "Standard Errors\n\n\nThe standard error of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) is shown below:\n\\[\n\\begin{align}\n  \\mathrm{SE}(\\hat{\\beta}_1)^2 &= \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}, \\\\\n  \\mathrm{SE}(\\hat{\\beta}_0)^2 &= \\sigma^2 \\biggl[ \\frac{1}{n} +  \\frac{\\bar{x}^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\biggr],\n\\end{align}\n\\]\nwhere \\(\\sigma^2 = \\operatorname{Var}(\\epsilon)\\).\n\n\n\nBut, wait, we don‚Äôt know \\(\\epsilon\\)! How would we compute \\(\\sigma^2\\)?\nIn practice, we aproximate \\(\\sigma^2 \\approx \\mathrm{RSE} = \\sqrt{\\mathrm{RSS}/(n-2)}\\).\n\n\n\n\n\n\n\n\n\nImportant\n\n\nüí° Standard errors are a type of standard deviation but are not the same! See (Altman and Bland 2005) for more on this.\n\n\n\n\n\n\nThese formulas are only valid if we assume the errors \\(\\epsilon_i\\) have common variance \\(\\sigma^2\\) and are uncorrelated.\nRSE makes a comeback in Section 3.1.3"
  },
  {
    "objectID": "slides/week02_slides_part1.html#back-to-our-advertising-linear-models",
    "href": "slides/week02_slides_part1.html#back-to-our-advertising-linear-models",
    "title": "üóìÔ∏è Week 02:Linear Regression",
    "section": "Back to our Advertising linear models",
    "text": "Back to our Advertising linear models\nWhat are the confidence intervals of our independent linear models?\n\n\n\nTV üì∫\n\n\nconfint(tv_model)\n\n                 2.5 %     97.5 %\n(Intercept) 6.12971927 7.93546783\nTV          0.04223072 0.05284256\n\n\n\nRadio üìª\n\n\nconfint(radio_model)\n\n                2.5 %     97.5 %\n(Intercept) 8.2015885 10.4216877\nradio       0.1622443  0.2427472\n\n\n\n\nNewspaper üì∞\n\n\nconfint(newspaper_model)\n\n                  2.5 %      97.5 %\n(Intercept) 11.12595560 13.57685854\nnewspaper    0.02200549  0.08738071\n\n\n\nüó®Ô∏è What does it mean?\n\n\n\n\n\n\nFor every additional $1000 invested in Radio, we can expect an increase in sales of between 162 and 242 units.\n\n\n\nUse the function confint to compute confidence intervals in R."
  },
  {
    "objectID": "slides/week02_slides_part1.html#p-values",
    "href": "slides/week02_slides_part1.html#p-values",
    "title": "üóìÔ∏è Week 02:Linear Regression",
    "section": "p-values",
    "text": "p-values\n\nTo test the null hypothesis, we compute a t-statistic, given by \\[\nt = \\frac{\\hat{\\beta}_1 - 0}{\\mathrm{SE}(\\hat{\\beta}_1)},\n\\]\nThis will have a t-distribution1 with \\(n - 2\\) degrees of freedom, assuming \\(\\beta_1 = 0\\).\nUsing statistical software, it is easy to compute the probability of observing any value equal to \\(\\mid t \\mid\\) or larger.\nWe call this probability the p-value.\n\nü§î How are the t-distribution and the Normal distribution related? Check this link to find out."
  },
  {
    "objectID": "slides/week02_slides_part1.html#back-to-our-advertising-linear-models-1",
    "href": "slides/week02_slides_part1.html#back-to-our-advertising-linear-models-1",
    "title": "üóìÔ∏è Week 02:Linear Regression",
    "section": "Back to our Advertising linear models",
    "text": "Back to our Advertising linear models\nHow significant are the linear models?\n\n\n\nTV üì∫\n\n\nout <- capture.output(summary(tv_model))\ncat(paste(out[9:15]), sep=\"\\n\")\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 7.032594   0.457843   15.36   <2e-16 ***\nTV          0.047537   0.002691   17.67   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nRadio üìª\n\n\nout <- capture.output(summary(radio_model))\ncat(paste(out[9:15]), sep=\"\\n\")\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  9.31164    0.56290  16.542   <2e-16 ***\nradio        0.20250    0.02041   9.921   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nNewspaper üì∞\n\n\nout <- capture.output(summary(newspaper_model))\ncat(paste(out[9:15]), sep=\"\\n\")\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 12.35141    0.62142   19.88  < 2e-16 ***\nnewspaper    0.05469    0.01658    3.30  0.00115 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nüó®Ô∏è What does it mean?"
  },
  {
    "objectID": "slides/week02_slides_part1.html#references",
    "href": "slides/week02_slides_part1.html#references",
    "title": "üóìÔ∏è Week 02:Linear Regression",
    "section": "References",
    "text": "References\n\n\nAltman, Douglas G, and J Martin Bland. 2005. ‚ÄúStandard Deviations and Standard Errors.‚Äù BMJ 331 (7521): 903. https://doi.org/10.1136/bmj.331.7521.903.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York NY: Springer. https://www.statlearning.com/.\n\n\nKarafiath, Imre. 2009. ‚ÄúIs There a Viable Alternative to Ordinary Least Squares Regression When Security Abnormal Returns Are the Dependent Variable?‚Äù Review of Quantitative Finance and Accounting 32 (1): 17‚Äì31. https://doi.org/10.1007/s11156-007-0079-y.\n\n\n\n\n\nDS202 - Data Science for Social Scientists ü§ñ ü§π"
  },
  {
    "objectID": "slides/week02_slides_part2.html#residual-standard-errors-rse",
    "href": "slides/week02_slides_part2.html#residual-standard-errors-rse",
    "title": "üóìÔ∏è Week 02:Multiple Linear Regression",
    "section": "Residual Standard Errors (RSE)",
    "text": "Residual Standard Errors (RSE)\n\n\nRecall the ‚Äútrue model‚Äù: \\(Y = f(X) + \\epsilon\\)\nEven if we knew the true values of \\(\\beta_0\\) and \\(\\beta_1\\) ‚Äî not just the estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) ‚Äî our predictions of sales might still be off.\nBy how much?"
  },
  {
    "objectID": "slides/week02_slides_part2.html#residual-standard-errors-rse-1",
    "href": "slides/week02_slides_part2.html#residual-standard-errors-rse-1",
    "title": "üóìÔ∏è Week 02:Multiple Linear Regression",
    "section": "Residual Standard Errors (RSE)",
    "text": "Residual Standard Errors (RSE)\n\n\nThis can be estimated by the variance of errors: \\(\\sigma^2 = \\operatorname{Var}(\\epsilon)\\).\nAs said earlier, this quantity can be approximated, for the simple linear regression case, by the Residual Standard Errors (\\(\\mathrm{RSE}\\)) formula below:\n\n\n\n\\[\n\\sigma^2 \\approx \\mathrm{RSE} = \\sqrt{\\frac{\\mathrm{RSS}}{(n-\\mathrm{df})}}\n\\]\nwhere \\(\\mathrm{RSS} = \\sum_i^n{(y_i - \\hat{y}_i)^2}\\) represents the residual sum of squares and \\(\\mathrm{df}\\) represents the degrees of freedom in our model.\n\n\n\n\n‚û°Ô∏è It turns out that \\(\\mathrm{RSE}\\) is a good way to assess the goodness-of-fit of a model."
  },
  {
    "objectID": "slides/week02_slides_part2.html#back-to-our-advertising-linear-models",
    "href": "slides/week02_slides_part2.html#back-to-our-advertising-linear-models",
    "title": "üóìÔ∏è Week 02:Multiple Linear Regression",
    "section": "Back to our Advertising linear models",
    "text": "Back to our Advertising linear models\nLet‚Äôs compare the linear models we fitted earlier:\n\n\n\n\n\n\nTV üì∫\n\n\nout <- capture.output(summary(tv_model))\ncat(paste(out[16:16]), sep=\"\\n\")\n\nResidual standard error: 3.259 on 198 degrees of freedom\n\n\n\nRadio üìª\n\n\nout <- capture.output(summary(radio_model))\ncat(paste(out[16:16]), sep=\"\\n\")\n\nResidual standard error: 4.275 on 198 degrees of freedom\n\n\n\n\nNewspaper üì∞\n\n\nout <- capture.output(summary(newspaper_model))\ncat(paste(out[16:16]), sep=\"\\n\")\n\nResidual standard error: 5.092 on 198 degrees of freedom\n\n\n\nüó®Ô∏è What does it mean?"
  },
  {
    "objectID": "slides/week02_slides_part2.html#the-r2-statistic",
    "href": "slides/week02_slides_part2.html#the-r2-statistic",
    "title": "üóìÔ∏è Week 02:Multiple Linear Regression",
    "section": "The \\(R^2\\) statistic",
    "text": "The \\(R^2\\) statistic\n\nR-squared or fraction of variance explained is defined as:\n\n\\[\nR^2 = \\frac{\\mathrm{TSS - RSS}}{\\mathrm{TSS}} = 1 - \\frac{\\mathrm{RSS}}{\\mathrm{TSS}}\n\\]\nwhere TSS = \\(\\sum_{i=1}^n (y_i - \\bar{y})^2\\) is the total sum of squares. \n\n\n\n\n\n\n\nTip\n\n\nIntuitively, \\(R^2\\) measures the proportion of variability in \\(Y\\) that can be explained using \\(X\\).\n\n\\(R^2\\) close to 1 means that a large proportion of the variance in \\(Y\\) is explained by the regression.\n\\(R^2\\) close to 0 means that the regression does not explain much of the variability in \\(Y\\)."
  },
  {
    "objectID": "slides/week02_slides_part2.html#sample-correlation-coefficient",
    "href": "slides/week02_slides_part2.html#sample-correlation-coefficient",
    "title": "üóìÔ∏è Week 02:Multiple Linear Regression",
    "section": "Sample correlation coefficient",
    "text": "Sample correlation coefficient\nBy the way, in the simple linear regression setting, it can be shown that \\(R^2 = (\\operatorname{Cor}(X, Y))^2\\), where \\(\\operatorname{Cor}(X, Y)\\) is the correlation between \\(X\\) and \\(Y\\):\n\\[\n\\operatorname{Cor}(X, Y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}}.\n\\]\n\n\nüìù Give it a go! Play around with the definition of \\(R^2\\) shown in the previous slide and verify that \\(R^2 = (\\operatorname{Cor}(X, Y))^2\\)."
  },
  {
    "objectID": "slides/week02_slides_part2.html#f-statistic",
    "href": "slides/week02_slides_part2.html#f-statistic",
    "title": "üóìÔ∏è Week 02:Multiple Linear Regression",
    "section": "F-statistic",
    "text": "F-statistic\nWe used t-statistic to compute p-values for the coefficients (\\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\)).Now how do I test whether the model, as a whole, makes sense?\n\n\n\n\nFor this, we perform the hypothesis test: \\[\n\\begin{align}\n&~~~~H_0:&\\beta_1 = \\beta_2 = \\ldots = \\beta_j = 0 \\\\\n&\\text{vs} \\\\\n&~~~~H_A:& \\text{at least one } \\beta_j \\neq 0.\n\\end{align}\n\\]\n\n\n\n\nwhich is performed by computing the F-statistic: \\[\nF = \\frac{(TSS - RSS) / p}{RSS/(n - p - 1)} \\sim F_{p, n-p-1}\n\\]\n\n\n\n\n\nIf F is close to 1, there is no relationship between the response and the predictor(s).\nIf \\(H_A\\) is true, then we expect \\(F\\) to be greater than 1.\nCheck (James et al. 2021, 75‚Äì77) for an in-depth explanation of this test.\n\n\n\n\n\n\n\n\nNote that the F-statistic applies to both simple and multiple linear regression models.\nCheck this link if you want to understand the difference between the t-test and the the F-test."
  },
  {
    "objectID": "slides/week02_slides_part2.html#back-to-our-advertising-linear-models-1",
    "href": "slides/week02_slides_part2.html#back-to-our-advertising-linear-models-1",
    "title": "üóìÔ∏è Week 02:Multiple Linear Regression",
    "section": "Back to our Advertising linear models",
    "text": "Back to our Advertising linear models\nHow well do our models explain the variability of the response?\n\n\n\nTV üì∫\n\n\nout <- capture.output(summary(tv_model))\ncat(paste(out[17:18]), sep=\"\\n\")\n\nMultiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: < 2.2e-16\n\n\n\nRadio üìª\n\n\nout <- capture.output(summary(radio_model))\ncat(paste(out[17:18]), sep=\"\\n\")\n\nMultiple R-squared:  0.332, Adjusted R-squared:  0.3287 \nF-statistic: 98.42 on 1 and 198 DF,  p-value: < 2.2e-16\n\n\n\n\nNewspaper üì∞\n\n\nout <- capture.output(summary(newspaper_model))\ncat(paste(out[17:18]), sep=\"\\n\")\n\nMultiple R-squared:  0.05212,   Adjusted R-squared:  0.04733 \nF-statistic: 10.89 on 1 and 198 DF,  p-value: 0.001148\n\n\n\nüó®Ô∏è What does it mean?"
  },
  {
    "objectID": "slides/week02_slides_part2.html#a-multiple-linear-regression-to-advertising",
    "href": "slides/week02_slides_part2.html#a-multiple-linear-regression-to-advertising",
    "title": "üóìÔ∏è Week 02:Multiple Linear Regression",
    "section": "A multiple linear regression to Advertising",
    "text": "A multiple linear regression to Advertising\n\n\nWhen you run a linear model in R, you can call the summary function to see and check all of these statistics we‚Äôve covered so far.\nBy now, you should be able to understand its full output\n\n\nFitting all predictors:\n\n\nTV üì∫ + Radio üìª + Newspaper üì∞\n\nfull_model <- lm(sales ~ ., data=advertising)\nsummary(full_model)\n\n\nCall:\nlm(formula = sales ~ ., data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.938889   0.311908   9.422   <2e-16 ***\nTV           0.045765   0.001395  32.809   <2e-16 ***\nradio        0.188530   0.008611  21.893   <2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: < 2.2e-16\n\n\n\nConfidence Intervals\n\nconfint(full_model)\n\n                  2.5 %     97.5 %\n(Intercept)  2.32376228 3.55401646\nTV           0.04301371 0.04851558\nradio        0.17154745 0.20551259\nnewspaper   -0.01261595 0.01054097"
  },
  {
    "objectID": "slides/week02_slides_part2.html#interpreting-the-coefficients",
    "href": "slides/week02_slides_part2.html#interpreting-the-coefficients",
    "title": "üóìÔ∏è Week 02:Multiple Linear Regression",
    "section": "Interpreting the coefficients",
    "text": "Interpreting the coefficients\n\nRecall the multiple regression model:\n\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p + \\epsilon ,\n\\]\n\n\nWe interpret \\(\\beta_j\\) as the average effect on \\(Y\\) of a one unit increase in \\(X_j\\), holding all other predictors fixed. In the advertising example, the model becomes\n\n\\[\n\\mathrm{sales} = \\beta_0 + \\beta_1 \\times \\mathrm{TV} + \\beta_2 \\times \\mathrm{radio} + \\beta_3 \\times \\mathrm{newspaper} + \\epsilon .\n\\]"
  },
  {
    "objectID": "slides/week02_slides_part2.html#interpreting-the-coefficients-1",
    "href": "slides/week02_slides_part2.html#interpreting-the-coefficients-1",
    "title": "üóìÔ∏è Week 02:Multiple Linear Regression",
    "section": "Interpreting the coefficients",
    "text": "Interpreting the coefficients\n\n\nThe ideal scenario is when the predictors are uncorrelated ‚Äì a balanced design:\n\nEach coefficient can be estimated and tested separately.\nInterpretations such as ‚Äúa unit change in \\(X_j\\) is associated with a \\(\\beta_j\\) change in \\(Y\\), while all the other variables stay fixed‚Äù, are possible.\n\nCorrelations amongst predictors cause problems:\n\nThe variance of all coefficients tends to increase, sometimes dramatically\nInterpretations become hazardous ‚Äì when \\(X_j\\) changes, everything else changes.\n\nClaims of causality should be avoided for observational data."
  },
  {
    "objectID": "slides/week02_slides_part2.html#references",
    "href": "slides/week02_slides_part2.html#references",
    "title": "üóìÔ∏è Week 02:Multiple Linear Regression",
    "section": "References",
    "text": "References\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York NY: Springer. https://www.statlearning.com/.\n\n\n\n\n\nDS202 - Data Science for Social Scientists ü§ñ ü§π"
  },
  {
    "objectID": "slides/week03_slides_part1.html#classification",
    "href": "slides/week03_slides_part1.html#classification",
    "title": "üóìÔ∏è Week 03 Classifiers - Part I",
    "section": "Classification",
    "text": "Classification\n\n\nWe have so far only modelled quantitative responses.\nToday, we focus on predicting categorical, or qualitative, responses.\n\n\n\n\n\nThe generic supervised model:\n\\[\nY = \\operatorname{f}(X) + \\epsilon\n\\]\nstill applies, only this time \\(Y\\) is categorical. ‚û°Ô∏è\n\n\n\nOur categorical variables of interest take values in an unordered set \\(\\mathcal{C}\\), such as:\n\n\\(\\text{eye color} \\in \\mathcal{C} = \\{\\color{brown}{brown},\\color{blue}{blue},\\color{green}{green}\\}\\)\n\\(\\text{email} \\in \\mathcal{C} = \\{spam, ham\\}\\)\n\\(\\text{football results} \\in \\mathcal{C} \\{away\\ win,draw,home\\ win\\}\\)\n\n\n\n\n\nOpening slides - Unordered here is an important distinction. - We can also call it a class"
  },
  {
    "objectID": "slides/week03_slides_part1.html#why-cant-i-use-linear-regression",
    "href": "slides/week03_slides_part1.html#why-cant-i-use-linear-regression",
    "title": "üóìÔ∏è Week 03 Classifiers - Part I",
    "section": "Why can‚Äôt I use linear regression?",
    "text": "Why can‚Äôt I use linear regression?\n\n\nWhat if I just coded each category as a number?\n\\[\nY =\n    \\begin{cases}\n        1 &\\text{if}~\\color{brown}{brown},\\\\\n        2 &\\text{if}~\\color{blue}{blue},\\\\\n        3 &\\text{if}~\\color{green}{green}.\n    \\end{cases}\n\\]\n\nWhat could go wrong?\n\n\n\n\n\n\nHow would you interpret a particular prediction if your model returned:\n\n\\(\\hat{y} = ~~1.5\\) or\n\\(\\hat{y} = ~~0.1\\) or\n\\(\\hat{y} = 20.0\\)?\n\n\n\n\n\n\n\nKey takeaway: Regression is not suitable for all problems. - regression cannot accommodate a qualitative response with more than two classes - regression will not provide meaningful estaimtes of Pr(Y|X)"
  },
  {
    "objectID": "slides/week03_slides_part1.html#more-on-classification",
    "href": "slides/week03_slides_part1.html#more-on-classification",
    "title": "üóìÔ∏è Week 03 Classifiers - Part I",
    "section": "More on Classification",
    "text": "More on Classification\n\n\nOften we are more interested in estimating the probabilities that \\(X\\) belongs to each category in \\(\\mathcal{C}\\).\n\n\n\n\nFor example, it is sometimes more valuable to have an estimate of the probability that an insurance claim is fraudulent, than a classification fraudulent or not.\n\n\n\n\n\n\n\nA successful gambling strategy, for instance, requires placing bets on outcomes to which you believe the bookmakers have assigned incorrect probabilities. Knowing the most likely outcome is not enough!\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nStatistical models for ordinal response, when sets are discrete but have an order, are outside the scope of this course. Should you need to create models for ordinal variables, consult ‚Äúordinal logistic regression‚Äù. A good reference about this is (Agresti 2019, chap. 6).\n\n\n\n\n\nKey takeaway: normally, we estimate"
  },
  {
    "objectID": "slides/week03_slides_part1.html#speaking-of-probabilities",
    "href": "slides/week03_slides_part1.html#speaking-of-probabilities",
    "title": "üóìÔ∏è Week 03 Classifiers - Part I",
    "section": "Speaking of Probabilities‚Ä¶",
    "text": "Speaking of Probabilities‚Ä¶\nLet‚Äôs talk about three possible interpretations of probability:\n\n\n\n\nClassical\n\n\n\n\nFrequentist\n\n\n\n\nBayesian\n\n\n\n\n\nEvents of the same kind can be reduced to a certain number of equally possible cases.\nExample: coin tosses lead to either heads or tails \\(1/2\\) of the time ( \\(50\\%/50\\%\\))\n\n\n\n\nWhat would be the outcome if I repeat the process many times?\nExample: if I toss a coin \\(1,000,000\\) times, I expect \\(\\approx 50\\%\\) heads and \\(\\approx 50\\%\\) tails outcome.\n\n\n\n\nWhat is your judgement of the likelihood of the outcome? Based on previous information.\nExample: if I know that this coin has symmetric weight, I expect a \\(50\\%/50\\%\\) outcome.\n\n\n\n\n\n\nSource: (DeGroot and Schervish 2003)"
  },
  {
    "objectID": "slides/week03_slides_part1.html#speaking-of-probabilities-1",
    "href": "slides/week03_slides_part1.html#speaking-of-probabilities-1",
    "title": "üóìÔ∏è Week 03 Classifiers - Part I",
    "section": "Speaking of Probabilities‚Ä¶",
    "text": "Speaking of Probabilities‚Ä¶\nFor our purposes:\n\nProbabilities are numbers between 0 and 1\nThe sum of all possible outcomes of an event must sum to 1.\nIt is useful to think of things as probabilities\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nüí° Although there is no such thing as ‚Äúa probability of \\(120\\%\\)‚Äù or ‚Äúa probability of \\(-23\\%\\)‚Äù, you could still use this language to refer to increase or decrease in an outcome."
  },
  {
    "objectID": "slides/week03_slides_part1.html#the-logistic-regression-model",
    "href": "slides/week03_slides_part1.html#the-logistic-regression-model",
    "title": "üóìÔ∏è Week 03 Classifiers - Part I",
    "section": "The Logistic Regression model",
    "text": "The Logistic Regression model\n\nConsider a binary response:\n\\[\nY = \\begin{cases}\n0 \\\\\n1\n\\end{cases}\n\\]\n\n\nWe model the probability that \\(Y = 1\\) using the logistic function (aka. sigmoid curve):\n\\[\nPr(Y = 1|X) = p(X) = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1 X}}\n\\]\n\n\n\n\n\nSource of illustration: TIBCO\n\n\n\nThis is how this function looks like"
  },
  {
    "objectID": "slides/week03_slides_part1.html#the-logistic-function",
    "href": "slides/week03_slides_part1.html#the-logistic-function",
    "title": "üóìÔ∏è Week 03 Classifiers - Part I",
    "section": "The Logistic function",
    "text": "The Logistic function\n\nChanging \\(\\beta_0\\) while keeping \\(\\beta_1 = 1\\):"
  },
  {
    "objectID": "slides/week03_slides_part1.html#the-logistic-function-cont.",
    "href": "slides/week03_slides_part1.html#the-logistic-function-cont.",
    "title": "üóìÔ∏è Week 03 Classifiers - Part I",
    "section": "The Logistic function (cont.)",
    "text": "The Logistic function (cont.)\n\nKeep \\(\\beta_0 = 0\\) but vary \\(\\beta_1\\):"
  },
  {
    "objectID": "slides/week03_slides_part1.html#maximum-likelihood-estimate",
    "href": "slides/week03_slides_part1.html#maximum-likelihood-estimate",
    "title": "üóìÔ∏è Week 03 Classifiers - Part I",
    "section": "Maximum likelihood estimate",
    "text": "Maximum likelihood estimate\n\nAs with linear regression, the coefficients are unknown and need to be estimated from training data:\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1X}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}}\n\\]\n\n\nWe estimate these by maximising the likelihood function:\n\\[\n\\max \\ell(\\beta_0, \\beta_1) = \\prod_{i:y_i=1}{p(x_i)} \\prod_{i':y_{i'}=0} (1 - p(x_{i'})),\n\\]\nand we call this method the Maximum Likelihood Estimate (MLE).\n\n\n‚û°Ô∏è As usual, there are multiple ways to solve this equation!\n\n\n\nKey takeaway of this slide: MLE (logistic regression) is analogous to OLS (linear regression).\nIntuition: What are the values for \\(\\alpha\\) and \\(\\beta\\) that generate predicted probabilities, \\(\\hat{Y}_i\\) for each training observation that are as close as possible to the realised outcomes, \\(Y_i\\)?\n\n\n\nüí° If you want to read on how exactly this is solved check this link"
  },
  {
    "objectID": "slides/week03_slides_part1.html#solutions-to-mle",
    "href": "slides/week03_slides_part1.html#solutions-to-mle",
    "title": "üóìÔ∏è Week 03 Classifiers - Part I",
    "section": "Solutions to MLE",
    "text": "Solutions to MLE\n\n\nMLE is much more difficult to solve than the least squares formulations.\nMost solutions rely on a variant of the Hill Climbing algorithm\n\n\n\n\n\n\n\n\n\nHow do you find the latitude and longitude of a mountain peak if you can‚Äôt see very far?\n\n\n\nStart somewhere.\nLook around for the best way to go up.\nGo a small distance in that direction.\nLook around for the best way to go up.\nGo a small distance in that direction.\n\\(\\cdots\\)\n\n\n\n\n\n\n\nAdvanced: If for whatever random reason, you find yourself enamored with the Maximum Likelihood Estimate, check (Agresti 2019) for a recent take on the statistical properties of this method."
  },
  {
    "objectID": "slides/week03_slides_part1.html#the-concept-of-odds",
    "href": "slides/week03_slides_part1.html#the-concept-of-odds",
    "title": "üóìÔ∏è Week 03 Classifiers - Part I",
    "section": "The concept of odds",
    "text": "The concept of odds\nThe quantity below is called the odds:\n\\[\n\\frac{p(X)}{1 - p(X)} = e^{\\beta_0 + \\beta_1 X}\n\\]\n\n\n\n\n\n\nExample\n\n\nIf the odds are 9, then \\(\\frac{p(X)}{(1-p(X))} = 9 \\Rightarrow p(X) = 0.9\\).\nThis means that 9 out of 10 people will default.\n\n\n\n\n\n\n\n\n\nTip\n\n\nHow to interpret \\(\\beta_1\\)\nIf X increases one unit then the odds increase by a factor of \\(e^{\\beta_1}\\)\n\n\n\n\n\nüìù Give it a go! Using algebra, can you re-arrange the equation for \\(p(X)\\) presented in the Logistic regression model slides to arrive at the odds quantity shown above?"
  },
  {
    "objectID": "slides/week03_slides_part1.html#log-odds-or-logit",
    "href": "slides/week03_slides_part1.html#log-odds-or-logit",
    "title": "üóìÔ∏è Week 03 Classifiers - Part I",
    "section": "Log odds or logit",
    "text": "Log odds or logit\n\nIt is also useful to think of the odds in log terms.\n\n\\[\nlog\\left(\\frac{p(X)}{1 - p(X)}\\right) = \\beta_0 + \\beta_1 X\n\\]\n\nWe call the quantity above the log odds or logit\n\n\n\n\n\n\n\nTip\n\n\nHow to interpret \\(\\beta_1\\)\nIf X increases one unit then the log odds increase by \\(\\beta_1\\)"
  },
  {
    "objectID": "slides/week03_slides_part1.html#example-default-data",
    "href": "slides/week03_slides_part1.html#example-default-data",
    "title": "üóìÔ∏è Week 03 Classifiers - Part I",
    "section": "Example: Default data",
    "text": "Example: Default data\n\n\nA sample of the data:\n\nlibrary(ISLR2)\n\nhead(ISLR2::Default, n=15)\n\n   default student   balance    income\n1       No      No  729.5265 44361.625\n2       No     Yes  817.1804 12106.135\n3       No      No 1073.5492 31767.139\n4       No      No  529.2506 35704.494\n5       No      No  785.6559 38463.496\n6       No     Yes  919.5885  7491.559\n7       No      No  825.5133 24905.227\n8       No     Yes  808.6675 17600.451\n9       No      No 1161.0579 37468.529\n10      No      No    0.0000 29275.268\n11      No     Yes    0.0000 21871.073\n12      No     Yes 1220.5838 13268.562\n13      No      No  237.0451 28251.695\n14      No      No  606.7423 44994.556\n15      No      No 1112.9684 23810.174\n\n\n\nHow the data is spread:\n\nsummary(ISLR2::Default$default)\n\n  No  Yes \n9667  333 \n\n\n\nsummary(ISLR2::Default$balance)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    0.0   481.7   823.6   835.4  1166.3  2654.3 \n\n\n\nsummary(ISLR2::Default$income)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    772   21340   34553   33517   43808   73554 \n\n\n\nsummary(ISLR2::Default$student)\n\n  No  Yes \n7056 2944"
  },
  {
    "objectID": "slides/week03_slides_part1.html#simple-logistic-regression-models",
    "href": "slides/week03_slides_part1.html#simple-logistic-regression-models",
    "title": "üóìÔ∏è Week 03 Classifiers - Part I",
    "section": "Simple logistic regression models",
    "text": "Simple logistic regression models\n\n\n\nIncome üí∞\n\n\nincome_model <- \n  glm(default ~ income, data=ISLR2::Default, family=binomial)\ncat(sprintf(\"beta_0 = %.5f | beta_1 = %e\",\n            income_model$coefficients[\"(Intercept)\"],\n            income_model$coefficients[\"income\"]))\n\nbeta_0 = -3.09415 | beta_1 = -8.352575e-06\n\n\n\nBalance üí∏\n\n\nbalance_model <- \n  glm(default ~ balance, data=ISLR2::Default, family=binomial)\ncat(sprintf(\"beta_0 = %.5f | beta_1 = %.4f\",\n            balance_model$coefficients[\"(Intercept)\"],\n            balance_model$coefficients[\"balance\"]))\n\nbeta_0 = -10.65133 | beta_1 = 0.0055\n\n\n\n\nStudent üßë‚Äçüéì\n\n\nstudent_model <- \n  glm(default ~ student, data=ISLR2::Default, family=binomial)\ncat(sprintf(\"beta_0 = %.5f | beta_1 = %.4f\",\n            student_model$coefficients[\"(Intercept)\"],\n            student_model$coefficients[\"studentYes\"]))\n\nbeta_0 = -3.50413 | beta_1 = 0.4049\n\n\n\n\n\n\n\n\n\nNote\n\n\nLogistic regression coefficients are a bit trickier to interpret when compared to those of linear regression. Let‚Äôs look at how it works ‚û°Ô∏è\n\n\n\n\n\n\n\n\nGather answers from students."
  },
  {
    "objectID": "slides/week03_slides_part1.html#example-default-vs-balance",
    "href": "slides/week03_slides_part1.html#example-default-vs-balance",
    "title": "üóìÔ∏è Week 03 Classifiers - Part I",
    "section": "Example: Default vs Balance",
    "text": "Example: Default vs Balance\n\n\n\nModel: Default vs Balance üí∏\n\n\\[\n\\hat{y} = \\frac{e^{-10.65133 + 0.005498917X}}{1 + e^{-10.65133 + 0.005498917X}}\n\\]\n\n  That is:\n\\[\n\\begin{align}\n\\hat{\\beta}_0 &= -10.65133\\\\\n\\hat{\\beta}_1 &= 0.005498917\n\\end{align}\n\\]\n\n\n\n\n\nInterpreting \\(\\hat{\\beta}_0\\):\n\nIn the absence of balance information:\n\nLog odds: \\(-10.65133\\)\nOdds : \\(e^{-10.65133} = 2.366933 \\times 10^{-5}\\) \\[\n\\begin{align}\np(\\text{default}=\\text{Yes}) &= \\frac{\\text{odds}}{(1 + \\text{odds})} \\\\\n                           &= 2.366877 \\times 10^{-5}\\end{align}\n\\]\n\n\n\n\n\nInterpreting \\(\\hat{\\beta}_1\\):\n\nWith balance information:\n\nLog odds: \\(0.005498917\\)\nOdds : \\(e^{0.005498917} = 1.005514\\)\nThat is, for every \\(\\$1\\) increase in balance, the probability of default increases \n\n\n\n\n\n\n\nNote that the increase is cumulative, not linear. It depends on where X is."
  },
  {
    "objectID": "slides/week03_slides_part1.html#example-default-vs-income",
    "href": "slides/week03_slides_part1.html#example-default-vs-income",
    "title": "üóìÔ∏è Week 03 Classifiers - Part I",
    "section": "Example: Default vs Income",
    "text": "Example: Default vs Income\n\n\n\nModel: Default vs Income üí∞\n\n\\[\n\\hat{y} = \\frac{e^{-3.094149 - 8.352575 \\times 10^{-6} X}}{1 + e^{-3.094149 - 8.352575 \\times 10^{-6} X}}\n\\]\n\n  That is:\n\\[\n\\begin{align}\n\\hat{\\beta}_0 &= - 3.094149\\\\\n\\hat{\\beta}_1 &= - 8.352575 \\times 10^{-6}\n\\end{align}\n\\]\n\n\n\n\n\nInterpreting \\(\\hat{\\beta}_0\\):\n\nIn the absence of balance information:\n\nLog odds: \\(- 3.094149\\)\nOdds : \\(e^{- 3.094149} = 0.04531355\\) \\[\n\\begin{align}\np(\\text{default}=\\text{Yes}) &= \\frac{\\text{odds}}{(1 + \\text{odds})} \\\\\n                           &= 0.04334924 = 4.33\\%\\end{align}\n\\]\n\n\n\n\n\nInterpreting \\(\\hat{\\beta}_1\\):\n\nWith balance information:\n\nLog odds: \\(- 8.352575\\)\nOdds : \\(e^{- 8.352575} = 0.9999916\\)\nThat is, for every \\(\\$1\\) increase in income, the probability of default decreases \n\n\n\n\n\n\n\nNote that the increase is cumulative, not linear. It depends on where X is."
  },
  {
    "objectID": "slides/week03_slides_part1.html#example-default-vs-is-student",
    "href": "slides/week03_slides_part1.html#example-default-vs-is-student",
    "title": "üóìÔ∏è Week 03 Classifiers - Part I",
    "section": "Example: Default vs Is Student?",
    "text": "Example: Default vs Is Student?\n\n\n\nModel: Default vs Student üßë‚Äçüéì\n\n\\[\n\\hat{y} = \\frac{e^{-3.504128 + 0.4048871 X}}{1 + e^{-3.504128 + 0.4048871 X}}\n\\]\n\n  That is:\n\\[\n\\begin{align}\n\\hat{\\beta}_0 &= -3.504128\\\\\n\\hat{\\beta}_1 &= +0.4048871\n\\end{align}\n\\]\n\n\n\n\n\nInterpreting \\(\\hat{\\beta}_0\\):\n\nIn the absence of balance information:\n\nLog odds: \\(-3.504128\\)\nOdds : \\(e^{- 3.504128} = 0.03007299\\) \\[\n\\begin{align}\np(\\text{default}=\\text{Yes}) &= \\frac{\\text{odds}}{(1 + \\text{odds})} \\\\\n                           &= 0.02919501 \\approx 2.92\\%\\end{align}\n\\]\n\n\n\n\n\nInterpreting \\(\\hat{\\beta}_1\\):\n\nWith balance information:\n\nLog odds: \\(0.4048871\\)\nOdds : \\(e^{0.4048871} = 1.499133\\)\n\nIf person is a student, then the probability of default increases"
  },
  {
    "objectID": "slides/week03_slides_part1.html#model-info",
    "href": "slides/week03_slides_part1.html#model-info",
    "title": "üóìÔ∏è Week 03 Classifiers - Part I",
    "section": "Model info",
    "text": "Model info\nThe output of summary is similar to that of linear regression:\n\n\n\nModel: Default vs Balance üí∏\n\n\nsummary(balance_model)\n\n\nCall:\nglm(formula = default ~ balance, family = binomial, data = ISLR2::Default)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.2697  -0.1465  -0.0589  -0.0221   3.7589  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.065e+01  3.612e-01  -29.49   <2e-16 ***\nbalance      5.499e-03  2.204e-04   24.95   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1596.5  on 9998  degrees of freedom\nAIC: 1600.5\n\nNumber of Fisher Scoring iterations: 8\n\n\n\nConfidence Intervals\n\nconfint(balance_model)\n\n                    2.5 %       97.5 %\n(Intercept) -11.383288936 -9.966565064\nbalance       0.005078926  0.005943365"
  },
  {
    "objectID": "slides/week03_slides_part1.html#wraping-up-on-coefficients",
    "href": "slides/week03_slides_part1.html#wraping-up-on-coefficients",
    "title": "üóìÔ∏è Week 03 Classifiers - Part I",
    "section": "Wraping up on coefficients:",
    "text": "Wraping up on coefficients:\n\n\n\nPay attention to the sign of the coefficient. The sign of the coefficients indicate the direction of the association.\nIf the value of a predictor increases, we look at the sign of its coefficient:\n\nIf it is a ‚ûï positive coefficient, we predict an increase in the probability of the class\nIf it is a ‚ûñ negative coefficient, we predict a decrease in the probability of the class"
  },
  {
    "objectID": "slides/week03_slides_part1.html#multiple-logistic-regression-1",
    "href": "slides/week03_slides_part1.html#multiple-logistic-regression-1",
    "title": "üóìÔ∏è Week 03 Classifiers - Part I",
    "section": "Multiple Logistic Regression",
    "text": "Multiple Logistic Regression\n\nIt is straightforward to extend the logistic model to include multiple predictors:\n\n\\[\nlog \\left( \\frac{p(X)}{1-p(X)} \\right)=\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p\n\\]\n\\[\np(X) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p}}\n\\]\n\nMost things are still available (hypothesis test, confidence intervals, etc.)\nLet‚Äôs explore the output and summary of the full model ‚è≠Ô∏è"
  },
  {
    "objectID": "slides/week03_slides_part1.html#fitting-all-predictors-of-default",
    "href": "slides/week03_slides_part1.html#fitting-all-predictors-of-default",
    "title": "üóìÔ∏è Week 03 Classifiers - Part I",
    "section": "Fitting all predictors of Default",
    "text": "Fitting all predictors of Default\n\n\nFull Model\n\nfull_model <- glm(default ~ ., data=ISLR2::Default, family=binomial)\nsummary(full_model)\n\n\nCall:\nglm(formula = default ~ ., family = binomial, data = ISLR2::Default)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.4691  -0.1418  -0.0557  -0.0203   3.7383  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.087e+01  4.923e-01 -22.080  < 2e-16 ***\nstudentYes  -6.468e-01  2.363e-01  -2.738  0.00619 ** \nbalance      5.737e-03  2.319e-04  24.738  < 2e-16 ***\nincome       3.033e-06  8.203e-06   0.370  0.71152    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1571.5  on 9996  degrees of freedom\nAIC: 1579.5\n\nNumber of Fisher Scoring iterations: 8\n\n\n\nConfidence Intervals\n\nconfint(full_model)\n\n                    2.5 %        97.5 %\n(Intercept) -1.185902e+01 -9.928174e+00\nstudentYes  -1.109018e+00 -1.822147e-01\nbalance      5.294898e-03  6.204587e-03\nincome      -1.304712e-05  1.912447e-05"
  },
  {
    "objectID": "slides/week03_slides_part1.html#references",
    "href": "slides/week03_slides_part1.html#references",
    "title": "üóìÔ∏è Week 03 Classifiers - Part I",
    "section": "References",
    "text": "References\n\n\nAgresti, Alan. 2019. An Introduction to Categorical Data Analysis. Third edition. Wiley Series in Probability and Statistics. Hoboken, NJ: John Wiley & Sons.\n\n\nDeGroot, Morris H., and Mark J. Schervish. 2003. Probability and Statistics. 3. ed., international edition. Boston Munich: Addison-Wesley.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York NY: Springer. https://www.statlearning.com/.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDS202 - Data Science for Social Scientists ü§ñ ü§π"
  },
  {
    "objectID": "slides/week03_slides_part2.html#bayes-theorem",
    "href": "slides/week03_slides_part2.html#bayes-theorem",
    "title": "üóìÔ∏è Week 03:Classifiers - Part II",
    "section": "Bayes‚Äô Theorem",
    "text": "Bayes‚Äô Theorem\n\nBefore we go on to explain what Naive Bayes is about, we need to understand the formula below.\n\n\\[\nP(\\mathbf{Y} = k | \\mathbf{X} = x) = \\frac{P(k)P(\\mathbf{X}|\\mathbf{Y}=k)}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}\n\\]\n\n\nLet‚Äôs look at it step-by-step ‚è≠Ô∏è"
  },
  {
    "objectID": "slides/week03_slides_part2.html#bayes-theorem-1",
    "href": "slides/week03_slides_part2.html#bayes-theorem-1",
    "title": "üóìÔ∏è Week 03:Classifiers - Part II",
    "section": "Bayes‚Äô Theorem",
    "text": "Bayes‚Äô Theorem\n\\[\nP(\\mathbf{Y} = k | \\mathbf{X} = x) = \\frac{P(k)P(\\mathbf{X}|\\mathbf{Y}=k)}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}\n\\]\nNew variables\n\n\n\\(K \\Rightarrow\\) is the set of classes. In the binary case, \\(K = \\{0, 1\\}\\).\n\\(P(k) \\Rightarrow\\) is the probability that a random sample belongs to class \\(k\\).\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe textbook uses a slightly different notation."
  },
  {
    "objectID": "slides/week03_slides_part2.html#bayes-theorem-2",
    "href": "slides/week03_slides_part2.html#bayes-theorem-2",
    "title": "üóìÔ∏è Week 03:Classifiers - Part II",
    "section": "Bayes‚Äô Theorem",
    "text": "Bayes‚Äô Theorem\n\\[\n\\color{blue}{P(\\mathbf{Y} = k | \\mathbf{X} = x)} \\color{Gainsboro}{= \\frac{P(k)P(\\mathbf{X}|\\mathbf{Y}=k)}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}}\n\\]\n\n\nThe quantity above (in blue) is called the posterior distribution\nIt is what we are interested in when making inferences/predictions\n\n\n\nRead it as:\n\nWhat is the probability that the class is \\(k\\) given that the sample is \\(x\\)?"
  },
  {
    "objectID": "slides/week03_slides_part2.html#bayes-theorem-3",
    "href": "slides/week03_slides_part2.html#bayes-theorem-3",
    "title": "üóìÔ∏è Week 03:Classifiers - Part II",
    "section": "Bayes‚Äô Theorem",
    "text": "Bayes‚Äô Theorem\n\\[\n\\color{Gainsboro}{P(\\mathbf{Y} = k | \\mathbf{X} = x) =} \\frac{\\color{blue}{P(k)}\\color{Gainsboro}{P(\\mathbf{X}|\\mathbf{Y}=k)}}{\\color{Gainsboro}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}}\n\\]\n\n\nThe quantity above (in blue) is called the prior distribution\nIt represents the proportion of samples of class \\(k\\) we believe (estimate) we would find if sampling at random.\n\n\n\nRead it as:\n\nWhat is the probability that the class is \\(k\\) given a random sample?"
  },
  {
    "objectID": "slides/week03_slides_part2.html#bayes-theorem-4",
    "href": "slides/week03_slides_part2.html#bayes-theorem-4",
    "title": "üóìÔ∏è Week 03:Classifiers - Part II",
    "section": "Bayes‚Äô Theorem",
    "text": "Bayes‚Äô Theorem\n\\[\n\\color{Gainsboro}{P(\\mathbf{Y} = k | \\mathbf{X} = x) =} \\frac{\\color{Gainsboro}{P(k)}\\color{blue}{P(\\mathbf{X}|\\mathbf{Y}=k)}}{\\color{Gainsboro}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}}\n\\]\n\n\nThe quantity above (in blue) is often called the likelihood\nIt represents the density function of \\(\\mathbf{X}\\) for samples of class \\(k\\).\n\n\n\nThink of it as:\n\nWhat values would I expect \\(X\\) to take when the class is \\(\\mathbf{Y} = k\\)?"
  },
  {
    "objectID": "slides/week03_slides_part2.html#bayes-theorem-5",
    "href": "slides/week03_slides_part2.html#bayes-theorem-5",
    "title": "üóìÔ∏è Week 03:Classifiers - Part II",
    "section": "Bayes‚Äô Theorem",
    "text": "Bayes‚Äô Theorem\n\\[\n\\color{Gainsboro}{P(\\mathbf{Y} = k | \\mathbf{X} = x) =} \\frac{\\color{Gainsboro}{P(k)}\\color{Gainsboro}{P(\\mathbf{X}|\\mathbf{Y}=k)}}{\\color{blue}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}}\n\\]\n\n\nThe quantity above (in blue) represents the density function of \\(\\mathbf{X}\\) regardless of the class\nIt is often called the marginal probability of \\(\\mathbf{X}\\).\n\nNote that \\(\\color{blue}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}} = P(\\mathbf{X})\\)\n\n\n\n\nThink of it as:\n\nWhat values would I expect \\(X\\) if ignored the class?"
  },
  {
    "objectID": "slides/week03_slides_part2.html#bayes-theorem-6",
    "href": "slides/week03_slides_part2.html#bayes-theorem-6",
    "title": "üóìÔ∏è Week 03:Classifiers - Part II",
    "section": "Bayes‚Äô Theorem",
    "text": "Bayes‚Äô Theorem\n\\[\nP(\\mathbf{Y} = k | \\mathbf{X} = x) = \\frac{P(k)P(\\mathbf{X}|\\mathbf{Y}=k)}{\\sum_{l=1}^{K}{P(l)P(\\mathbf{X}|\\mathbf{Y}=l)}}\n\\]\n\nLet‚Äôs look at how different algorithms explore this rule ‚è≠Ô∏è"
  },
  {
    "objectID": "slides/week03_slides_part2.html#linear-discriminant-analysis-lda",
    "href": "slides/week03_slides_part2.html#linear-discriminant-analysis-lda",
    "title": "üóìÔ∏è Week 03:Classifiers - Part II",
    "section": "Linear Discriminant Analysis (LDA)",
    "text": "Linear Discriminant Analysis (LDA)\n\n\nAssumptions:\n\nLikelihood follows a Gaussian distribution\n\nEach class has its own mean, \\(\\mu_k\\)\nAll classes have the same standard deviation\n\nThat is, \\(\\sigma^2_1 = \\sigma^2_2 = \\ldots = \\sigma^2_K\\), or simply \\(\\sigma^2\\)\n\n\nWe denote this as: \\(P(\\mathbf{X}|\\mathbf{Y}=k) \\sim N(\\mu_k, \\sigma^2)\\)"
  },
  {
    "objectID": "slides/week03_slides_part2.html#lda---estimates",
    "href": "slides/week03_slides_part2.html#lda---estimates",
    "title": "üóìÔ∏è Week 03:Classifiers - Part II",
    "section": "LDA - Estimates",
    "text": "LDA - Estimates\n\nWe estimate the mean per class and the shared standard deviation as follows:\n\n\n\\[\n\\begin{align}\n\\hat{\\mu}_k &= \\frac{1}{n_k}\\sum_{i:y_i=k}{x_i}\\\\\n\\hat{\\sigma}^2 &= \\frac{1}{n - K}\\sum_{k=1}^K{\\sum_{i:y_i=k}{\\left(x_i - \\hat{\\mu}_k\\right)^2}} \\\\\n\\hat{P}(k) &= \\frac{n_k}{n}\n\\end{align}\n\\]\n\n\n\nwhere:\n\n\\(n\\) is the total number of training observations\n\\(n_k\\) is the number of training observations in the \\(k\\)th class\n\n\n\n\n\nRead (James et al. 2021, sec. 4.4) to understand why these estimates are the way they are.\n\n\n\n\nMention that priors could come from prior knowledge"
  },
  {
    "objectID": "slides/week03_slides_part2.html#naive-bayes-classifier-1",
    "href": "slides/week03_slides_part2.html#naive-bayes-classifier-1",
    "title": "üóìÔ∏è Week 03:Classifiers - Part II",
    "section": "Naive Bayes Classifier",
    "text": "Naive Bayes Classifier\n\n\nMain Assumption:\n\n\nWithin the \\(k\\)th class, the \\(p\\) predictors are independent\n\n\n\n\nAssuming features are not associated (not correlated), the likelihood becomes: \\[\nP(\\mathbf{X}|\\mathbf{Y}=k) = \\underbrace{P(x_1 |\\mathbf{Y}=k)}_{1\\text{st} \\text{ predictor}} \\times \\underbrace{P(x_2 |\\mathbf{Y}=k)}_{2\\text{nd} \\text{ predictor}} \\times \\ldots \\times \\underbrace{P(x_p |\\mathbf{Y}=k)}_{p\\text{-th} \\text{ predictor}}\n\\]\n\n\n\n\nThis means the posterior is given by: \\[\nP(\\mathbf{Y} = k| \\mathbf{X} = x) = \\frac{\\quad\\quad P(k) \\times P(x_1 |\\mathbf{Y}=k) \\times P(x_2 |\\mathbf{Y}=k) \\times \\ldots \\times P(x_p |\\mathbf{Y}=k)}{\\sum_{l=1}^K{P(l) \\times P(x_1 |\\mathbf{Y}=l) \\times P(x_2 |\\mathbf{Y}=l) \\times \\ldots \\times P(x_p |\\mathbf{Y}=l)}}\n\\]"
  },
  {
    "objectID": "slides/week03_slides_part2.html#a-naive-approach-indeed",
    "href": "slides/week03_slides_part2.html#a-naive-approach-indeed",
    "title": "üóìÔ∏è Week 03:Classifiers - Part II",
    "section": "A naive approach indeed",
    "text": "A naive approach indeed\n\n\nThis may all look very complicated but it is actually quite simple\n\n\n\n\nIf data is discrete (categorical), you just count the proportion of each category.\n\nExample:\n\\[\nP(\\mathbf{Y} = k| \\mathbf{X}_j = x_j) =\n\\begin{cases}\n0.32 & \\text{if } x_j = 1\\\\\n0.55 & \\text{if } x_j = 2\\\\\n0.13 & \\text{if } x_j = 3\n\\end{cases}\n\\]\n\n\n\nIf data is continuous, use a histogram as an estimate for the true density of \\(x_p\\)\n\nAlternatively, use a kernel density estimator"
  },
  {
    "objectID": "slides/week03_slides_part2.html#default-yes-or-no",
    "href": "slides/week03_slides_part2.html#default-yes-or-no",
    "title": "üóìÔ∏è Week 03:Classifiers - Part II",
    "section": "Default: Yes or No?",
    "text": "Default: Yes or No?\n\n\nWe have looked at how the probabilities (risk of default) change according to the value of predictors\nBut in practice we need to decide whether the risk is too high or tolerable\nIn our example, we might want to ask:\n\n\n\n\n‚ÄúWill this person default on their credit card? YES or NO?‚Äù"
  },
  {
    "objectID": "slides/week03_slides_part2.html#default-yes-or-no-1",
    "href": "slides/week03_slides_part2.html#default-yes-or-no-1",
    "title": "üóìÔ∏è Week 03:Classifiers - Part II",
    "section": "Default: Yes or No?",
    "text": "Default: Yes or No?\n\n\nHow would you classify the following customers?\n\n\nCode\nlibrary(tidyverse)\n\nfull_model <- \n  glm(default ~ ., data=ISLR2::Default, family=binomial)\n\nset.seed(40)\nsample_customers <- \n  ISLR2::Default %>% \n  slice(9986, 9908, 6848, 9762, 9979, 7438)\npred <- predict(full_model, sample_customers, type=\"response\")\n# Format it as percentage\nsample_customers$prediction <- \n  sapply(pred, function(x){sprintf(\"%.2f %%\", 100*x)})\nsample_customers\n\n\n  default student   balance   income prediction\n1      No      No  842.9494 39957.13     0.27 %\n2      No      No 1500.5721 39891.86    10.53 %\n3     Yes     Yes 1957.1203 18805.95    44.23 %\n4      No      No 1902.1499 35008.67    53.71 %\n5     Yes      No 2202.4624 47287.26    87.09 %\n6     Yes     Yes 2461.5070 11878.56    93.34 %\n\n\n\n\n\n\n\n\n\nImage created with the DALL¬∑E algorithm using the prompt: ‚Äò35mm macro photography of a robot holding a question mark card, white background‚Äô\n\n\nFull model expression: \\[\n\\hat{y} \\approxeq \\frac{e^{-10.87 - 0.65\\times\\text{student[Yes]} + 5.74 \\times 10^{-3}\\times\\text{balance} + 3\\times 10^{-6}\\times\\text{income}}}{1 + e^{-10.87 - 0.65\\times\\text{student[Yes]} + 5.74 \\times 10^{-3}\\times\\text{balance} + 3\\times 10^{-6}\\times\\text{income}}}\n\\]"
  },
  {
    "objectID": "slides/week03_slides_part2.html#default-yes-or-no-2",
    "href": "slides/week03_slides_part2.html#default-yes-or-no-2",
    "title": "üóìÔ∏è Week 03:Classifiers - Part II",
    "section": "Default: Yes or No?",
    "text": "Default: Yes or No?\n\n\nHow would you classify the following customers?\n\n\nCode\nlibrary(tidyverse)\n\nfull_model <- \n  glm(default ~ ., data=ISLR2::Default, family=binomial)\n\nset.seed(40)\nsample_customers <- \n  ISLR2::Default %>% \n  slice(9986, 9908, 6848, 9762, 9979, 7438)\npred <- predict(full_model, sample_customers, type=\"response\")\n# Format it as percentage\nsample_customers$prediction <- \n  sapply(pred, function(x){sprintf(\"%.2f %%\", 100*x)})\nsample_customers\n\n\n  default student   balance   income prediction\n1      No      No  842.9494 39957.13     0.27 %\n2      No      No 1500.5721 39891.86    10.53 %\n3     Yes     Yes 1957.1203 18805.95    44.23 %\n4      No      No 1902.1499 35008.67    53.71 %\n5     Yes      No 2202.4624 47287.26    87.09 %\n6     Yes     Yes 2461.5070 11878.56    93.34 %\n\n\n\n\nIf we set our threshold \\(= 50\\%\\), we get the following confusion matrix:\n\n\n\n\n\nActual\n\n\n\n\n\nPredicted\nNo\nYes\n\n\nNo\n2\n1\n\n\nYes\n1\n2\n\n\n\n\n\n\nIf we set our threshold \\(= 40\\%\\), we get the following confusion matrix:\n\n\n\n\n\nActual\n\n\n\n\n\nPredicted\nNo\nYes\n\n\nNo\n2\n0\n\n\nYes\n1\n3\n\n\n\n\n\n\n\n\n\nWhich of the two is more accurate?"
  },
  {
    "objectID": "slides/week03_slides_part2.html#thresholds",
    "href": "slides/week03_slides_part2.html#thresholds",
    "title": "üóìÔ∏è Week 03:Classifiers - Part II",
    "section": "Thresholds",
    "text": "Thresholds\n\nWhen making predictions about classes, we always have to make decisions.\nThresholds, applied to the predicted probability scores, are a way to decide whether to favour a particular class over another\n‚è≠Ô∏è Next, we will explore several metrics that can help us decide whether our classification model is good or bad."
  },
  {
    "objectID": "slides/week03_slides_part2.html#confusion-matrix",
    "href": "slides/week03_slides_part2.html#confusion-matrix",
    "title": "üóìÔ∏è Week 03:Classifiers - Part II",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\nLet‚Äôs take another look at the confusion matrix. We can think of the numbers in each cell as the following: \n\n\n\n\n\n\n\n\n\n\nActual\n\n\n\n\n\nPredicted\nNo\nYes\n\n\nNo\nTrue Negative (TN)\nFalse Negative (FN)\n\n\nYes\nFalse Positive (FP)\nTrue Positive (TP)\n\n\n\n\n\nIdeally, we would have no False Negatives and no False Positives but, of course, that is never the case."
  },
  {
    "objectID": "slides/week03_slides_part2.html#classification-metrics-1",
    "href": "slides/week03_slides_part2.html#classification-metrics-1",
    "title": "üóìÔ∏è Week 03:Classifiers - Part II",
    "section": "Classification metrics",
    "text": "Classification metrics\n\n\nIt is convenient to aggregate those quantities into a few other metrics\nTwo of the most common ones are called sensitivity and specificity\n\n\n\n\\[\n\\begin{align}\n\\text{Sensitivity} &= \\text{True Positive Rate (TPR)} = \\frac{TP}{P} \\\\\n\\text{Specificity} &= \\text{True Negative Rate (TNR)} = \\frac{TN}{N}\n\\end{align}\n\\]\n\n\n\nAnother common one is accuracy:\n\n\\[\n\\text{Accuracy} = \\frac{TP + TN}{P + N}\n\\]\n\n\nA good model has high sensitivity and high specificity and high accuracy.\n\n\n\nThere are many other ways to assess the results of a classification model"
  },
  {
    "objectID": "slides/week03_slides_part2.html#which-threshold-is-better",
    "href": "slides/week03_slides_part2.html#which-threshold-is-better",
    "title": "üóìÔ∏è Week 03:Classifiers - Part II",
    "section": "Which threshold is better?",
    "text": "Which threshold is better?\n\n\n\nüìù Now, looking at the logistic regression model we built for the entire dataset, work out the sensitivity, specificity and accuracy of the following confusion matrices:\n\n\n\n\n\n\n\nPractice\n\n\n\n‚è≤Ô∏è 5 min to work out the math\nüó≥Ô∏è Vote on your preferred threshold (on  Slack)\n\n\n\n\n\n\\(\\text{Threshold} = 50\\%\\):\n\n\n\n\nActual\n\n\n\n\n\nPredicted\nNo\nYes\n\n\nNo\n9627\n228\n\n\nYes\n40\n105\n\n\n\n\n\\(\\text{Threshold} = 40\\%\\):\n\n\n\n\nActual\n\n\n\n\n\nPredicted\nNo\nYes\n\n\nNo\n9588\n199\n\n\nYes\n79\n134"
  },
  {
    "objectID": "slides/week03_slides_part2.html#meet-the-roc-curve",
    "href": "slides/week03_slides_part2.html#meet-the-roc-curve",
    "title": "üóìÔ∏è Week 03:Classifiers - Part II",
    "section": "Meet the ROC curve",
    "text": "Meet the ROC curve\n\n\n\nThe Receiver Operating Characteristic (ROC) curve is another way to assess the model.\nIt shows how sensitivity and specificity change as we vary the threshold from 0 to 1 (threshold not shown).\n\n\n\n\n\n\n\n\nAsk: how would an ideal curve look like?"
  },
  {
    "objectID": "slides/week03_slides_part2.html#generalisation-problems",
    "href": "slides/week03_slides_part2.html#generalisation-problems",
    "title": "üóìÔ∏è Week 03:Classifiers - Part II",
    "section": "Generalisation problems",
    "text": "Generalisation problems\n\n\nThe data used to train algorithms is called training data\nOften, we want to use the fitted models to make predictions on new previously unseen data\n\n\n\n\n\n\n\n\n\nImportant\n\n\n‚ö†Ô∏è A model that performs well on training data will not necessarily perform well on new data ‚ö†Ô∏è\n\n\n\n\n\n\nTo make a robust assessment of our model, we have to split the data in two:\n\nthe training data and\nthe test data\n\nWe do NOT use the test data to fit the model\nWe will come back to this next week, this is the topic of üóìÔ∏è Week 04."
  },
  {
    "objectID": "slides/week03_slides_part2.html#inappropriate-reliance-on-metrics",
    "href": "slides/week03_slides_part2.html#inappropriate-reliance-on-metrics",
    "title": "üóìÔ∏è Week 03:Classifiers - Part II",
    "section": "Inappropriate reliance on metrics",
    "text": "Inappropriate reliance on metrics\n\n\nAccuracy can be very misleading when classes are imbalanced\nConsider the following model: \\(\\hat{y} = \\text{Yes}\\) (always)\n\nOnly \\(3\\%\\) of customers default on their credit cards\nTherefore, this model would have a \\(97\\%\\) accuracy!\nIt is correct ninety-seven percent of times. But is it a good model?\n\nüôÖ‚Äç‚ôÇÔ∏è NO!\n\n\nSimilarly, you have to ask yourself about the usefulness of any other metric\n\nIs True Positive Rate more or less important than True Negative Rate for the classification problem at hand?\nWhy? Why not?\n\nUltimately, it boils down to how you plan to use this model afterwards."
  },
  {
    "objectID": "slides/week03_slides_part2.html#references",
    "href": "slides/week03_slides_part2.html#references",
    "title": "üóìÔ∏è Week 03:Classifiers - Part II",
    "section": "References",
    "text": "References\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York NY: Springer. https://www.statlearning.com/.\n\n\n\n\n\nDS202 - Data Science for Social Scientists ü§ñ ü§π"
  }
]