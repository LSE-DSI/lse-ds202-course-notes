---
title: "ðŸ’» Week 08 - Lab Roadmap (90 min)"
date: 14 November 2022
date-meta: 14 November 2022
from: markdown+emoji
author: Dr. Stuart Bramwell/Dr. Jon Cardoso-Silva
editor: visual
---

**Context**

This lab session draws on [:spiral_calendar: Week 04 lecture/workshop](/weeks/week04/lecture.qmd) and [:spiral_calendar: Week 05 lecture](/weeks/week05/lecture.qmd) content.

It also reuses elements of **tidymodels** introduced in previous labs where we used functions from `library(broom)` or `library(rsample)`. All these packages are part of `tidymodels`. For the record, tidymodel's ['Get Started' tutorials](https://www.tidymodels.org/start/) are really good.

:::{.callout-important}
Download this page as an RMarkdown file from Moodle.
:::

**Topics**

More specifically, these are the things we will explore or revisit:

-   Difference between Classification vs Regression ([:spiral_calendar: Week 03 lecture](/weeks/week03/lecture.qmd))
-   [k-fold cross-validation](https://lse-dsi.github.io/lse-ds202-course-notes/weeks/week04/lecture.html#the-cross-validation-setup)
-   [Support Vector Machine (SVM)](https://lse-dsi.github.io/lse-ds202-course-notes/weeks/week05/lecture.html#part-ii---non-linear-algorithms-support-vector-machines-45-50-min)
-   Overfitting
-   Hyperparameters

<details><summary>**Setup**</summary>

## Setup

> :bulb: *Some of you mentioned that your R version cannot handle tidymodels. We recommend you update R to version 4.2.0 or above.*

**Packages you will need**

```{r, message=FALSE, warning=FALSE}
library('ISLR2')       # for the data
library('tidyverse')   # to use things like the pipe (%>%)
library('e1071')       # for SVM model
library('tidymodels')  # for model tuning, cross-validation etc.

# Vanity packages:
library('GGally')      # for pretty correlation plot
library('ggsci')       # for pretty plot colours
library('cvms')        # for pretty confusion matrix plots
```

</details>

<details><summary>The Data</summary>

## The Data

### :tangerine:Orange Juice

This week we will use a different ISLR2 dataset: `OJ` . We will perform a **classification** task with the goal to predict the `Purchase` column.

> *The data contains 1070 **purchases** where the customer either purchased Citrus Hill or Minute Maid Orange Juice. A number of characteristics of the customer and product are recorded.*

```{r}
ISLR2::OJ %>% head()
```

To understand what each variable represent, open the R Console, type the following and hit ENTER:

``` r
?ISLR2::OJ
```

**Which variables can help us distinguish the two different brands?**

To simplify our plots later on, let's focus on just two predictors:

```{r, message=FALSE, dpi=300}
plot_df <- ISLR2::OJ %>% select(Purchase, LoyalCH, PriceDiff)

g = (
  ggpairs(plot_df, aes(colour=Purchase))
  
  # Customizing the plot
  + scale_colour_startrek()
  + scale_fill_startrek()
  + theme_bw()
)
g 
```

## :chart_with_upwards_trend: Stock Market

We will also use the `Smarket` dataset from the ISLR2 package. We will perform a **regression** task with the goal to predict the percentage of return of the S&P 500 stock index on any given day, as represented by the `Today` column.

> *Daily percentage returns for the S&P 500 stock index between 2001 and 2005.*

```{r}
ISLR2::Smarket %>% head()
```

```{r, message=FALSE, dpi=300}
plot_df <- ISLR2::Smarket %>% select(Today, Volume, Lag1, Lag2)

g = (
  ggpairs(plot_df)
  
  # Customizing the plot
  + scale_colour_startrek()
  + scale_fill_startrek()
  + theme_bw()
)
g 
```

To understand what each variable represent, open the R Console, type the following and hit ENTER:

``` r
?ISLR2::Smarket
```
</details>

<details><summary>Step 1: SVM models for classification (25 min)</summary>

## Step 1: SVM models for classification (25 min)

SVM stands for **S**upport **V**ector **M**achines. Revisit [:spiral_calendar: Week 05 lecture](/weeks/week05/lecture.qmd) or Chapter 9 of our textbook to understand more about this algorithm.

R does not come with SVM, so we need to import it from a library. Let's start with the function `svm` we used in the Week 05 lecture, imported from the [`e1071` package](https://cran.r-project.org/web/packages/e1071/e1071.pdf). Here are a few things to know about the `svm` function:

-   The `svm` command is largely similar to other commands such as `lm` and `glm` in that the first parameter is an [R formula](https://www.datacamp.com/tutorial/r-formula-tutorial) and the second is the data set.
-   There are a few other options, but we will focus on specifying a radial kernel using `kernel = 'radial'`.
-   We *can* specify the type of machine learning task we are performing. Since we are doing classification, we use the option `type = 'C-classification'`.

### Step 1.1: Training the SVM model

```{r}
filtered_data <- ISLR2::OJ %>% select(Purchase, LoyalCH, PriceDiff)

orange_svm_model <- svm(Purchase ~ .,
                        data=filtered_data,
                        kernel='radial', 
                        type='C-classification')
orange_svm_model
```

:dart: **ACTION POINT**: What does the 'Number of Support Vectors' represent?

> your text here

:dart: **ACTION POINT**: What would happened if we changed `kernel` to `kernel="linear"`?

> your text here

### Step 1.2: Goodness-of-Fit of the SVM

Let's investigate how well our model fits the data. Let's reuse the model we trained (`orange_svm_model`) and predict the *same samples* we used to train it. To avoid modifying our original dataframe, let's save the output of the prediction in an auxiliary df (`plot_df`):

```{r, output=FALSE}
plot_df <- 
    filtered_data %>% 
    mutate(class_pred = predict(orange_svm_model, newdata = .))
head(plot_df)
```

**Add a `is_correct` column** to indicate whether the prediction was correct or not:

```{r}
plot_df <- plot_df %>% mutate(is_correct = class_pred == Purchase)
head(plot_df)
```

Remember what we did in the Decision Tree example last week!

#### Simple confusion matrix:

```{r, output=FALSE}
confusion_matrix <- 
    table(expected=plot_df$Purchase, class_pred=plot_df$class_pred)
print(confusion_matrix)
```

#### Nicer looking confusion matrix:

```{r, output=FALSE, message=FALSE}
plot_confusion_matrix(as_tibble(confusion_matrix), 
                      target_col = "expected", 
                      prediction_col = "class_pred",
                      
                      ## Customizing the plot
                      add_normalized = TRUE,
                      add_col_percentages = FALSE,
                      add_row_percentages = FALSE,
                      counts_col = "n",
                      )
```

:dart: **ACTION POINT**: How well does the model fit the data? What is your opinion?

#### Measure: Precision

Remember from Week 04 Lecture/Workshop (notebook can be found under:inbox_tray:W04 Lecture Files on Moodle):

> **PRECISION:** Given all predictions for a specific class, how many were True Positives? In other words, Precision = True Positives/(True Positives + False Positives).

Let's calculate the score for the CH class. That is, as if CH="Yes" and MM="No".

```{r}
## expected == CH & predicted == CH
total_correct_CH   <- confusion_matrix["CH", "CH"]  

## sum of samples predicted == CH
total_predicted_CH <- sum(confusion_matrix[, "CH"]) 

precision          <- total_correct_CH/total_predicted_CH
cat(sprintf("%.2f%%", 100*precision))
```

#### Measure: Recall

> Also called True Positive Rate = True Positive/(True Positives + False Negatives)

```{r}
## number of samples of brand CH
total_real_CH      <- sum(confusion_matrix["CH", ]) 

recall <- total_correct_CH/total_real_CH
cat(sprintf("%.2f%%", 100*recall))
```

#### Measure: F1-SCORE

> **F1-SCORE:** A combination of Precision and Recall
>
> $$
> \operatorname{F1-score} = \frac{2 \times \operatorname{Precision} \times \operatorname{Recall}}{(\operatorname{Precision} + \operatorname{Recall})}
> $$

```{r}
f1_score <- (2*precision*recall)/(precision + recall)

cat(f1_score)
```

### Step 1.3: Visualize the SVM decision space

Here we will demonstrate how you could simulate some data to cover the entire feature space of the data we are modelling. What do we mean by that? By inspecting `LoyalCH` and `PriceDiff`, we see the range values these variables can assume:

```{r}
filtered_data %>% 
    select(c(LoyalCH, PriceDiff)) %>%
    summary()
```

:bulb: We can simulate data to account for all possible combinations of `LoyalCH` and `PriceDiff`. We achieve this using `crossing`, another `tidyverse` function:

-   We feed `crossing` a sequence of numbers that range from the minimal and maximal values of both variables, incremented by 0.1 values.
-   We then create a new variable `class_pred` which uses the SVM model object to predict the brand of orange juice purchased by the customer
-   Note that we say `newdata = .` to indicate that we simply want to use the data set created with `crossing` as our new data.

```{r}
sim.data <- 
  crossing(LoyalCH   = seq(0,1,0.05),
           PriceDiff = seq(-1,1,0.1)) %>% 
  mutate(class_pred = predict(orange_svm_model, newdata = .))
head(sim.data)
```

The data above is all synthetic ("fake")! But it is very useful to colour the background of our plot.

We use `geom_tile` to show the area the SVM model identifies as Chinstrap penguins. We then use `geom_point` to overlay the actual data. Red dots in blue areas and vice versa indicate cases where the SVM model makes errors.

```{r, dpi=300}

g <- (
  plot_df %>%   
    ggplot()
  
    ## Tile the background of the plot with SVM predictions
    + geom_tile(data = sim.data, aes(x=LoyalCH, y=PriceDiff, fill = class_pred), alpha = 0.25)
  
    ## Actual data
    + geom_point(aes(x=LoyalCH, y=PriceDiff, colour = Purchase, shape = is_correct), size=2.5, stroke=0.95, alpha=0.7)
  
    ## Define X and Os
    + scale_shape_manual(values = c(4, 1))
    
    ## (OPTIONAL) Customizing the colours and theme of the plot
    + scale_x_continuous(labels=scales::percent)
    + scale_colour_startrek()
    + scale_fill_startrek()
    + theme_minimal()
    + theme(panel.grid = element_blank(), legend.position = 'bottom', plot.title = element_text(hjust = 0.5))
    + labs(x = 'Customer brand loyalty for CH', y = 'Sale price of MM less sale price of CH', fill = 'Brand', colour = 'Brand', shape = 'Correct prediction?', title = sprintf('Overall Training Accuracy = %.2f %%', 100*(sum(plot_df$correct)/nrow(plot_df))))
)

g
```

:handshake: **WORKING TOGETHER** In pairs, discuss what you see in the plot:

-   What do the shape of dots represent? The `X` and `O`s?

> your text here

-   What do the colours of the dots represent?

> your text here

-   What do the background colour in the plot represent?

> your text here

-   Can you point in the plot roughly which dots you would expect to be the support vectors?

</details>

<details><summary>Step 2: Doing the same with `tidymodels` (15 min)</summary>

## Step 2: Doing the same with `tidymodels` (15 min)

The function `svm` from `library(e1071)` package is not the only way to run SVM in R. The package `parsnip` also have its own SVM functions. The functionality is roughly the same but there are differences in how you write the code.

*`parsnip` already comes installed in `tidymodels`, so we do not need to import or install anything else.*

### Step 2.1 Training the SVM model

We specify a **radial basis function** SVM (see the part about kernels in the [:spiral_calendar: Week 05 lecture](/weeks/week05/lecture.qmd)) with the function `svm_rbf`.

In the spirit of `tidyverse`, we pipe the SVM algorithm into the `fit` function, where we can define the R formula like we have been doing with other algorithms:

```{r}

orange_tidymodel <-
  svm_rbf() %>% 
  set_mode('classification') %>% 
  fit(Purchase ~ ., data = filtered_data)

orange_tidymodel
```

:dart: **ACTION POINT**: Compare the output above to that of another colleague. Why don't you get the exact same output?

> your personal notes go here

:bulb: If you want to try different kernels, you will need to replace `svm_rbf()` by `svm_linear()` or `svm_poly()`.

### Step 2.2: Goodness-of-Fit of the SVM

Let's investigate how well our model fits the data. Let's reuse the model we trained (`orange_tidymodel`) and predict the *same samples* we used to train it.

Function `augment(<model>, <df>)` of `tidymodels` applies a model to a dataframe and return the same data plus a few columns:

```{r, output=FALSE}
plot_df <- augment(orange_tidymodel, filtered_data)
head(plot_df)
```

:dart: **ACTION POINT**: How is the `plot_df` data frame above different to the first plot_df we created in **Step 1.2**?

> your notes go here

**Add a `is_correct` column** to indicate whether the prediction was correct or not:

```{r}
plot_df <- plot_df %>% mutate(is_correct = .pred_class == Purchase)
head(plot_df)
```

#### Measure: Precision

You don't need to calculate precision by hand, just use the `precision()` function from tidymodels:

```{r}
plot_df %>% precision(Purchase, .pred_class) %>% head()
```

#### Measure: Recall

You don't need to calculate recall by hand, just use the `recall()` function from tidymodels:

```{r}
plot_df %>% recall(Purchase, .pred_class) %>% head()
```

#### Measure: F1-score

You don't need to calculate F1-score by hand, just use the `f_meas()` function from tidymodels:

```{r}
plot_df %>% f_meas(Purchase, .pred_class) %>% head()
```

#### (Optional) ROC curve

Plot the ROC curve for class `Purchase=="CH"` :

```{r}
plot_df %>% 
  roc_curve(Purchase, .pred_CH) %>% 
  autoplot
```

#### :house: **Take-home exercise Q1:**

Edit the cell below modifying `event_level` from `"second"` to `"first"`. Why do you get different results? What do you think is going on?

``` r
plot_df %>% f_meas(Purchase, .pred_class, event_level=...)
```

:bulb:Tip: Read the documentation of `f_meas` to understand what `event_level` represents. (Type `?f_meas`)

:bulb: **Gold** **Tip**: note the **Levels** of the `factor` variable called `Purchase`:

``` r
plot_df$Purchase
```

#### :house: **Take-home exercise Q2:**

Create a plot of the confusion matrix for the `orange_tidymodel` like we did in Step 1.

```{r}

## your code goes here
```

#### :house: **Take-home exercise Q3:**

Create a plot of SVM decision space for the `orange_tidymodel` like we did in Step 1.

```{r}

## your code goes here
```

:dart: **ACTION POINT**: If you were to run the SVM algorithm by yourself in another dataset, which version would you prefer, the one in Step 1 or the one in Step 2?

> your notes go here

</details>

<details><summary>Step 3: What about regression? (10 min)</summary>

## Step 3.1: Train the model

Let's select just the predictors `Volume` and `Lag1` and fit a regression model to predict `Today`:

```{r}
# Remove Direction, otherwise we would be "cheating" 
filtered_data <- ISLR2::Smarket %>% select(Today, Volume, Lag1)

smarket_tidymodel <-
  svm_rbf() %>% 
  set_mode('regression') %>% 
  fit(Today ~ ., data = filtered_data)

smarket_tidymodel
```

## Step 3.2: Goodness-of-Fit of the SVM

Since the target variable is continuous, not discrete, we cannot plot confusion matrix nor anything like that. We will have to go back to the idea of residuals ([:spiral_calendar: Week 02 Lecture](https://lse-dsi.github.io/lse-ds202-course-notes/weeks/week02/lecture.html) & [:computer: Week 03 - Lab](https://lse-dsi.github.io/lse-ds202-course-notes/weeks/week03/lab.html)).

```{r}
plot_df <- 
  augment(smarket_tidymodel, filtered_data) %>% 
  mutate(row_number=row_number()) # adding this here just to make our plot easier
plot_df %>% head()
```

:dart: **ACTION POINT**: What do the different columns mean?

> your text go here

```{r}
g <- (
  ggplot(plot_df, aes(x=row_number, y=.resid, colour=abs(.resid)))
  + geom_point(alpha=0.6)
  
  + theme_bw()
  + scale_colour_viridis_b(name="Absolute residual")
)

g
```

### Measure: Mean Absolute Error (MAE)

$$
MAE = \frac{\sum_{i=1}^{n}{|y_i - \hat{y}_i|}}{n}
$$

```{r}
plot_df %>% mae(Today, .pred)
```

### Measure: Root Mean Squared Error (RMSE)

$$
RMSE = \frac{\sum_{i=1}^{n}{(y_i - \hat{y}_i)^2}}{n}
$$

```{r}
plot_df %>% rmse(Today, .pred)
```

:dart: **ACTION POINT**: Would a better model have a larger or smaller value of MAE/RMSE?

> your text go here

### :house: **Take-home exercise Q4 (might be challenging):**

Try to replicate the visualization of the SVM Decision Space (Step 1.3) for the `Smarket` data.

```{r}
# your code goes here
```

:bulb:Tip: Remove the `scale_â€¦` functions, otherwise it will throw an error.

:bulb:Tip: After constructing the graph for the first time, try adding `scale_fill_viridis_c()` and `scale_colour_viridis_c()` to visualize predictions.

</details>

<details><summary>Step 4: k-fold cross validation with `tidymodels` (40 min)</summary>

## Step 4: k-fold cross validation with `tidymodels` (40 min)

Here we will replicate the cross-validation setup used to generate the structure [:spiral_calendar: Week 04 lecture/workshop](/weeks/week04/lecture.qmd).

**:previous_track_button:Recap:**

-   We also explored training vs test splits, albeit in a different way, in [Step 2.1](https://lse-dsi.github.io/lse-ds202-course-notes/weeks/week04/lab.html#step-2.1-separate-some-data-just-for-training) of [:computer: Week 04 - Lab](https://lse-dsi.github.io/lse-ds202-course-notes/weeks/week04/lab.html)

-   k-fold cross-validation was also present in [Step 2](https://lse-dsi.github.io/lse-ds202-course-notes/weeks/week05/lab.html#step-2-k-fold-cross-validation) of [:computer: Week 05 - Lab](https://lse-dsi.github.io/lse-ds202-course-notes/weeks/week05/lab.html)

-   Cross-validation was also mentioned in the [:spiral_calendar: Week 05 lecture](/weeks/week05/lecture.qmd), when you were introduced the problem of **overfitting**.

*But how exactly can cross-validation help overcome overfitting?* This is what we will explore in this section of the lab.

### Step 4.1: Create training / test split

We start by creating a training / test split using the functions `initial_split` packages for :chart_with_upwards_trend: `SMarket` data.

```{r}

set.seed(123)

default_split <- initial_split(filtered_data, prop = 0.75, strata = Today)

internal_validation_set <- training(default_split)

external_validation_set <- testing(default_split)

```

How many samples are in the `internal validation set`?

```{r}
nrow(internal_validation_set)
```

How many samples were left in the `external validation set`?

```{r}
nrow(external_validation_set)
```

The external validation set will only be used at the end

### Step 4.2: Create resampling folds for cross validation

Next, let's create **10-fold** cross-validation data using `internal_validation_set`. We can achieve this by using the `vfold_cv` command, specifying `v = 10`.

```{r}
k_folds <- vfold_cv(internal_validation_set, v = 10)
k_folds

```

*Notice anything odd? The output is a tibble but the first column `splits` is a series of lists. This is another thing that makes tibbles different to data frames - you can nest lists within tibbles but not data frames. We will use this more explicitly in the next lab when we build k-means clustering models.*

### Step 4.3: **:fried_egg:**Specifying a recipe

The next step is to create a recipe. Luckily, `recipe` function takes the same values as the `lm` model! We first create a formula `default ~ .` and then use `data = internal_validation_set`. Printing `smarket_recipe`, we have one outcome and three predictors.

```{r}

smarket_recipe <- recipe(Today ~ ., data = internal_validation_set)
smarket_recipe

```

### Step 4.4: Specify a model

Next, we will specify a support vector machine model. Here's where things get a bit more involved.

We specify a radial basis function SVM. `svm_rbf` takes two **hyperparameters**: `cost` and `rbf_sigma`. Instead of specifying a single value for each, we will instead set them equal to `tune()`. This indicates that we want to try a range of different values.

```{r}
svm_regressor <-
  svm_rbf(cost = tune(), rbf_sigma = tune()) %>% 
  set_mode('regression') 

```

### Step 4.5: Create a hyperparameter grid

Which values for `cost` and `rbf_sigma` should we choose? It is often hard to tell, so instead we can experiment with different values.

We can use `grid_regular` to create a tibble of different hyperparameter combinations. `levels = 5` indicates that we want to try out five different values for each hyperparameter.

```{r}

set.seed(234)

svm_grid <- grid_regular(cost(), rbf_sigma(), levels = 5)
svm_grid

```

### Step 4.6: Perform cross-validation

We now have all we need to run cross-validation, and the function `tune_grid` puts everything together. Let's think intuitively what this command is doing.

-   We are telling `tune_grid` that we want to run a classification model on a recipe using different **resampling** folds.

-   Instead of specifying **hyperparameter** values we want to run a combination of different values.

-   After this, we want to choose a metric to evaluate different combinations.

-   We opt for `rmse` but we can specify several metrics however using the `metric_set` command.

```r

smarket_tuned <-
  tune_grid(object = svm_regressor,
            preprocessor = smarket_recipe,
            resamples = k_folds,
            grid = svm_grid,
            metrics = metric_set(rmse),
            control = control_grid(save_pred=TRUE))

```

### Step 4.7: Which combination of hyperparameters works best?

Now we have tuned our models, let's find out which combination of hyperparameters work best. We can create a ggplot easily using the `autoplot` command.

```r

smarket_tuned %>% 
  autoplot() +
  theme_minimal() +
  ggsci::scale_color_jco() +
  labs(y = 'RMSE', colour = 'Sigma')

```

:dart: **ACTION POINT**: Can you explain what we see in the plot above?

> your text go here

We can use the function `select_best` to identify the hyperparameter combination that leads to the highest precision.

```r

select_best(smarket_tuned)

```

</details>

<details><summary>:house: **Take-home exercises**</summary>

## :house: **Take-home exercises**

Get ready for the next summative problem set by answering the following questions:

### Q5: Re-run Step3 (Regression)

Build a standalone SVM model (`tidymodels` version) on the same data we used in Step 3, only this time set the parameters of the SVM to the optimal parameters identified in Step 4.

```{r}
## your code goes here
```

### Q6: Predict the external validation set

Use the model you built in Q5 and make predictions on the external validation set. How does the RMSE of these predictions compare to the RMSE of the internal validation set?

```{r}
## your code goes here
```

### Q7: SVM Decision Space (Regression)

Do the same thing you did to answer Q4, only this time using the model from Q5.

```{r}
## your code goes here
```

### Q8: Compare Q4 x Q7 plots

Explain if and how the decision space you obtained in Q8 differs from the one in Q4.

> your text goes here

### Q9: Grid search for Orange Juice (Classification)

-   For this task, you will use the Orange Juice data set (`ISLR2::OJ`) with ALL the predictors included

-   Replicate the entire procedure of Step 4, making the necessary adjustments to predict `Purchase`

:bulb:You will have two tweak at least two main things: `metric_set` and `set_mode`

```{r}
## your code goes here
```

### Q10: Challenge

Replicate the same steps above using different SVM kernels (linear, polynomial, etc.)

```{r}
# your code goes here
```

## Final step:

*You don't need to submit this file but practice **knitting this RMarkdown as HTML**. Received any error? Reach out to your colleagues either on Slack or on your student-led Whatsapp group chat.*

</summary>