---
title: "ðŸ’» Week 03 - Lab Roadmap (90 min)"
date: 20 September 2022
date-meta: 20 September 2022
from: markdown+emoji
author: Yijun Wang
---

This week, we will fit simple and multiple linear regression models in R and learn to interpret the R output. We will apply this method to practical cases and deal with problems that commonly arise during this process.

<details><summary>Step 1: Simple linear regression (15 min)</summary>

## Step 1: Simple linear regression

We will follow the instructions below step by step together while answering whatever questions you might encounter along the way.

1. Install and load the `ISLR2` package, which contains a large collection of data sets and functions.

    ```r
    install.packages("ISLR2").
    library (ISLR2)
    ```
    
    The function `install.packages()` is used to download packages that don't come with R. This installation only needs to be done the first time you use a package. However, the `library()` function must be called within each R session to load packages.

2. Use the `Boston` data set in the `ISLR2` library. It records `medv` (median house value) for 506 census tracts in Boston. Have a look at the first few rows of the Boston data set:

    ```r
    head (Boston)
        crim zn indus chas   nox    rm  age    dis rad tax ptratio lstat medv
    1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3  4.98 24.0
    2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8  9.14 21.6
    3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8  4.03 34.7
    4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7  2.94 33.4
    5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7  5.33 36.2
    6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7  5.21 28.7
    ```

    We want to predict `medv` using the available predictors, such as `rm` (average number of rooms per house), `age` (average age of houses), and `lstat` (percentage of households with low socioeconomic status). To find out more about the data set, we can type `?Boston`.

3. Fit a simple linear regression `lm()` model, with `medv` as the response and `lstat` as the predictor:

    ```r
    lm.fit <- lm(medv ~ lstat , data = Boston)
    ```

    The basic syntax is `lm(y âˆ¼ x, data)`, where `y` is the response, `x` is the predictor, and `data` is the data set in which we keep these two variables.

4. Use the function `summary()` to understand detailed information about the model lm.fit we just fitted, like p-values and standard errors for the coefficients, as well as the $R^2$ statistic and F-statistic:

    ```r
    summary(lm.fit)

    Call:
    lm(formula = medv ~ lstat, data = Boston)

    Residuals:
        Min      1Q  Median      3Q     Max 
    -15.168  -3.990  -1.318   2.034  24.500 

    Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
    (Intercept) 34.55384    0.56263   61.41   <2e-16 ***
    lstat       -0.95005    0.03873  -24.53   <2e-16 ***
    ---
    Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

    Residual standard error: 6.216 on 504 degrees of freedom
    Multiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 
    F-statistic: 601.6 on 1 and 504 DF,  p-value: < 2.2e-16
    ```

    Because `lm.fit` is a simple linear regression model, there are only two coefficients: $\hat{\beta}_0$ and $\hat{\beta}_1$. The Coefficients in the output shows the estimated value, standard error, t-test result and p-value for t-test of $\hat{\beta}_0$ and $\hat{\beta}_1$. The goodness-of-fit of the model can be measured by the RSE and $R^2$ in the output.

    We can also use `lm.fit$coefficients` or `coef(lm.fit)` to check the coefficients of the regression model:

    ```r
    > coef(lm.fit)
    (Intercept)       lstat 
     34.5538409  -0.9500494 
    ```
    
5. Obtain a confidence interval for the coefficient estimates using the `confint()` function:

    ```r
    confint (lm.fit)
                    2.5 %     97.5 %
    (Intercept) 33.448457 35.6592247
    lstat       -1.026148 -0.8739505
    ```

6. Plot `medv` and `lstat` along with the least squares regression line using the `plot()` and `abline()` functions.:

    ```r
    plot(Boston$lstat, Boston$medv, col = "red", pch = "+")
    abline(lm.fit , lwd = 3, col = "blue")
    ```

</details>



<details><summary>Step 2: Multiple linear regression (10 min)</summary>

## Step 2:  Multiple linear regression

We will still use the `Boston` data set to fit multiple linear regression. The fitting process is similar to simple linear regression.

1. Fit a multiple linear regression `lm()` model, with `medv` as the response, `lstat` and `age` as the predictors:
    
    ```r
    lm.fit <- lm(medv ~ lstat + age , data = Boston)
    summary (lm.fit)

    Call:
    lm(formula = medv ~ lstat + age, data = Boston)

    Residuals:
        Min      1Q  Median      3Q     Max 
    -15.981  -3.978  -1.283   1.968  23.158 

    Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
    (Intercept) 33.22276    0.73085  45.458  < 2e-16 ***
    lstat       -1.03207    0.04819 -21.416  < 2e-16 ***
    age          0.03454    0.01223   2.826  0.00491 ** 
    ---
    Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

    Residual standard error: 6.173 on 503 degrees of freedom
    Multiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 
    F-statistic:   309 on 2 and 503 DF,  p-value: < 2.2e-16
    ```
    The syntax `lm(y ~ x1 + x2 + x3)` is used to fit a model with three predictors, `x1`, `x2`, and `x3`. The `summary()` function now outputs the regression coefficients for all the predictors.

2. Fit a multiple linear regression `lm()` model, with `medv` as the response, all rest variables as the predictors:

    ```r
    lm.fit <- lm(medv ~ ., data = Boston)
    summary(lm.fit)

    Call:
    lm(formula = medv ~ ., data = Boston)

    Residuals:
         Min       1Q   Median       3Q      Max 
    -15.1304  -2.7673  -0.5814   1.9414  26.2526 

    Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
    (Intercept)  41.617270   4.936039   8.431 3.79e-16 ***
    crim         -0.121389   0.033000  -3.678 0.000261 ***
    zn            0.046963   0.013879   3.384 0.000772 ***
    indus         0.013468   0.062145   0.217 0.828520    
    chas          2.839993   0.870007   3.264 0.001173 ** 
    nox         -18.758022   3.851355  -4.870 1.50e-06 ***
    rm            3.658119   0.420246   8.705  < 2e-16 ***
    age           0.003611   0.013329   0.271 0.786595    
    dis          -1.490754   0.201623  -7.394 6.17e-13 ***
    rad           0.289405   0.066908   4.325 1.84e-05 ***
    tax          -0.012682   0.003801  -3.337 0.000912 ***
    ptratio      -0.937533   0.132206  -7.091 4.63e-12 ***
    lstat        -0.552019   0.050659 -10.897  < 2e-16 ***
    ---
    Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

    Residual standard error: 4.798 on 493 degrees of freedom
    Multiple R-squared:  0.7343,    Adjusted R-squared:  0.7278 
    F-statistic: 113.5 on 12 and 493 DF,  p-value: < 2.2e-16   
    ```

    We can access the individual components of a summary object by name (type `?summary.lm` to see what is available). Hence `summary(lm.fit)$r.sq` gives us the R2, and `summary(lm.fit)$sigma` gives us the RSE.

3. Select variables:
    
    In these two multiple linear regression models, the t-tests and F-test results suggest that all predictors are significant for the response variable. However,  it is more often the case that the response is only associated with a subset of the predictors. We call the process of determining which predictors are associated with the response as **variable selection**.

    If the number of predictors is very small, we could perform the variable selection by trying out a lot of different models, each containing a different subset of the predictors. We can then select the best model out of all of the models we have considered.

    If the number of predictors is not very small, the function `regsubsets()` could be used to select the best subset of variables. 

    ```r
    install.packages('leaps')
    library(leaps)
    regfit.full <- regsubsets(medv ~ ., Boston)
    summary (regfit.full)
    ```  

    The `regsubsets()` function identifies the best model with a given number of predictors, where the best is quantified using RSS. The syntax is the same as for `lm()`. The `summary()` command outputs the best set of variables for each model size. You can use the argument `method` = "forward" or "backward" or "seqrep" choose selection methods. 

</details>



<details><summary> Step 3: Some potential problems (15 min)</summary>
## Step 3: Some potential problems

Many problems may occur when we fit a linear regression model to a particular data set. These problems will lead to inaccurate estimation. In this step, we will identify and overcome potential problems such as outliers, collinearity and interaction effects. 

We present a few of the many methods available, but those interested can explore more after class.

1. Handle interaction terms:
    
    In regression, an interaction effect exists when the effect of an independent variable on the response variable changes, depending on the values of one or more independent variables. When you believe there is an interaction effect, it is easy to include interaction terms in a linear model using the `lm()` function.

    ```r
    summary (lm(medv ~ lstat * age , data = Boston))

    Call:
    lm(formula = medv ~ lstat * age, data = Boston)

    Residuals:
        Min      1Q  Median      3Q     Max 
    -15.806  -4.045  -1.333   2.085  27.552 

    Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
    (Intercept) 36.0885359  1.4698355  24.553  < 2e-16 ***
    lstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***
    age         -0.0007209  0.0198792  -0.036   0.9711    
    lstat:age    0.0041560  0.0018518   2.244   0.0252 *  
    ---
    Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

    Residual standard error: 6.149 on 502 degrees of freedom
    Multiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 
    F-statistic: 209.3 on 3 and 502 DF,  p-value: < 2.2e-16
    ```
    
    The syntax `lstat:age` tells R to include an interaction term between `lstat` and `age`. The syntax `lstat*age` simultaneously includes `lstat`, `age`, and the interaction term `lstatÃ—age` as predictors; it is a shorthand for `lstat+age+lstat:age`.

2. Identify outliers through residual plots:

    An outlier is a point for which $\hat{y}_i$ is far from the value predicted by the model. Outliers can arise for a variety of reasons, such as incorrect recording of observation during data collection. Outliers could be identified through residual plots:

    ```r
    par(mfrow = c(2, 2))
    plot(lm.fit)
    ```
    
    The `plot` function automatically produces four diagnostic plots when you pass the output from `lm()`. Plots on the left column are residual plots, indicating the relationship between residuals and fitted values.

    In practice, it can be difficult to decide how large a residual needs to be before we consider the point to be an outlier. Instead of plotting the residuals, we can address this problem by plotting the studentized residuals. These are computed by dividing each residual e<sub>i</sub> by its estimated standard studentized residual error. Observations with studentized residuals greater than 3 in absolute value are possible outliers. Using the plot() function to plot the studentized residuals:

    
    ```r
    plot(predict(lm.fit), rstudent(lm.fit)
    ```

3. Handle outliers:

   If we believe an outlier is due to an error in data collection or recording, then one solution is to simply remove the observation. However, care should be taken, as an outlier may instead signal a deficiency with our model, such as a missing predictor.

4. Detect collinearity using the correlation matrix:

    Collinearity refers to the situation in which two or more predictor variables collinearity are closely related to one another. It can be detected through the correlation matrix:
    
    ```r
    cor(Boston)
    ```
    
    Ignoring the last row and the last column in the matrix, which indicate the relationship with response variable `medv`, an element of this matrix that is large in absolute value indicates a pair of highly correlated variables, and therefore a collinearity problem in the data.

    Or we could detect collinearity quantitatively using `vif()` function:

    ```r
    install.packages("car")
    library(car)

    vif(lm.fit)
    ```

    Instead of inspecting the correlation matrix, a better way to assess multicollinearity is to compute the variance inflation factor (VIF). As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity.

:::{.callout-tip collapse="true"}
## Read more about VIF

Check out our textbook [@james_introduction_2021, pages 99-103] for a description of the Variance Inflation Factor (VIF).

:::

5. Handle collinearity:

    When faced with the problem of collinearity, there are two simple solutions. 
    
    The first is to drop one of the problematic variables from the regression. Doing this does not compromise the regression fit much since collinearity implies that the information this variable provides about the response is redundant in the presence of the other variables.   

    The second solution is to combine the collinear variables into a single predictor.

</details>

<details><summary> Step 4: Practical exercises (50 min)</summary>
## Step 4: Practical exercises (in pairs)

So far, we have learnt to fit simple and multiple linear regression models in R. In this practical case, we will continue to use the data set `Auto` studied in the last lab. Make sure that the missing values have been removed from the data.

Eight questions are listed below. You are required to try to answer these questions in pairs using R commands. We will go over the solutions once everyone has finished these questions.

ðŸŽ¯ **Questions** 

1. Use the `lm()` function to perform a simple linear regression with `mpg` as the response and `horsepower` as the predictor. Use the `summary()` function to print the results. Comment on the output. For example:

   i. Is there a relationship between the predictor and the response?
   ii. How strong is the relationship between the predictor and the response?
   iii. Is the relationship between the predictor and the response positive or negative?
   iv. What is the predicted `mpg` associated with a `horsepower` of 98? What are the associated 95 % confidence intervals?

2. Plot the response and the predictor. Use the `abline()` function to display the least squares regression line.
    
3. Use the `plot()` function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.

4. Produce a scatterplot matrix that includes all the variables in the data set.

5. Compute the matrix of correlations between the variables using the function `cor()`. You will need to exclude the name variable, which is qualitative.

6. Use the `lm()` function to perform a multiple linear regression with `mpg` as the response and all other variables except `name` as the predictors. Use the `summary()` function to print the results. Comment on the output. For instance:

   i. Is there a relationship between the predictors and the response?
   ii. Which predictors appear to have a statistically significant relationship to the response?
   iii. What does the coefficient for the year variable suggest?

7. Use the `plot()` function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? 

8. Use the `*` and `:` symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?

:::{.callout-tip}

If you could not finish all eight questions during the lab, take that as a home exercise. 

Use the `#week03` channel on Slack if you have any questions.

:::

</details>
