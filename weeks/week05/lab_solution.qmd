---
title: "‚úîÔ∏è Week 05 - Lab Solutions"
date: 1 November 2022
date-meta: 1 November 2022
from: markdown+emoji
author: Xiaowei Gao/Mustafa Can Ozkan
---

## üîë Solutions to exercises

1. For the `Default` dataset, please Split the sample set into a training set and a validation set, then fit a logistic regression model that uses `income` and `balance` to predict `default`. As following,

    i.  Use three different splits of the observations into a training set and a validation set.
    <br>

    ```r
    > train.1 = sample(dim(Default)[1], 0.70*dim(Default)[1])
    > train.2 = sample(dim(Default)[1], 0.70*dim(Default)[1])
    > train.3 = sample(dim(Default)[1], 0.70*dim(Default)[1])
    ```

    ii. Fit three multiple logistic regression models using only the training observations.
    <br>

    ```r
    #just give one sample here with train.1
    > glm.fit.1 = glm(default ~ income + balance, data = Default, subset = train.1, family = "binomial")
    
    ```


    iii. Based on the three models,obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the `default` category if the posterior probability is greater than 0.5.
    <br>

    ```r
    #just give one sample here with train.1
    > glm.probs = predict(glm.fit.1, Default[-train.1, ], type = "response")
    > glm.preds = rep("No", dim(Default)[1])
    > glm.preds[glm.probs > 0.5] = "Yes"

    ```
    
    iv. Based on the three models, compute the validation set error, which is the fraction of the observations in the validation set that are misclassified.

    ```r
    #just give one sample here
    > mean(glm.preds != Default[-train, "default"])

    ```

2. For the `Default` dataset, We continue to consider the use of a logistic regression model to predict the probability of `default` using `income` and `balance` on the `Default` data set. In particular, we will now compute estimates for the standard errors of the `income` and `balance` logistic regression coefficients in two different ways: 


    2.1. Using the `bootstrap`.

    2.2. Using the standard formula for computing the standard errors in the `glm()` function. 


    As following,


   i. Using the `summary()` and `glm()` functions, determine the estimated standard errors for the coefficients associated with `income` and `balance` in a multiple logistic regression model that uses both predictors.
    <br>

        ```r
        > log_def <- glm(default ~ income + balance, data = Default, family = "binomial")
        > summary(log_def)$coefficients[, 2]
        [1]  (Intercept)       income      balance 
            4.347564e-01 4.985167e-06 2.273731e-04 

        ```

    ii. Write a function,`boot.fn()`,that takes as input the `Default` data set as well as an index of the observations, and that outputs the coefficient estimates for `income` and `balance` in the multiple logistic regression model.
    <br>

        ```r
            > boot.fn <- function(data, index = 1:nrow(data)) {
            +   coef(glm(default ~ income + balance, data = data, subset = index, family = "binomial"))[-1]
            + }
            > boot.fn(Default)
           [1]  income      balance 
            2.080898e-05 5.647103e-03 

        ```

    iii. Use the `boot()` function together with your `boot.fn()` function to estimate the standard errors of the logistic regression coefficients for `income` and `balance`. Then, Create a histogram of the bootstrap parameter estimates with `ggplot2`, and also set the `bins=20`, title as `1,000 Bootstrap Parameter Estimates - 'balance' & 'income`. 
    <br>


        ```r
        > set.seed(101)
        > boot_results <- boot(data = Default, statistic = boot.fn, R = 1000)

        > as.data.frame(boot_results$t) %>%
            rename(income = V1, balance = V2) %>%
            gather(key = "variable", value = "estimate") %>%
            ggplot(aes(x = estimate, fill = factor(variable))) + 
            geom_histogram(bins = 20) + 
            facet_wrap(~ variable, scales = "free_x") + 
            labs(title = "1,000 Bootstrap Parameter Estimates - 'balance' & 'income'", 
                subtitle = paste0("SE(balance) = ", formatC(sd(boot_results$t[ ,2]), format = "e", digits = 6), 
                                    ", SE(income) = ", formatC(sd(boot_results$t[ ,1]), format = "e", digits = 6)), 
                x = "Parameter Estimate", 
                y = "Count") + 
            theme(legend.position = "none")
        ```

3. We saw that the `cv.glm()` function can be used in order to compute the LOOCV test error estimate. However, one could compute those quantities using just the `glm()` and `predict.glm()` functions, and a for loop. You will now take this approach in order to compute the LOOCV error for a simple logistic regression model on the `Weekly` data set. As following,

    i. Logistic Regression- Fit a logistic regression model that predicts `Direction` using `Lag1` and `Lag2`.
    <br>

    ```r
        > log_dir <- glm(Direction ~ Lag1 + Lag2, data = Weekly, family = "binomial")
        > summary(log_dir)

    ```

    ii. Omitting One Observation from Training- Fit a logistic regression model that predicts `Direction` using `Lag1` and `Lag2` *using all but the first observation* .
    <br>

    ```r
        > log_dir_2 <- glm(Direction ~ Lag1 + Lag2, data = Weekly[-1, ], family = "binomial")
        > summary(log_dir_2)

    ```

    iii. Predicting the Omitted Observation- Use the model from ii. to predict the `direction` of the first observation. You can do this by predicting that the first observation will go up if $P(\text { Direction }=' U p ' \mid \operatorname{Lag} 1, \text { Lag2 })>0.5$. Was this observation correctly classified?
    <br>

    ```r
        # methods 1
        > predict(log_dir_2, Weekly[1, ])
        > Weekly[1, ]

        # methods 2
        > ifelse(predict(log_dir_2, newdata = Weekly[1, ], type = "response") > 0.5, "Up", "Down")

    ```

    iv. Writing a LOOCV Loop- Write a for loop from i = 1 to i = n, where n is the number of observations in the data set, that performs each of the following steps:
        1. Fit a logistic regression model using all but the ith observation to predict `Direction` using `Lag1` and `Lag2`.
        2.  Compute the posterior probability of the market moving up for the `i` th observation.
        3. Use the posterior probability for the `i` th observation in order to predict whether or not the market moves up.
        4. Determine whether or not an error was made in predicting the direction for the `i` th observation. If an error was made, then indicate this as a `1` , and otherwise indicate it as a `0` .
    <br>

    ```r
        # method 1
        > error <- c()
        > for (i in 1:nrow(Weekly)) {
        log_dir <- glm(Direction ~ Lag1 + Lag2, data = Weekly[-i, ], family = "binomial") # 1.
        prediction <- ifelse(predict(log_dir, newdata = Weekly[i, ], type = "response") > 0.5, "Up", "Down") # 2. & 3.
        error[i] <- as.numeric(prediction != Weekly[i, "Direction"]) # 4.
        }
        > head(error)
        > error[1:10]
        [1]  1 1 0 1 0 1 0 0 0 1

        #method 2
        > n <- dim(Weekly)[1]
        > errors <- rep(0, n)
        > for (i in 1:n){
            glm.fit.loo <- glm(Direction ~ Lag1 + Lag2, data = Weekly, family = "binomial", subset <- c(-i))
            pred = "Down"
            if (predict(glm.fit.loo, Weekly[i, ], type = "response") > 0.5){
                pred = "Up"
            }
            if (pred != Weekly[i, "Direction"]){
                errors[i] <- 1
            }
        }
        > head(errors)
        > errors[1:10]
        [1]  1 1 0 1 0 1 0 0 0 1

    ```

    v. The LOOCV Estimate for the Test Error- Take the average of the `n` numbers obtained in `(iv)-4`. in order to obtain the LOOCV estimate for the test error. Comment on the results.
    
    ```r

    #method 1
    > mean(error)
    [1] 0.4499541
    > prop.table(table(Weekly$Direction))

        Down        Up 
    0.4444444 0.5555556

    #method 2
    > mean(errors)
    [1] 0.4499541
    > mean(Weekly["Direction"] != "Up")
    [1] 0.4444444
    > mean(Weekly["Direction"] == "Up")
    [1] 0.5555556
    ```

    
</details>