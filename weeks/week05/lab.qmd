---
title: "üíª Week 05 - Lab Roadmap (90 min)"
date: 06 October 2022
date-meta: 06 October 2022
from: markdown+emoji
author: Xiaowei Gao & Stuart Bramwell
---

<p> Last week we learnt how to do the Classification Methods in R. As data scientists, we need to know how to examine the results we generated and also to justify our results.  So, we will focus on and practice the validation methods in the R language in this week's lab. 

If you feel confident at the current stage, free to explore more on your own. We have provided you with some supplementary online resources : 

<br>
üëâ [R for Data Analytics](https://rforanalytics.com/13.6-k-fold-cross-validation.html) (part of the courses by Abhay Singh)

üëâ [R for Data Science](https://r4ds.had.co.nz/index.html) (book by Hadley Wickham & Garrett Grolemund)
</p>

<details><summary> Step 1: The Validation Set Approach (15mins) </summary>
<br>
In this step, we will explore how to extract the subset of the whole dataset as a training dataset, and then estimate the test error rates of various linear models. The dataset is `Auto` from the `ISLR2` package.

1. Split the `Auto` dataset into two halves, as randomly selecting 196 observations out of the original 392 observations.
	
	```{r}
	library(ISLR2)
	set.seed(1)
	train <- sample(392, 196)
	```
	Please note that, the `set.seed()` is important here as it set a **seed** for the random number generator. Literally, the same results will be replicated in the following steps. Further information can be found in the official documentation.[Seeding Random Variate Generators](https://www.rdocumentation.org/packages/simEd/versions/2.0.0/topics/set.seed) Also, I provide some details to help you understand this function in the following part.

	```r
	> set.seed(123)
	> x<-rnorm(3) # randomly generate three numbers
	[1]Ôºö-0.5604756 -0.2301775  1.5587083
	```

	```r
	> y<-rnorm(3) #try to replicate the result from x, but totally different
	[1]Ôºö0.07050839 0.12928774 1.71506499
	```

	```r
	> set.seed(123)
	> y<-rnorm(3) #try to replicate the result from x again with the same seed
	[1]Ôºö-0.5604756 -0.2301775  1.5587083
	```
2. Fit a linear regression model using the training dataset (`train`), making the `mpg` as the dependent variable(x) and `horsepower` as the independent variable(y). Then, using the fitted model to estimate the `mpg` from the whole dataset, `Auto`. Finally, calculating the MSE of the 196 observations in the validation set.

	```r
	# use the lm() function to fit a linear regression model
	> lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = train)
	# estimate the 'mpg' values by the lm.fit model
	> lm.pre <- predict(lm.fit, Auto)
	# calculate MSE
	> attach(Auto)
	> mean((mpg - lm.pre)[-train]^2)
	[1] 23.27
	```
	Therefore, we have estimated the test MSE for the linear regression model, **23.27**. (Well Done! üí™ )
	<br>
    Please note, here we use new functions, namely `lm`, `predict`,and `attach`. By typing `?`, you can find the official explanation of these functions. We just want to point out `attach`, which will make your coding easier without quoting `dataframe$colnames`.
	
    ‚ö°Ô∏èThe `-train` index selects only the observations that are not in the training set.

	```r
	> attach(Auto)
	> mpg
	[1] You can see all the values from the 'mpg' column.
	```
	However, if you do not use `attach` to call the dataframe, you have to use the `$` function to have the same results.

	```r
	> Auto$mpg
	[1] You can see all the values from the 'mpg' column.
	```

3. Repeat the second part to estimate the test error for the quadratic and cubic regressions.

	```r
	> lm.fit2 <- lm(mpg ‚àº poly(horsepower, 2), data = Auto, subset = train)
	> mean((mpg - predict(lm.fit2, Auto))[-train]^2)
	[1] 18.72
	> lm.fit3 <- lm(mpg ‚àº poly(horsepower, 3), data = Auto, subset = train)
	> mean((mpg - predict(lm.fit3, Auto))[-train]^2)
	[1] 18.79
	```

	By far, we have got the MSE for the quadratic and cubic regressions. Hence, we can see a model that predicts `mpg` using a quadratic function of `horsepower` performs better than a model that involves only a linear function of `horsepower`, and there is little evidence in favor of a model that uses a cubic function of `horsepower`. 

</details>

<details><summary> Step 2: Leave-One-Out Cross-Validation (15mins) </summary>
<br>
Hereby, We will compute the LOOCV values by using the function `glm` rather than `lm`.

1. Generate the cross-validation results with the generalized linear model.
	
    ```r
	> library(boot)

	#fit the glm model
	> glm.fit <- glm(mpg ‚àº horsepower, data = Auto)

	# get the information about the model
	> summary(glm.fit)

	[1] Call: glm(formula = mpg ~ horsepower, data = Auto)
        Deviance Residuals: 
            Min        1Q    Median        3Q       Max  
        -13.5710   -3.2592   -0.3435    2.7630   16.9240  

        Coefficients:
                    Estimate Std. Error t value Pr(>|t|)    
        (Intercept) 39.935861   0.717499   55.66   <2e-16 ***
        horsepower  -0.157845   0.006446  -24.49   <2e-16 ***
        ---
        Signif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1

        (Dispersion parameter for gaussian family taken to be 24.06645)

            Null deviance: 23819.0  on 391  degrees of freedom
        Residual deviance:  9385.9  on 390  degrees of freedom
        AIC: 2363.3

        Number of Fisher Scoring iterations: 2
	
	# get the cross-validation results by the 'cv.glm' function
	> cv.err <- cv.glm(Auto, glm.fit)
	> cv.err$delta
	[1]   1   1 
	   24.23 24.23

	# explore cv.err 
	> summary(cv.err)

    [1]	      Length Class  Mode   
        call    3    -none- call   
        K       1    -none- numeric
        delta   2    -none- numeric
        seed  626    -none- numeric
	
	```

	Please be aware that, The `cv.glm()` function is part of the `boot` library. Meanwhile, you can explore the `cv.err` by yourself to see what is the meaning of 	`call`,`K`,`delta` and `seed`. This online webpage will be useful when interpreting the results. [Intepretation of crossvalidation result](https://stats.stackexchange.com/questions/48766/intepretation-of-crossvalidation-result-cv-glm)

	üèãÔ∏è There is a bonus for the senior students. You can explore the package `caret` for the LOOCV estimation. 

2. Repeat the former step and fit increasingly complex polynomial models
	
    ```r
	# crate a vactor with length equal to 10
	> cv.error <- rep(0, 10) 

	> for(i in 1:10){
	  glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
	  cv.error[i] <- cv.glm(data = Auto, glmfit = glm.fit)$delta[1]
	}

	#check the cv.error 
	> cv.error
	[1] 24.23151 19.24821 19.33498 19.42443 19.03321 18.97864 18.83305 18.96115 19.06863 19.49093
	```

	`rep` means repeat. In this line, we can also use `cv.error <- numeric(10)` to get the same result. <br> **It does not matter whether we have rep(0,10) or rep(1,10). We only want to get a vector with a length of 10.**  You can think of the reason by yourself. <br> We hereby use the `for` loop to iterate the number from 1 to 10, and store the first `delta` number to the **i** th position of `cv.error`. 

	I would also encourage you to basically visualise the results, as `plot(cv.error)`. We can clearly see a sharp drop in the estimated test MSE between the linear and quadratic fits, from 24.27 to 19.27, but then no clear improvement from using higher-order polynomials. 

</details>

<details><summary> Step 3: k-Fold Cross-Validation (5mins)</summary>
<br>
It will be easy to follow the former procedure in Step 2, by the `cv.glm` to implement ***K-fold*** CV.

1. Estimate CV errors corresponding to the polynomial fits of orders one to ten. 

	```r
	> set.seed(17)
	> cv.error.10 <- rep(100, 10) 
	> for(i in 1:10){
	  glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
	  cv.error.10[i] <- cv.glm(data = Auto, glmfit = glm.fit, K=10)$delta[1]
	}

	> cv.error.10
	[1] 24.27207 19.26909 19.34805 19.29496 19.03198 18.89781 19.12061 19.14666 18.87013 20.95520
	```
Note that in the line of `cv.error.10[i] <- cv.glm(data = Auto, glmfit = glm.fit, K=10)$delta[1]`, it will be very strict to `K` rather than `k`.

</details>

<details><summary> Step 4: The Bootstrap (20mins) </summary>
<br>
We have learnt the theoretical method regarding Bootstrap. I understand there may be a bit difficult for beginners in statistics, but we will mainly focus on the coding implementation here. Two datasets will be used, `Portfolio` from the `ISLR2` package, and also the `Auto` dataset. So please ensure you have called the `library(ISLR2)` at the very first beginning. Also, we will introduce how to create a `funcion` below. 

**Functions are ‚Äúself contained‚Äù modules of code that accomplish a specific task.** referenced from [Functions and theri argumens](https://hbctraining.github.io/Intro-to-R/lessons/03_introR-functions-and-arguments.html)
<br>

With the help of a function, you can reuse the same pattern codes with a simple function name.  

### Step 4.1 Estimating the Accuracy of a Statistic of Interest
<br>

1. Explore the ___Portfolio___ data with ggplot.

    ```r
    #fundamental view the dataset
    > View(Portfolio)
    > dim(Portfolio)
    > colnames(Portfolio)

    # plot the distribution of each columns
    > ggplot(Portfolio) + geom_histogram(aes(x= X),bins = 10)
    > ggplot(Portfolio) + geom_histogram(aes(x= Y),bins = 10)
    

    #plot the linear relationship among x and y
    > ggplot(Portfolio) + 
        geom_jitter(aes(x=X, y=Y), alpha=0.1) + 
        geom_smooth(aes(x=X, y=Y), method='lm')
    ```
You could also try to change `bins` in `ggplot`, which will help you to better understand the hist plot.

2. Create a function to estimate $\alpha$ with input from __Portfolio___ data and name it as `alpha.fn()`

    ```r
    #write a function
   > alpha.fn <- function(data, index) {
        X <- data$X[index]
        Y <- data$Y[index]
        (var(Y) - cov(X, Y)) / (var(X) + var(Y) - 2 * cov(X, Y)) 
        } 
    #use the function
    > alpha.fn(Portfolio, 1:100)
    [1] 0.576
    ```

    When you write a function, you need to clearly know three points, namely the input data (the first and second arguments in the parentheses), the processes (within the curly braces) and the results you need. 
<br>

    Hence, in this step, you may want to retrieve every value of X and Y within the index (the first 100 observations). Finally, you will have the estimated $\alpha$ with the processes from this function.   
<br>
3. Randomly select the 100 observations from the whole dataset, with replacement, and  produce R = 1,000 bootstrap estimates for $\alpha$.

    ```r
    > set.seed(7)
    # We also used the function created before
    > ?sample 
    > alpha.fn(Portfolio, sample(100, 100, replace = T)) 
    [1] 0.539

    # Perform the bootstrap analysis with 1000 times, and compute the resulting standard deviation for alpha
    > ?boot
    > boot(Portfolio, alpha.fn, R = 1000)

    [1] ORDINARY NONPARAMETRIC BOOTSTRAP

        Call:
        boot(data = Portfolio, statistic = alpha.fn, R = 1000)


        Bootstrap Statistics :
            original       bias    std. error
        t1* 0.5758321 -0.001787503  0.08969266
    ```
    The final output shows that using the original data, $\hat{\alpha}=0.5758$, and that the bootstrap estimate for $\mathrm{SE}(\hat{\alpha}) = 0.0897$ . 
        
You may find the code hard to understand, especially when we cannot fully digest the logic of bootstrap. So I would suggest we can quickly review the lecture at this stage. Please believe in ourself, you will get through the stage sooner and make the codes much clear step by step. Be patient, and be confident üíª .
<br>

### Step 4.2 Estimating the Accuracy of a Linear Regression Model
<br>
In this step, we will use the boostrap approach to assess the variability of the coefficient estimates and predictions from a statistical learning method.
<br>

1. Create a function as `boot.fn()`.

    ```r
    > boot.fn <- function(data, index)
    + coef(lm(mpg ‚àº horsepower, data = data, subset = index)) 
    
    > boot.fn(Auto, 1:392)
    [1](Intercept)  horsepower 
        39.9358610  -0.1578447
    ```

    You may find the `function` here is a bit different with the former one. So, I provide an another option to make it the same, with curly braces.

    ```r
    > boot.fn.1 <- function(data, index){
        data<- data
        coef(lm(mpg ~ horsepower, data = data, subset = index)) 
        }
    ```
2. Compare the results from bootstrap estimates and the standard estimates 
    
    ```r
    #bootstrap with 1000 times
    > boot(Auto, boot.fn, 1000)
    [1]     ORDINARY NONPARAMETRIC BOOTSTRAP
        Call:
        boot(data = Auto, statistic = boot.fn, R = 1000)


        Bootstrap Statistics :
            original       bias    std. error
        t1* 39.9358610  0.082482836 0.847514357
        t2* -0.1578447 -0.000988235 0.007241942

    # standard estimates
    > summary(lm(mpg~ horsepower, data = Auto))$coef
    [1]     Estimate  Std. Error   t value      Pr(>|t|)
        (Intercept) 39.9358610 0.717498656  55.65984 1.220362e-187
        horsepower  -0.1578447 0.006445501 -24.48914  7.031989e-81
    ```

    We can find that in the bootstrap esimation process, $\mathrm{SE}(\hat{\beta}_{1}) = 0.8475$ and $\mathrm{SE}(\hat{\beta}_{2}) = 0.0072$ , while in the standard esimation process, $\hat{\beta}_{Intercept}=0.7175$ and $\hat{\beta}_{horsepower}=0.0064$. You can just think why there is a difference here between the two results. Possibly, you can use the below code to visualize the linear/non-linear relationship between these two variables. It may give you some suggestions on this issue.


    ```r
    > ggplot(Auto, aes(x=mpg, y=horsepower)) + 
        geom_point() +
        geom_smooth()

    ``` 



</details>

<details><summary> Step 5: Exercise (35mins)</summary>
<br>
Since then, we have known and implemented the coding with Cross-validation and Bootstrap.  In this practical case, we will use the new dataset `Default`  and also `Weekly` from the `ISRL` package. **Do not forget to set a random seed before beginning your analysis.**

Some questions are listed below. You are required to try to answer these questions in pairs using R commands. We will go over the solutions once everyone has finished these questions

üéØ **Questions** 

1. For the `Default` dataset, please Split the sample set into a training set and a validation set, then fit a logistic regression model that uses `income` and `balance` to predict `default`. As following,

    i.  Use three different splits of the observations into a training set and a validation set.
    <br>

    ii. Fit three multiple logistic regression models using only the training observations.
    <br>

    iii. Based on the three models,obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the `default` category if the posterior probability is greater than 0.5.
    <br>

    iv. Based on the three models, compute the validation set error, which is the fraction of the observations in the validation set that are misclassified.

2. For the `Default` dataset, We continue to consider the use of a logistic regression model to predict the probability of `default` using `income` and `balance` on the `Default` data set. In particular, we will now compute estimates for the standard errors of the `income` and `balance` logistic regression coefficients in two different ways: 2.1. Using the `bootstrap`, 2.2. Using the standard formula for computing the standard errors in the `glm()` function. As following,

    i. Using the `summary()` and `glm()` functions, determine the estimated standard errors for the coefficients associated with `income` and `balance` in a multiple logistic regression model that uses both predictors.
    <br>

    ii. Write a function,`boot.fn()`,that takes as input the `Default` data set as well as an index of the observations, and that outputs the coefficient estimates for `income` and `balance` in the multiple logistic regression model.
    <br>

    iii. Use the `boot()` function together with your `boot.fn()` function to estimate the standard errors of the logistic regression coefficients for `income` and `balance`. Then, Create a histogram of the bootstrap parameter estimates with `ggplot2`, and also set the `bins=20`, title as `1,000 Bootstrap Parameter Estimates - 'balance' & 'income`.
    <br>

3. We saw that the `cv.glm()` function can be used in order to compute the LOOCV test error estimate. However, one could compute those quantities using just the `glm()` and `predict.glm()` functions, and a for loop. You will now take this approach in order to compute the LOOCV error for a simple logistic regression model on the `Weekly` data set. As following,

    i. Logistic Regression- Fit a logistic regression model that predicts `Direction` using `Lag1` and `Lag2`.
    <br>

    ii. Omitting One Observation from Training- Fit a logistic regression model that predicts `Direction` using `Lag1` and `Lag2` *using all but the first observation* .
    <br>

    iii. Predicting the Omitted Observation- Use the model from ii. to predict the `direction` of the first observation. You can do this by predicting that the first observation will go up if $P(\text { Direction }=' U p ' \mid \operatorname{Lag} 1, \text { Lag2 })>0.5$. Was this observation correctly classified?
    <br>

    iv. Writing a LOOCV Loop- Write a for loop from i = 1 to i = n, where n is the number of observations in the data set, that performs each of the following steps:
        1. Fit a logistic regression model using all but the ith observation to predict `Direction` using `Lag1` and `Lag2`.
        2.  Compute the posterior probability of the market moving up for the `i` th observation.
        3. Use the posterior probability for the `i` th observation in order to predict whether or not the market moves up.
        4. Determine whether or not an error was made in predicting the direction for the `i` th observation. If an error was made, then indicate this as a `1` , and otherwise indicate it as a `0` .
    <br>

    v. The LOOCV Estimate for the Test Error- Take the average of the `n` numbers obtained in `(iv)-4`. in order to obtain the LOOCV estimate for the test error. Comment on the results.
    
</details>






















