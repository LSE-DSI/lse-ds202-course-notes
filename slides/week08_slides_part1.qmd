---
subtitle: "PCA"
title: "üóìÔ∏è Week 08:<br/> Principal Component Analysis"
author: Dr. [Jon Cardoso-Silva](https://www.lse.ac.uk/DSI/People/Jonathan-Cardoso-Silva)
institute: '[\@LSEDataScience](https://twitter.com/lsedatascience)'
date: 18 November 2022
date-meta: 18 November 2022
toc: true
toc-depth: 1
toc-title: "What we will cover today"
center-title-slide: false
from: markdown+emoji
format:
  revealjs: 
    pdf-separate-fragments: true
    code-fold: false
    theme: simple
    slide-number: true
    mouse-wheel: true
    chalkboard: 
      buttons: true
    preview-links: auto
    logo: /figures/logos/LSE Data Science Institute.png
    css: /css/styles_slides.css
    footer: 'DS202 - Data Science for Social Scientists :robot: :juggling_person:'
---

# Unsupervised Learning

(Quick recap)

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)      # to use things like the pipe (%>%), mutate and if_else
library(ISLR2)
library(GGally)
library(ggsci)
```

## Comparison

::: columns

::: {.column .fragment .fade-in-then-semi-out style="width:45%; border-style: dotted;border-width: 1px;border-color: \"#fefefe\"; margin-right:0.5%;padding:1%;font-size:0.65em;"}
### Supervised Learning

- We want to predict $Y$

```{r, warning=FALSE}
#| echo: false
df <- tibble(
   X1=runif(9),
   X2=runif(9)*10,
   X3=runif(9)*1.2+0.5,
   X4=runif(9)*0.01,
   X5=runif(9)*5,
   Y=sample(c("A", "B", "C"), 9, replace=TRUE)
)

knitr::kable(df, digits=2)
```


:::

::: {.column .fragment .fade-in style="width:45%; border-style: dotted;border-width: 1px;border-color: \"#fefefe\"; margin-right:0.5%;padding:1%;font-size:0.65em;"}
### Unsupervised Learning

- We work with $X$ only

```{r, warning=FALSE}
#| echo: false
df$Y <- "-"
knitr::kable(df, digits=2)
```

:::

:::

## How is it different?

::: columns

::: {.column style="width:45%; border-style: dotted;border-width: 1px;border-color: \"#fefefe\"; margin-right:0.5%;padding:1%;font-size:0.65em;"}
### Supervised Learning

- We have historic $X$ and $Y$ data

- Our main goal is to predict future values of $Y$

- Algorithms fit the data and supervise themselves objectively (e.g.: residuals)

- We can validate how well the model fits training data and how it generalises beyond that.

:::

::: {.column style="width:45%; border-style: dotted;border-width: 1px;border-color: \"#fefefe\"; margin-right:0.5%;padding:1%;font-size:0.65em;"}
### <mark>Unsupervised Learning</mark>

- We only have $X$ data

- The main goal is to observe (dis-)similarities in $X$

- There is no $Y$ variable to "supervise" how models should fit the data

- Validation is a lot more subjective. There is no objective way to check our work.

:::

:::


# Example: `USArrests`

## What the data looks like {.smaller .scrollable}

- How to visualise this data?

```{r}
#| echo: true
knitr::kable(USArrests)
```

:::notes
- This data set contains statistics, in arrests per 100,000 residents for assault, murder, and rape in each of the 50 US states in 1973. Also given is the percent of the population living in urban areas.
- A data frame with 50 observations on 4 variables.
[,1]	Murder	numeric	Murder arrests (per 100,000)
[,2]	Assault	numeric	Assault arrests (per 100,000)
[,3]	UrbanPop	numeric	Percent urban population
[,4]	Rape	numeric	Rape arrests (per 100,000)
:::

## EDA with our friend the `ggpairs` plot

```{r}
ggpairs(USArrests) + theme_bw()
```

----

### When you have too many features... 

![](/figures/week08/example_ggpairs_plot.png)

::: footer
üö¥ data from a cycling accident dataset we have compiled 
:::


::: notes
- It might be difficult to visualize and compare all features
- Recap this plot
:::

## When ggpairs becomes impractical

::: incremental
- If we have $p$ features, there will be $\frac{p \times (p - 1)}{2}$ different pair plots!
- Think about it:
   - 10 features -> 45 pair plots
   - 100 features -> 4950 pair plots
   - 1000 features -> 499500 pair plots <br/>(nearly 500k combinations!)
:::

## When you have too many features... {.smaller}

... you might want to <mark>reduce the dimensionality</mark> of your dataset

::: columns

::: {.column .fragment style="width:30%; }

::: {.callout-tip }

What if I told you you can reduce a matrix of **10** features into just **2**? 
:::

:::

::: {.column}

:::

:::

# PCA

Principal Component Analysis

## What is PCA? {.smaller}

Let's start with terminology:

::: incremental
- Principal Component Analysis (PCA) recombines the matrix of features $\mathbf{X}$
- If $\mathbf{X}$ has $n$ observations and $p$ features ($n \times p$), PCA will produce a new $n \times p$ matrix  $\mathbf{Z}$ 
   - Instead of "features" or "predictors", we call these new columns <mark>principal components</mark>
- You might be thinking: ü§î "wait, didn't you just say that PCA would help to _reduce_ the number of features?!"
- The thing is: because of the way this matrix is constructed, it is often enough to just use the first few columns of $\mathbf{Z}$.
:::

## The _first_ principal component {.smaller}

- The first column of $\mathbf{Z}$ is a linear combination of the $p$ features of $\mathbf{X}$:

$$
Z_1 = \phi_{11}X_1 + \phi_{21}X2 + \ldots + \phi_{p1}X_p
$$

subject to:

$$
\sum_{j=1}^p \phi_{j1}^2 = 1
$$

::: incremental
- We refer to the elements $\phi_{11}, \phi_{21}, \ldots, \phi_{p1}$ as the <mark>loadings</mark> of the first principal component.
- We can also refer to them collective, as the <mark>loading vector</mark> $\phi_1 = (\phi_{11}, \phi_{21}, \ldots, \phi_{p1})$.
- Each one of the new elements, say $z_{11}$ are referred to as <mark>scores</mark>
:::

## A closer look at the optimisation task: {.smaller}

$$
\begin{eqnarray}
&\operatorname{maximize}_{\phi_{11}, \ldots, \phi_{p1}}&\left\{\frac{1}{n}\sum_i^{n}\left(\sum_i^p{\phi_{j1}x_{ij}}\right)\right\}
\\
&\text{subject to} &\\
&&\sum_{j=1}^p \phi_{j1}^2 = 1
\end{eqnarray}
$$

- In practice, this maximizes the _sample variance_ of the $n$ values of $z_{i1}$
- Collective, the loading vector $\phi_1$ points to the direction of largest variance in the data
   - We will see an example soon!

## The _second_ principal component {.smaller}

::: {.fragment}
- As I said, $\mathbf{Z}$ has $p$ dimensions. We only talked about the first one.
:::

:::{.fragment}

- The second column of $\mathbf{Z}$ is **also** linear combination of the $p$ features of $\mathbf{X}$:

$$
Z_2 = \phi_{12}X_1 + \phi_{22}X2 + \ldots + \phi_{p2}X_p
$$

subject to:

$$
\sum_{j=1}^p \phi_{j2}^2 = 1
$$

but also:

- <mark>$\phi_2$ should be orthogonal to $\phi_1$</mark>

:::

## Example of two PCs

![](/figures/week08/Fig6_14.png)

::: footer
- You will find this as Figure 6.14 of our Textbook
:::

## Example: USArrests

![](/figures/week08/PCA_USArrest.png)


## üìπ YouTube Recommendation {.smaller}

- [3Blue1Brown](https://www.youtube.com/@3blue1brown) is fantastic! Do check it out.
- The video below helps to visualize how linear matrix transformations work

<iframe width="860" height="480" src="https://www.youtube.com/embed/kYB8IZa5AuE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## What's Next

- I will show you your Summative 02!