---
title: "DS202 Data Science for Social Scientists"
subtitle: "üóìÔ∏è Week 01 - Part II: Overview of core concepts"
author: Dr. [Jon Cardoso Silva](https://www.lse.ac.uk/DSI/People/Jonathan-Cardoso-Silva)
institute: '[LSE Data Science Institute](https://twitter.com/lsedatascience)'
toc: true
toc-depth: 1
toc-title: "What we will cover today"
center-title-slide: false
from: markdown+emoji
format:
  revealjs: 
    pdf-separate-fragments: true
    code-fold: true
    theme: simple
    slide-number: true
    mouse-wheel: true
    chalkboard: 
      buttons: true
    preview-links: auto
    background-color: '#ededed'
    logo: /figures/logos/LSE Data Science Institute.png
    css: /css/styles_slides.css
    footer: 'DS202 - Data Science for Social Scientists :robot: :juggling_person:'
---

# What do we mean by data science?

## Data science is...

::: {.fragment .fade-in}
> *"\[...\] a field of study and practice that involves the **collection**, **storage**, and **processing** of data in order to derive important üí° **insights** into a problem or a phenomenon.*
:::

::: {.fragment .fade-in}
> *Such data may be generated by **humans** (surveys, logs, etc.) or **machines** (weather data, road vision, etc.),*
:::

::: {.fragment .fade-in}
> *and could be in **different formats** (text, audio, video, augmented or virtual reality, etc.)."*
:::

::: aside
[@shah_hands-introduction_2020, chapter 1] - _Emphasis and emojis are of my own making._
:::

## The mythical unicorn :unicorn:

![](/figures/week01/unicorn.svg){.absolute left="10%" height="65%"}

::: {.fragment .absolute left=0% top=20% style="font-size: 0.6em; color: #f2f2f2; background-color: #1F497D; max-width:7em; padding-left:0.8em; padding-right:0.5em; margin-top:0em; margin-bottom:0em;"}
knows 
**everything** 
about statistics
:::

::: {.fragment .absolute left=0% bottom=23% style="font-size: 0.6em; color: #f2f2f2; background-color: #F4112C; max-width:8em; padding-left:0.8em; padding-right:0.6em; margin-top:0em; margin-bottom:0em;"}
able to
communicate 
**insights** perfectly
:::

::: {.fragment .absolute top=20% right=30% style="font-size: 0.6em; color: #f2f2f2; background-color: #7030A0; max-width:7.7em; padding-left:0.8em; padding-right:0.8em; margin-top:0em; margin-bottom:0em;"}
**fully** understands
businesses like
no one
:::

::: {.fragment .absolute bottom=25% right=17% style="font-size: 0.6em; color: #f2f2f2; background-color: #D99838; max-width:6em; padding-left:0.8em; padding-right:0.8em; margin-top:0em; margin-bottom:0em;"}
is a **fluent**
computer
programmer
:::

::: aside
See [@davenport_beyond_2020] for a more in-depth discussion about this
:::

::: notes
Of course, such a person does not exist!
:::

------------------------------------------------------------------------

## In reality...

::: columns

::: {.column style="width:50%; font-size:0.75em;"}
We are all jugglers ü§π

::: incremental
- Everyone brings a different skill set.
- We need multi-disciplinary teams.
- Good data scientists know **a bit** of everything.
  - Not fluent in _all_ things
  - Understands their strenghts and weaknessess
  - They know when and where to interface with others
:::

:::

::: {.column width="50%"}
![](/figures/week01/we_need_teams.png){.absolute height="72%"}
:::

:::

::: aside
See [@schutt_doing_2013, chapter 1] for more on this.
:::

# The <br/> Data <br/> Science <br/> Workflow

## The Data Science Workflow
```{dot}
//| data-id: 1
//| fig-width: 10
//| 
digraph {
  pad=0.5;
  margin=0;
	fontname="Helvetica,Arial,sans-serif";
	node [fontname="Helvetica,Arial,sans-serif", fontsize="20", fontcolor="#8431A6", shape=plaintext, margin=0.2]
	edge [fontname="Helvetica,Arial,sans-serif", penwidth=4, color="#8431A6";]
	
	rankdir=TB;
	
	
	start       [shape=box, margin=0.2, fontcolor="#8431A6", style=filled, color="#f2eaf6", label="Start"];
	gather      [label="Gather\ldata  ", imagepos="tc"];
	store       [label="Store it         \lsomewhere"];
	clean       [label="Clean &        \lpre-process"];
	build       [label="Build a \ldataset"];
	eda         [label="Exploratory    \ldata analysis"];
    ml          [label="Machine\llearning"];
	insight     [label="Obtain   \linsights"];
	communicate [label="Communicate\lresults         "];
	end         [shape=box, margin=0.1, fontcolor="#8431A6", color="#f2eaf6", style=filled, label="End"];
	
	subgraph cluster_80_percent {
	  color=blue;
		gather -> store  [label="      "];
	  store  -> clean  [label="      "];
	  clean  -> build  [label="      "];
	}

  build -> eda;

  subgraph cluster_20_percent{
    color=blue;
	  eda     -> ml           [label="      "];
      ml      -> insight      [label="      "];
	  insight -> communicate  [label="      "];
  }
    
  {rank = same; gather; store; clean; build;}
  {rank = same; eda; ml; insight; communicate;}
  
  start -> gather;
  communicate -> end;
}
```

::: aside
‚ö†Ô∏è Note that this is a simplified version of what happens in a data science project. <br/>
In practice, the process is not linear and there are many feedback loops.
:::

## The Data Science Workflow {auto-animate=true auto-animate-easing="ease-in-out"}
```{dot}
//| data-id: 1
//| fig-width: 10

digraph {
  pad=0.5;
  margin=0;
	fontname="Helvetica,Arial,sans-serif";
	node [fontname="Helvetica,Arial,sans-serif", fontsize="20", margin=0.2]
	edge [fontname="Helvetica,Arial,sans-serif", penwidth=4, color="#8431A6";]
	
	rankdir=TB;
	
  node [shape=box, margin=0.2, fontcolor="#8431A6", style=filled, color="#f2eaf6"];
  	
  start       [label="Start"];
  end         [margin=0.1, label="End"];
  
  node [fontcolor="#fcfcfc", style=filled, color="#8431A6"];
	gather      [label="Gather\ldata  ", imagepos="tc"];
	store       [label="Store it         \lsomewhere"];
	clean       [label="Clean &        \lpre-process"];
	build       [label="Build a \ldataset"];
	eda         [label="Exploratory    \ldata analysis"];

  node [fontcolor="#dac1e4", style=filled, color="#f2eaf6"];
  ml          [label="Machine\llearning"];
	insight     [label="Obtain   \linsights"];
	communicate [label="Communicate\lresults         "];

	start -> gather;
	subgraph cluster_80_percent {
	  color=blue;
		gather -> store  [label="      "];
	  store  -> clean  [label="      "];
	  clean  -> build  [label="      "];
	}

  build -> eda;

  edge [color="#f2eaf6"];
  subgraph cluster_20_percent{
    color=blue;
	  eda     -> ml           [label="      "];
    ml      -> insight      [label="      "];
	  insight -> communicate  [label="      "];
  }
    
  {rank = same; gather; store; clean; build;}
  {rank = same; eda; ml; insight; communicate;}

  communicate -> end;
}
```

It is often said that **80%** of the time and effort spent on a data science project goes to the tasks highlighted above.

## The Data Science Workflow {auto-animate=true auto-animate-easing="ease-in-out"}
```{dot}
//| data-id: 1
//| fig-width: 10

digraph {
  pad=0.5;
  margin=0;
	fontname="Helvetica,Arial,sans-serif";
	node [fontname="Helvetica,Arial,sans-serif", fontsize="20", margin=0.2]
	edge [fontname="Helvetica,Arial,sans-serif", penwidth=4, color="#f2eaf6"]
	
	rankdir=TB;
	
  node [shape=box, margin=0.2, fontcolor="#8431A6", style=filled, color="#f2eaf6"];
  	
	start       [label="Start"];
  end         [margin=0.1, label="End"];
  
  node [fontcolor="#dac1e4", style=filled, color="#f2eaf6"];
	gather      [label="Gather\ldata  ", imagepos="tc"];
	store       [label="Store it         \lsomewhere"];
	clean       [label="Clean &        \lpre-process"];
	build       [label="Build a \ldataset"];
	eda         [label="Exploratory    \ldata analysis", fontcolor="#fcfcfc", style=filled, color="#8431A6", margin=0.3];
    ml          [label="Machine\llearning", fontcolor="#fcfcfc", style=filled, color="#8431A6", margin=0.3];
	insight     [label="Obtain   \linsights"];
	communicate [label="Communicate\lresults         "];

	start -> gather;
	subgraph cluster_80_percent {
	  color=blue;
		gather -> store  [label="      "];
	  store  -> clean  [label="      "];
	  clean  -> build  [label="      "];
	}

  build -> eda;

  subgraph cluster_20_percent{
    color=blue;
	  eda     -> ml           [label="      "];
    ml      -> insight      [label="      "];
	  insight -> communicate  [label="      "];
  }
    
  {rank = same; gather; store; clean; build;}
  {rank = same; eda; ml; insight; communicate;}

  communicate -> end;
}
```

This course is about **Machine Learning**. So, in most examples and tutorials, we will assume that we already have good quality data. 

# How is that different to what I have learned in my previous stats courses?

## Data Science and Social Science

::: notes
In reality, data scientists work as a multidisciplinary group, collaborating towards a common goal.

Content borrowed from [ME314 Day 1](https://github.com/lse-me314/lectures)
:::

::: incremental
-   Social science: The goal is typically **explanation**
-   Data science: The goal is frequently **prediction**, or data exploration
-   Many of the same methods are used for both objectives
:::

::: aside
Check [@shmueli_explain_2010] for a discussion about this topic.
:::


# Machine Learning 

---

## What does it mean to learn something?

:::: {.columns}

::: {.column width="70%"}
![](/figures/week01/dalle_robot_holding_question_mark.png){.absolute height="80%"}
:::

::: {.column width="30%"}
:::


::: footer 
[Image created with the [DALL¬∑E](https://openai.com/blog/dall-e/) algorithm using the prompt: _'35mm macro photography of a robot holding a question mark card, white background'_]{style="font-size:0.6em;"}
:::

::::

## Predicting a sequence intuitively


::: {.incremental}
- Say our data is the following simple sequence: <font color="#4285F4"> $6, 9, 12, 15, 18, 21, 24, ...$ </font>
- What number do you expect to come next? Why?
- It is very likely that you guessed that <br/><font color="#4285F4">$\operatorname{next number}=27$</font>
- We spot that the sequence follows a pattern
- From this, we notice ‚Äî we **learn** ‚Äî that the sequence is governed by:<br/> <font color="#4285F4">$\operatorname{next number} = \operatorname{previous number} + 3$</font>

:::

## Predicting a sequence (formula)

The next number is a **function** of the previous one:

<font color="#4285F4">
$$
\operatorname{next number} = f(\operatorname{previous number})
$$
</font>

## Predicting a sequence (generic formula)

In general terms, we can represented it as:

<font color="#274f92">
$$
\operatorname{Y} = f(\operatorname{X})
$$
</font>

::: {.incremental .r-fit-text}
where:

- <font color="#274f92">$Y$</font>: a quantitative **response**. <br/> It goes by many names: dependent variable, response, target, outcome
- <font color="#274f92">$X$</font>: a set of **predictors**, <br/> also called inputs, regressors, covariates, features, independent variables.
- <font color="#274f92">$f$</font>: the *systematic* information that $X$ provides about $Y$

:::

## Predicting a sequence (generic formula)


In general terms, we can represented it as:

<font color="#274f92">

$$
\operatorname{Y} = f(\operatorname{X}) + \epsilon
$$

</font>

where:

- <font color="#274f92">$Y$</font>: the output
- <font color="#274f92">$X$</font>: a set of inputs
- <font color="#274f92">$f$</font>: the *systematic* information that $X$ provides about $Y$
- <font color="#274f92">$\epsilon~~$</font>: a **random error term**

::: {.notes}
In reality, there is some error $\epsilon$ that cannot be reduced.
:::


## Approximating $f$

::: incremental
- <font color="#274f92">$f$</font> is almost always unknown
- We aim to find an approximation (a **model**). Let's call it <font color="#274f92">$\hat{f}$</font>
- that can then use it to predict values of $Y$ for whatever <font color="#274f92">$X$</font>.
- That is: <font color="#274f92">$\hat{Y} = \hat{f}(X)$</font>
:::

## What is Machine Learning?

::: incremental
- Statistical learning, or Machine learning, refers to a set of approaches for estimating <font color="#274f92">$f$</font>.
- Each algorithm you will learn on this course has its own way to determine <font color="#274f92">$\hat{f}$</font> given data
:::

## Types of learning {.smaller}
<br/>

In general terms, there are two main ways to learn from data:

::: columns
::: {.column style="width: 45%;border-style: dotted;border-width: 1px;border-color: \"#fefefe\"; margin-right:0.5%;padding:1%;"}
### Supervised Learning {style="margin: 0em;"}
- Each observation ($x_i$) has an outcome associated with it ($y_i$).
- Your goal is to find a <font color="#274f92">$\hat{f}$</font> that produces <font color="#274f92">$\hat{Y}$</font> value close to the true <font color="#274f92">$Y$</font> values.
- Our focus on üóìÔ∏è Weeks 2, 3, 4 & 5.
:::

::: {.column style="width: 45%;border-style: dotted;border-width: 1px;border-color: \"#fefefe\"; margin-right:0.5%;padding:1%;"}
### Unsupervised Learning {style="margin: 0em;"}
- You have observations ($x_i$) but there is no response variable.
- Your goal is to find a <font color="#274f92">$\hat{f}$</font>, focused only on <font color="#274f92">$X$</font> that _best_ represents the patterns in the data.
- Our focus on üóìÔ∏è Weeks 7 & 8.
:::

:::

# Training algorithm

Now let's shift our attention to understanding:

- how we structure our data for supervised learning
- the different sources of statistical errors


## Data Structure {.smaller}

Let's go back to our example:

::: columns

::: {.column width=30%}

::: {.fragment}
Our simple sequence:

<font color="#4285F4"> $6, 9, 12, 15, 18, 21, 24$ </font>

:::
:::

::: {.column width=30%}

::: {.fragment}
Becomes:

| <font color="#274f92">$X$</font>  | <font color="#274f92">$Y$</font> |
|------|-----|
|   6  |  9  |
|   9  | 12  |
|  12  | 15  |
|  15  | 18  |
|  18  | 21  |
|  21  | 24  |

:::
:::

::: {.column style="width:30%; border-left-color: coral; border-left-width: 0.2em;"}

::: {.fragment}
And for prediction:

| <font color="#274f92">$X$</font>  | <font color="#274f92">$\hat{Y}$</font> |
|------|-----|
|  24  |  ?  |

we present the $X$ values and ask the **fitted model** to give us $\hat{Y}$.

:::
:::

:::

::: notes
Same data but now in tabular format


a few other terms:
- training data/test data
- fitted model
:::


## The ground truth

Let's create a dataframe to illustrate the process of training an algorithm:


```{r}
#| output-location: fragment
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap

library(tidyverse)

df = tibble(X=as.integer(seq(6, 21, 3)),
		    Y=as.integer(seq(6+3, 21+3, 3)))
print(df)
```


## Adding noise {.smaller}

Let's simulate the introduction of some random error:

```{r}
#| output-location: fragment
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap

# Let's simulate some noise
gaussian_noise = rnorm(n=nrow(df), mean=0, sd=1.5)

# Call it "observed Y"
df$obsY = df$Y + gaussian_noise
print(df)
```

## Visualizing the data 

```{r out.width="90%",out.height="40%", dpi=300}
library(ggpubr)
g <- ggplot(df %>% 
			gather("y", "pos", -X) %>% 
			filter(y == "Y"),
			aes(x=X, y=pos, color=y, shape=y)) +
	geom_point(size=3, stroke = 1.5) +
	stat_regline_equation(aes(label = ..eq.label..), size=7, color="#E69F25", show.legend =FALSE) +
	theme_bw() +
	theme(legend.position = "bottom",
		  legend.text = element_text(size=16),
	      axis.title.x = element_text(size=20),
		  axis.title.y = element_text(size=20),
		  axis.text.x = element_text(size=14),
		  axis.text.y = element_text(size=14)) +
	scale_shape_manual(name="", values=c("Y"=0, "obsY"=1)) +
	scale_color_manual(name="", values=c("Y"="#E69F25", "obsY"="#5CB4E4")) +
	scale_x_continuous(expression(X), 
					   limits=c(3, 27),
					   breaks=seq(3,27, 3)) +
	scale_y_continuous(expression("Y"), 
					   limits=c(3, 27),
					   breaks=seq(3,27, 3))

coeffs_Y = coefficients(lm(Y ~ X, data=df))
g <- g + geom_abline(intercept=coeffs_Y[1], slope=coeffs_Y[2], size=1.3, color="#E69F25", alpha=0.7)

g
```

## Visualizing the data (w/ noise)

```{r out.width="90%",out.height="40%", dpi=300}
library(ggpubr)

g <- ggplot(df %>% 
			gather("y", "pos", -X),
			aes(x=X, y=pos, color=y, shape=y)) +
	geom_point(size=3, stroke = 1.5) +
	theme_bw() +
	theme(legend.position = "bottom",
		  legend.text = element_text(size=16),
		  axis.title.x = element_text(size=20),
		  axis.title.y = element_text(size=20),
		  axis.text.x = element_text(size=14),
		  axis.text.y = element_text(size=14)) +
	scale_shape_manual(name="", values=c("Y"=0, "obsY"=1)) +
	scale_color_manual(name="", 
					   values=c("Y"="#E69F25", "obsY"="#5CB4E4")) +
	scale_x_continuous(expression(X), 
					   limits=c(3, 27),
					   breaks=seq(3,27, 3)) +
	scale_y_continuous(expression("Y"), 
					   limits=c(3, 27),
					   breaks=seq(3,27, 3)) 

g
```


## Visualizing the data (w/ noise)

```{r out.width="90%",out.height="40%", dpi=300}
library(ggpubr)

g <- ggplot(df %>% 
			gather("y", "pos", -X),
			aes(x=X, y=pos, color=y, shape=y)) +
	geom_point(size=3, stroke = 1.5) +
	stat_regline_equation(aes(label = ..eq.label.., color=y), size=7, show.legend =FALSE) +
	theme_bw() +
	theme(legend.position = "bottom",
		  legend.text = element_text(size=16),
		  axis.title.x = element_text(size=20),
		  axis.title.y = element_text(size=20),
		  axis.text.x = element_text(size=14),
		  axis.text.y = element_text(size=14)) +
	scale_shape_manual(name="", values=c("Y"=0, "obsY"=1)) +
	scale_color_manual(name="", 
					   values=c("Y"="#E69F25", "obsY"="#5CB4E4")) +
	scale_x_continuous(expression(X), 
					   limits=c(3, 27),
					   breaks=seq(3,27, 3)) +
	scale_y_continuous(expression("Y"), 
					   limits=c(3, 27),
					   breaks=seq(3,27, 3)) 


coeffs_obsY = coefficients(lm(obsY ~ X, data=df))
g <- g + 
	geom_abline(intercept=coeffs_Y[1], slope=coeffs_Y[2], size=1.3, color="#E69F25", alpha=0.5) +
	geom_abline(intercept=coeffs_obsY[1], slope=coeffs_obsY[2], size=1.3, color="#5CB4E4", alpha=0.8)
g
```

::: aside
Which line is closer to the "truth"?
:::

## Visualizing the data (w/ noise)

```{r out.width="90%",out.height="40%", dpi=300}
library(ggpubr)

g <- ggplot(df %>% 
			gather("y", "pos", -X),
			aes(x=X, y=pos, color=y, shape=y)) +
	geom_point(size=0.5, stroke = 1.5) +
	stat_regline_equation(aes(label = ..eq.label.., color=y), size=7, show.legend =FALSE) +
	theme_bw() +
	theme(legend.position = "bottom",
		  legend.text = element_text(size=16),
		  axis.title.x = element_text(size=20),
		  axis.title.y = element_text(size=20),
		  axis.text.x = element_text(size=14),
		  axis.text.y = element_text(size=14)) +
	scale_shape_manual(name="", values=c("Y"=0, "obsY"=1)) +
	scale_color_manual(name="", 
					   values=c("Y"="#E69F25", "obsY"="#5CB4E4")) +
	scale_x_continuous(expression(X), 
					   limits=c(0, 300)) +
	scale_y_continuous(expression("Y"), 
					   limits=c(0, 300)) 


coeffs_obsY = coefficients(lm(obsY ~ X, data=df))
g <- g + 
	geom_abline(intercept=coeffs_Y[1], slope=coeffs_Y[2], size=1.3, color="#E69F25", alpha=0.5) +
	geom_abline(intercept=coeffs_obsY[1], slope=coeffs_obsY[2], size=1.3, color="#5CB4E4", alpha=0.8)
g
```

::: aside
How much error can we accept?
:::

## Assessing error {.smaller}

How much error was introduced by <font color="#274f92">$\epsilon$</font> per sample?

```{r}
#| output-location: fragment
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap
#| code-line-numbers: "|1|2"

df$error    <- df$Y - df$obsY  # Calculate the error
df$absError <- abs(df$error)   # Ignore the sign of error
df
```

::: {.fragment}
On average, what is the error?

```{r}
#| output-location: fragment
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap

mean(df$absError)
```
:::

::: {.fragment}
This measure is called the [**M**ean **A**bsolute **E**rror](https://www.wikiwand.com/en/Mean_absolute_error).
:::

## Measures of error {.smaller}

This is what we computed:

$$
\operatorname{MAE} = \frac{\sum_{i=1}^n{|(y_i + \epsilon) - y_i|}}{n}
$$

::: {.incremental}
- We were able to compute this error because we **knew** what the ground truth <font color="#274f92">$Y$</font>, we knew what its _real_ value was.
- It was only possible because it was a simulation, not real data.
- In practice, we will almost never be able to assess the impact of <font color="#274f92">$\epsilon$</font>.
- We will use this same way of thinking to assess how good and accurate our models are. üîú
:::

# What's Next?

::: {.incremental .r-fit-text}
- We will introduce different measures of error and goodness-of-fit throughout this course. 
- Next week we will cover **Simple and Multiple Linear Regression**
- Join our ![](/figures/logos/slack_logo_simple.svg){height="1em"} [Slack group](/main/communication.qmd) if you haven't done so yet.
- Use the time before our first lab to revisit basic R programming skills.
- Head over to the üîñ [Week 01 - Appendix](/weeks/week01/appendix.qmd) page for:
	- Indicative & recommended reading
	- Programming Resources
:::

# Thank you

---
## References {style="word-break:break-all;"}

::: {#refs .r-fit-text}
:::
