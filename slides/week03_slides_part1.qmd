---
subtitle: "DS202 Data Science for Social Scientists"
title: "<font style='font-size:1em;'>üóìÔ∏è Week 03<br/> Classifiers - Part I</font>"
author: Dr. [Jon Cardoso-Silva](https://www.lse.ac.uk/DSI/People/Jonathan-Cardoso-Silva)
institute: '[\@LSEDataScience](https://twitter.com/lsedatascience)'
date: 14 October 2022
date-meta: 14 October 2022
toc: true
toc-depth: 1
toc-title: "What we will cover today"
center-title-slide: false
from: markdown+emoji
server: shiny
format:
  revealjs: 
    pdf-separate-fragments: true
    code-fold: false
    theme: simple
    slide-number: true
    mouse-wheel: true
    chalkboard: 
      buttons: true
    preview-links: auto
    logo: /figures/logos/LSE Data Science Institute.png
    css: /css/styles_slides.css
    footer: 'DS202 - Data Science for Social Scientists :robot: :juggling_person:'
---

# Regression vs Classification

## Classification {.smaller}

::: {.incremental}
- We have so far only modelled **quantitative** responses. 
- Today, we focus on predicting <mark>categorical</mark>, or <mark>qualitative</mark>, responses.
:::

::: columns

::: {.column style="width: 30%;margin-right:0.5%;padding:1%;"}
::: {.fragment .fade-in-then-semi-out}
The generic supervised model:

$$
Y = \operatorname{f}(X) + \epsilon
$$

still applies, only this time $Y$ is categorical. ‚û°Ô∏è

:::
:::


::: {.column style="width: 64%;margin-right:0.5%;padding:1%;"}
::: {.fragment}
Our categorical variables of interest take values in an **unordered set** $\mathcal{C}$, such as: 

- $\text{eye color} \in \mathcal{C} = \{\color{brown}{brown},\color{blue}{blue},\color{green}{green}\}$
- $\text{email} \in \mathcal{C} = \{spam, ham\}$
- $\text{football results} \in \mathcal{C} \{away\ win,draw,home\ win\}$
:::
:::

:::



::: notes
**Opening slides**
- Unordered here is an important distinction.
- We can also call it a class
:::

## Why can't I use linear regression? {.smaller}

::: columns

::: {.column style="width: 60%;margin-right:0.5%;padding:1%;"}
What if I just coded each category as a number?

$$
Y = 
    \begin{cases} 
        1 &\text{if}~\color{brown}{brown},\\
        2 &\text{if}~\color{blue}{blue},\\
        3 &\text{if}~\color{green}{green}.
    \end{cases}
$$
:::

::: {.column style="width: 34%;margin-right:0.5%;padding:1%;"}
What could go wrong?

![](/figures/week01/dalle_robot_holding_question_mark.png){.absolute height="25%"}
:::
:::

::: columns

::: {.column style="width: 60%;margin-right:0.5%;padding:1%;"}
::: fragment

How would you interpret a particular prediction if your model returned:

- $\hat{y} = ~~1.5$ or 
- $\hat{y} = ~~0.1$ or 
- $\hat{y} = 20.0$?
:::
:::

::: {.column style="width: 34%;margin-right:0.5%;padding:1%;"}


:::
:::

::: notes
Key takeaway: Regression is not suitable for all problems.
- regression cannot accommodate a qualitative response with more than two classes
- regression will not provide meaningful estaimtes of Pr(Y|X)

:::

## More on Classification {.smaller}

::: {.fragment fragment-index=1 style="margin-bottom:-2.5%;"}
- Often we are more interested in estimating the <mark>probabilities</mark> that $X$ belongs to each category in $\mathcal{C}$.
:::

::: {.fragment fragment-index=2 style="margin-bottom:-2.5%;"}
- For example, it is sometimes more valuable to have an estimate of the *probability* that an **insurance claim** is fraudulent, than a *classification* fraudulent or not.
:::

::: {.fragment fragment-index=2 .fade-in-then-out}
![](/figures/week03/pexels-monstera-5849597.jpg){.absolute width="50%" top="37.5%" left="25%"}
:::

::: {.fragment fragment-index=3 style="margin-bottom:-2.5%;"}
- A successful **gambling strategy**, for instance, requires placing bets on outcomes to which you believe the bookmakers have assigned incorrect probabilities. Knowing the most likely outcome is not enough!
:::

::: {.fragment fragment-index=3 .fade-in-then-out}
![](/figures/week03/pexels-sascha-d%C3%BCser-187333.jpg){.absolute width="40%" top="50%" left="60%"}
:::

::: {.fragment fragment-index=4 style="margin-top:10%;"}

::: callout-note
Statistical models for ordinal response, when sets are discrete but have an order, are outside the scope of this course. Should you need to create models for ordinal variables, consult "ordinal logistic regression". A good reference about this is [@agresti_introduction_2019, chapter 6].
:::
:::

::: notes
Key takeaway: normally, we estimate
:::


## Speaking of Probabilities... {.smaller}


Let's talk about three possible <mark>interpretations of probability</mark>:

::: columns

::: {.column width=25%}

::: {.fragment fragment-index=1}
::: {style="min-height:15vh;display:block;"}
**Classical**
:::
:::

::: {.fragment fragment-index=2}
::: {style="min-height:15vh;display:block;"}
**Frequentist**
:::
:::

::: {.fragment fragment-index=3}
::: {style="min-height:15vh;display:block;"}
**Bayesian**
:::
:::

:::

::: {.column width=75%}

::: {.fragment fragment-index=1}
::: {style="min-height:15vh;display:block;font-size:0.75em;"}
Events of the same kind can be reduced to a certain number of equally possible cases.

Example: coin tosses lead to either heads or tails $1/2$ of the time ( $50\%/50\%$)
:::
:::

::: {.fragment fragment-index=2}
::: {style="min-height:15vh;display:block;font-size:0.75em;"}
What would be the outcome if I repeat the process many times? 

Example: if I toss a coin $1,000,000$ times, I expect $\approx 50\%$ heads and $\approx 50\%$ tails outcome.
:::
:::

::: {.fragment fragment-index=3}
::: {style="min-height:15vh;display:block;font-size:0.75em;"}
What is your judgement of the likelihood of the outcome? Based on previous information. 

Example: if I know that this coin has symmetric weight, I expect a $ 50\%/50\%$ outcome.
:::
:::

:::

:::

::: aside
Source: [@degroot_probability_2003]
:::

## Speaking of Probabilities... {.smaller}


**For our purposes:**

- Probabilities are numbers between 0 and 1
- The sum of all possible outcomes of an event must sum to 1.
- It is <mark>useful</mark> to think of things as probabilities

![](/figures/week03/lithium-ion-battery.jpeg){.absolute width="45%" top="45.5%" left="55%"}

::: columns

::: {.column style="width: 45%;"}
::: {.fragment}

:::{.callout-note}
üí° Although there is no such thing as "a probability of $120\%$" or "a probability of $-23\%$", you could still use this language to refer to increase or decrease in an outcome.

::: 
:::
:::

::: {.column style="width: 55%;"}
<!-- Ghost column -->
:::

::: 

# Logistic Regression

## The Logistic Regression model {.smaller}

::: {.fragment}
Consider a <mark>binary</mark> response:

$$
Y = \begin{cases}
0 \\
1
\end{cases}
$$
:::

::: {.fragment}
We model the probability that $Y = 1$ as:

$$
Pr(Y = 1|X) = p(X) = \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 + \beta_1 X}}
$$
:::

::: {.fragment}
![](/figures/week03/logistic-regression-diagram.svg){.absolute height=35% top=60% left=25%}
:::

::: footer
Source of illustration: [TIBCO](https://www.tibco.com/reference-center/what-is-logistic-regression)
:::


::: notes
- This is how this function looks like
:::

## The Logistic function {.smaller}


- Changing $\beta_0$ while keeping $\beta_1 = 1$:

```{r out.width="100%", echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
#| echo: false
#| eval: true
#| code-fold: show
#| code-overflow: wrap

library(tidyverse)

X <- (-400:400)/100

df <- expand.grid(X=X, alpha=c(-2, 0, 1), beta=c(-1, 0, 1, 2))

df <- 
  df %>% group_by(alpha, beta) %>% 
  transform(X=X, Y=exp(alpha+beta*X)/(1+exp(alpha+beta*X)))

df <- df %>% 
  transform(alpha=factor(alpha, ordered = TRUE),
            beta=factor(beta, ordered = TRUE))

library(tidyverse)

ggplot(df %>% filter(beta == 1), 
       aes(x=X, y=Y, color=alpha, linetype=alpha)) +

  geom_line(size=1.4) +

  scale_linetype_manual(name="",
                 values=c("dotted", "solid", "dashed"),
                 labels=c(bquote(beta[0] == -2),
                          bquote(beta[0] == 0),
                          bquote(beta[0] == 1))) +
  scale_color_brewer(name="", type="qual",  palette="Set2",
    labels=c(bquote(beta[0] == -2), 
             bquote(beta[0] == 0), 
             bquote(beta[0] == 1))) +

  xlab("X") + ylab("Y") +

  # THEMING
  theme_bw() +
	theme(
        axis.title.x = element_text(size=24,margin = margin(t = 10, r = 0, b = 0, l = 0)),
        axis.title.y = element_text(size=24, margin = margin(t = 0,  r = 10, b = 0, l = 0)),
        legend.text = element_text(size=18),
        legend.position="right",
        axis.text.x = element_text(size=18),
        axis.text.y = element_text(size=18),
        panel.grid.minor = element_blank(),
        aspect.ratio = 1/2)
```


## The Logistic function (cont.) {.smaller}


- Keep $\beta_0 = 0$ but vary $\beta_1$:

```{r out.width="100%", out.height="100%", dpi=300, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}}
#| echo: false
#| eval: true
#| code-fold: show
#| code-overflow: wrap

library(tidyverse)

ggplot(df %>% filter(alpha == 0), 
       aes(x=X, y=Y, color=beta, linetype=beta)) +

  geom_line(size=1.4) +

  scale_linetype_manual(name="",
                 labels=c(bquote(beta[1] == -2),
                          bquote(beta[1] == 0),
                          bquote(beta[1] == 1),
                          bquote(beta[1] == 2)),
                  values=c("twodash", "dotted", "solid", "dashed")) +
  scale_color_brewer(name="", type="qual", palette="Set1",
    labels=c(bquote(beta[1] == -2), 
             bquote(beta[1] == 0), 
             bquote(beta[1] == 1),
             bquote(beta[1] == 2))) +

  # THEMING
  theme_bw() +
	theme(
        axis.title.x = element_text(size=24,margin = margin(t = 10, r = 0, b = 0, l = 0)),
        axis.title.y = element_text(size=24, margin = margin(t = 0,  r = 10, b = 0, l = 0)),
        legend.text = element_text(size=18),
        legend.position="right",
        axis.text.x = element_text(size=18),
        axis.text.y = element_text(size=18),
        panel.grid.minor = element_blank(),
        aspect.ratio = 1/2)
```


## Maximum likelihood estimate {.smaller}

::: {.fragment}
As with linear regression, the coefficients are unknown and need to be estimated from training data:

$$
\hat{p}(X) = \frac{e^{\hat{\beta}_0 + \hat{\beta}_1X}}{1 + e^{\hat{\beta}_0 + \hat{\beta}_1 X}}
$$
:::

::: {.fragment} 
We estimate these by maximising the <mark>likelihood function</mark>:

$$
\max \ell(\beta_0, \beta_1) = \prod_{i:y_i=1}{p(x_i)} \prod_{i':y_{i'}=0} (1 - p(x_{i'})), 
$$

and we call this method the <mark>Maximum Likelihood Estimate</mark> (MLE).
:::

::: {.fragment}

‚û°Ô∏è As usual, there are [multiple ways](https://www.wikiwand.com/en/Maximum_likelihood_estimation#/Iterative_procedures) to solve this equation!
:::


::: notes
Key takeaway of this slide: MLE (logistic regression) is analogous to OLS (linear regression).

<mark>Intuition:</mark> What are the values for $\alpha$ and $\beta$ that generate predicted probabilities, $\hat{Y}_i$ for each training observation that are as close as possible to the realised outcomes, $Y_i$?

:::


## Solutions to MLE {.smaller}

::: incremental
- MLE is much more difficult to solve than the least squares formulations.
- Most solutions rely on a variant of the [Hill Climbing](https://www.wikiwand.com/en/Hill_climbing) algorithm
:::

::: {.columns style="margin-top:-1%;"}

::: {.column width=30%}

::: {.fragment}
![](/figures/week03/pexels-eberhard-grossgasteiger-1366909.jpg){width="80%"}
:::
:::

::: {.column style="width:70%;font-size:0.95em;padding-top:1.75%;"}

::: {.fragment}
How do you find the latitude and longitude of a mountain peak if you can't see very far?
:::

::: {.incremental}

- Start somewhere.
- Look around for the best way to go up.
- Go a small distance in that direction.
- Look around for the best way to go up.
- Go a small distance in that direction.
- $\cdots$
:::
:::

:::

<!-- In R, all GLMs are solved with https://www.wikiwand.com/en/Iteratively_reweighted_least_squares -->



::: footer

::: {style="font-size:0.75em;"}
**Advanced:** If for whatever random reason, you find yourself enamored with the Maximum Likelihood Estimate, check [@agresti_introduction_2019] for a recent take on the statistical properties of this method.
:::

:::



## Example: `Default` data

::: columns

::: {.column width="45%"}
A sample of the data:

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| output-location: fragment
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap

library(ISLR2)

head(ISLR2::Default, n=15)
```
:::

::: {.column style="width:50%;margin-left:5%;"}

How the data is spread:

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| output-location: fragment
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap
summary(ISLR2::Default$default)
```

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| output-location: fragment
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap
summary(ISLR2::Default$balance)
```

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| output-location: fragment
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap
summary(ISLR2::Default$income)
```


```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| output-location: fragment
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap
summary(ISLR2::Default$student)
```
:::

:::

-------

### Who is more likely to default?

```{r out.width="100%", echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
#| echo: false
#| eval: true
#| code-fold: show
#| code-overflow: wrap
library(ggplot2)

plot_df <- ISLR2::Default
plot_df$proxy_alpha <-  plot_df$default == "Yes"

g <- 
  # AES  
  ggplot(plot_df, 
    aes(x=balance, 
        y=income, 
        color=default, 
        fill=default,
        alpha=default,
        shape=default)) + 
  geom_point(size=2.5) +
  

  # CUSTOMIZING
  scale_color_manual(name="Default", 
                     values=c("No"="#50bb9c", "Yes"="#D35F27")) +
  scale_alpha_discrete(name="Default", range=c(0.25, 0.9)) +                     
  scale_fill_manual(name="Default", 
                     values=c("No"="#69c5aa", "Yes"="#d76f3c")) +
  scale_shape_manual(name="Default", values=c(21, 21)) +
  scale_x_continuous(name="Monthly credit card balance ($)", 
                     breaks=function(x){seq(0, x[2]+500, 500)}) +
  scale_y_continuous(name="Monthly Income ($)", 
                     labels=scales::label_number(suffix = "k", scale = 1e-3)) +

  # THEMING
  theme_bw() +
	theme(
        axis.title.x = element_text(size=14, margin = margin(t = 10, r = 0, b = 0, l = 0)),
        axis.title.y = element_text(size=14, margin = margin(t = 0,  r = 10, b = 0, l = 0)),
        axis.text.x = element_text(size=12),
        axis.text.y = element_text(size=12),
        panel.grid.minor = element_blank(),
        aspect.ratio = 1.2/3)

  

g
```

::: aside
This is essentially the same plot as Figure 4.1 in [@james_introduction_2021]
:::

::: notes
- You can see that large balance leads more easily to default
:::

--------

### Does it matter if customer is a student? 


```{r out.width="100%", echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
#| echo: false
#| eval: true
#| code-fold: show
#| code-overflow: wrap
library(tidyverse)

plot_df <- ISLR2::Default
plot_df$proxy_alpha <-  plot_df$default == "Yes"

max_student_income <- 
  max((plot_df %>% filter(student == "Yes"))$income)

g <- 
  # AES  
  ggplot(plot_df, 
    aes(x=balance, 
        y=income, 
        color=default, 
        fill=default,
        alpha=default,
        shape=default)) + 
  geom_point(size=2.5) +
  

  # CUSTOMIZING
  scale_color_manual(name="Default", 
                     values=c("No"="#50bb9c", "Yes"="#D35F27")) +
  scale_alpha_discrete(name="Default", 
                       range=c(0.25, 0.9)) +                     
  scale_fill_manual(name="Default", 
                    values=c("No"="#69c5aa", "Yes"="#d76f3c")) +
  scale_shape_manual(name="Default", values=c(21, 21)) +
  scale_x_continuous(name="Monthly credit card balance ($)", 
                     breaks=function(x){seq(0, x[2]+500, 500)}) +
  scale_y_continuous(name="Monthly Income ($)", 
                     labels=scales::label_number(suffix = "k", 
                                                 scale = 1e-3)) +

  facet_grid(student  ~ ., labeller = "label_both") +


  # THEMING
  theme_bw() +
	theme(
        axis.title.x = element_text(size=14, margin = margin(t = 10, r = 0, b = 0, l = 0)),
        axis.title.y = element_text(size=14, margin = margin(t = 0,  r = 10, b = 0, l = 0)),
        axis.text.x = element_text(size=12),
        axis.text.y = element_text(size=12),
        panel.grid.minor = element_blank(),
        aspect.ratio = 1.2/5)

  

g
```

# Interpreting coefficients

<br/>
Since we now mostly care about <mark>probabilities</mark>, how do the <mark>odds</mark> change according to features of the customers?

## Simple logistic regression models


::: columns

::: {.column width="50%"}
- **Income** üí∞
```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap

income_model <- 
  glm(default ~ income, data=ISLR2::Default, family=binomial)
cat(sprintf("beta_0 = %.5f | beta_1 = %e",
            income_model$coefficients["(Intercept)"],
            income_model$coefficients["income"]))
```


- **Balance** üí∏

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap

balance_model <- 
  glm(default ~ balance, data=ISLR2::Default, family=binomial)
cat(sprintf("beta_0 = %.5f | beta_1 = %.4f",
            balance_model$coefficients["(Intercept)"],
            balance_model$coefficients["balance"]))
```


:::

::: {.column width="50%"}
- **Student** üßë‚Äçüéì

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap

student_model <- 
  glm(default ~ student, data=ISLR2::Default, family=binomial)
cat(sprintf("beta_0 = %.5f | beta_1 = %.4f",
            student_model$coefficients["(Intercept)"],
            student_model$coefficients["studentYes"]))
```


::: {.fragment style="font-size:0.70em;margin-top:15%;margin-left:6%;margin-right:6%;"}
::: {.callout-note }
Logistic regression coefficients are a bit trickier to interpret when compared to those of linear regression. Let's look at how it works ‚û°Ô∏è
:::
:::
:::

:::


::: notes
- Gather answers from students.

:::

## The concept of <mark>odds</mark> {.smaller}

The quantity below is called the <mark>odds</mark>:

$$
\frac{p(X)}{1 - p(X)} = e^{\beta_0 + \beta_1 X}
$$


::: {.callout-note style="margin-top:15%;"}
## Example

If the odds are 9, then $\frac{p(X)}{(1-p(X))} = 9 \Rightarrow p(X) = 0.9$. 

This means that 9 out of 10 people will default. 
:::

::: footer

::: {style="font-size:0.75em;"}
üìù Give it a go! Using algebra, can you re-arrange the equation for $p(X)$ presented in the [_Logistic regression model_ slides](#the-logistic-regression-model) to arrive at the odds quantity shown above?
:::
:::

## Log odds or _logit_ {.smaller}

- It is also useful to think of the odds in log terms.

$$
log\left(\frac{p(X)}{1 - p(X)}\right) = \beta_0 + \beta_1 X
$$

- We call the quantity above the <mark>log odds</mark> or <mark>logit</mark>

::: callout-tip

**How to interpret $\beta_1$**

If X increases one unit then the odds increase by $e^{\beta_1}$
:::

## Example: Default vs Balance


::: columns

::: {.column style="width:50%;font-size:0.75em;"}
- Model: **Default vs Balance** üí∏

$$
\hat{y} = \frac{e^{-10.65133 + 0.005498917X}}{1 + e^{-10.65133 + 0.005498917X}}
$$

:::

::: {.column style="width:50%;font-size:0.55em;"}
<br/>
<br/>
That is:

$$
\begin{align}
\hat{\beta}_0 &= -10.65133\\
\hat{\beta}_1 &= 0.005498917
\end{align}
$$

:::

:::

::: columns

::: {.column style="width:50%;font-size:0.55em;"}

::: {.fragment}
**Interpreting $\hat{\beta}_0$**:

- In the absence of `balance` information:
  - Log odds: $-10.65133$
  - Odds    : $e^{-10.65133} = 2.366933 \times 10^{-5}$
  $$
  \begin{align}
  p(\text{default}=\text{Yes}) &= \frac{\text{odds}}{(1 + \text{odds})} \\
                               &= 2.366877 \times 10^{-5}\end{align}
  $$
:::
:::

::: {.column style="width:50%;font-size:0.55em;"}

::: {.fragment}
**Interpreting $\hat{\beta}_1$**:

- With `balance` information:
  - Log odds: $0.005498917$
  - Odds    : $e^{0.005498917} = 1.005514$
  - That is, for every $\$1$ increase in balance, the probability of default <mark style="color:#069F72;">increases</mark> by $+0.5514\%$

:::
:::

:::

::: notes
- Note that the increase is cumulative, not linear. It depends on where X is.
:::

## Example: Default vs Income


::: columns

::: {.column style="width:50%;font-size:0.75em;"}
- Model: **Default vs Income** üí∞

$$
\hat{y} = \frac{e^{-3.094149 - 8.352575 \times 10^{-6} X}}{1 + e^{-3.094149 - 8.352575 \times 10^{-6} X}}
$$

:::

::: {.column style="width:50%;font-size:0.55em;"}
<br/>
<br/>
That is:

$$
\begin{align}
\hat{\beta}_0 &= - 3.094149\\
\hat{\beta}_1 &= - 8.352575 \times 10^{-6}
\end{align}
$$

:::

:::

::: columns

::: {.column style="width:50%;font-size:0.55em;"}

::: {.fragment}
**Interpreting $\hat{\beta}_0$**:

- In the absence of `balance` information:
  - Log odds: $- 3.094149$
  - Odds    : $e^{- 3.094149} = 0.04531355$
  $$
  \begin{align}
  p(\text{default}=\text{Yes}) &= \frac{\text{odds}}{(1 + \text{odds})} \\
                               &= 0.04334924 = 4.33\%\end{align}
  $$
:::
:::

::: {.column style="width:50%;font-size:0.55em;"}

::: {.fragment}
**Interpreting $\hat{\beta}_1$**:

- With `balance` information:
  - Log odds: $- 8.352575$
  - Odds    : $e^{- 8.352575} = 0.9999916$
  - That is, for every $\$1$ increase in income, the probability of default <mark style="color:#c9594c;">decreases</mark> by $0.000835254\%$

:::
:::

:::

::: notes
- Note that the increase is cumulative, not linear. It depends on where X is.
:::

## Example: Default vs Is Student?


::: columns

::: {.column style="width:50%;font-size:0.75em;"}
- Model: **Default vs Student** üßë‚Äçüéì

$$
\hat{y} = \frac{e^{-3.504128 + 0.4048871 X}}{1 + e^{-3.504128 + 0.4048871 X}}
$$

:::

::: {.column style="width:50%;font-size:0.55em;"}
<br/>
<br/>
That is:

$$
\begin{align}
\hat{\beta}_0 &= -3.504128\\
\hat{\beta}_1 &= +0.4048871
\end{align}
$$

:::

:::

::: columns

::: {.column style="width:50%;font-size:0.55em;"}

::: {.fragment}
**Interpreting $\hat{\beta}_0$**:

- In the absence of `balance` information:
  - Log odds: $-3.504128$
  - Odds    : $e^{- 3.504128} = 0.03007299$
  $$
  \begin{align}
  p(\text{default}=\text{Yes}) &= \frac{\text{odds}}{(1 + \text{odds})} \\
                               &= 0.02919501 \approx 2.92\%\end{align}
  $$
:::
:::

::: {.column style="width:50%;font-size:0.55em;"}

::: {.fragment}
**Interpreting $\hat{\beta}_1$**:

- With `balance` information:
  - Log odds: $0.4048871$
  - Odds    : $e^{0.4048871} =  1.499133$
  - If person is a student, then the probability of default <mark style="color:#069F72;">increases</mark> by $\approx 49\%$
    - $p(\text{default}=\text{Yes}|\text{student}=\text{Yes}) \approx 4.3\%$
:::
:::

:::

## Model info {.smaller}

The output of `summary` is similar to that of linear regression:

::: columns

::: {.column width="60%"}

- Model: **Default vs Balance** üí∏
```{r}
#| output-location: column-fragment
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap

summary(balance_model)
```

:::

::: {.column style="width:35%;margin-left:5%;"}

**Confidence Intervals**
```{r}
#| output-location: column-fragment
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap

confint(balance_model)
```


:::

:::

## Wraping up on coefficients: {.smaller}

::: columns

::: {.column width=80%}

- Pay attention to the **sign** of the coefficient. The sign of the coefficients indicate the direction of the association.
- If the value of a predictor increases, we look at the sign of its coefficient:
  - If it is a ‚ûï **positive coefficient**, we predict an <mark style="color:#069F72;">increase</mark> in the probability of the class
  - If it is a ‚ûñ **negative coefficient**, we predict a <mark style="color:#c9594c;">decrease</mark> in the probability of the class

:::

::: {.column width=20%}

:::

:::

# Multiple <br/> Logistic <br/> Regression {.smaller}

## Multiple Logistic Regression {.smaller}

- It is straightforward to extend the logistic model to include multiple predictors:

$$
log \left( \frac{p(X)}{1-p(X)} \right)=\beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p
$$

$$
p(X) = \frac{e^{\beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p}}{1 + e^{\beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p}}
$$

- Most things are still available (hypothesis test, confidence intervals, etc.)
- Let's explore the output and summary of the full model ‚è≠Ô∏è

## Fitting all predictors of `Default` {.smaller}

::: columns

::: {.column width="60%"}

**Full Model**
```{r}
#| output-location: column-fragment
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap

full_model <- glm(default ~ ., data=ISLR2::Default, family=binomial)
summary(full_model)
```

:::

::: {.column style="width:35%;margin-left:5%;"}

**Confidence Intervals**
```{r}
#| output-location: column-fragment
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap

confint(full_model)
```


:::

:::


# What's Next

After our 10-min break ‚òï:

- Bayes' Theorem
- The Naive Bayes algorithm
- Thresholds
- Confusion Matrix
- ROC Curve
- What will happen in our üíª labs this week?

---
## References {style="word-break:break-all;"}

::: {#refs .r-fit-text}
:::
