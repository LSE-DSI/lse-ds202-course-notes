---
subtitle: "DS202 Data Science for Social Scientists"
title: "üóìÔ∏è Week 03:<br/>Classifiers - Part II"
author: Dr. [Jon Cardoso-Silva](https://www.lse.ac.uk/DSI/People/Jonathan-Cardoso-Silva)
institute: '[\@LSEDataScience](https://twitter.com/lsedatascience)'
date: 14 October 2022
date-meta: 14 October 2022
toc: true
toc-depth: 1
toc-title: "What we will cover today"
center-title-slide: false
tbl-cap-location: bottom
from: markdown+emoji
format:
  revealjs: 
    html-q-tag: true
    pdf-separate-fragments: true
    code-fold: false
    theme: simple
    slide-number: true
    mouse-wheel: true
    chalkboard: 
      buttons: true
    preview-links: auto
    logo: /figures/logos/LSE Data Science Institute.png
    css: /css/styles_slides.css
    footer: 'DS202 - Data Science for Social Scientists :robot: :juggling_person:'
---

# Generative models

## Bayes' Theorem

- Before we go on to explain what Naive Bayes is about, we need to understand the formula below. 


$$
P(\mathbf{Y} = k | \mathbf{X} = x) = \frac{P(k)P(\mathbf{X}|\mathbf{Y}=k)}{\sum_{l=1}^{K}{P(l)P(\mathbf{X}|\mathbf{Y}=l)}}
$$

::: {.fragment}
- Let's look at it step-by-step ‚è≠Ô∏è
:::

## Bayes' Theorem {.smaller}


$$
P(\mathbf{Y} = k | \mathbf{X} = x) = \frac{P(k)P(\mathbf{X}|\mathbf{Y}=k)}{\sum_{l=1}^{K}{P(l)P(\mathbf{X}|\mathbf{Y}=l)}}
$$

**New variables**

::: incremental
- $K \Rightarrow$ is the set of classes. In the binary case, $K = \{0, 1\}$.
- $P(k) \Rightarrow$ is the probability that a random sample belongs to class $k$.
:::

::: columns

::: {.column width="40%"}
::: callout-note
The textbook uses a slightly different notation.
:::
:::

::: {.column width="60%"}
<!-- Ghost column -->
:::

:::


## Bayes' Theorem {.smaller}


$$
\color{blue}{P(\mathbf{Y} = k | \mathbf{X} = x)} \color{Gainsboro}{= \frac{P(k)P(\mathbf{X}|\mathbf{Y}=k)}{\sum_{l=1}^{K}{P(l)P(\mathbf{X}|\mathbf{Y}=l)}}}
$$

::: incremental
- The quantity above (in blue) is called the <mark>posterior distribution</mark>
- It is what we are interested in when making inferences/predictions
:::

::: fragment
Read it as:
<blockquote>
_What is the probability that the class is $k$ given that the sample is $x$?_
</blockquote>
:::

## Bayes' Theorem {.smaller}


$$
\color{Gainsboro}{P(\mathbf{Y} = k | \mathbf{X} = x) =} \frac{\color{blue}{P(k)}\color{Gainsboro}{P(\mathbf{X}|\mathbf{Y}=k)}}{\color{Gainsboro}{\sum_{l=1}^{K}{P(l)P(\mathbf{X}|\mathbf{Y}=l)}}}
$$

::: incremental
- The quantity above (in blue) is called the <mark>prior distribution</mark>
- It represents the proportion of samples of class $k$ we _believe_ (estimate) we would find if sampling at random.
:::

::: fragment
Read it as:
<blockquote>
_What is the probability that the class is $k$ given a random sample?_
</blockquote>
:::

## Bayes' Theorem {.smaller}


$$
\color{Gainsboro}{P(\mathbf{Y} = k | \mathbf{X} = x) =} \frac{\color{Gainsboro}{P(k)}\color{blue}{P(\mathbf{X}|\mathbf{Y}=k)}}{\color{Gainsboro}{\sum_{l=1}^{K}{P(l)P(\mathbf{X}|\mathbf{Y}=l)}}}
$$

::: incremental
- The quantity above (in blue) is often called the <mark>likelihood</mark>
- It represents the density function of $\mathbf{X}$ for samples of class $k$.
:::

::: fragment
Think of it as:
<blockquote>
_What values would I expect $X$ to take when the class is $\mathbf{Y} = k$?_
</blockquote>
:::

## Bayes' Theorem {.smaller}


$$
\color{Gainsboro}{P(\mathbf{Y} = k | \mathbf{X} = x) =} \frac{\color{Gainsboro}{P(k)}\color{Gainsboro}{P(\mathbf{X}|\mathbf{Y}=k)}}{\color{blue}{\sum_{l=1}^{K}{P(l)P(\mathbf{X}|\mathbf{Y}=l)}}}
$$

::: incremental
- The quantity above (in blue) represents the density function of $\mathbf{X}$ regardless of the class
- It is often called the <mark>marginal probability</mark> of $\mathbf{X}$.
  - Note that $\color{blue}{\sum_{l=1}^{K}{P(l)P(\mathbf{X}|\mathbf{Y}=l)}} = P(\mathbf{X})$
:::

::: fragment
Think of it as:
<blockquote>
_What values would I expect $X$ if ignored the class?_
</blockquote>
:::

## Bayes' Theorem {.smaller}


$$
P(\mathbf{Y} = k | \mathbf{X} = x) = \frac{P(k)P(\mathbf{X}|\mathbf{Y}=k)}{\sum_{l=1}^{K}{P(l)P(\mathbf{X}|\mathbf{Y}=l)}}
$$

- Let's look at how different algorithms explore this rule ‚è≠Ô∏è

# Linear Discriminant Analysis

## Linear Discriminant Analysis (LDA) {.smaller}

::: incremental
- Assumptions:
  - Likelihood follows a _Gaussian_ distribution
    - Each class has its own mean, $\mu_k$
    - All classes have the same standard deviation
      - That is, $\sigma^2_1 = \sigma^2_2 = \ldots = \sigma^2_K$, or simply $\sigma^2$ 
  - We denote this as: $P(\mathbf{X}|\mathbf{Y}=k) \sim N(\mu_k, \sigma^2)$ 
:::

## LDA - Estimates {.smaller}

- We estimate the mean per class and the shared standard deviation as follows:

::: {.fragment}
$$
\begin{align}
\hat{\mu}_k &= \frac{1}{n_k}\sum_{i:y_i=k}{x_i}\\
\hat{\sigma}^2 &= \frac{1}{n - K}\sum_{k=1}^K{\sum_{i:y_i=k}{\left(x_i - \hat{\mu}_k\right)^2}} \\
\hat{P}(k) &= \frac{n_k}{n} 
\end{align}
$$
:::

::: {.fragment}
- where:
  - $n$ is the total number of training observations
  - $n_k$ is the number of training observations in the $k$th class
:::


::: footer
::: {style="font-size:0.85em"}
Read [@james_introduction_2021, Section 4.4] to understand why these estimates are the way they are.
:::
:::

::: notes
- Mention that priors could come from prior knowledge
:::

# Naive Bayes Classifier

## Naive Bayes Classifier {.smaller}
::: {.fragment .fade-in-then-semi-out}
- Main Assumption:

<blockquote>
_Within the $k$th class, the $p$ predictors are **independent**_
</blockquote>

:::

::: {.fragment .fade-in-then-semi-out}
- Assuming features are not associated (not correlated), the likelihood becomes:
  $$ 
  P(\mathbf{X}|\mathbf{Y}=k) = \underbrace{P(x_1 |\mathbf{Y}=k)}_{1\text{st} \text{ predictor}} \times \underbrace{P(x_2 |\mathbf{Y}=k)}_{2\text{nd} \text{ predictor}} \times \ldots \times \underbrace{P(x_p |\mathbf{Y}=k)}_{p\text{-th} \text{ predictor}}
  $$
:::

::: {.fragment .fade-in-then-semi-out}
- This means the posterior is given by:
$$
P(\mathbf{Y} = k| \mathbf{X} = x) = \frac{\quad\quad P(k) \times P(x_1 |\mathbf{Y}=k) \times P(x_2 |\mathbf{Y}=k) \times \ldots \times P(x_p |\mathbf{Y}=k)}{\sum_{l=1}^K{P(l) \times P(x_1 |\mathbf{Y}=l) \times P(x_2 |\mathbf{Y}=l) \times \ldots \times P(x_p |\mathbf{Y}=l)}}
$$
:::

## A naive approach indeed {.smaller}

::: {.fragment .fade-in-then-semi-out}
- This may all look very complicated but it is actually quite simple
:::

::: {.fragment .fade-in-then-semi-out}
- If data is discrete (categorical), you just count the proportion of each category.

Example:

$$
P(\mathbf{Y} = k| \mathbf{X}_j = x_j) = 
\begin{cases}
0.32 & \text{if } x_j = 1\\
0.55 & \text{if } x_j = 2\\
0.13 & \text{if } x_j = 3
\end{cases}
$$
:::

::: {.fragment .fade-in-then-semi-out}
- If data is continuous, use a histogram as an estimate for the true density of $x_p$
  - Alternatively, use a <mark>kernel density estimator</mark>
:::


# Making decisions

## Default: Yes or No? {.smaller}

::: {.incremental}
- We have looked at how the probabilities (risk of default) change according to the value of predictors
- But in practice we need to decide whether the risk is too high or tolerable
- In our example, we might want to ask:

:::

::: {.fragment}
<blockquote>_"Will this person default on their credit card? **YES** or **NO**?"_</blockquote>
:::

## Default: Yes or No? {.smaller}

::: columns

::: {.column width="40%"}
How would you classify the following customers?

```{r}
#| echo: true
#| eval: true
#| code-fold: true
#| code-overflow: wrap
library(tidyverse)

full_model <- 
  glm(default ~ ., data=ISLR2::Default, family=binomial)

set.seed(40)
sample_customers <- 
  ISLR2::Default %>% 
  slice(9986, 9908, 6848, 9762, 9979, 7438)
pred <- predict(full_model, sample_customers, type="response")
# Format it as percentage
sample_customers$prediction <- 
  sapply(pred, function(x){sprintf("%.2f %%", 100*x)})
sample_customers
```
:::

::: {.column width="60%"}
<!-- Ghost column -->
:::

:::

::: aside

Full model expression:
$$
\hat{y} \approxeq \frac{e^{-10.87 - 0.65\times\text{student[Yes]} + 5.74 \times 10^{-3}\times\text{balance} + 3\times 10^{-6}\times\text{income}}}{1 + e^{-10.87 - 0.65\times\text{student[Yes]} + 5.74 \times 10^{-3}\times\text{balance} + 3\times 10^{-6}\times\text{income}}}
$$

:::

![](/figures/week01/dalle_robot_holding_question_mark.png){.absolute width="10%" bottom="2.5%" left="85%"}


::: footer
[Image created with the [DALL¬∑E](https://openai.com/blog/dall-e/) algorithm using the prompt: *'35mm macro photography of a robot holding a question mark card, white background'*]{style="font-size:0.6em;"}
:::

## Default: Yes or No? {.smaller}

::: columns


::: {.column width="40%"}
How would you classify the following customers?

```{r}
#| echo: true
#| eval: true
#| code-fold: true
#| code-overflow: wrap
library(tidyverse)

full_model <- 
  glm(default ~ ., data=ISLR2::Default, family=binomial)

set.seed(40)
sample_customers <- 
  ISLR2::Default %>% 
  slice(9986, 9908, 6848, 9762, 9979, 7438)
pred <- predict(full_model, sample_customers, type="response")
# Format it as percentage
sample_customers$prediction <- 
  sapply(pred, function(x){sprintf("%.2f %%", 100*x)})
sample_customers
```
:::


::: {.column style="width:30%;font-size:0.75em;"}

- If we set our <mark>threshold</mark> $= 50\%$, we get the following <mark>confusion matrix</mark>:

|                | Actual    |          |
|:--------------:|:---------:|:--------:|
| Predicted      |   **No**  |  **Yes** |
|    **No**      |    2      |   1      |
|   **Yes**      |    1      |   2      |

:::

::: {.column style="width:30%;font-size:0.75em;"}

::: {.fragment}

- If we set our <mark>threshold</mark> $= 40\%$, we get the following <mark>confusion matrix</mark>:

|                | Actual    |          |
|:--------------:|:---------:|:--------:|
| Predicted      |   **No**  |  **Yes** |
|    **No**      |    2      |   0      |
|   **Yes**      |    1      |   3      |
:::

:::

:::

<br/>

::: {.fragment}
### Which of the two is more accurate?
:::

## Thresholds {.smaller}

- When making predictions about classes, we always have to make decisions.
- <mark>Thresholds</mark>, applied to the predicted probability scores, are a way to decide whether to favour a particular class over another
- ‚è≠Ô∏è Next, we will explore several metrics that can help us decide whether our classification model is good or bad.  

# Classification Metrics

## Confusion Matrix {.smaller}

- Let's take another look at the confusion matrix. We can think of the numbers in each cell as the following:
<br/>

|                | Actual    |          |
|:--------------:|:---------:|:--------:|
| Predicted      |   **No**  |  **Yes** |
|    **No**      |    <mark style="color:#069F72;">True Negative (TN)</mark>     |   <mark style="color:#c9594c;">False Negative (FN)</mark>      |
|   **Yes**      |    <mark style="color:#c9594c;">False Positive (FP)</mark>      |   <mark style="color:#069F72;">True Positive (TP)</mark>      |

:  {tbl-colwidths="[20,40,40]"}

<br/>

- Ideally, we would have no False Negatives and no False Positives but, of course, that is never the case.

## Classification metrics {.smaller}

::: {.incremental}
- It is convenient to aggregate those quantities into a few other metrics
- Two of the most common ones are called <mark>sensitivity</mark> and <mark>specificity</mark>
:::

::: {.fragment}

$$
\begin{align}
\text{Sensitivity} &= \text{True Positive Rate (TPR)} = \frac{TP}{P} \\
\text{Specificity} &= \text{True Negative Rate (TNR)} = \frac{TN}{N} 
\end{align}
$$

:::

::: {.fragment}

- Another common one is <mark>accuracy</mark>:

$$
\text{Accuracy} = \frac{TP + TN}{P + N}
$$

<!-- :::

::: {.fragment} -->

- A good model has **high** sensitivity and **high** specificity and **high** accuracy.

:::

::: footer
There are [many other ways](https://www.wikiwand.com/en/Sensitivity_and_specificity) to assess the results of a classification model 
:::

## Which threshold is better? {.smaller}

::: columns

<!-- ```{r}
#| echo: true
#| eval: true
#| code-fold: true
#| code-overflow: wrap

require(caret)

threshold <- 0.5

full_predictions <-
  predict(full_model, ISLR2::Default, type="response")
full_predictions <- 
  ifelse(full_predictions > threshold, "Yes", "No")

comp_table <- 
  table(predicted = full_predictions, 
        actual = ISLR2::Default$default)

caret::confusionMatrix(comp_table, positive="Yes")
``` -->

::: {.column width="40%"}
üìù Now, looking at the logistic regression model we built for the entire dataset, work out the sensitivity, specificity and accuracy of the following confusion matrices:

<br/>

::: {.callout-tip}
## Practice
- ‚è≤Ô∏è 5 min to work out the math
- üó≥Ô∏è Vote on your preferred threshold <br/>(on ![](/figures/logos/slack_logo_simple.svg){height=0.75em} Slack)
:::
:::

::: {.column style="width:30%;font-size:0.75em;margin-left:3%;"}

$\text{Threshold} = 50\%$:

|                | Actual    |          |
|:--------------:|:---------:|:--------:|
| Predicted      |   **No**  |  **Yes** |
|    **No**      |    9627      |   228      |
|   **Yes**      |    40      |   105      |
:::

::: {.column style="width:30%;font-size:0.75em;margin-left:-3%;"}

$\text{Threshold} = 40\%$:

|                | Actual    |          |
|:--------------:|:---------:|:--------:|
| Predicted      |   **No**  |  **Yes** |
|    **No**      |    9588      |   199      |
|   **Yes**      |    79      |   134      |

:::


:::

## Meet the ROC curve {.smaller}

::: columns

::: {.column style="width:100%;font-size:0.9em;"}
- The <mark>Receiver Operating Characteristic</mark> (ROC) curve is another way to assess the model. 
- It shows how sensitivity and specificity change as we vary the threshold from 0 to 1 <br/>(threshold not shown).
:::

::: {.column width=0%}
<!-- Ghost column -->
:::

:::

```{r out.width="85%", dpi=100}
#| echo: false
#| eval: true
#| code-fold: show
#| code-overflow: wrap

library(tidyverse)
library(pROC)

full_predictions <-
  predict(full_model, ISLR2::Default, type="response")

rocobj <- roc(ISLR2::Default$default, full_predictions)


ggroc(rocobj, colour = 'steelblue', size = 2) + 

  xlab("Specificity (TNR)") + 
  ylab("Sensitivity (TPR)") + 

  # THEMING
  theme_bw() +
	theme(
        axis.title.x = element_text(size=24,margin = margin(t = 10, r = 0, b = 0, l = 0)),
        axis.title.y = element_text(size=24, margin = margin(t = 0,  r = 10, b = 0, l = 0)),
        legend.text = element_text(size=18),
        legend.position="right",
        axis.text.x = element_text(size=18),
        axis.text.y = element_text(size=18),
        panel.grid.minor = element_blank(),
        aspect.ratio = 1,
        plot.margin=grid::unit(c(0,0,0,0), "mm"))
```

::: notes
- Ask: how would an ideal curve look like?
:::

# What could go wrong?

## Generalisation problems {.smaller}

::: incremental
- The data used to train algorithms is called <mark>training data</mark>
- Often, we want to use the fitted models to make **predictions** on new previously unseen data
:::

::: {.fragment}

::: callout-important

‚ö†Ô∏è **A model that performs well on training data will not _necessarily_ perform well on new data** ‚ö†Ô∏è

:::

:::
::: incremental

- To make a robust assessment of our model, we have to split the data in two:
  - the training data and 
  - the <mark>test data</mark>
- We do NOT use the test data to fit the model
- We will come back to this next week, this is the topic of üóìÔ∏è Week 04.

:::

## Inappropriate reliance on metrics {.smaller}

::: incremental

- Accuracy can be very misleading when classes are <mark>imbalanced</mark>
- Consider the following model: $\hat{y} = \text{Yes}$ (always)
  - Only $3\%$ of customers default on their credit cards
  - Therefore, this model would have a $97\%$ accuracy! 
  - It is correct ninety-seven percent of times. But is it a good model?
    - üôÖ‚Äç‚ôÇÔ∏è **NO!**
- Similarly, you have to ask yourself about the usefulness of any other metric
  - Is True Positive Rate more or less important than True Negative Rate for the classification problem at hand?
  - Why? Why not?
- Ultimately, it boils down to how you plan to use this model afterwards.
:::

# What's Next

**Your Checklist:**

- :orange_book: Read [@james_introduction_2021, chapter 4]

- :eyes: Browse the slides again

- :memo: Take note of anything that isn't clear to you

- :pager: Share your questions on `/week03` channel on Slack

- :computer: In our labs [this week](/weeks/week03/lab.qmd), we will reinforce our knowledge of linear regression.

---
## References {style="word-break:break-all;"}

::: {#refs .r-fit-text}
:::
