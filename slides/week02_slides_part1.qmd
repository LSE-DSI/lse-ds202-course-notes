---
title: "DS202 Data Science for Social Scientists"
subtitle: "üóìÔ∏è Week 02: Linear Regression ‚Äî a predictive approach, motivation, and examples"
author: Dr. [Jon Cardoso Silva](https://www.lse.ac.uk/DSI/People/Jonathan-Cardoso-Silva)
institute: '[LSE Data Science Institute](https://twitter.com/lsedatascience)'
toc: true
toc-depth: 1
toc-title: "What we will cover today"
center-title-slide: false
from: markdown+emoji
format:
  revealjs: 
    pdf-separate-fragments: true
    code-fold: false
    theme: simple
    slide-number: true
    mouse-wheel: true
    chalkboard: 
      buttons: true
    preview-links: auto
    background-color: '#ededed'
    logo: /figures/logos/LSE Data Science Institute.png
    css: /css/styles_slides.css
    footer: 'DS202 - Data Science for Social Scientists :robot: :juggling_person:'
---

# What is Linear Regression

## The basic models {.smaller}

::: {.fragment}
Linear regression is a simple approach to supervised learning. 
:::

::: columns
::: {.column style="width: 29%;margin-right:0.5%;padding:1%;"}
::: {.fragment .fade-in-then-semi-out}
The generic supervised model:

$$
Y = \operatorname{f}(X) + \epsilon
$$

is defined more explicitly as follows ‚û°Ô∏è

:::
:::

::: {.column style="width: 31%;margin-right:0.5%;padding:1%;"}
::: {.fragment}
#### Simple linear regression

<mark class="math">

$$
\begin{align}
Y = \beta_0 +& \beta_1 X + \epsilon, \\ 
\\ 
\\
\end{align}
$$
</mark>

when we use a single predictor, $X$.

:::
:::

::: {.column style="width: 32%;margin-right:0.5%;padding:1%;"}
::: {.fragment}
#### Multiple linear regression

<mark class="math">

$$ 
\begin{align}
Y = \beta_0 &+ \beta_1 X_1 + \beta_2 X_2 \\
   &+ \dots \\
   &+ \beta_p X_p + \epsilon
\end{align}
$$

</mark>

when there are multiple predictors, $X_p$.
:::
:::

:::

::: columns

::: {.column style="width: 29%;margin-right:0.5%;padding:1%;"}
<!-- Ghost column -->
:::

::: {.column style="width: 65%;margin-right:0.5%;padding:1%;"}
::: {.fragment}

:::{.callout-warning}
- True regression functions are never linear!
- Although it may seem overly simplistic, linear regression is extremely useful both conceptually and practically.
::: 

:::
:::
::: 

::: {.notes}
- See [üì∫ Regression: Crash Course Statistics on YouTube](https://www.youtube.com/watch?v=WWqE7YHR4Jc&ab_channel=CrashCourse) for inspiration on how to present linear regression to students.
<video src="https://www.youtube.com/watch?v=WWqE7YHR4Jc&ab_channel=CrashCourse"> </video>

- We will talk about both types of models, how we can estimate the values of all $\beta$ and assess how good our models are.

:::

# Regression coefficients


## Linear Regression with a single predictor {.smaller}

::: columns

::: {.column style="width: 31%;margin-right:0.5%;padding:1%;"}

We assume a model:

<mark class="math">

$$
Y = \beta_0 + \beta_1 X + \epsilon ,
$$

</mark>
:::

::: {.column style="width: 40%;margin-left:20%;padding:0%;"}
```{r dpi=300}
# | data-id=1
library(ggplot2)

g <- 
  ggplot() + 
  theme_bw() +
	theme(legend.position = "bottom",
        legend.text = element_text(size=16),
        axis.title.x = element_text(size=25),
        axis.title.y = element_text(size=25),
        axis.text.x = element_text(size=20),
        axis.text.y = element_text(size=20),
        panel.grid.minor = element_blank(),
        aspect.ratio=3/10) +
  scale_x_continuous(name="X", limits=c(0, 10), breaks=seq(-1,10,1)) +
  scale_y_continuous(name="Y", limits=c(0, 3), breaks=seq(-1,4,1)) +
  geom_abline(intercept=1, slope=0.15, size=1.5, color="#E69F25", alpha=1.0) +
  geom_vline(xintercept = 0, linetype="dashed") +
  geom_hline(yintercept = 0, linetype="dashed") +
  annotate(geom="text", x=2.1, y=2.25, label="Y = 1 + 0.15 X", color="#E69F25", size=11)
g
```
:::

:::

::: {.incremental .r-fit-text}
where:

- <mark class="math">$\beta_0$</mark>: an unknown constant that represents the **intercept** of the line.
- <mark class="math">$\beta_1$</mark>: an unknown constant that represents the **slope** of the line
- <mark class="math">$\epsilon$</mark>: the random error term (irreducible)
:::

::: aside
::: {.fragment}
$\beta_0$ and $\beta_1$ are also known as <mark>coefficients</mark> or <mark>parameters</mark> of the model.
:::
:::


## Linear Regression with a single predictor {.smaller}

::: columns

::: {.column style="width: 31%;margin-right:0.5%;padding:1%;"}

We want to estimate:

$$
\hat{y} = \hat{\beta_0} + \hat{\beta_1} x
$$

:::

::: {.column style="width: 40%;margin-left:20%;padding:0%;"}
```{r dpi=300}
# | data-id=1

library(ggplot2)

g <- 
  ggplot() + 

  # TRUE LINE
  geom_abline(intercept=1, slope=0.15, size=1.5, color="#E69F25", alpha=0.7) +
  # ESTIMATE
  geom_abline(intercept=0.90, slope=0.17, size=1.4, color="#585858", alpha=0.9) +

  # TEXT
  annotate(geom="text", x=2.1, y=2.35, color="#585858", size=11,
           label=expression(hat(y) == hat(beta)[0] + hat(beta)[1] * x)) +
  
  # EXTRA
  geom_vline(xintercept = 0, linetype="dashed") +
  geom_hline(yintercept = 0, linetype="dashed") +
  # THEMING
  theme_bw() +
	theme(legend.position = "bottom",
        legend.text = element_text(size=16),
        axis.title.x = element_text(size=25),
        axis.title.y = element_text(size=25),
        axis.text.x = element_text(size=20),
        axis.text.y = element_text(size=20),
        panel.grid.minor = element_blank(),
        aspect.ratio=3/10) +
  scale_x_continuous(name="X", limits=c(0, 10), breaks=seq(-1,10,1)) +
  scale_y_continuous(name="Y", limits=c(0, 3), breaks=seq(-1,4,1))


g
```
:::

:::

::: {.incremental}
where:

- <mark class="math">$\hat{y}$</mark>: is a prediction of $Y$ on the basis of $X = x$.
- <mark class="math">$\hat{\beta_0}$</mark>: is an estimate of the "true" $\beta_0$.
- <mark class="math">$\hat{\beta_1}$</mark>: is an estimate of the "true" $\beta_1$.

:::

::: aside
::: {.fragment}
The <mark>hat</mark> symbol denotes an estimated value.
:::
:::


## Different estimators, different equations {.smaller}


::: columns

::: {.column width="55%"}

::: {.incremental .r-fit-text}
There are multiple ways to estimate the coefficients. 

- If you use different techniques, you might get different equations
- The most common algorithm is called <br/> <mark>Ordinary Least Squares (OLS)</mark>
- Just to name a few other estimators [@karafiath_is_2009]:
  - Least Absolute Deviation (LAD)
  - Weighted Least Squares (WLS)
  - Generalized Least Squares (GLS)
  - Heteroskedastic-Consistent (HC) variants
:::

:::


::: {.column style="width: 40%;margin-left:0%;padding:0%;"}

```{r out.height="100%", dpi=300}
# | data-id=1

library(ggplot2)

g <- 
  ggplot() + 

  # TRUE LINE
  geom_abline(intercept=1, slope=0.15, size=1.5, color="#E69F25", alpha=1.0) +
  # ESTIMATES
  geom_abline(intercept=0.80, slope=0.17, size=1, color="#585858", alpha=0.4) +
  geom_abline(intercept=0.90, slope=0.13, size=1, color="#585858", alpha=0.4) +
  geom_abline(intercept=0.95, slope=0.20, size=1, color="#585858", alpha=0.4) +
  geom_abline(intercept=1.10, slope=0.13, size=1, color="#585858", alpha=0.4) +
  geom_abline(intercept=1.05, slope=0.17, size=1, color="#585858", alpha=0.4) +
  geom_abline(intercept=1.07, slope=0.15, size=1, color="#585858", alpha=0.4) +

  # TEXT
  annotate(geom="text", x=1.4, y=0.55, color="#585858", size=11, hjust=0,
           label=expression(hat(y) == hat(beta)[0] + hat(beta)[1] * ~~~~~x)) +
  annotate(geom="text", x=1.4, y=0.25, color="#E69F25", size=11, hjust=0,
           label=expression(Y == ~1 + 0.15 * ~X)) +

  # EXTRA
  geom_vline(xintercept = 0, linetype="dashed") +
  geom_hline(yintercept = 0, linetype="dashed") +
  # THEMING
  theme_bw() +
	theme(legend.position = "bottom",
        legend.text = element_text(size=16),
        axis.title.x = element_text(size=25),
        axis.title.y = element_text(size=25),
        axis.text.x = element_text(size=20),
        axis.text.y = element_text(size=20),
        panel.grid.minor = element_blank()) +
  scale_x_continuous(name="X", limits=c(0, 3), breaks=seq(-1,10,0.5)) +
  scale_y_continuous(name="Y", limits=c(0, 2), breaks=seq(-1,4,0.5))




g
```
:::


:::

::: aside
::: {.fragment}
We will only cover <mark>OLS</mark> in this course.
:::
:::

# Residual errors

To understand Ordinary Least square, we first need to understand the concept of residuals.

## The concept of <mark style="color:#c9594c;">residuals</mark>

Suppose you came across some data:

```{r fig.height=4, dpi=300}
# | data-id=1

library(ggplot2)

set.seed(10)
x <- rnorm(100, mean=1.5, sd=1.5)
y_true <- 0.9 + 0.17*x
y_real <- y_true + rnorm(100, mean=0, sd=0.25)

df <- data.frame(x=x, y_true=y_true, y_real=y_real)

g <- 
  ggplot(df, aes(x=x, y=y_real)) + 

  geom_point(shape=21, size=5, fill="#274f92", alpha=0.5) +


  # EXTRA
  geom_vline(xintercept = 0, linetype="dashed") +
  geom_hline(yintercept = 0, linetype="dashed") +
  # THEMING
  theme_bw() +
	theme(legend.position = "bottom",
        legend.text = element_text(size=16),
        axis.title.x = element_text(size=25),
        axis.title.y = element_text(size=25),
        axis.text.x = element_text(size=20),
        axis.text.y = element_text(size=20),
        panel.grid.minor = element_blank()) +
  scale_x_continuous(name="x", limits=c(0, 3), breaks=seq(-1,10,0.5)) +
  scale_y_continuous(name=expression(paste("Estimated Y (", hat(y), ")")), 
                     limits=c(0, 2), breaks=seq(-1,4,0.5))




g
```

::: aside
_And you suspect there is a linear relationship between X and Y._
:::

::: notes
First, let's think of the concept of residuals...
:::

## The concept of <mark style="color:#c9594c;">residuals</mark>

So, you decide to fit a line to it.

```{r fig.height=4, dpi=300}
# | data-id=1

library(ggplot2)

set.seed(10)
x <- rnorm(100, mean=1.5, sd=1.5)
y_true <- 0.9 + 0.17*x
y_real <- y_true + rnorm(100, mean=0, sd=0.25)

df <- data.frame(x=x, y_true=y_true, y_real=y_real)

g <- 
  ggplot(df, aes(x=x, y=y_real)) + 
  geom_point(shape=21, size=5, fill="#274f92", alpha=0.5) +
 

  # ESTIMATES
  geom_abline(intercept=0.90, slope=0.17, size=1.4, color="#585858", alpha=0.8) +


  annotate(geom="text", x=1.51, y=0.25, color="#585858", size=7, hjust=0,
           label=expression(paste("Fitted line: ", hat(y) == ~0.90 + 0.17 * ~X))) +

  # EXTRA
  geom_vline(xintercept = 0, linetype="dashed") +
  geom_hline(yintercept = 0, linetype="dashed") +
  # THEMING
  theme_bw() +
	theme(legend.position = "bottom",
        legend.text = element_text(size=16),
        axis.title.x = element_text(size=25),
        axis.title.y = element_text(size=25),
        axis.text.x = element_text(size=20),
        axis.text.y = element_text(size=20),
        panel.grid.minor = element_blank()) +
  scale_x_continuous(name="x", limits=c(0, 3), breaks=seq(-1,10,0.5)) +
  scale_y_continuous(name=expression(paste("Estimated Y (", hat(y), ")")), limits=c(0, 2), breaks=seq(-1,4,0.5))




g
```

::: aside
_A line that goes right through the middle of the cloud of data._
:::

## The concept of <mark style="color:#c9594c;">residuals</mark>

<!-- <p class="r-fit-text"> -->
<mark style="color:#c9594c;">Residuals</mark> are the distances from each data point to this line.
<!-- </p> -->

```{r fig.height=4, dpi=300}
# | data-id=1

library(ggplot2)

set.seed(10)
x <- rnorm(100, mean=1.5, sd=1.5)
y_true <- 0.9 + 0.17*x
y_real <- y_true + rnorm(100, mean=0, sd=0.25)

df <- data.frame(x=x, y_true=y_true, y_real=y_real)

g <- 
  ggplot(df, aes(x=x, y=y_real)) + 

  geom_segment(aes(x=x, xend=x, y=y_true, yend=y_real), alpha=0.6, color="#c9594c") +
  geom_point(shape=21, size=5, fill="#274f92", alpha=0.5) +
 

  # ESTIMATES
  geom_abline(intercept=0.90, slope=0.17, size=1.4, color="#585858", alpha=0.8) +


  annotate(geom="text", x=1.51, y=0.25, color="#585858", size=7, hjust=0,
           label=expression(paste("Fitted line: ", hat(y) == ~0.90 + 0.17 * ~X))) +

  # EXTRA
  geom_vline(xintercept = 0, linetype="dashed") +
  geom_hline(yintercept = 0, linetype="dashed") +
  # THEMING
  theme_bw() +
	theme(legend.position = "bottom",
        legend.text = element_text(size=16),
        axis.title.x = element_text(size=25),
        axis.title.y = element_text(size=25),
        axis.text.x = element_text(size=20),
        axis.text.y = element_text(size=20),
        panel.grid.minor = element_blank()) +
  scale_x_continuous(name="x", limits=c(0, 3), breaks=seq(-1,10,0.5)) +
  scale_y_continuous(name=expression(paste("Estimated Y (", hat(y), ")")), limits=c(0, 2), breaks=seq(-1,4,0.5))




g
```

::: aside
<mark style="color:#c9594c;">$e_i$</mark>$=y_i-\hat{y}_i$ represents the $i$th <mark style="color:#c9594c;">residual</mark>
:::

## Residual Sum of Squares (RSS) {.smaller}

From this, we can define the <mark> Residual Sum of Squares </mark> (RSS) as 

$$
\mathrm{RSS}= e_1^2 + e_2^2 + \dots + e_n^2,
$$ 

::: {.fragment}
or equivalently as

$$
\mathrm{RSS}= (y_1 - \hat{\beta}_0 - \hat{\beta}_1 x_1)^2 + (y_2 - \hat{\beta}_0 - \hat{\beta}_1 x_2)^2 + \dots + (y_n - \hat{\beta}_0 - \hat{\beta}_1 x_n)^2.
$$ 
:::

<br/></br>

::: {.fragment}

::: {.callout-note}
The (ordinary) least squares approach chooses $\hat{\beta}_0$ and $\hat{\beta}_1$ to minimize the RSS.
:::

:::

::: notes
That is how it does its job.
:::

## A question for you

::: columns
::: {.column width="70%"}
![](/figures/week01/dalle_robot_holding_question_mark.png){.absolute height="80%"}
:::

::: {.column width="30%"}

Why the squares and not, say, just the sum of residuals?

:::

::: footer
[Image created with the [DALL¬∑E](https://openai.com/blog/dall-e/) algorithm using the prompt: *'35mm macro photography of a robot holding a question mark card, white background'*]{style="font-size:0.6em;"}
:::
:::

::: {.notes}
- Explain that the sum penalizes individual large errors a lot more
<mark class="idea">Consider adding a visualisation to illustrate this point.</mark>
:::


# Ordinary Least Squares

How do we estimate the coefficients using the ordinary least squares algorithm?

## The objective function {.smaller}

We treat this as an optimisation problem. We want to minimize RSS:
$$
\begin{align}
\min \mathrm{RSS} =& \sum_i^n{e_i^2} \\
             =& \sum_i^n{\left(y_i - \hat{y}_i\right)^2} \\
             =& \sum_i^n{\left(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i\right)^2}
\end{align}
$$

## Estimating $\hat{\beta}_0$ {.smaller}

To find $\hat{\beta}_0$, we have to solve the following partial derivative:

$$
\frac{\partial ~\mathrm{RSS}}{\partial \hat{\beta}_0}{\sum_i^n{(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2}} = 0
$$

::: {.fragment}
...
which will lead you to:
:::

::: {.fragment}
$$
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x},
$$
:::

::: {.fragment}
where we made use of the sample means:

- $\bar{y} \equiv \frac{1}{n} \sum_{i=1}^n y_i$
- $\bar{x} \equiv \frac{1}{n} \sum_{i=1}^n x_i$ 

:::


::: {.aside}

::: {.fragment}
üìù Give it a go! Pretend $\hat{\beta}_1$ is constant and use the [power rule](https://brilliant.org/wiki/derivatives-of-polynomials/) to solve the equation and reach the same result.
:::
:::

::: notes
Full derivation if needed:

$$
\begin{align}
0 &= \frac{\partial ~\mathrm{RSS}}{\partial \hat{\beta}_0}{\sum_i^n{(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2}} & (\text{chain rule})\\
0 &= \sum_i^n{-2 (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)} & (\text{take $-2$ out})\\
0 &= -2 \sum_i^n{ (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)}  & (\div -2) \\
0 &=\sum_i^n{ (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)}  & (\text{sep. sums}) \\
0 &=\sum_i^n{y_i} - \sum_i^n{\hat{\beta}_0} - \sum_i^n{\hat{\beta}_1 x_i}  & (\text{simplify}) \\
0 &=\sum_i^n{y_i} - n\hat{\beta}_0 - \hat{\beta}_1\sum_i^n{ x_i}  & (+ n\hat{\beta}_0) \\
n\hat{\beta}_0 &= \sum_i^n{y_i} - \hat{\beta}_1\sum_i^n{ x_i} & (\text{isolate }\hat{\beta}_0 ) \\
\hat{\beta}_0 &= \frac{\sum_i^n{y_i} - \hat{\beta}_1\sum_i^n{ x_i}}{n} & (\text{rearranging}) \\
\hat{\beta}_0 &= \frac{\sum_i^n{y_i}}{n} - \hat{\beta}_1\frac{\sum_i^n{x_i}}{n} & (\text{or simply}) \\
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x} & \blacksquare
\end{align} 
$$

:::

## Estimating $\hat{\beta}_1$ 

Similarly, to find $\hat{\beta}_1$ we solve:

$$
\frac{\partial ~\mathrm{RSS}}{\partial \hat{\beta}_1}{[\sum_i^n{y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i}]} = 0
$$

::: {.fragment}
...
which will lead you to:
:::

::: {.fragment}
$$
\hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i-\bar{x})^2}
$$
:::

::: {.aside}

::: {.fragment style="font-size:1rem;"}
üìù Give it a go! Use the same method as before to solve the equation and isolate $\hat{\beta}_1$. Tip: Use the previous formula to substitute $\hat{\beta}_0$.
:::
:::

::: notes
Full derivation if needed:

$$
\begin{align}
0 &= \frac{\partial ~\mathrm{RSS}}{\partial \hat{\beta}_1}{\sum_i^n{(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2}} & (\text{chain rule})\\
0 &= \sum_i^n{\left(-2x_i~ (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)\right)} & (\text{take $-2$ out})\\
0 &= -2\sum_i^n{\left( x_i~ (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)\right)} & (\div -2) \\
0 &= \sum_i^n{\left(x_i~ (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)\right)} & (\text{distribute } x_i) \\
0 &= \sum_i^n{\left(y_ix_i - \hat{\beta}_0x_i - \hat{\beta}_1 x_i^2\right)} & (\text{replace } \hat{\beta}_0) \\
0 &= \sum_i^n{\left(y_ix_i - (\bar{y} - \hat{\beta}_1 \bar{x})x_i - \hat{\beta}_1 x_i^2\right)} & (\text{rearrange}) \\
0 &= \sum_i^n{\left(y_ix_i - \bar{y}x_i + \hat{\beta}_1 \bar{x}x_i - \hat{\beta}_1 x_i^2\right)} & (\text{separate sums}) \\
0 &= \sum_i^n{\left(y_ix_i - \bar{y}x_i\right)} + \sum_i^n{\left(\hat{\beta}_1 \bar{x}x_i - \hat{\beta}_1 x_i^2\right)} & (\text{take $\hat{\beta}_1$ out}) \\
0 &= \sum_i^n{\left(y_ix_i - \bar{y}x_i\right)} + \hat{\beta}_1\sum_i^n{\left(\bar{x}x_i - x_i^2\right)} & (\text{isolate}) \\
\hat{\beta}_1 &= \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i-\bar{x})^2}
\end{align} 
$$

:::

## Parameter Estimation (OLS)

And that is how OLS works!

$$
\begin{align}
\hat{\beta}_1 &= \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i-\bar{x})^2} \\
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x}
\end{align}
$$

## Estimates for Multiple Regression {.smaller}

::: {.fragment .r-fit-text}
The process of estimation is similar when we have more than one predictor. To estimate:

$$
\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 + \dots + \hat{\beta}_p x_p.
$$

:::

::: {.fragment .r-fit-text}
We aim to minimize Residual Sum of Squares as before:

$$
\min \mathrm{RSS} = \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \sum_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_{i1} - \hat{\beta}_2 x_{i2} - \dots - \hat{\beta}_p x_{ip})^2.
$$

This is done using standard statistical software ‚Äî you need a good linear algebra solver. 
:::

::: {.fragment .r-fit-text}
The values $\hat{\beta}_0, \hat{\beta}_1, \dots, \hat{\beta}_p$ that minimize RSS are the multiple least squares regression coefficient estimates.
:::

## Example: Advertising data

::: columns

::: {.column width="45%"}
A sample of the data:

```{r}
#| output-location: fragment
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap

library(tidyverse)

file = "https://www.statlearning.com/s/Advertising.csv"
advertising <- read_csv(file) %>% select(-1)
head(advertising, 11)
```
:::

::: {.column style="width:50%;margin-left:5%;"}

How the data is spread:

```{r}
#| output-location: fragment
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap
summary(advertising$TV)
```

```{r}
#| output-location: fragment
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap
summary(advertising$radio)
```

```{r}
#| output-location: fragment
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap
summary(advertising$newspaper)
```


```{r}
#| output-location: fragment
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap
summary(advertising$sales)
```
:::

:::

---------------------------------

### Relationship: advertising budget and sales

Is advertising spending related to sales?

```{r out.width="100%"}
#| echo: false
#| eval: true
#| code-fold: show
#| code-overflow: wrap

plot_df <- 
  advertising %>% 
  gather("source", "investment", -sales)

plot_df$source <- 
  sub('^(\\w?)', '\\U\\1', plot_df$source, perl=T)

plot_df$source <-
  factor(plot_df$source, ordered=TRUE,
         levels=c("TV", "Radio", "Newspaper"))

g <- 
  # AES  
  ggplot(plot_df, aes(x=investment, y=sales)) + 
  geom_point(size=2.5, shape=21, 
             fill="#c9594c", alpha=0.6) +

  geom_smooth(size=1.3, method="lm", color="#1e90ff", 
              se = FALSE, show.legend = FALSE) +

  # Faceting
  facet_grid(~ source, scales = "free") +

  # CUSTOMIZING
  scale_x_continuous(name="Investment in the media channel (in 1,000 $)") +
  scale_y_continuous(name="Sales (in 1,000 units)") +

  # THEMING
  theme_bw() +
	theme(
        axis.title.x = element_text(size=14, margin = margin(t = 10, r = 0, b = 0, l = 0)),
        axis.title.y = element_text(size=14, margin = margin(t = 0,  r = 10, b = 0, l = 0)),
        axis.text.x = element_text(size=12),
        axis.text.y = element_text(size=12),
        panel.grid.minor = element_blank(),
        aspect.ratio = 1)

g
```

::: aside
This is the plot you see in the book
:::

-----------------------------------

### Relationship: advertising budget and sales

Is advertising spending related to sales?

```{r out.width="100%"}
#| echo: false
#| eval: true
#| code-fold: show
#| code-overflow: wrap

plot_df <- 
  advertising %>% 
  gather("source", "investment", -sales)

plot_df$source <- 
  sub('^(\\w?)', '\\U\\1', plot_df$source, perl=T)

plot_df$source <-
  factor(plot_df$source, ordered=TRUE,
         levels=c("TV", "Radio", "Newspaper"))

g <- 
  # AES  
  ggplot(plot_df, aes(x=investment, y=sales)) + 
  geom_point(size=2.5, shape=21, 
             fill="#c9594c", alpha=0.6) +

  geom_smooth(size=1.3, method="lm", color="#1e90ff", 
              se = FALSE, show.legend = FALSE) +

  # Faceting
  facet_grid(~ source) +

  # CUSTOMIZING
  scale_x_continuous(name="Investment in the media channel (in 1,000 $)") +
  scale_y_continuous(name="Sales (in 1,000 units)") +

  # THEMING
  theme_bw() +
	theme(
        axis.title.x = element_text(size=14, margin = margin(t = 10, r = 0, b = 0, l = 0)),
        axis.title.y = element_text(size=14, margin = margin(t = 0,  r = 10, b = 0, l = 0)),
        axis.text.x = element_text(size=12),
        axis.text.y = element_text(size=12),
        panel.grid.minor = element_blank(),
        aspect.ratio = 1)

g
```

::: aside
This is the same plot but now all scales match
:::

## Simple linear regression models


::: columns

::: {.column width="50%"}

- **TV** üì∫
```{r}
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap

tv_model <- lm(sales ~ TV, data=advertising)
cat(sprintf("Sales (1k units) = %.4f %+.4f TV ($ 1k)\n", 
            tv_model$coefficients["(Intercept)"], 
            tv_model$coefficients["TV"]))
```

- **Radio** üìª
```{r}
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap

radio_model <- lm(sales ~ radio, data=advertising)
cat(sprintf("Sales (1k units) = %.4f %+.4f Radio ($ 1k)\n", 
            radio_model$coefficients["(Intercept)"], 
            radio_model$coefficients["radio"]))
```

:::

::: {.column width="50%"}
- **Newspaper** üì∞
```{r}
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap

newspaper_model <- lm(sales ~ newspaper, data=advertising)
cat(sprintf("Sales (1k units) = %.4f %+.4f Newspaper ($ 1k)\n", 
            newspaper_model$coefficients["(Intercept)"], 
            newspaper_model$coefficients["newspaper"]))
```

:::{.fragment style="font-size:1.5rem;overflow:wrap;margin-left:5%;margin-top:20%;"}

üó®Ô∏è How should we interpret these models?
:::

:::

:::


::: notes
- Gather answers from students.
- For every 1k dollars spent in advertising on a particular media channel, we expect more $\hat{\beta}_1$ thousand units of the product to be sold.
- Why don't the models agree about the intercept?
:::

# Confidence Intervals {.smaller}

::: columns

::: {.column width="60%"}

::: {.incremental}
- If we were to fit a linear model from repeated samples of the data, we would get different coefficients every time.
- Because of the [Central Limit Theorem](https://statisticsbyjim.com/basics/central-limit-theorem/), we know that the **mean of this sampling distribution** can be approximated by a <mark>Normal Distribution</mark>.
- We know from [Normal Theory](http://www.homepages.ucl.ac.uk/~ucaktwa/teaching/NormalTheory.pdf) that 95% of the distribution lies in $[\mu - 2 \sigma, \mu + 2 \sigma]$.  
:::

::: {.fragment fragment-index=5}
> A 95% <mark>confidence interval</mark> is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter.
:::

:::

::: {.column style="width:30%;margin-left:10%;"}

::: {.fragment fragment-index=4}
![](/figures/week02/NormalDist1.96.png){width="100%"}
:::
:::
:::

::: notes
- How well does my model fit the data?
- What is the confidence interval of the estimates?
(under a scenario where we got repeated samples like the present sample). 
:::

## Confidence Interval of coefficients {.smaller}

<!-- .smaller does not work well when the title is H3. I often need to make manual adjustments with font-size -->

::: {.fragment}
- The confidence interval of an estimate has the form:
$$
\hat{\beta}_1 \pm 2 \times \mathrm{SE}(\hat{\beta}_1).
$$
where $SE$ is the <mark>standard error</mark> and reflects how the estimate varies under repeated sampling. 
:::

::: {.fragment}
- That is, there is approximately a 95% chance that the interval
$$
\biggl[ \hat{\beta}_1 - 2 \times \mathrm{SE}(\hat{\beta}_1), \hat{\beta}_1 + 2 \times \mathrm{SE}(\hat{\beta}_1) \biggr]
$$
will contain the true value of $\beta_1$.
:::

::: notes
How SE differs from STD? See [@altman_standard_2005]
:::

## Standard Errors {.smaller}
<!-- .smaller does not work well when the title is H3. I often need to make manual adjustments with font-size -->

::: {.fragment}
The <mark>standard error</mark> of $\hat{\beta}_0$ and $\hat{\beta}_1$ is shown below:


$$
\begin{align}
  \mathrm{SE}(\hat{\beta}_1)^2 &= \frac{\sigma^2}{\sum_{i=1}^n (x_i - \bar{x})^2}, \\
  \mathrm{SE}(\hat{\beta}_0)^2 &= \sigma^2 \biggl[ \frac{1}{n} +  \frac{\bar{x}^2}{\sum_{i=1}^n (x_i - \bar{x})^2} \biggr],
\end{align}
$$

where $\sigma^2 = \operatorname{Var}(\epsilon)$.
:::

::: {.incremental }
- But, wait, we don't know $\epsilon$! How would we compute $\sigma^2$?
- In practice, we aproximate $\sigma^2 \approx \mathrm{RSE} = \sqrt{\mathrm{RSS}/(n-2)}$.
:::

::: {.fragment}
::: callout-important
üí° Standard errors are a type of standard deviation but are not the same! See [@altman_standard_2005] for more on this.
:::
:::


::: notes
- These formulas are only valid if we assume the errors $\epsilon_i$ have common variance $\sigma^2$ and are uncorrelated.
- RSE makes a comeback in Section 3.1.3
:::

## Back to our Advertising linear models {.smaller}

What are the confidence intervals of our independent linear models?

::: columns

::: {.column width="50%"}

- **TV** üì∫
```{r}
#| output-location: fragment
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap

confint(tv_model)
```

- **Radio** üìª
```{r}
#| output-location: fragment
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap

confint(radio_model)
```

:::

::: {.column width="50%"}
- **Newspaper** üì∞
```{r}
#| output-location: fragment
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap

confint(newspaper_model)
```

:::{.fragment style="font-size:1.5rem;overflow:wrap;margin-left:5%;margin-top:14%;"}

üó®Ô∏è What does it mean?
:::

:::

:::

::: aside
Use the function [`confint`](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/confint) to compute confidence intervals in `R`.
::: 

::: notes
- For every additional $1000 invested in Radio, we can expect an increase in sales of between 162 and 242 units.
:::

# Hypothesis Testing {.smaller}

::: incremental
- Standard errors can also be used to perform <mark>hypothesis tests</mark> on the coefficients. 
- The most common hypothesis test involves testing the <mark>null hypothesis</mark> of:
  - $H_0$: There is no relationship between $X$ and $Y$ versus the \alert{alternative hypothesis}. 
  - $H_A$: There is some relationship between $X$ and $Y$. 
:::

::: {.fragment style="margin-top:-1%;"}
- Mathematically, this corresponds to testing:

$$
\begin{align}
&~~~~H_0:&\beta_1 &= 0 \\
&\text{vs} \\
&~~~~H_A:& \beta_1 &\neq 0,
\end{align}
$$

since if $\beta_1=0$ then the model reduces to $Y = \beta_0 + \epsilon$, and $X$ and $Y$ are not associated.
:::

## p-values {.smaller}


- To test the null hypothesis, we compute a <mark>t-statistic</mark>, given
by
$$
t = \frac{\hat{\beta}_1 - 0}{\mathrm{SE}(\hat{\beta}_1)},
$$
::: incremental
- This will have a t-distribution with $n - 2$ degrees of freedom, assuming $\beta_1 = 0$.
- Using statistical software, it is easy to compute the probability of observing any value equal to $\mid t \mid$ or larger.
- We call this probability the <mark>p-value</mark>.
:::

## Back to our Advertising linear models {.smaller}

How significant are the linear models?

::: columns

::: {.column width="50%"}

- **TV** üì∫
```{r}
#| output-location: fragment
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap

out <- capture.output(summary(tv_model))
cat(paste(out[9:15]), sep="\n")
```

- **Radio** üìª
```{r}
#| output-location: fragment
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap

out <- capture.output(summary(radio_model))
cat(paste(out[9:15]), sep="\n")
```

:::

::: {.column width="50%"}
- **Newspaper** üì∞
```{r}
#| output-location: fragment
#| echo: true
#| eval: true
#| code-fold: false
#| code-overflow: wrap

out <- capture.output(summary(newspaper_model))
cat(paste(out[9:15]), sep="\n")
```

:::{.fragment style="font-size:1.5rem;overflow:wrap;margin-left:5%;margin-top:14%;"}

üó®Ô∏è What does it mean?
:::

:::

:::

::: notes

:::

# What's Next

After our 10-min break ‚òï:

- Metrics to assess goodness-of-fit
- Interpreting Multiple Linear Regression üìà
- Interaction effects
- Outliers
- Collinearity
- What will happen in our üíª labs this week?

---
## References {style="word-break:break-all;"}

::: {#refs .r-fit-text}
:::
