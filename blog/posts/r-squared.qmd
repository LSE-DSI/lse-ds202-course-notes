---
title: "What happens to R-squared when we add more predictors?"
subtitle: "DS202 Blog Post"
date: 10 October 2022
date-meta: 10 October 2022
categories: [week02, R-squared, equations]
theme: lumen
---

::: callout-tip
## TLDR

When using [OLS](/slides/week02_slides_part1.html#/ordinary-least-squares), $R^2$ will always stay the same or increase you add more features to a linear regression, even if those features are "useless".

:::

## Data and Variables

Suppose you have the following data:

$$
\mathbf{X} = \left[
\begin{array}{cc}
x_{11}&x_{12}\\
x_{21}&x_{22}\\
\vdots&\vdots\\
x_{n1}&x_{n2} 
\end{array}\right],
$$

 that is, $p=2$ features for $n$ observations, and you want to fit a linear regression to attempt to predict the $\mathbf{y}$ column below:
 
$$
\mathbf{y} = \left[
\begin{array}{c}
y_{1}\\
y_2\\
\vdots\\
y_n
\end{array}\right].
$$

## About $R^2$

As we saw briefly on [Week 02](/weeks/week02/lecture.qmd) and as defined in Chapter 3 of the textbook [@james_introduction_2021], $R^2$ represents the proportion of variability in the response variable that can be explained using the dependent variables (features). More specifically:

$$
R^2 = 1 - \frac{\operatorname{RSS}}{\operatorname{TSS}}
$$

where $\operatorname{TSS}$ represents the <mark>Total Sum of Squares</mark>:

$$
\operatorname{TSS} =  \sum_i^n{\left(y_i - \bar{y}\right)^2},
$$

and $\bar{y}$ is the mean of $\mathbf{y}$.

As for the $\operatorname{RSS}$, it represents the <mark>Residual Sum of Squares</mark>. 

To understand why $R^2$ can never decrease if we add more features, I think it is useful to visualise linear regression in matrix format.

## Linear Regression in matrix format

We can also represent the regression coefficients obtained by <mark>Ordinary Least Squares</mark> ([OLS](/slides/week02_slides_part1.html#/ordinary-least-squares)) in matrix format. $\hat{\boldsymbol{\beta}}$ is a $1 \times (p+1)$ matrix:

$$
\hat{\boldsymbol{\beta}} = \left[
\begin{array}{c}
\hat{\beta}_{0}\\
\hat{\beta}_1\\
\hat{\beta}_2
\end{array}\right].
$$

When thinking about regression this way, it is often useful to add an additional column of 1s to $\mathbf{X}$:

$$
\mathbf{X} = \left[
\begin{array}{ccc}
1 & x_{11}&x_{12}\\
1 & x_{21}&x_{22}\\
1 & \vdots&\vdots\\
1 & x_{n1}&x_{n2} 
\end{array}\right],
$$

so that:

$$
\hat{\boldsymbol{\beta}}\mathbf{X} =\left[
\begin{array}{c}
\hat{\beta}_{0} \times 1 + \hat{\beta}_1 \times x_{11} + \hat{\beta}_2 \times x_{12}\\
\hat{\beta}_{0} \times 1 + \hat{\beta}_1 \times x_{21} + \hat{\beta}_2 \times x_{22}\\
\vdots  \\ 
\hat{\beta}_{0} \times 1 + \hat{\beta}_1 \times x_{n1} + \hat{\beta}_2 \times x_{n2}\\
\end{array}\right] = \hat{\mathbf{y}}
$$

represents the estimated values $\hat{\mathbf{y}}$.

Residuals can then be represented as $\mathbf{e} = (\mathbf{y} - \hat{\mathbf{y}})$, or

$$
\mathbf{e} =\left[
\begin{array}{c}
y_{1} - (\hat{\beta}_{0} \times 1 + \hat{\beta}_1 \times x_{11} + \hat{\beta}_2 \times x_{12})\\
y_{2} - (\hat{\beta}_{0} \times 1 + \hat{\beta}_1 \times x_{21} + \hat{\beta}_2 \times x_{22})\\
\vdots  \\ 
y_{n} = (\hat{\beta}_{0} \times 1 + \hat{\beta}_1 \times x_{n1} + \hat{\beta}_2 \times x_{n2})\\
\end{array}\right] 
$$

OLS provides an optimal way to find coefficients $\mathbf{\hat{\beta}}$ that minimizes $\operatorname{RSS}$.

In matrix format, the $\operatorname{RSS}$ can be represented as:

$$
\operatorname{RSS} = \mathbf{e}^T\mathbf{e}.
$$

## Regression with all features

If we use all features ($p=2$) of our example, then OLS will find an optimal solution to $\text{min } \operatorname{RSS}$. Let's call it $$

## Now, increase one feature (p=2)

When we add the second column of $\mathbf{x}_2$ to the linear regression and we fit $\mathbf{y} \sim \mathbf{x}_1 + \mathbf{x}_2$, we will obtain a new set of regression coefficients — ${\beta}_0''$,  ${\beta}_1''$ and ${\beta}_2''$ — as well as new corresponding $\operatorname{RSS}_{(p=2)}$ and $\operatorname{R^2}_{(p=2)}$ values. Note that $\operatorname{TSS}$ is always the same, irrespective of the number of features.

To understand why $R^2$ always stays the same or increases when $p$ increases, let's compare $\operatorname{RSS}_{(p=2)}$ to $\operatorname{RSS}_{(p=1)}$. First, let's expand $\operatorname{RSS}_{(p=2)}$:

$$
\begin{align}
\operatorname{RSS}_{(p=2)} =& \sum_i^n{\left(y_i - \hat{y}_i''\right)^2}\\
                           =& \sum_i^n{\left( y_i - {\beta}_0'' - {\beta}_1'' x_{i1} - {\beta}_2'' x_{i2} \right)^2}\\
\end{align}
$$

## Recall the objective function of OLS

In this course we are only interested in the <mark>Ordinary Least Squares</mark> ([OLS](/slides/week02_slides_part1.html#/ordinary-least-squares)) method to estimate the regression coefficients of our linear model. OLS will always try to minimize $\operatorname{RSS}$ and so, what does that mean to $\operatorname{RSS}_{(p=2)}$?

Now, <mark style="color:red">let's plug the previous estimate, $y_i = \beta_0' + \beta_1' x_{i1}$</mark> and re-arrange the terms:

$$
\begin{align}
& \sum_i^n{\left( {\beta}_0' + {\beta}_1' x_{i1} - {\beta}_0'' - {\beta}_1'' x_{i1} - {\beta}_2'' x_{i2} \right)^2} & \\
=& \sum_i^n{\left( ({\beta}_0' - {\beta}_0'') + ({\beta}_1'  - {\beta}_1'') x_{i1} - {\beta}_2'' x_{i2} \right)^2} & \text{  Eq. A}
\end{align}
$$

From the above, you can see that the signs of coefficients do not interfer in $\operatorname{RSS}$, as the equation takes the **square** of errors but their magnitudes do. 

In the simple case where ${\beta}_0' = {\beta}_0''$ and ${\beta}_1' = {\beta}_1''$, we are left with the expression $\operatorname{RSS}_{(p=2)} = \sum_i^n{\left(- {\beta}_2'' x_{i2}\right)^2}$, which goes from $0$ to $\infty$ but never negative. Note that the expression in Eq. A can never go below 0, even if ${\beta}_0' \neq {\beta}_0''$ and ${\beta}_1' \neq {\beta}_1''$. Thus, $\operatorname{RSS}_{(p=2)} - \operatorname{RSS}_{(p=1)} \geq 0$ and  $R^2_{(p=2)} \geq R^2_{(p=1)}$.

<!-- 
<HIDDEN> A longer route:

We want to show that the following is always true:

$$
\begin{align}
\operatorname{RSS}_{(p=1)} \leq \operatorname{RSS}_{(p=2)}.
\end{align}
$$

So let's expand these expressions and see what we find.

$$
\begin{align}
\operatorname{RSS}_{(p=1)} &\leq \operatorname{RSS}_{(p=2)} & \text{}\\
\sum_i^n{\left(y_i - \hat{y}_i'\right)^2} &\leq \sum_i^n{\left(y_i - \hat{y}_i''\right)^2} & \text{expanding}\\
\sum_i^n{y_i^2 + (\hat{y}_i')^2 - 2y_i\hat{y}_i'} &\leq \sum_i^n{y_i^2 + (\hat{y}_i'')^2 - 2y_i\hat{y}_i''} & \text{expanding square}\\
\sum_i^n{(\hat{y}_i')^2 - 2y_i\hat{y}_i'} &\leq \sum_i^n{(\hat{y}_i'')^2 - 2y_i\hat{y}_i''} & \text{subtract} \sum_i^n{y_i^2}\\
\end{align}
$$

which, if we expand $\hat{y}_i'$ and $\hat{y}_i''$, we arrive at:

$$
\begin{align}
\sum_i^n{({\beta}_0' + {\beta}_1' x_{i1})^2 - 2y_i({\beta}_0' + {\beta}_1' x_{i1})} &\leq \sum_i^n{({\beta}_0'' + {\beta}_1'' x_{i1} + {\beta}_2'' x_{i2})^2 - 2y_i({\beta}_0'' + {\beta}_1'' x_{i1} + {\beta}_2'' x_{i2})} \\
\end{align}
$$

Keep on expanding:


$$
\begin{align}
\operatorname{RSS}_{(p=1)} &= \sum_i^n{{\beta}_0'^2 + ({\beta}_1' x_{i1})^2 + 2\beta_0'\beta_1'x_{i1} - 2y_i{\beta}_0' - 2y_i{\beta}_1' x_{i1}} \\ 
=& \sum_i^n{{\beta}_1'^2 x_{i1}^2 + (2\beta_0'\beta_1' - 2y_i{\beta}_1')x_{i1} + {\beta}_0'^2}
\end{align}
$$

$$
\begin{align}
\operatorname{RSS}_{(p=2)} &=
\sum_i^n{({\beta}_0'' + {\beta}_1'' x_{i1} + {\beta}_2'' x_{i2})^2 - 2y_i{\beta}_0'' - 2y_i{\beta}_1'' x_{i1} - 2y_i{\beta}_2'' x_{i2}} \\
\sum_i^n{{\beta}_0''^2 + ({\beta}_1'' x_{i1})^2 + ({\beta}_2'' x_{i2})^2 + 2 {\beta}_0''{\beta}_1'' x_{i1} + 2 {\beta}_1'' x_{i1} {\beta}_2'' x_{i2} + 2 {\beta}_2'' x_{i2}{\beta}_0 - 2y_i{\beta}_0'' - 2y_i{\beta}_1'' x_{i1} - 2y_i{\beta}_2'' x_{i2}} \\
\ldots
\end{align}
$$ -->


[Reference](https://math.stackexchange.com/questions/1976747/prove-that-r2-cannot-decrease-when-adding-a-variable)