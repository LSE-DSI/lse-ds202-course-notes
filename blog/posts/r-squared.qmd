---
title: "What happens to R-squared when we add more predictors?"
subtitle: "DS202 Blog Post"
date: 10 October 2022
date-meta: 10 October 2022
categories: [week02, R-squared, equations]
theme: lumen
---

::: callout-tip
## TLDR

When using [OLS](/slides/week02_slides_part1.html#/ordinary-least-squares), $R^2$ will always stay the same or increase you add more features to a linear regression, even if those features are "useless".

:::

## Data and Variables

Suppose you have the following data:

$$
\mathbf{X} = \left[
\begin{array}{cc}
x_{11}&x_{12}\\
x_{21}&x_{22}\\
\vdots&\vdots\\
x_{n1}&x_{n2} 
\end{array}\right],
$$

 that is, $p=2$ features for $n$ observations, and you want to fit a linear regression to attempt to predict the $\mathbf{y}$ column below:
 
$$
\mathbf{y} = \left[
\begin{array}{c}
y_{1}\\
y_2\\
\vdots\\
y_n
\end{array}\right].
$$


## A simple linear regression (p=1)

Let's start with the simplest case where you use just one feature, $\mathbf{y} \sim \mathbf{x}_1$. To simplify the notation later on, I will denote the estimated regression coefficients $\beta_0'$ and $\beta_1'$.

As we saw briefly on [Week 02](/weeks/week02/lecture.qmd) and as defined in Chapter 3 of the textbook [@james_introduction_2021], $R^2$ represents the proportion of variability in the response variable that can be explained using the dependent variables (features). More specifically:

$$
R^2 = 1 - \frac{\operatorname{RSS}}{\operatorname{TSS}}
$$

where $\operatorname{TSS}$ represents the <mark>Total Sum of Squares</mark>:

$$
\operatorname{TSS} =  \sum_i^n{\left(y_i - \bar{y}\right)^2},
$$

and $\bar{y}$ is the mean of $\mathbf{y}$.

As for the $\operatorname{RSS}$, it represents the <mark>Residual Sum of Squares</mark>. I will denote the RSS of the simple linear regression as  $\operatorname{RSS}_{(p=1)}$:

$$
\operatorname{RSS}_{(p=1)} = \sum_i^n{e_i^2} = \sum_i^n{\left(y_i - \hat{y}_i'\right)^2} = \sum_i^n{\left(y_i - \beta_0' - \beta_1' x_{i1}\right)^2}
$$



## Now, increase one feature (p=2)

When we add the second column of $\mathbf{x}_2$ to the linear regression and we fit $\mathbf{y} \sim \mathbf{x}_1 + \mathbf{x}_2$, we will obtain a new set of regression coefficients — ${\beta}_0''$,  ${\beta}_1''$ and ${\beta}_2''$ — as well as new corresponding $\operatorname{RSS}_{(p=2)}$ and $\operatorname{R^2}_{(p=2)}$ values. Note that $\operatorname{TSS}$ is always the same, irrespective of the number of features.

To understand why $R^2$ always stays the same or increases when $p$ increases, let's compare $\operatorname{RSS}_{(p=2)}$ to $\operatorname{RSS}_{(p=1)}$. First, let's expand $\operatorname{RSS}_{(p=2)}$:

$$
\begin{align}
\operatorname{RSS}_{(p=2)} =& \sum_i^n{\left(y_i - \hat{y}_i''\right)^2}\\
                           =& \sum_i^n{\left( y_i - {\beta}_0'' - {\beta}_1'' x_{i1} - {\beta}_2'' x_{i2} \right)^2}\\
\end{align}
$$

## Recall the objective function of OLS

In this course we are only interested in the <mark>Ordinary Least Squares</mark> ([OLS](/slides/week02_slides_part1.html#/ordinary-least-squares)) method to estimate the regression coefficients of our linear model. OLS will always try to minimize $\operatorname{RSS}$ and so, what does that mean to $\operatorname{RSS}_{(p=2)}$?

Now, <mark style="color:red">let's plug the previous estimate, $y_i = \beta_0' + \beta_1' x_{i1}$</mark> and re-arrange the terms:

$$
\begin{align}
& \sum_i^n{\left( {\beta}_0' + {\beta}_1' x_{i1} - {\beta}_0'' - {\beta}_1'' x_{i1} - {\beta}_2'' x_{i2} \right)^2} & \\
=& \sum_i^n{\left( ({\beta}_0' - {\beta}_0'') + ({\beta}_1'  - {\beta}_1'') x_{i1} - {\beta}_2'' x_{i2} \right)^2} & \text{  Eq. A}
\end{align}
$$

From the above, you can see that the signs of coefficients do not interfer in $\operatorname{RSS}$, as the equation takes the **square** of errors but their magnitudes do. 

In the simple case where ${\beta}_0' = {\beta}_0''$ and ${\beta}_1' = {\beta}_1''$, we are left with the expression $\operatorname{RSS}_{(p=2)} = \sum_i^n{\left(- {\beta}_2'' x_{i2}\right)^2}$, which goes from $0$ to $\infty$ but never negative. Note that the expression in Eq. A can never go below 0, even if ${\beta}_0' \neq {\beta}_0''$ and ${\beta}_1' \neq {\beta}_1''$. Thus, $\operatorname{RSS}_{(p=2)} - \operatorname{RSS}_{(p=1)} \geq 0$ and  $R^2_{(p=2)} \geq R^2_{(p=1)}$.

<!-- 
<HIDDEN> A longer route:

We want to show that the following is always true:

$$
\begin{align}
\operatorname{RSS}_{(p=1)} \leq \operatorname{RSS}_{(p=2)}.
\end{align}
$$

So let's expand these expressions and see what we find.

$$
\begin{align}
\operatorname{RSS}_{(p=1)} &\leq \operatorname{RSS}_{(p=2)} & \text{}\\
\sum_i^n{\left(y_i - \hat{y}_i'\right)^2} &\leq \sum_i^n{\left(y_i - \hat{y}_i''\right)^2} & \text{expanding}\\
\sum_i^n{y_i^2 + (\hat{y}_i')^2 - 2y_i\hat{y}_i'} &\leq \sum_i^n{y_i^2 + (\hat{y}_i'')^2 - 2y_i\hat{y}_i''} & \text{expanding square}\\
\sum_i^n{(\hat{y}_i')^2 - 2y_i\hat{y}_i'} &\leq \sum_i^n{(\hat{y}_i'')^2 - 2y_i\hat{y}_i''} & \text{subtract} \sum_i^n{y_i^2}\\
\end{align}
$$

which, if we expand $\hat{y}_i'$ and $\hat{y}_i''$, we arrive at:

$$
\begin{align}
\sum_i^n{({\beta}_0' + {\beta}_1' x_{i1})^2 - 2y_i({\beta}_0' + {\beta}_1' x_{i1})} &\leq \sum_i^n{({\beta}_0'' + {\beta}_1'' x_{i1} + {\beta}_2'' x_{i2})^2 - 2y_i({\beta}_0'' + {\beta}_1'' x_{i1} + {\beta}_2'' x_{i2})} \\
\end{align}
$$

Keep on expanding:


$$
\begin{align}
\operatorname{RSS}_{(p=1)} &= \sum_i^n{{\beta}_0'^2 + ({\beta}_1' x_{i1})^2 + 2\beta_0'\beta_1'x_{i1} - 2y_i{\beta}_0' - 2y_i{\beta}_1' x_{i1}} \\ 
=& \sum_i^n{{\beta}_1'^2 x_{i1}^2 + (2\beta_0'\beta_1' - 2y_i{\beta}_1')x_{i1} + {\beta}_0'^2}
\end{align}
$$

$$
\begin{align}
\operatorname{RSS}_{(p=2)} &=
\sum_i^n{({\beta}_0'' + {\beta}_1'' x_{i1} + {\beta}_2'' x_{i2})^2 - 2y_i{\beta}_0'' - 2y_i{\beta}_1'' x_{i1} - 2y_i{\beta}_2'' x_{i2}} \\
\sum_i^n{{\beta}_0''^2 + ({\beta}_1'' x_{i1})^2 + ({\beta}_2'' x_{i2})^2 + 2 {\beta}_0''{\beta}_1'' x_{i1} + 2 {\beta}_1'' x_{i1} {\beta}_2'' x_{i2} + 2 {\beta}_2'' x_{i2}{\beta}_0 - 2y_i{\beta}_0'' - 2y_i{\beta}_1'' x_{i1} - 2y_i{\beta}_2'' x_{i2}} \\
\ldots
\end{align}
$$ -->


[Reference](https://math.stackexchange.com/questions/1976747/prove-that-r2-cannot-decrease-when-adding-a-variable)